,source,target
0,"py::sequence_view seq = sizes;
auto size = seq.size();
self->bind_len(size);
    for (ssize_t i = 0; i < size; ++i) {
self->dims_[i]->set_size(py::to_int(seq[i]));
}
Py_RETURN_NONE;
","py::sequence_view seq = sizes;
auto size = seq.size();
self->bind_len(size);
    for (Py_ssize_t i = 0; i < size; ++i) {
self->dims_[i]->set_size(py::to_int(seq[i]));
}
Py_RETURN_NONE;
"
1,"size_t size_at_dim = 0;
for (const auto i : c10::irange(materialized.size())) {
const Tensor& t = materialized[i];
if (!at::native::cat_should_skip_tensor(t)) {
at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);
size_at_dim += t.size(dim);
all_contiguous = all_contiguous && t.is_contiguous(memory_format);
        all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();
all_same_sizes_and_stride = all_same_sizes_and_stride &&
t.sizes() == materialized[valid].get().sizes() &&
t.strides() == materialized[valid].get().strides();
","size_t size_at_dim = 0;
for (const auto i : c10::irange(materialized.size())) {
const Tensor& t = materialized[i];
      all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();
if (!at::native::cat_should_skip_tensor(t)) {
at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);
size_at_dim += t.size(dim);
all_contiguous = all_contiguous && t.is_contiguous(memory_format);
all_same_sizes_and_stride = all_same_sizes_and_stride &&
t.sizes() == materialized[valid].get().sizes() &&
t.strides() == materialized[valid].get().strides();
"
2,"atol: float,
check_shape: bool,
check_dtype: bool,
):
pt_outs, _ = torch.jit._flatten(pt_outs)
pt_outs = _unpack_to_numpy(pt_outs, cast_onnx_accepted=False)
assert len(ort_outs) == len(
pt_outs
), f""Number of outputs differ ONNX runtime: ({len(ort_outs)}) PyTorch: ({len(pt_outs)})""
for ort_out, pt_out in zip(ort_outs, pt_outs):
        # TODO: Remove `check_shape` option once every shape inconsistent issue is addressed.
        if not check_shape:
            # Allow different but broadcastable output shapes.
            ort_out, pt_out = np.broadcast_arrays(ort_out, pt_out)
        torch.testing.assert_close(
            ort_out,
            pt_out,
            rtol=rtol,
            atol=atol,
            check_dtype=check_dtype,
            equal_nan=True,
        )
def _prepare_input_for_pytorch(args, kwargs):
","atol: float,
check_shape: bool,
check_dtype: bool,
    acceptable_error_percentage: Optional[float],
):
    """"""
    Compare ONNX Runtime and PyTorch outputs.

    Args:
        ort_outs: outputs from ONNX Runtime.
        pt_outs: outputs from PyTorch.
        rtol (float, optional): relative tolerance in comparison between ONNX and PyTorch outputs.
        atol (float, optional): absolute tolerance in comparison between ONNX and PyTorch outputs.
        acceptable_error_percentage (float, optional): acceptable percentage of element mismatches in comparison.
            It should be a float of value between 0.0 and 1.0.

    Raises:
        AssertionError: if outputs from ONNX model and PyTorch model are not
            equal up to specified precision.
        ValueError: if arguments provided are invalid.
    """"""
pt_outs, _ = torch.jit._flatten(pt_outs)
pt_outs = _unpack_to_numpy(pt_outs, cast_onnx_accepted=False)
assert len(ort_outs) == len(
pt_outs
), f""Number of outputs differ ONNX runtime: ({len(ort_outs)}) PyTorch: ({len(pt_outs)})""
    if acceptable_error_percentage and (
        acceptable_error_percentage > 1.0 or acceptable_error_percentage < 0.0
    ):
        raise ValueError(
            ""If set, acceptable_error_percentage should be between 0.0 and 1.0""
        )
for ort_out, pt_out in zip(ort_outs, pt_outs):
        try:
            # TODO: Remove `check_shape` option once every shape inconsistent issue is addressed.
            if not check_shape:
                # Allow different but broadcastable output shapes.
                ort_out, pt_out = np.broadcast_arrays(ort_out, pt_out)
            torch.testing.assert_close(
                ort_out,
                pt_out,
                rtol=rtol,
                atol=atol,
                check_dtype=check_dtype,
                equal_nan=True,
            )
        except AssertionError as e:
            if acceptable_error_percentage:
                error_percentage = 1 - np.sum(
                    np.isclose(ort_out, pt_out, rtol=rtol, atol=atol)
                ) / np.prod(ort_out.shape)
                if error_percentage <= acceptable_error_percentage:
                    warnings.warn(
                        f""Suppressed AssertionError:\n{e}.\n""
                        f""Error percentage {error_percentage} ""
                        f""within acceptable range {acceptable_error_percentage}.""
                    )
                    continue
            raise
def _prepare_input_for_pytorch(args, kwargs):
"
3,"ort_providers: Sequence[str] = _ORT_PROVIDERS,
rtol: float = 0.001,
atol: float = 1e-7,
**_,
):
""""""Verify model export to ONNX with ONNX Runtime.
","ort_providers: Sequence[str] = _ORT_PROVIDERS,
rtol: float = 0.001,
atol: float = 1e-7,
    acceptable_error_percentage: Optional[float] = None,
**_,
):
""""""Verify model export to ONNX with ONNX Runtime.
"
4,"return decorator
def _scalar(x):
""""""Convert a scalar tensor into a Python value.""""""
if isinstance(x, torch.Tensor) and x.shape == ():
return x.item()
","return decorator
def _scalar(x: torch.Tensor):
""""""Convert a scalar tensor into a Python value.""""""
if isinstance(x, torch.Tensor) and x.shape == ():
return x.item()
"
5,"weight = g.op(""Constant"", value_t=weight_value)
if bias is None or _is_none(bias):
if channel_size is None:
            raise RuntimeError(
                ""Unsupported: ONNX export of batch_norm for unknown "" ""channel size.""
)
bias_value = torch.tensor(
[0.0] * channel_size,
","weight = g.op(""Constant"", value_t=weight_value)
if bias is None or _is_none(bias):
if channel_size is None:
            raise errors.SymbolicValueError(
                ""Unsupported: ONNX export of batch_norm for unknown channel size."",
                input,
)
bias_value = torch.tensor(
[0.0] * channel_size,
"
6,"return self
self_rank = symbolic_helper._get_tensor_rank(self)
perm = list(range(self_rank))
","return self
self_rank = symbolic_helper._get_tensor_rank(self)
    assert self_rank is not None
perm = list(range(self_rank))
"
7,"const auto input_ = moveBatchDimToFront(input, input_bdim);
auto weight_flatten = moveBatchDimToFront(weight, weight_bdim);
if (weight_flatten.dim() > 1) {
// for an input [N, C, ...]
// weight can be a non-vector but the total number of elements must be the same as C
","const auto input_ = moveBatchDimToFront(input, input_bdim);
auto weight_flatten = moveBatchDimToFront(weight, weight_bdim);
  const auto weight_logical_dim = rankWithoutBatchDim(weight, weight_bdim);
  TORCH_CHECK(weight_logical_dim == 0 || weight_logical_dim == 1,
      ""prelu: Expected `weight` to be a scalar or 1D tensor, but got ndim = "",
      weight_logical_dim);

if (weight_flatten.dim() > 1) {
// for an input [N, C, ...]
// weight can be a non-vector but the total number of elements must be the same as C
"
8,"build-in method                 |build-in method
...                         |    aten::to
aten::fill_/aten::zero_ |        aten::_to_copy
                                    |            aten::copy_
                                    |                cudaMemcpyAsync
Algorithm:
We start at node aten::to, go parent events' previous events,
","build-in method                 |build-in method
...                         |    aten::to
aten::fill_/aten::zero_ |        aten::_to_copy
Algorithm:
We start at node aten::to, go parent events' previous events,
"
9,"def _prepare_sharded_tensor_write(
sharded_tensor: ShardedTensor,
storage_key: str,
storage_key_to_fqn: Dict[str, str]
) -> Tuple[List[TensorWriteRequest], ShardedTensorStorageMetadata]:
""""""
Prepare sharded tensor write.
Args:
sharded_tensor: The sharded tensor to persist.
storage_key: The identifier for `sharded_tensor`.
storage_key_to_fqn: dict used to produce storage keys

Returns:
        Write requests for persisting the sharded tensor, and metadata
        describing the persisted sharded tensor.
NB `storage_key` is used to compose the key names of the local shards.

""""""
write_requests = []
shard_to_storage_key: Dict[str, str] = dict()
for shard_md in sharded_tensor.metadata().shards_metadata:
shard_storage_key = _get_shard_storage_key(storage_key, shard_md, storage_key_to_fqn)
shard_to_storage_key[_get_shard_key(shard_md)] = shard_storage_key
for shard in sharded_tensor.local_shards():
tensor = shard.tensor.detach()
","def _prepare_sharded_tensor_write(
    fqn: str,
sharded_tensor: ShardedTensor,
storage_key: str,
storage_key_to_fqn: Dict[str, str]
) -> Tuple[List[TensorWriteRequest], TensorStorageMetadata, Dict[MetadataIndex, str]]:
""""""
Prepare sharded tensor write.
Args:
        fqn: The FQN of ``sharded_tensor`` in the state_dict.
sharded_tensor: The sharded tensor to persist.
storage_key: The identifier for `sharded_tensor`.
storage_key_to_fqn: dict used to produce storage keys
Returns:
        A 3-element tuple with the following values:
            List of ``TensorWriteRequest`` for the tensor
            Metadada describing the tensor.
            Dictionary describing storage information for this tensor
NB `storage_key` is used to compose the key names of the local shards.
""""""
write_requests = []
shard_to_storage_key: Dict[str, str] = dict()
    storage_md = {}
for shard_md in sharded_tensor.metadata().shards_metadata:
shard_storage_key = _get_shard_storage_key(storage_key, shard_md, storage_key_to_fqn)
shard_to_storage_key[_get_shard_key(shard_md)] = shard_storage_key
        storage_md[MetadataIndex(fqn, shard_md.shard_offsets)] = shard_storage_key
for shard in sharded_tensor.local_shards():
tensor = shard.tensor.detach()
"
10,"tensor.
""""""
return _prepare_generic_tensor_read(
        metadata.storage_metadata,
        sharded_tensor_out.local_shards())
def _prepare_generic_tensor_read(
    checkpoint_shards: List[ShardStorageMetadata], local_shards: List[Shard]
) -> List[TensorReadRequest]:
read_reqs = []
# this is a naive quadratic algo that can be optimized later
for shard in local_shards:
# scan all mds looking for chunks
for storage_md in checkpoint_shards:
            shard_md_from_storage = storage_md.shard_metadata
# do they overlap?
if not _check_shard_metadata_pair_overlap(
","tensor.
""""""
return _prepare_generic_tensor_read(
        fqn,
        metadata.chunks,
        sharded_tensor_out.local_shards(),
        storage_metadata)
def _prepare_generic_tensor_read(
    fqn: str,
    checkpoint_shards: List[ChunkStorageMetadata],
    local_shards: List[Shard],
    storage_metadata: Dict[MetadataIndex, str]
) -> List[TensorReadRequest]:
read_reqs = []
# this is a naive quadratic algo that can be optimized later
for shard in local_shards:
# scan all mds looking for chunks
for storage_md in checkpoint_shards:
            shard_md_from_storage = _chunk_to_shard_md(storage_md)
# do they overlap?
if not _check_shard_metadata_pair_overlap(
"
11,"try:
result = map_fun()
except BaseException as e:
                result = CheckpointException(step, {self.rank: e})
final_result = self.broadcast_object(result)
if isinstance(final_result, CheckpointException):
raise final_result
","try:
result = map_fun()
except BaseException as e:
                result = CheckpointException(step, {self.rank: _wrap_exception(e)})
final_result = self.broadcast_object(result)
if isinstance(final_result, CheckpointException):
raise final_result
"
12,"namespace at { namespace functorch {
static void handleScalarTypePromotion(Tensor& logical_scalar_tensor, Tensor& second) {
  auto result_type = at::native::result_type(logical_scalar_tensor[0], second);
  if (logical_scalar_tensor.scalar_type() != result_type) {
    logical_scalar_tensor = logical_scalar_tensor.to(result_type);
  }
  if (second.scalar_type() != result_type) {
    second = second.to(result_type);
  }
}

std::tuple<Tensor, Tensor> _binary_pointwise_helper(
    const Tensor& tensor, optional<int64_t> tensor_batch_dim,
    const Tensor& other, optional<int64_t> other_batch_dim) {
  // compute max logical rank
  auto tensor_logical_rank = rankWithoutBatchDim(tensor, tensor_batch_dim);
  auto other_logical_rank = rankWithoutBatchDim(other, other_batch_dim);
  auto max_logical_rank = std::max(tensor_logical_rank, other_logical_rank);

  auto tensor_ = moveBatchDimToFront(tensor, tensor_batch_dim);
  auto other_ = moveBatchDimToFront(other, other_batch_dim);

  // In the (0D, ND) case, type promotion semantics are different :/
  auto tensor_is_logical_scalar = (tensor_logical_rank == 0 && tensor_batch_dim.has_value());
  auto other_is_logical_scalar = (other_logical_rank == 0 && other_batch_dim.has_value());
  if (tensor_is_logical_scalar && !other_is_logical_scalar) {
    handleScalarTypePromotion(tensor_, other_);
  }
  if (other_is_logical_scalar && !tensor_is_logical_scalar) {
    handleScalarTypePromotion(other_, tensor_);
  }

  // If the dimensions aren't aligned, we need to line them up.
  // Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]
  // Note that only tensors that have a batch dim need to be modified.
  // Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed
  tensor_ = maybePadToLogicalRank(tensor_, tensor_batch_dim, max_logical_rank);
  other_ = maybePadToLogicalRank(other_, other_batch_dim, max_logical_rank);

  return std::make_tuple(tensor_, other_);
}

template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
const Tensor& tensor, optional<int64_t> tensor_batch_dim,
","namespace at { namespace functorch {
template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
const Tensor& tensor, optional<int64_t> tensor_batch_dim,
"
13,"OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
","OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
  OP_DECOMPOSE(inverse);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
"
14,"// This file registers special JIT operators used to implement the PyTorch CUDA
// API in TorchScript.
#if !defined(USE_ROCM)
#include <torch/csrc/api/include/torch/utils.h>
#include <torch/csrc/jit/cuda/cuda.h>
#include <torch/csrc/jit/ir/ir.h>
","// This file registers special JIT operators used to implement the PyTorch CUDA
// API in TorchScript.
#include <torch/csrc/api/include/torch/utils.h>
#include <torch/csrc/jit/cuda/cuda.h>
#include <torch/csrc/jit/ir/ir.h>
"
15,"instance should be enabled at any given time.
Args:
        enabled (bool, optional, default=True): Setting ``enabled=False`` makes this context manager a no-op.
Default: ``True``.
        record_shapes (bool, optional, default=False): If ``record_shapes=True``, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``
","instance should be enabled at any given time.
Args:
        enabled (bool, optional): Setting ``enabled=False`` makes this context manager a no-op.
Default: ``True``.
        record_shapes (bool, optional): If ``record_shapes=True``, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``
"
16,"passed as the tuple. For example, in LSTM, if user passes
``(activation, hidden)``, :attr:`function` should correctly use the
first input as ``activation`` and the second input as ``hidden``
        preserve_rng_state(bool, optional, default=True):  Omit stashing and restoring
the RNG state during each checkpoint.
        use_reentrant(bool, optional, default=True): Use checkpointing
implementation that requires re-entrant autograd.
If ``use_reentrant=False`` is specified, ``checkpoint`` will use an
implementation that does not require re-entrant autograd. This
","passed as the tuple. For example, in LSTM, if user passes
``(activation, hidden)``, :attr:`function` should correctly use the
first input as ``activation`` and the second input as ``hidden``
        preserve_rng_state(bool, optional):  Omit stashing and restoring
the RNG state during each checkpoint.
            Default: ``True``
        use_reentrant(bool, optional): Use checkpointing
implementation that requires re-entrant autograd.
If ``use_reentrant=False`` is specified, ``checkpoint`` will use an
implementation that does not require re-entrant autograd. This
"
17,"functions (comprising the model) to run sequentially.
segments: Number of chunks to create in the model
input: A Tensor that is input to :attr:`functions`
        preserve_rng_state(bool, optional, default=True):  Omit stashing and restoring
the RNG state during each checkpoint.
Returns:
Output of running :attr:`functions` sequentially on :attr:`*inputs`
","functions (comprising the model) to run sequentially.
segments: Number of chunks to create in the model
input: A Tensor that is input to :attr:`functions`
        preserve_rng_state(bool, optional):  Omit stashing and restoring
the RNG state during each checkpoint.
            Default: ``True``
Returns:
Output of running :attr:`functions` sequentially on :attr:`*inputs`
"
18,"const Tensor& input,
IntArrayRef padding,
IntArrayRef stride,
    IntArrayRef kernel_size) {
const int64_t kernel_height = kernel_size[0];
const int64_t kernel_width = kernel_size[1];
const int64_t pad_height = padding[0];
","const Tensor& input,
IntArrayRef padding,
IntArrayRef stride,
    IntArrayRef kernel_size,
    bool is_channels_last) {
const int64_t kernel_height = kernel_size[0];
const int64_t kernel_width = kernel_size[1];
const int64_t pad_height = padding[0];
"
19,"Argument,
BaseTy,
BaseType,
ListType,
NativeFunction,
OptionalType,
","Argument,
BaseTy,
BaseType,
    FunctionSchema,
ListType,
NativeFunction,
OptionalType,
"
20,"}}""""""
def gen_returns(returns: List[Return], cur_level_var: str, results_var: str) -> str:
idx = 0
wrapped_returns = []
for ret in returns:
","}}""""""
def gen_returns(
    returns: Tuple[Return, ...], cur_level_var: str, results_var: str
) -> str:
idx = 0
wrapped_returns = []
for ret in returns:
"
21,"eps = group['eps']
lr = group['lr']
beta1, beta2 = group['betas']
            maximize = group['maximize']
for p in group['params']:
if p.grad is not None:
","eps = group['eps']
lr = group['lr']
beta1, beta2 = group['betas']
            maximize = group.get('maximize', False)
for p in group['params']:
if p.grad is not None:
"
22,"else:
d_p = buf
        alpha = lr if maximize else -lr
        param.add_(d_p, alpha=alpha)
def _multi_tensor_sgd(params: List[Tensor],
","else:
d_p = buf
        param.add_(d_p, alpha=-lr)
def _multi_tensor_sgd(params: List[Tensor],
"
23,"#ifndef USE_NUMPY
namespace torch {
namespace utils {
PyObject* tensor_to_numpy(const at::Tensor& tensor) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
at::Tensor tensor_from_numpy(
","#ifndef USE_NUMPY
namespace torch {
namespace utils {
PyObject* tensor_to_numpy(const at::Tensor&, bool) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
at::Tensor tensor_from_numpy(
"
24,"std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return at::split_with_sizes(self, split_sizes, dim);
}
auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
auto dim_physical = self_physical.getPhysicalDim(dim);
  auto result = at::split_with_sizes(self_physical.tensor(), split_sizes, dim_physical);
self_physical.getPhysicalToLogicalMap().applyInplace(result);
return result;
}
","std::vector<Tensor> split_with_sizes_batching_rule(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return split_with_sizes(self, split_sizes, dim);
}
auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
auto dim_physical = self_physical.getPhysicalDim(dim);
  auto result = split_with_sizes(self_physical.tensor(), split_sizes, dim_physical);
self_physical.getPhysicalToLogicalMap().applyInplace(result);
return result;
}
"
25,"batch_size = sample_grads[0].shape[0]
norms = [sample_grad.view(batch_size, -1).norm(2, dim=-1) for sample_grad in sample_grads]
norms = torch.stack(norms, dim=0).norm(2, dim=0)
    return norms
def clip_and_accumulate_and_add_noise(model, max_per_sample_grad_norm=1.0, noise_multiplier=1.0):
sample_grads = tuple(param.grad_sample for param in model.parameters())
# step 0: compute the norms
    sample_norms = compute_norms(sample_grads)
# step 1: compute clipping factors
clip_factor = max_per_sample_grad_norm / (sample_norms + 1e-6)
","batch_size = sample_grads[0].shape[0]
norms = [sample_grad.view(batch_size, -1).norm(2, dim=-1) for sample_grad in sample_grads]
norms = torch.stack(norms, dim=0).norm(2, dim=0)
    return norms, batch_size
def clip_and_accumulate_and_add_noise(model, max_per_sample_grad_norm=1.0, noise_multiplier=1.0):
sample_grads = tuple(param.grad_sample for param in model.parameters())
# step 0: compute the norms
    sample_norms, batch_size = compute_norms(sample_grads)
# step 1: compute clipping factors
clip_factor = max_per_sample_grad_norm / (sample_norms + 1e-6)
"
26,"# step 4: assign the new grads, delete the sample grads
for param, param_grad in zip(model.parameters(), grads):
        param.grad = param_grad
del param.grad_sample
","# step 4: assign the new grads, delete the sample grads
for param, param_grad in zip(model.parameters(), grads):
        param.grad = param_grad/batch_size
del param.grad_sample
"
27,"else:
args.append(tensor_args[tensor_index])
tensor_index += 1
while tensor_index < len(tensor_args):
args.append(tensor_args[tensor_index])
","else:
args.append(tensor_args[tensor_index])
tensor_index += 1
        index += 1
while tensor_index < len(tensor_args):
args.append(tensor_args[tensor_index])
"
28,"return required_fw_nodes, required_bw_nodes, unclaimed_nodes
required_fw_nodes, required_bw_nodes, unclaimed_nodes = classify_nodes(joint_module)
    cache = {}
    def dist_from_fw(node):
        if node in cache:
            return cache[node]
if node not in required_fw_nodes:
return 0
dist = int(1e9)
for n in node.users:
            dist = min(dist_from_fw(n) + 1, dist)
        cache[node] = dist
return dist
    for node in joint_module.graph.nodes:
        node.dist_from_fw = dist_from_fw(node)
aten = torch.ops.aten
","return required_fw_nodes, required_bw_nodes, unclaimed_nodes
required_fw_nodes, required_bw_nodes, unclaimed_nodes = classify_nodes(joint_module)
    def dist_from_bw(node):
        print(node)
if node not in required_fw_nodes:
return 0
dist = int(1e9)
for n in node.users:
            dist = min(dist_from_bw(n) + 1, dist)
return dist
    for node in reversed(joint_module.graph.nodes):
        if node not in required_fw_nodes:
            node.dist_from_bw = 0
        else:
            node.dist_from_bw = int(1e9)
            for n in node.users:
                node.dist_from_bw = min(node.dist_from_bw, n.dist_from_bw + 1)
aten = torch.ops.aten
"
29,"return _old_backward(*args, **kwargs)
torch.Tensor.backwrd = _backward
","return _old_backward(*args, **kwargs)
torch.Tensor.backward = _backward
"
30,"primal_inputs = list(filter(_is_primal, joint_module.graph.nodes))
fwd_outputs, bwd_outputs = _extract_fwd_bwd_outputs(joint_module)
forward_only_graph = _extract_graph_with_inputs_outputs(joint_module.graph, primal_inputs, fwd_outputs)
    forward_node_names = set([node.name for node in forward_only_graph.nodes if node.op != 'output'])
saved_values = []
for node in joint_module.graph.nodes:
if node.name not in forward_node_names:
","primal_inputs = list(filter(_is_primal, joint_module.graph.nodes))
fwd_outputs, bwd_outputs = _extract_fwd_bwd_outputs(joint_module)
forward_only_graph = _extract_graph_with_inputs_outputs(joint_module.graph, primal_inputs, fwd_outputs)
    forward_node_names = {node.name for node in forward_only_graph.nodes if node.op != 'output'}
saved_values = []
for node in joint_module.graph.nodes:
if node.name not in forward_node_names:
"
31,"if node.op == 'placeholder':
return True
        return not all([is_fusible(node, user) for user in node.users])
def get_node_weight(node):
mem_sz = _size_of(node.meta['tensor_meta'])
","if node.op == 'placeholder':
return True
        return not all(is_fusible(node, user) for user in node.users)
def get_node_weight(node):
mem_sz = _size_of(node.meta['tensor_meta'])
"
32,"return (out, mean, rstd)
@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return torch.clamp(self, min=min)
","return (out, mean, rstd)
@register_decomposition(aten.isnan)
def isnan(self: Tensor) -> Tensor:
    return torch.where(self != self, self.new_ones((), dtype=torch.bool), self.new_zeros((), dtype=torch.bool))


@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return torch.clamp(self, min=min)
"
33,"#                                  self * aten.log(other)))
@register_decomposition(aten.var)
def var_decomposition(x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False):
if dims is None:
dims = []

    if isinstance(dims, (tuple, list)) and len(dims) == 0:
n = x.numel()
else:
n = 1
","#                                  self * aten.log(other)))
@register_decomposition(aten.var.correction)
def var_decomposition(x: Tensor, dims: Optional[List[int]], correction: int = 0, keepdim: bool = False):
if dims is None:
dims = []
    if len(dims) == 0:
n = x.numel()
else:
n = 1
"
34,"args = list(node.args)
args[1] = [1]
node.args = tuple(args)
        elif node.target == torch.ops.aten.avg_pool2d_backward:
            # Handle empty strides
            if node.args[3] == []:
                args = list(node.args)
                args[3] = [1, 1]
                node.args = tuple(args)
for node in fx_g.graph.nodes:
new_kwargs = {}
","args = list(node.args)
args[1] = [1]
node.args = tuple(args)
for node in fx_g.graph.nodes:
new_kwargs = {}
"
35,"return std::make_tuple(at::where(condition_, self_, other_), 0);
}
std::tuple<Tensor,optional<int64_t>> masked_select_batch_rule(
const Tensor& self, optional<int64_t> self_bdim,
const Tensor& mask, optional<int64_t> mask_bdim) {
","return std::make_tuple(at::where(condition_, self_, other_), 0);
}
std::tuple<Tensor, optional<int64_t>> gelu_backward_batch_rule(
    const Tensor& grad_out, optional<int64_t> grad_out_bdim, const Tensor& input, optional<int64_t> input_bdim,
    c10::string_view approximate) {

  // repeat the preprocessing from _binary_pointwise_batch_rule
  const auto tensor_other = _binary_pointwise_helper(grad_out, grad_out_bdim, input, input_bdim);
  auto grad_out_ = std::get<0>(tensor_other);
  auto input_ = std::get<1>(tensor_other);

  // gelu_backward doesn't broadcast well so we need to insist all inputs have a bdim
  const auto batch_size = get_bdim_size2(grad_out, grad_out_bdim, input, input_bdim);
  grad_out_ = ensure_has_bdim(grad_out_, grad_out_bdim.has_value(), batch_size);
  input_ = ensure_has_bdim(input_, input_bdim.has_value(), batch_size);

  return std::make_tuple(at::gelu_backward(grad_out_, input_, approximate), 0);
}

std::tuple<Tensor,optional<int64_t>> masked_select_batch_rule(
const Tensor& self, optional<int64_t> self_bdim,
const Tensor& mask, optional<int64_t> mask_bdim) {
"
36,"return std::make_tuple(result0, mean, rstd);
}
std::tuple<Tensor,Tensor,Tensor> native_group_norm_backward_plumbing(
const Tensor & grad_out, const Tensor & input, const Tensor & mean,
const Tensor & rstd, const c10::optional<Tensor> & weight_opt,
","return std::make_tuple(result0, mean, rstd);
}
std::tuple<at::Tensor,optional<int64_t>> group_norm_backward_no_weight_bias_batch_rule(
    const at::Tensor & grad_out, optional<int64_t> grad_out_bdim,
    const at::Tensor & input, optional<int64_t> input_bdim,
    const at::Tensor & mean, optional<int64_t> mean_bdim,
    const at::Tensor & rstd, optional<int64_t> rstd_bdim,
    int64_t N, int64_t C, int64_t HxW, int64_t group) {
  auto grad_out_ = moveBatchDimToFront(grad_out, grad_out_bdim);
  auto input_ = moveBatchDimToFront(input, input_bdim);
  auto mean_ = moveBatchDimToFront(mean, mean_bdim);
  auto rstd_ = moveBatchDimToFront(rstd, rstd_bdim);

  const auto bdim_size = get_bdim_size2(grad_out, grad_out_bdim, input, input_bdim);
  grad_out_ = ensure_has_bdim(grad_out, grad_out_bdim.has_value(), bdim_size);
  input_ = ensure_has_bdim(input_, input_bdim.has_value(), bdim_size);
  mean_ = ensure_has_bdim(mean_, mean_bdim.has_value(), bdim_size);
  rstd_ = ensure_has_bdim(rstd_, rstd_bdim.has_value(), bdim_size);

  grad_out_ = reshape_dim_into(0, 0, grad_out_); // [B0 * N, C, *]
  input_ = reshape_dim_into(0, 0, input_);       // [B0 * N, C, *]
  mean_ = reshape_dim_into(0, 0, mean_);         // [B0 * N, G]
  rstd_ = reshape_dim_into(0, 0, rstd_);         // [B0 * N, G]

  const auto result = native_group_norm_backward(
      grad_out_.contiguous(),
      input_.contiguous(),
      mean_.contiguous(),
      rstd_.contiguous(),
      nullopt, N * bdim_size, C, HxW, group, {true, false, false});
  auto result0 = std::get<0>(result);
  result0 = reshape_dim_outof(0, bdim_size, result0);
  return std::make_tuple(result0, 0);
}

std::tuple<Tensor,Tensor,Tensor> native_group_norm_backward_plumbing(
const Tensor & grad_out, const Tensor & input, const Tensor & mean,
const Tensor & rstd, const c10::optional<Tensor> & weight_opt,
"
37,"if output_mask[0]:
d_input = aten.mul(aten.div(rstd, N), inner)
else:
        d_input = None
if output_mask[1] and weight is not None:
if len(outer_dim_indices) > 0:
","if output_mask[0]:
d_input = aten.mul(aten.div(rstd, N), inner)
else:
        d_input = aten.new_empty(input, (0,))
if output_mask[1] and weight is not None:
if len(outer_dim_indices) > 0:
"
38,"aten.silu_backward,
]
)
default_decompositions = {
    k: v for k, v in decomposition_table.items() if k in default_decompositions
}
def print_compile(fx_g, _):
","aten.silu_backward,
]
)
default_decompositions = get_decompositions(default_decompositions)
def print_compile(fx_g, _):
"
39,"pointwise_ops = [aten.add, aten.sub, aten.div, aten.atan2, aten.mul, aten.max, aten.min, aten.pow, aten.remainder, aten.fmod, aten.__and__, aten.__or__, aten.__xor__, aten.__lshift__, aten.__rshift__, aten.eq, aten.ne, aten.ge, aten.gt, aten.le, aten.lt, aten.abs, aten.bitwise_not, aten.ceil, aten.floor, aten.frac, aten.neg, aten.relu, aten.round, aten.silu, aten.trunc, aten.log, aten.log10, aten.log1p, aten.log2, aten.lgamma, aten.exp, aten.expm1, aten.erf, aten.erfc, aten.cos, aten.acos, aten.cosh, aten.sin, aten.asin, aten.sinh, aten.tan, aten.atan, aten.tanh, aten.atanh, aten.sqrt, aten.rsqrt,  aten.reciprocal, aten.sigmoid, aten.softplus, aten.threshold, aten.threshold_backward, aten.clamp, aten.where, aten.lerp, aten.addcmul, aten.gelu, aten.gelu_backward]  # noqa: E501
misc_ops = [aten.to, aten.type_as, operator.getitem]
    # Ban reductions for now due to it being unnecessary/running into pathological situations
    # todo(chilli): add a heuristic to allow reduction only if output node is much smaller than input node
reduction_ops = [aten.softmax, aten._softmax, aten._softmax_backward_data, aten.sum, aten.mean, aten._grad_sum_to_size, aten.sum_to_size, aten.amax]  # noqa: E501
# not recomputed by default since these are kinda expensive/hard to fuse into
","pointwise_ops = [aten.add, aten.sub, aten.div, aten.atan2, aten.mul, aten.max, aten.min, aten.pow, aten.remainder, aten.fmod, aten.__and__, aten.__or__, aten.__xor__, aten.__lshift__, aten.__rshift__, aten.eq, aten.ne, aten.ge, aten.gt, aten.le, aten.lt, aten.abs, aten.bitwise_not, aten.ceil, aten.floor, aten.frac, aten.neg, aten.relu, aten.round, aten.silu, aten.trunc, aten.log, aten.log10, aten.log1p, aten.log2, aten.lgamma, aten.exp, aten.expm1, aten.erf, aten.erfc, aten.cos, aten.acos, aten.cosh, aten.sin, aten.asin, aten.sinh, aten.tan, aten.atan, aten.tanh, aten.atanh, aten.sqrt, aten.rsqrt,  aten.reciprocal, aten.sigmoid, aten.softplus, aten.threshold, aten.threshold_backward, aten.clamp, aten.where, aten.lerp, aten.addcmul, aten.gelu, aten.gelu_backward]  # noqa: E501
misc_ops = [aten.to, aten.type_as, operator.getitem]
reduction_ops = [aten.softmax, aten._softmax, aten._softmax_backward_data, aten.sum, aten.mean, aten._grad_sum_to_size, aten.sum_to_size, aten.amax]  # noqa: E501
# not recomputed by default since these are kinda expensive/hard to fuse into
"
40,"proxy_out.node.meta['tensor_meta'] = _extract_tensor_metadata(args[0])
with no_dispatch():
            real_out = func(*args, **kwargs)
def wrap_with_proxy(e, proxy):
# Some ops (like native_batch_norm_backward) return undefined tensors that get
","proxy_out.node.meta['tensor_meta'] = _extract_tensor_metadata(args[0])
with no_dispatch():
            real_out = func_overload(*args, **kwargs)
def wrap_with_proxy(e, proxy):
# Some ops (like native_batch_norm_backward) return undefined tensors that get
"
41,"const c10::optional<Tensor>& running_var_opt, optional<int64_t> running_var_bdim,
bool training, double momentum, double eps) {
return batch_norm_batch_rule<F, Func>(
        input, input_bdim, weight_opt, weight_bdim, bias_opt, bias_bdim,
running_mean_opt, running_mean_bdim, running_var_opt, running_var_bdim, training, momentum, eps);
}
};
","const c10::optional<Tensor>& running_var_opt, optional<int64_t> running_var_bdim,
bool training, double momentum, double eps) {
return batch_norm_batch_rule<F, Func>(
        input, input_bdim, weight_opt, weight_bdim, bias_opt, bias_bdim,
running_mean_opt, running_mean_bdim, running_var_opt, running_var_bdim, training, momentum, eps);
}
};
"
42,"auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
","auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  if (!isBatchedAtLevel(self, cur_level) && !isBatchedAtLevel(indices, cur_level)) {
    return at::index(self, indices);
  }
Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
"
43,"#include <functorch/csrc/Constants.h>
#include <functorch/csrc/TensorWrapper.h>
#include <functorch/csrc/DynamicLayer.h>
#include <ATen/Context.h>
#include <ATen/MatrixRef.h>
","#include <functorch/csrc/Constants.h>
#include <functorch/csrc/TensorWrapper.h>
#include <functorch/csrc/DynamicLayer.h>
#include <functorch/csrc/PlumbingHelper.h>
#include <ATen/Context.h>
#include <ATen/MatrixRef.h>
"
44,"""an issue on github."");
}
// TODO: dedup
static bool participatesInCurrentLevel(const Tensor& self) {
  auto maybe_level = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_level.has_value());
  auto current_level = maybe_level->layerId();
  auto* maybe_batched_impl = maybeGetBatchedImpl(self);
  if (!maybe_batched_impl) {
    return false;
  }
  auto self_level = maybe_batched_impl->level();
  TORCH_INTERNAL_ASSERT(self_level <= current_level);
  return self_level == current_level;
}

static bool ivalueParticipatesInCurrentLevel(const IValue& ivalue) {
  if (!ivalue.isTensor()) {
    return false;
  }
  return participatesInCurrentLevel(ivalue.toTensor());
}

// TODO: Consider rewriting the following to look like:
// https://gist.github.com/zou3519/7b7c6a4a258d580f62d1d969851be6b1<Paste>
","""an issue on github."");
}
// TODO: Consider rewriting the following to look like:
// https://gist.github.com/zou3519/7b7c6a4a258d580f62d1d969851be6b1<Paste>
"
45,"return self;
}
Tensor trace_batching_rule(const Tensor& self) {
  if (!participatesInCurrentLevel(self)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return self.trace();
  }
  auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
  // Batched Diagonal View
  auto self_diag = at::diagonal(self_physical.tensor(), /*offset*/0, /*dim1*/-2, /*dim2*/-1);
  auto result =  at::sum(self_diag, -1);
  return self_physical.getPhysicalToLogicalMap().apply(result);
}

Tensor trace_backward_batching_rule(const Tensor& grad, IntArrayRef input_sizes) {
  if (!participatesInCurrentLevel(grad)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return at::trace_backward(grad, input_sizes);
  }
  auto grad_physical = MultiBatchVmapTransform::logicalToPhysical(grad);
  auto grad_input = at::zeros(grad_physical.getPhysicalShape(input_sizes), grad.options());
  // Batched Diagonal View
  auto grad_input_diag = at::diagonal(grad_input, /*offset*/0, /*dim1*/-2, /*dim2*/-1);
  // Append a dimension of size one to the grad output
  auto grad_physical_tensor = grad_physical.tensor().unsqueeze(-1);
  grad_input_diag.copy_(grad_physical_tensor);
  return grad_physical.getPhysicalToLogicalMap().apply(grad_input);
}

Tensor transpose_int_batching_rule(const Tensor& self, int64_t dim0, int64_t dim1) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return self;
}
Tensor transpose_int_batching_rule(const Tensor& self, int64_t dim0, int64_t dim1) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
46,"def clamp_max(self: Tensor, min: float):
return aten.clamp(self, max=max)
@register_decomposition(aten._fused_dropout)
def _fused_dropout_decomposition(input, p, generator=None):
mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
","def clamp_max(self: Tensor, min: float):
return aten.clamp(self, max=max)

@register_decomposition(aten._fused_dropout)
def _fused_dropout_decomposition(input, p, generator=None):
mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
"
47,"return std::get<0>(results);
}
// ['_fw_primal', '_dim_arange', 'diagflat', '_logcumsumexp', 'logcumsumexp', 'matrix_power', 'mvlgamma', 'pixel_shuffle', 'pixel_unshuffle', 'channel_shuffle', 'squeeze.dim', 'one_hot', 'unsqueeze', 'to_sparse.sparse_dim', 'diag', 'triu', 'tril', 'glu', 'special_multigammaln', 'linalg_tensorinv', 'linalg_matrix_power']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_4_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_4_t,Tensor,const Tensor &, int64_t>(
","return std::get<0>(results);
}
// ['_fw_primal', '_dim_arange', 'diagflat', '_logcumsumexp', 'logcumsumexp', 'matrix_power', 'mvlgamma', 'pixel_shuffle', 'pixel_unshuffle', 'channel_shuffle', 'native_channel_shuffle', 'round.decimals', 'squeeze.dim', 'one_hot', 'unsqueeze', 'to_sparse.sparse_dim', 'diag', 'triu', 'tril', 'glu', 'special_round', 'special_multigammaln', 'linalg_tensorinv', 'linalg_matrix_power']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_4_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_4_t,Tensor,const Tensor &, int64_t>(
"
48,"}
// ['_use_cudnn_ctc_loss']
typedef std::tuple<bool> (*batch_rule_11_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, int64_t);
template <>
bool lowerToNextLayer<batch_rule_11_t,bool,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, int64_t>(
  batch_rule_11_t batch_rule,
const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_use_cudnn_ctc_loss']
typedef std::tuple<bool> (*batch_rule_12_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, int64_t);
template <>
bool lowerToNextLayer<batch_rule_12_t,bool,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, int64_t>(
  batch_rule_12_t batch_rule,
const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
49,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['adaptive_avg_pool1d', 'broadcast_to', 'count_nonzero.dim_IntList', 'permute', 'repeat', 'reshape', '_mkldnn_reshape', 'sum_to_size', 'tile', 'flip', '_unsafe_view', '_sparse_sum.dim', 'view', 'trace_backward', 'adaptive_avg_pool2d', 'mkldnn_adaptive_avg_pool2d', '_adaptive_avg_pool2d', 'adaptive_avg_pool3d', '_adaptive_avg_pool3d', 'reflection_pad1d', 'reflection_pad2d', 'reflection_pad3d', 'replication_pad1d', 'replication_pad2d', 'replication_pad3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_19_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_19_t,Tensor,const Tensor &, IntArrayRef>(
  batch_rule_19_t batch_rule,
const Tensor & self, IntArrayRef output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['adaptive_avg_pool1d', 'broadcast_to', '_sparse_broadcast_to', 'count_nonzero.dim_IntList', 'permute', 'repeat', 'reshape', '_mkldnn_reshape', 'sum_to_size', 'tile', 'flip', '_unsafe_view', '_sparse_sum.dim', 'view', 'trace_backward', 'adaptive_avg_pool2d', 'mkldnn_adaptive_avg_pool2d', '_adaptive_avg_pool2d', 'adaptive_avg_pool3d', '_adaptive_avg_pool3d', 'reflection_pad1d', 'reflection_pad2d', 'reflection_pad3d', 'replication_pad1d', 'replication_pad2d', 'replication_pad3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_20_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_20_t,Tensor,const Tensor &, IntArrayRef>(
  batch_rule_20_t batch_rule,
const Tensor & self, IntArrayRef output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
50,"}
// ['as_strided']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_29_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_29_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<int64_t>>(
  batch_rule_29_t batch_rule,
const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['as_strided']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_30_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_30_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<int64_t>>(
  batch_rule_30_t batch_rule,
const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
51,"}
// ['bernoulli.p', 'normal.Tensor_float']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_35_t)(const Tensor &, c10::optional<int64_t>, double, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_35_t,Tensor,const Tensor &, double, c10::optional<Generator>>(
  batch_rule_35_t batch_rule,
const Tensor & self, double p, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['bernoulli.p', 'normal.Tensor_float']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_36_t)(const Tensor &, c10::optional<int64_t>, double, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_36_t,Tensor,const Tensor &, double, c10::optional<Generator>>(
  batch_rule_36_t batch_rule,
const Tensor & self, double p, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
52,"if (bias) {
std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
}
  auto results = batch_rule(input_value, input_bdim, weight_value, weight_bdim, bias_value, bias_bdim, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_convolution_mode', 'conv1d.padding', 'conv2d.padding', 'conv3d.padding']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_54_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, c10::string_view, IntArrayRef, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_54_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, c10::string_view, IntArrayRef, int64_t>(
  batch_rule_54_t batch_rule,
  const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, c10::string_view padding, IntArrayRef dilation, int64_t groups
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","if (bias) {
std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
}
  auto results = batch_rule(input_value, input_bdim, weight_value, weight_bdim, bias_value, bias_bdim, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_convolution.deprecated']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_55_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_55_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, bool, bool, bool>(
  batch_rule_55_t batch_rule,
  const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
53,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_copy_from', 'cholesky_solve', '_cholesky_solve_helper', '_convert_indices_from_csr_to_coo', 'linalg_pinv.rcond_tensor', 'linalg_matrix_rank.tol_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_61_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_61_t,Tensor,const Tensor &, const Tensor &, bool>(
  batch_rule_61_t batch_rule,
const Tensor & self, const Tensor & dst, bool non_blocking
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_copy_from', 'cholesky_solve', '_cholesky_solve_helper', 'linalg_pinv.rcond_tensor', 'linalg_matrix_rank.tol_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_62_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_62_t,Tensor,const Tensor &, const Tensor &, bool>(
  batch_rule_62_t batch_rule,
const Tensor & self, const Tensor & dst, bool non_blocking
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
54,"Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, padding, stride, dilation, groups, benchmark, deterministic);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution', 'cudnn_convolution_transpose_backward_input']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_70_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_70_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool>(
batch_rule_70_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution_transpose']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_70_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_70_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool>(
batch_rule_70_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
55,"}
// ['ctc_loss.IntList']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_84_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, int64_t, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_84_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, int64_t, int64_t, bool>(
  batch_rule_84_t batch_rule,
const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['ctc_loss.IntList']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_78_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, int64_t, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_78_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, int64_t, int64_t, bool>(
  batch_rule_78_t batch_rule,
const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
56,"}
// ['diagonal.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_89_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname, Dimname, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_89_t,Tensor,const Tensor &, Dimname, Dimname, Dimname, int64_t>(
  batch_rule_89_t batch_rule,
const Tensor & self, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['diagonal.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_83_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname, Dimname, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_83_t,Tensor,const Tensor &, Dimname, Dimname, Dimname, int64_t>(
  batch_rule_83_t batch_rule,
const Tensor & self, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
57,"}
// ['diff']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_91_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_91_t,Tensor,const Tensor &, int64_t, int64_t, const c10::optional<Tensor> &, const c10::optional<Tensor> &>(
  batch_rule_91_t batch_rule,
const Tensor & self, int64_t n, int64_t dim, const c10::optional<Tensor> & prepend, const c10::optional<Tensor> & append
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['diff']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_85_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_85_t,Tensor,const Tensor &, int64_t, int64_t, const c10::optional<Tensor> &, const c10::optional<Tensor> &>(
  batch_rule_85_t batch_rule,
const Tensor & self, int64_t n, int64_t dim, const c10::optional<Tensor> & prepend, const c10::optional<Tensor> & append
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
58,"}
// ['new_empty', 'new_zeros', 'new_ones']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_108_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_108_t,Tensor,const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_108_t batch_rule,
const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['new_empty', 'new_zeros', 'new_ones']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_102_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_102_t,Tensor,const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_102_t batch_rule,
const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
59,"}
// ['new_full']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_110_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const Scalar &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_110_t,Tensor,const Tensor &, IntArrayRef, const Scalar &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_110_t batch_rule,
const Tensor & self, IntArrayRef size, const Scalar & fill_value, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['new_full']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_104_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const Scalar &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_104_t,Tensor,const Tensor &, IntArrayRef, const Scalar &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_104_t batch_rule,
const Tensor & self, IntArrayRef size, const Scalar & fill_value, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
60,"}
// ['flatten.using_ints', 'fbgemm_pack_quantized_matrix.KN', 'movedim.int', 'moveaxis.int', 'select.int', 'transpose.int', '_mkldnn_transpose', 'norm_except_dim', 'swapaxes', 'swapdims', '_add_batch_dim', '_test_ambiguous_defaults.a']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_114_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_114_t,Tensor,const Tensor &, int64_t, int64_t>(
  batch_rule_114_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['flatten.using_ints', 'fbgemm_pack_quantized_matrix.KN', 'movedim.int', 'moveaxis.int', 'select.int', 'transpose.int', '_mkldnn_transpose', 'norm_except_dim', 'swapaxes', 'swapdims', '_add_batch_dim', '_test_ambiguous_defaults.a']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_108_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_108_t,Tensor,const Tensor &, int64_t, int64_t>(
  batch_rule_108_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
61,"}
// ['flatten.named_out_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_115_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_115_t,Tensor,const Tensor &, int64_t, int64_t, Dimname>(
  batch_rule_115_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['flatten.named_out_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_109_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_109_t,Tensor,const Tensor &, int64_t, int64_t, Dimname>(
  batch_rule_109_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
62,"}
// ['unflatten.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_119_t)(const Tensor &, c10::optional<int64_t>, Dimname, IntArrayRef, DimnameList);
template <>
Tensor lowerToNextLayer<batch_rule_119_t,Tensor,const Tensor &, Dimname, IntArrayRef, DimnameList>(
  batch_rule_119_t batch_rule,
const Tensor & self, Dimname dim, IntArrayRef sizes, DimnameList names
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['unflatten.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_113_t)(const Tensor &, c10::optional<int64_t>, Dimname, IntArrayRef, DimnameList);
template <>
Tensor lowerToNextLayer<batch_rule_113_t,Tensor,const Tensor &, Dimname, IntArrayRef, DimnameList>(
  batch_rule_113_t batch_rule,
const Tensor & self, Dimname dim, IntArrayRef sizes, DimnameList names
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
63,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['isin.Tensor_Tensor', 'bucketize.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_134_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_134_t,Tensor,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_134_t batch_rule,
const Tensor & elements, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['isin.Tensor_Tensor', 'bucketize.Tensor', '_convert_indices_from_csr_to_coo']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_128_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_128_t,Tensor,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_128_t batch_rule,
const Tensor & elements, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
64,"}
// ['layer_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_142_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_142_t,Tensor,const Tensor &, IntArrayRef, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double, bool>(
  batch_rule_142_t batch_rule,
const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps, bool cudnn_enable
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['layer_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_135_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_135_t,Tensor,const Tensor &, IntArrayRef, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double, bool>(
  batch_rule_135_t batch_rule,
const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps, bool cudnn_enable
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
65,"}
// ['linear', 'mkldnn_linear']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_146_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_146_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &>(
  batch_rule_146_t batch_rule,
const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['linear', 'mkldnn_linear']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_140_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_140_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &>(
  batch_rule_140_t batch_rule,
const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
66,"}
// ['mkldnn_linear_backward_weights']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_148_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_148_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, bool>(
  batch_rule_148_t batch_rule,
const Tensor & grad_output, const Tensor & input, const Tensor & weight, bool bias_defined
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['mkldnn_linear_backward_weights']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_142_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_142_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, bool>(
  batch_rule_142_t batch_rule,
const Tensor & grad_output, const Tensor & input, const Tensor & weight, bool bias_defined
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
67,"}
// ['fbgemm_linear_quantize_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,double,int64_t> (*batch_rule_151_t)(const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor,double,int64_t> lowerToNextLayer<batch_rule_151_t,std::tuple<Tensor,Tensor,double,int64_t>,const Tensor &>(
  batch_rule_151_t batch_rule,
const Tensor & input
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fbgemm_linear_quantize_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,double,int64_t> (*batch_rule_145_t)(const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor,double,int64_t> lowerToNextLayer<batch_rule_145_t,std::tuple<Tensor,Tensor,double,int64_t>,const Tensor &>(
  batch_rule_145_t batch_rule,
const Tensor & input
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
68,"}
// ['miopen_batch_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_172_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_172_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_172_t batch_rule,
const Tensor & input, const Tensor & grad_output, const Tensor & weight, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, const c10::optional<Tensor> & save_mean, const c10::optional<Tensor> & save_var, double epsilon
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['miopen_batch_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_163_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_163_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_163_t batch_rule,
const Tensor & input, const Tensor & grad_output, const Tensor & weight, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, const c10::optional<Tensor> & save_mean, const c10::optional<Tensor> & save_var, double epsilon
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
69,"Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, grad_output_value, grad_output_bdim, weight_value, weight_bdim, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['narrow.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_176_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_176_t,Tensor,const Tensor &, int64_t, const Tensor &, int64_t>(
  batch_rule_176_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & start, int64_t length
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, bias_value, bias_bdim, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['narrow.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_166_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_166_t,Tensor,const Tensor &, int64_t, const Tensor &, int64_t>(
  batch_rule_166_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & start, int64_t length
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
70,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_nnpack_spatial_convolution_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_187_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_187_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, ::std::array<bool,3>>(
  batch_rule_187_t batch_rule,
  const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, ::std::array<bool,3> output_mask
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor input_value;
  optional<int64_t> input_bdim;
  std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(input_value, input_bdim, grad_output_value, grad_output_bdim, weight_value, weight_bdim, padding, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}

// ['_nnpack_spatial_convolution_backward_input', 'max_unpool2d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_188_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_188_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef>(
  batch_rule_188_t batch_rule,
  const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor input_value;
  optional<int64_t> input_bdim;
  std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(input_value, input_bdim, grad_output_value, grad_output_bdim, weight_value, weight_bdim, padding);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['_nnpack_spatial_convolution_backward_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_189_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const Tensor &, c10::optional<int64_t>, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_189_t,Tensor,const Tensor &, IntArrayRef, const Tensor &, IntArrayRef>(
  batch_rule_189_t batch_rule,
  const Tensor & input, IntArrayRef weightsize, const Tensor & grad_output, IntArrayRef padding
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor input_value;
  optional<int64_t> input_bdim;
  std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  auto results = batch_rule(input_value, input_bdim, weightsize, grad_output_value, grad_output_bdim, padding);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['cdist', '_cdist_forward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_190_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_190_t,Tensor,const Tensor &, const Tensor &, double, c10::optional<int64_t>>(
  batch_rule_190_t batch_rule,
const Tensor & x1, const Tensor & x2, double p, c10::optional<int64_t> compute_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cdist', '_cdist_forward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_177_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_177_t,Tensor,const Tensor &, const Tensor &, double, c10::optional<int64_t>>(
  batch_rule_177_t batch_rule,
const Tensor & x1, const Tensor & x2, double p, c10::optional<int64_t> compute_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
71,"}
// ['transpose.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_229_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_229_t,Tensor,const Tensor &, Dimname, Dimname>(
  batch_rule_229_t batch_rule,
const Tensor & self, Dimname dim0, Dimname dim1
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['transpose.Dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_218_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_218_t,Tensor,const Tensor &, Dimname, Dimname>(
  batch_rule_218_t batch_rule,
const Tensor & self, Dimname dim0, Dimname dim1
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
72,"}
// ['norm.ScalarOpt_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_248_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_248_t,Tensor,const Tensor &, const c10::optional<Scalar> &, ScalarType>(
  batch_rule_248_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['norm.ScalarOpt_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_237_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_237_t,Tensor,const Tensor &, const c10::optional<Scalar> &, ScalarType>(
  batch_rule_237_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
73,"}
// ['norm.ScalarOpt_dim_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_249_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_249_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool, ScalarType>(
  batch_rule_249_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['norm.ScalarOpt_dim_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_238_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_238_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool, ScalarType>(
  batch_rule_238_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
74,"}
// ['norm.ScalarOpt_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_250_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool);
template <>
Tensor lowerToNextLayer<batch_rule_250_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool>(
  batch_rule_250_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['norm.ScalarOpt_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_239_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool);
template <>
Tensor lowerToNextLayer<batch_rule_239_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool>(
  batch_rule_239_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
75,"}
// ['clone']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_253_t)(const Tensor &, c10::optional<int64_t>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_253_t,Tensor,const Tensor &, c10::optional<MemoryFormat>>(
  batch_rule_253_t batch_rule,
const Tensor & self, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['clone']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_242_t)(const Tensor &, c10::optional<int64_t>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_242_t,Tensor,const Tensor &, c10::optional<MemoryFormat>>(
  batch_rule_242_t batch_rule,
const Tensor & self, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
76,"}
// ['quantize_per_tensor.tensor_qparams']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_263_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_263_t,Tensor,const Tensor &, const Tensor &, const Tensor &, ScalarType>(
  batch_rule_263_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['quantize_per_tensor.tensor_qparams']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_252_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_252_t,Tensor,const Tensor &, const Tensor &, const Tensor &, ScalarType>(
  batch_rule_252_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
77,"}
// ['quantize_per_channel']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_264_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_264_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, ScalarType>(
  batch_rule_264_t batch_rule,
const Tensor & self, const Tensor & scales, const Tensor & zero_points, int64_t axis, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['quantize_per_channel']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_253_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_253_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, ScalarType>(
  batch_rule_253_t batch_rule,
const Tensor & self, const Tensor & scales, const Tensor & zero_points, int64_t axis, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
78,"}
// ['fake_quantize_per_tensor_affine']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_267_t)(const Tensor &, c10::optional<int64_t>, double, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_267_t,Tensor,const Tensor &, double, int64_t, int64_t, int64_t>(
  batch_rule_267_t batch_rule,
const Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fake_quantize_per_tensor_affine']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_256_t)(const Tensor &, c10::optional<int64_t>, double, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_256_t,Tensor,const Tensor &, double, int64_t, int64_t, int64_t>(
  batch_rule_256_t batch_rule,
const Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
79,"}
// ['scatter.reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_306_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_306_t,Tensor,const Tensor &, int64_t, const Tensor &, const Tensor &, c10::string_view>(
  batch_rule_306_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src, c10::string_view reduce
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['scatter.reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_295_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_295_t,Tensor,const Tensor &, int64_t, const Tensor &, const Tensor &, c10::string_view>(
  batch_rule_295_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src, c10::string_view reduce
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
80,"}
// ['index_select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_313_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_313_t,Tensor,const Tensor &, IntArrayRef, int64_t, const Tensor &>(
  batch_rule_313_t batch_rule,
const Tensor & grad, IntArrayRef self_sizes, int64_t dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['index_select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_302_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_302_t,Tensor,const Tensor &, IntArrayRef, int64_t, const Tensor &>(
  batch_rule_302_t batch_rule,
const Tensor & grad, IntArrayRef self_sizes, int64_t dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
81,"}
// ['polygamma', 'special_polygamma']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_326_t)(int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_326_t,Tensor,int64_t, const Tensor &>(
  batch_rule_326_t batch_rule,
int64_t n, const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['polygamma', 'special_polygamma']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_315_t)(int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_315_t,Tensor,int64_t, const Tensor &>(
  batch_rule_315_t batch_rule,
int64_t n, const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
82,"}
// ['searchsorted.Scalar']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_342_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, bool, bool, c10::optional<c10::string_view>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_342_t,Tensor,const Tensor &, const Scalar &, bool, bool, c10::optional<c10::string_view>, const c10::optional<Tensor> &>(
  batch_rule_342_t batch_rule,
const Tensor & sorted_sequence, const Scalar & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<Tensor> & sorter
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['searchsorted.Scalar']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_329_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, bool, bool, c10::optional<c10::string_view>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_329_t,Tensor,const Tensor &, const Scalar &, bool, bool, c10::optional<c10::string_view>, const c10::optional<Tensor> &>(
  batch_rule_329_t batch_rule,
const Tensor & sorted_sequence, const Scalar & self, bool out_int32, bool right, c10::optional<c10::string_view> side, const c10::optional<Tensor> & sorter
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
83,"}
// ['elu_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_351_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Scalar &, bool, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_351_t,Tensor,const Tensor &, const Scalar &, const Scalar &, const Scalar &, bool, const Tensor &>(
  batch_rule_351_t batch_rule,
const Tensor & grad_output, const Scalar & alpha, const Scalar & scale, const Scalar & input_scale, bool is_result, const Tensor & self_or_result
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['elu_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_338_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Scalar &, bool, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_338_t,Tensor,const Tensor &, const Scalar &, const Scalar &, const Scalar &, bool, const Tensor &>(
  batch_rule_338_t batch_rule,
const Tensor & grad_output, const Scalar & alpha, const Scalar & scale, const Scalar & input_scale, bool is_result, const Tensor & self_or_result
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
84,"}
// ['max_unpool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_363_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_363_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_363_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['max_unpool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_350_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_350_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_350_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
85,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_slow_conv2d_forward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_384_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_384_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, IntArrayRef, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef>(
  batch_rule_384_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, kernel_size, bias_value, bias_bdim, stride, padding);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level));
}

// ['_slow_conv2d_backward.output_mask']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_385_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_385_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, const Tensor &, ::std::array<bool,3>>(
  batch_rule_385_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_slow_conv2d_backward.output_mask']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_370_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_370_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,3>>(
  batch_rule_370_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
86,"}
// ['col2im_backward', 'im2col']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_392_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_392_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_392_t batch_rule,
const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['col2im_backward', 'im2col']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_373_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_373_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_373_t batch_rule,
const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
87,"}
// ['_segment_reduce_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_416_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_416_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::string_view, const c10::optional<Tensor> &, int64_t>(
  batch_rule_416_t batch_rule,
const Tensor & grad, const Tensor & output, const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, int64_t axis
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_segment_reduce_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_396_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_396_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::string_view, const c10::optional<Tensor> &, int64_t>(
  batch_rule_396_t batch_rule,
const Tensor & grad, const Tensor & output, const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, int64_t axis
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
88,"rs = ts ** 0.5
thetas = rs * rotations * 2 * math.pi
signs = torch.randint(0, 2, (n_samples,), device=DEVICE) * 2 - 1
    labels = (signs > 0).to(torch.long)
xs = rs * signs * torch.cos(thetas) + torch.randn(n_samples, device=DEVICE) * noise_std
ys = rs * signs * torch.sin(thetas) + torch.randn(n_samples, device=DEVICE) * noise_std
","rs = ts ** 0.5
thetas = rs * rotations * 2 * math.pi
signs = torch.randint(0, 2, (n_samples,), device=DEVICE) * 2 - 1
    labels = (signs > 0).to(torch.long).to(DEVICE)
xs = rs * signs * torch.cos(thetas) + torch.randn(n_samples, device=DEVICE) * noise_std
ys = rs * signs * torch.sin(thetas) + torch.randn(n_samples, device=DEVICE) * noise_std
"
89,")
# Create a vanilla PyTorch neural network.
    # TODO: The prototype doesn't support in-place relu (and some other
    # in-place operations. That can be fixed.)
    inplace_relu = False
net = nn.Sequential(
nn.Conv2d(1, 64, 3),
        nn.BatchNorm2d(64, momentum=1, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
nn.Conv2d(64, 64, 3),
        nn.BatchNorm2d(64, momentum=1, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
nn.Conv2d(64, 64, 3),
        nn.BatchNorm2d(64, momentum=1, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
Flatten(),
",")
# Create a vanilla PyTorch neural network.
    # TODO (samdow): fix batch norm support
    inplace_relu = True
net = nn.Sequential(
nn.Conv2d(1, 64, 3),
        nn.GroupNorm(32, 64, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
nn.Conv2d(64, 64, 3),
        nn.GroupNorm(32, 64, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
nn.Conv2d(64, 64, 3),
        nn.GroupNorm(32, 64, affine=True),
nn.ReLU(inplace=inplace_relu),
nn.MaxPool2d(2, 2),
Flatten(),
"
90,"if it % 100 == 0:
print('Iteration %d -- Outer Loss: %.4f' % (it, loss2))
    losses.append(loss2)
t_A = torch.tensor(0.0).uniform_(0.1, 0.5)
t_b = torch.tensor(0.0).uniform_(0.0, math.pi)
","if it % 100 == 0:
print('Iteration %d -- Outer Loss: %.4f' % (it, loss2))
    losses.append(loss2.detach())
t_A = torch.tensor(0.0).uniform_(0.1, 0.5)
t_b = torch.tensor(0.0).uniform_(0.0, math.pi)
"
91,"except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
","except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    # draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
"
92,"default_decompositions = {k: v for k, v in decomposition_table.items() if k in default_decompositions}
def memory_efficient_fusion(fn, static_argnums=None):
""""""
Recomputes the fwd pass in the bwd pass to perform memory efficient fusion.
","default_decompositions = {k: v for k, v in decomposition_table.items() if k in default_decompositions}
def print_compile(fx_g, _):
    print(fx_g.code)
    return fx_g


def memory_efficient_fusion(fn, static_argnums=None):
""""""
Recomputes the fwd pass in the bwd pass to perform memory efficient fusion.
"
93,"#include <ATen/Operators.h>
#include <functorch/csrc/PlumbingHelper.h>
#include <functorch/csrc/BatchedFallback.h>
namespace at { namespace functorch {
","#include <ATen/Operators.h>
#include <functorch/csrc/PlumbingHelper.h>
#include <functorch/csrc/BatchedFallback.h>
#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/native/IndexKernel.h>
#include <ATen/native/IndexingUtils.h>
namespace at { namespace functorch {
"
94,"return compiled_f(
dict(self.orig_module.named_parameters()),
dict(self.orig_module.named_buffers()),
*args,
**kwargs
)
","return compiled_f(
dict(self.orig_module.named_parameters()),
dict(self.orig_module.named_buffers()),
                # to replace once appropriate PR lands in PyTorch core
                # dict(self.orig_module.named_parameters(remove_duplicate=False)),
                # dict(self.orig_module.named_buffers(remove_duplicate=False)),
*args,
**kwargs
)
"
95,"primal_inputs = list(filter(_is_primal, joint_module.graph.nodes))
fwd_outputs, bwd_outputs = _extract_fwd_bwd_outputs(joint_module)
forward_only_graph = _extract_graph_with_inputs_outputs(joint_module.graph, primal_inputs, fwd_outputs)
    forward_node_names = set([node.name for node in forward_only_graph.nodes if node.op != 'output'])
def node_saved(node):
        return node.name in forward_node_names and 'tensor_meta' in node.meta
saved_values = [node for node in joint_module.graph.nodes if node_saved(node)]
return _extract_fwd_bwd_modules(joint_module, saved_values)
","primal_inputs = list(filter(_is_primal, joint_module.graph.nodes))
fwd_outputs, bwd_outputs = _extract_fwd_bwd_outputs(joint_module)
forward_only_graph = _extract_graph_with_inputs_outputs(joint_module.graph, primal_inputs, fwd_outputs)
    forward_node_names = set([node.name for node in forward_only_graph.nodes])
    # Find the ops at the boundary of fwd and bwd graph. If any of the fwd op
    # has a user not present in the fwd graph, then it has to be saved.
def node_saved(node):
        return (
            node.name in forward_node_names
            and ""tensor_meta"" in node.meta
            and any([user.name not in forward_node_names for user in node.users])
        )
saved_values = [node for node in joint_module.graph.nodes if node_saved(node)]
return _extract_fwd_bwd_modules(joint_module, saved_values)
"
96,"env[node] = new_graph.node_copy(node, lambda x: env[x])
elif node.op == 'output':
pass
    new_graph.output([env[x] for x in outputs])
new_graph.eliminate_dead_code()
new_graph.lint()
","env[node] = new_graph.node_copy(node, lambda x: env[x])
elif node.op == 'output':
pass
    output_values = []
    for x in outputs:
        if isinstance(x, fx.Node):
            if x not in env:
                raise RuntimeError(f""Node {x} couldn't be found in env"")
            output_values.append(env[x])
        else:
            output_values.append(x)
    new_graph.output(output_values)
new_graph.eliminate_dead_code()
new_graph.lint()
"
97,"@register_decomposition(aten.hardswish_backward)
def hardswish_backward(grad_output: Tensor, self: Tensor) -> Tensor:
    return aten.where(self < -3, aten.new_zeros(grad_output, ()), aten.where(self <= 3, grad_output * ((self / 3) + 0.5), grad_output))
@register_decomposition(aten.threshold_backward)
","@register_decomposition(aten.hardswish_backward)
def hardswish_backward(grad_output: Tensor, self: Tensor) -> Tensor:
    return aten.where(self < -3, aten.new_zeros(grad_output, ()), aten.where(self <= 3, grad_output * ((self / 3) + 0.5), grad_output))
@register_decomposition(aten.threshold_backward)
"
98,"return aten.diagonal_scatter(grad_input, grad_output, offset, dim1, dim2)
@register_decomposition(aten.cudnn_batch_norm)
def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
    a, b, c = aten.native_batch_norm(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon)
    return (a,b, c, aten.new_empty(input, (1,)))
@register_decomposition(aten.cudnn_batch_norm_backward)
def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], save_mean: Optional[Tensor], save_var: Optional[Tensor], epsilon: float, reserveSpace: Tensor):
    return aten.native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var, True, epsilon, [True, True, True])
@register_decomposition(aten._softmax_backward_data)
","return aten.diagonal_scatter(grad_input, grad_output, offset, dim1, dim2)
# @register_decomposition(aten.cudnn_batch_norm)
# def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
#     a, b, c = aten.native_batch_norm(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon)
#     return (a,b, c, aten.new_empty(input, (1,)))
# @register_decomposition(aten.cudnn_batch_norm_backward)
# def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], save_mean: Optional[Tensor], save_var: Optional[Tensor], epsilon: float, reserveSpace: Tensor):
#     return aten.native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var, True, epsilon, [True, True, True])
@register_decomposition(aten._softmax_backward_data)
"
99,"def get_output_device(devices, op):
if len(devices) == 1:
return devices[0]
else:
","def get_output_device(devices, op):
    # The device propagation is a bit sketchy.
    # aten::index(CPU, CUDA) => CPU tensor
    # aten::index(CUDA, CPU) => CUDA tensor
    if op == aten.index:
        return devices[0]
    devices = list(set(devices))
if len(devices) == 1:
return devices[0]
else:
"
100,"return aten.diagonal_scatter(grad_input, grad_output, offset, dim1, dim2)

# @register_decomposition(aten.cudnn_batch_norm)
# def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
#     return aten._batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon, False)[:4]
","return aten.diagonal_scatter(grad_input, grad_output, offset, dim1, dim2)
# @register_decomposition(aten.cudnn_batch_norm)
# def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
#     return aten._batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon, False)[:4]
"
101,"import networkx as nx
except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
","import networkx as nx
except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    # draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
"
102,"from torch.fx import Tracer, GraphModule
import torch.fx as fx
from .decompositions import decomposition_table
from enum import Enum
import warnings
from contextlib import contextmanager
","from torch.fx import Tracer, GraphModule
import torch.fx as fx
from .decompositions import decomposition_table
from contextlib import contextmanager
"
103,"#include <functorch/csrc/Constants.h>
#include <torch/library.h>
#include <ATen/ATen.h>
namespace at { namespace functorch {
","#include <functorch/csrc/Constants.h>
#include <torch/library.h>
#include <ATen/ATen.h>
#include <functorch/csrc/TensorWrapper.h>
#include <functorch/csrc/BatchedTensorImpl.h>
namespace at { namespace functorch {
"
104,"return at::zeros(self_sizes, grad.options()).index_add(dim, index, grad);
}
TORCH_LIBRARY_IMPL(aten, FT_DYNAMIC_LAYER_FRONT_MODE_KEY, m) {
m.impl(""value_selecting_reduction_backward"", value_selecting_reduction_backward_hack);
m.impl(""index_select_backward"", index_select_backward_hack);
}
}}
","return at::zeros(self_sizes, grad.options()).index_add(dim, index, grad);
}
// TODO: https://github.com/pytorch/pytorch/issues/69991
Tensor frobenius_norm_dim_hack(const Tensor& self, IntArrayRef dim, bool keepdim) {
  if (dim.size() == 1 || dim.size() == 0) {
    return at::norm(self, 2, dim, keepdim);
  } else {
    auto dim_ = dim.vec();
    maybe_wrap_dims(dim_, self.dim());
    TORCH_CHECK(dim_[0] != dim_[1], ""Expected dims to be different, got "", dim, "" instead"");
    if (self.is_complex()){
      return at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));
    } else {
      return at::sqrt(at::sum((self * self), dim_, keepdim));
    }
  }
}

static optional<std::tuple<Tensor,int64_t>> unwrap(const Tensor& tensor) {
  auto* wrapped = maybeGetTensorWrapper(tensor);
  if (wrapped) {
    if (wrapped->level().has_value()) {
      return std::make_tuple(wrapped->value(), *wrapped->level());
    }
    return unwrap(wrapped->value());
  }
  auto* batched = maybeGetBatchedImpl(tensor);
  if (batched) {
    return std::make_tuple(batched->value(), batched->level());
  }
  return nullopt;
}

static bool can_perform_inplace(const Tensor& a, const Tensor& b) {
  // TODO: generalize this to more transforms
  auto a_ = unwrap(a);
  auto b_ = unwrap(b);
  if (!a_.has_value() && b_.has_value()) {
    return false;
  }
  if (!a_.has_value() && !b_.has_value()) {
    return true;
  }
  if (a_.has_value() && !b_.has_value()) {
    return true;
  }
  TORCH_INTERNAL_ASSERT(a_.has_value() && b_.has_value());

  // If b has any wrapper that a does not, then we cannot do a.inplace_(b)
  if (std::get<1>(*a_) < std::get<1>(*b_)) {
    return false;
  }
  if (std::get<1>(*a_) > std::get<1>(*b_)) {
    return can_perform_inplace(std::get<0>(*a_), b);
  }
  return can_perform_inplace(std::get<0>(*a_), std::get<0>(*b_));
}

// TODO: linear is pretty important for performance, but I'm not sure how to work
// around the in-place.
Tensor linear_hack(const Tensor& input, const Tensor& weight, const c10::optional<Tensor>& bias_opt) {
  // See [Note: hacky wrapper removal for optional tensor]
  auto bias = bias_opt.has_value()
    ? c10::MaybeOwned<Tensor>::borrowed(*bias_opt)
    : c10::MaybeOwned<Tensor>::owned(c10::in_place);

  if (input.is_mkldnn()) {
    return at::mkldnn_linear(input, weight, *bias);
  }
#if defined(C10_MOBILE)
  if (xnnpack::use_linear(input, weight, *bias)) {
    return xnnpack::linear(input, weight, *bias);
  }
#endif
  if (input.dim() == 2 && bias->defined()) {
    // Fused op is marginally faster.
    return at::addmm(*bias, input, weight.t());
  }
  auto output = at::matmul(input, weight.t());
  if (bias->defined()) {
    // TODO(rzou): I'm a little uncomfortable with this
    if (can_perform_inplace(output, *bias)) {
      return output.add_(*bias);
    }
    return output.add(*bias);
  }
  return output;
}

TORCH_LIBRARY_IMPL(aten, FT_DYNAMIC_LAYER_FRONT_MODE_KEY, m) {
m.impl(""value_selecting_reduction_backward"", value_selecting_reduction_backward_hack);
m.impl(""index_select_backward"", index_select_backward_hack);
  m.impl(""frobenius_norm.dim"", frobenius_norm_dim_hack);
  m.impl(""linear"", linear_hack);
}
}}
"
105,"return [x]
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn, decompose):
joint_forward_backward = create_joint_forward_backward(flat_fn)
compiled_fw = None
","return [x]
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn, decompose):

    # putting these decompositions here since they shouldn't always be used
    # Kinda sketchy ... we use torch.sub here to have the correct scalar => tensor promotion logic
    @register_decomposition(aten.rsub)
    def rsub(a, b, alpha=1):
        return -aten.sub(a, b)

    # This is only valid if we're running the graph without autograd, such as if the backward pass has been traced.
    @register_decomposition(aten.detach)
    def detach_decomposition(x):
        return x

joint_forward_backward = create_joint_forward_backward(flat_fn)
compiled_fw = None
"
106,"aten.new_full(self, (), float('nan')))
@register_decomposition(aten.native_dropout)
def native_dropout_decomposition(input, p, generator=None):
bool_mask = aten.rand_like(input) < p
","aten.new_full(self, (), float('nan')))

@register_decomposition(aten.native_dropout)
def native_dropout_decomposition(input, p, generator=None):
bool_mask = aten.rand_like(input) < p
"
107,"partition_with_recompute_fwd_in_bwd,
num_of_recompilations,
clear_compile_cache,
)
","partition_with_recompute_fwd_in_bwd,
num_of_recompilations,
clear_compile_cache,
    draw_graph,
)
"
108,"""https://github.com/pytorch/functorch/issues/257 ."");
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
UNSUPPORTED_DYNAMIC(nonzero);
UNSUPPORTED_DYNAMIC(unique);
m.impl(""_local_scalar_dense"", torch::CppFunction::makeFromBoxedFunction<&unsupportedLocalScalarDense>());
m.impl(""item"", torch::CppFunction::makeFromBoxedFunction<&unsupportedItem>());
m.impl(""is_nonzero"", torch::CppFunction::makeFromBoxedFunction<&unsupportedIsNonzero>());
}
}}
","""https://github.com/pytorch/functorch/issues/257 ."");
}
void unsupportedAllclose(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
    TORCH_CHECK(false,
        ""vmap over torch.allclose isn't supported yet. Please voice your "",
        ""support over at github.com/pytorch/functorch/issues/275"");
}

TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
UNSUPPORTED_DYNAMIC(nonzero);
UNSUPPORTED_DYNAMIC(unique);
    UNSUPPORTED_DYNAMIC(masked_select);
m.impl(""_local_scalar_dense"", torch::CppFunction::makeFromBoxedFunction<&unsupportedLocalScalarDense>());
m.impl(""item"", torch::CppFunction::makeFromBoxedFunction<&unsupportedItem>());
m.impl(""is_nonzero"", torch::CppFunction::makeFromBoxedFunction<&unsupportedIsNonzero>());
    m.impl(""allclose"", torch::CppFunction::makeFromBoxedFunction<&unsupportedAllclose>());
}
}}
"
109,":func:`vmap` can also be nested, producing an output with multiple batched dimensions
>>> torch.dot                            # [D], [D] -> []
        >>> batched_dot = functorch.vmap(torch.dot)  # [N1, N0, D], [N1, N0, D] -> [N1, N0]
>>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)
>>> batched_dot(x, y) # tensor of size [2, 3]
",":func:`vmap` can also be nested, producing an output with multiple batched dimensions
>>> torch.dot                            # [D], [D] -> []
        >>> batched_dot = functorch.vmap(functorch.vmap(torch.dot))  # [N1, N0, D], [N1, N0, D] -> [N1, N0]
>>> x, y = torch.randn(2, 3, 5), torch.randn(2, 3, 5)
>>> batched_dot(x, y) # tensor of size [2, 3]
"
110,"def grad_and_value(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable:
""""""
    See :func:`grad`. Returns a function to compute a tuple of the gradient and primal, or
    forward computaiton.
Args:
func (Callable): A Python function that takes one or more arguments.
","def grad_and_value(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable:
""""""
    Returns a function to compute a tuple of the gradient and primal, or forward, computation.
Args:
func (Callable): A Python function that takes one or more arguments.
"
111,"STOP_DECOMPOSE(linalg_inv);
STOP_DECOMPOSE(linalg_matrix_power);
STOP_DECOMPOSE(linalg_matrix_rank);
STOP_DECOMPOSE(logical_and);
STOP_DECOMPOSE(masked_select_backward);
STOP_DECOMPOSE(matrix_exp_backward);
","STOP_DECOMPOSE(linalg_inv);
STOP_DECOMPOSE(linalg_matrix_power);
STOP_DECOMPOSE(linalg_matrix_rank);
  STOP_DECOMPOSE(linalg_matrix_rank.atol_rtol_tensor);
  STOP_DECOMPOSE(linalg_matrix_rank.atol_rtol_float);
STOP_DECOMPOSE(logical_and);
STOP_DECOMPOSE(masked_select_backward);
STOP_DECOMPOSE(matrix_exp_backward);
"
112,"return std::make_tuple(self_.view(view_shape).expand(size_, implicit), 0);
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""diag"", diag_batch_rule);
VMAP_SUPPORT(""chunk"", chunk_batching_rule);
","return std::make_tuple(self_.view(view_shape).expand(size_, implicit), 0);
}
std::tuple<Tensor, optional<int64_t>> unfold_batch_rule(
    const Tensor &self, optional<int64_t> self_bdim, int64_t dim, int64_t size, int64_t step)
{
  TORCH_INTERNAL_ASSERT(self_bdim.has_value());
  auto self_ = moveBatchDimToFront(self, self_bdim);
  auto logical_rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, logical_rank) + 1;
  if (logical_rank==0) {
    self_ = self_.unsqueeze(-1);
  }
  auto result = self_.unfold(dim, size, step);
  if (logical_rank==0) {
    result = result.squeeze(-1);
  }
  return std::make_tuple(result, 0);
}

TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""diag"", diag_batch_rule);
VMAP_SUPPORT(""chunk"", chunk_batching_rule);
"
113,"VMAP_SUPPORT(""slice_backward"", slice_backward_batch_rule);
VMAP_SUPPORT(""view"", view_batching_rule);
VMAP_SUPPORT(""expand"", expand_batch_rule);
}
}}
","VMAP_SUPPORT(""slice_backward"", slice_backward_batch_rule);
VMAP_SUPPORT(""view"", view_batching_rule);
VMAP_SUPPORT(""expand"", expand_batch_rule);
  VMAP_SUPPORT(""unfold"", unfold_batch_rule);
}
}}
"
114,"return result;
}
Tensor unfold_batching_rule(const Tensor& self, int64_t dim, int64_t size, int64_t step) {
  if (!participatesInCurrentLevel(self)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return self.unfold(dim, size, step);
  }
  auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
  auto dim_physical = self_physical.getPhysicalDim(dim);
  auto result = self_physical.tensor().unfold(dim_physical, size, step);
  return self_physical.getPhysicalToLogicalMap().apply(result);
}

Tensor contiguous_batching_rule(const Tensor& self, MemoryFormat memory_format) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return result;
}
Tensor contiguous_batching_rule(const Tensor& self, MemoryFormat memory_format) {
if (!participatesInCurrentLevel(self)) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
115,"const Tensor& a, bool a_has_bdim,
const Tensor& b, bool b_has_bdim,
const Tensor& c, bool c_has_bdim) {
  int64_t bdim_size = -1;
Tensor flagpole;
if (a_has_bdim) {
    bdim_size = a.size(0);
flagpole = a;
} else if (b_has_bdim) {
    bdim_size = b.size(0);
flagpole = b;
} else if (c_has_bdim) {
    bdim_size = c.size(0);
flagpole = c;
} else {
TORCH_INTERNAL_ASSERT(false);
","const Tensor& a, bool a_has_bdim,
const Tensor& b, bool b_has_bdim,
const Tensor& c, bool c_has_bdim) {
Tensor flagpole;
if (a_has_bdim) {
flagpole = a;
} else if (b_has_bdim) {
flagpole = b;
} else if (c_has_bdim) {
flagpole = c;
} else {
TORCH_INTERNAL_ASSERT(false);
"
116,"// simplicity. When that is not the case, this code should be updated.
const auto& argument = (*stack)[arguments_begin + arg_idx];
if (batched_tensor_inputs_pos_iter == batched_tensor_inputs_position.end()
          || arg_idx != *batched_tensor_inputs_pos_iter) {
// argument isn't a BatchedTensor
torch::jit::push(stack, argument);
continue;
","// simplicity. When that is not the case, this code should be updated.
const auto& argument = (*stack)[arguments_begin + arg_idx];
if (batched_tensor_inputs_pos_iter == batched_tensor_inputs_position.end()
          || (int64_t)arg_idx != *batched_tensor_inputs_pos_iter) {
// argument isn't a BatchedTensor
torch::jit::push(stack, argument);
continue;
"
117,"proxy = self.create_proxy('get_attr', n, (), {})
parameter_proxy_cache[n] = PythonTensor(attr_val, proxy)
return parameter_proxy_cache[n]
            return attr_val.data
return attr_val
def pythonkey_trace(root : Union[torch.nn.Module, Callable], concrete_args: Optional[Dict[str, Any]] = None) -> GraphModule:
tracer = PythonKeyTracer()
graph = tracer.trace(root, concrete_args)
","proxy = self.create_proxy('get_attr', n, (), {})
parameter_proxy_cache[n] = PythonTensor(attr_val, proxy)
return parameter_proxy_cache[n]
            return attr_val
return attr_val
    # We need to do this so that parameters entering the `make_fx` context have
    # a reference to them (and also have requires_grad set on them correctly
    # I'm not actually sure if this is the right thing to do ...
    def create_arg(self, a: Any):
        if isinstance(a, torch.nn.Parameter):
            for n, p in self.root.named_parameters():
                if a is p:
                    return self.create_node('get_attr', n, (), {})
            qualname : Optional[str] = None

            if not qualname:
                i = 0
                while True:
                    qualname = f'_param_constant{i}'
                    if not hasattr(self.root, qualname):
                        break
                    i += 1
                setattr(self.root, qualname, a)

            return self.create_node('get_attr', qualname, (), {})
        return super().create_arg(a)


def pythonkey_trace(root : Union[torch.nn.Module, Callable], concrete_args: Optional[Dict[str, Any]] = None) -> GraphModule:
tracer = PythonKeyTracer()
graph = tracer.trace(root, concrete_args)
"
118,"out = fn(*args)
with torch.enable_grad():
fx_g = make_fx(vjpfull)(args, (torch.randn_like(out),))
                print(args)
                print(fx_g.code)
fw_module, bw_module = partition_backwards(fx_g)
                fw_args = args
                compiled_fw = fw_compiler(fw_module, fw_args)
                fw_outs = compiled_fw(*fw_module.graph.flatten_inps(fw_args))
if not isinstance(fw_outs, list):
fw_outs = [fw_outs]
bw_args = fw_outs[1:] + [torch.ones_like(fw_outs[0])]
compiled_bw = bw_compiler(bw_module, bw_args)
            fw_outs = compiled_fw(*fw_module.graph.flatten_inps(fw_args))
if not isinstance(fw_outs, list):
fw_outs = [fw_outs]
ctx.activations = fw_outs[1:]
","out = fn(*args)
with torch.enable_grad():
fx_g = make_fx(vjpfull)(args, (torch.randn_like(out),))
fw_module, bw_module = partition_backwards(fx_g)
                compiled_fw = fw_compiler(fw_module, args)
                fw_outs = compiled_fw(*fw_module.graph.flatten_inps(args))
if not isinstance(fw_outs, list):
fw_outs = [fw_outs]
bw_args = fw_outs[1:] + [torch.ones_like(fw_outs[0])]
compiled_bw = bw_compiler(bw_module, bw_args)

            fw_outs = compiled_fw(*fw_module.graph.flatten_inps(args))
if not isinstance(fw_outs, list):
fw_outs = [fw_outs]
ctx.activations = fw_outs[1:]
"
119,"def unwrap_tensor(e):
return e.elem if isinstance(e, PythonTensor) else e
        aten_func = getattr(torch.ops.aten, func.__name__)
proxy_args = pytree.tree_map(unwrap_proxy, args)
proxy_kwargs = pytree.tree_map(unwrap_proxy, kwargs)
        proxy_out = aten_func(*proxy_args, **proxy_kwargs)
        real_out = aten_func(*pytree.tree_map(unwrap_tensor, args), **pytree.tree_map(unwrap_tensor, kwargs))
def wrap_with_proxy(e, idx):
return PythonTensor(e, proxy_out[idx]) if type(e) == torch.Tensor else e
","def unwrap_tensor(e):
return e.elem if isinstance(e, PythonTensor) else e
proxy_args = pytree.tree_map(unwrap_proxy, args)
proxy_kwargs = pytree.tree_map(unwrap_proxy, kwargs)
        proxy_out = func(*proxy_args, **proxy_kwargs)
        real_out = func(*pytree.tree_map(unwrap_tensor, args), **pytree.tree_map(unwrap_tensor, kwargs))
def wrap_with_proxy(e, idx):
return PythonTensor(e, proxy_out[idx]) if type(e) == torch.Tensor else e
"
120,"with torch.enable_grad():
fx_g = make_fx(vjpfull)(fn, args, (torch.ones_like(out),))
fw_module, bw_module = partition_backwards(fx_g)
garbage_hack = torch.randn(())
fw_args = (garbage_hack,) + args
","with torch.enable_grad():
fx_g = make_fx(vjpfull)(fn, args, (torch.ones_like(out),))
fw_module, bw_module = partition_backwards(fx_g)
                print(fw_module.code, bw_module.code)
garbage_hack = torch.randn(())
fw_args = (garbage_hack,) + args
"
121,"// However, registering e.g. CppFunction::makeNamedNotSupported() as an implementation
// only works for operators that support boxing.
#define TENSOROPTIONS c10::optional<c10::ScalarType>, c10::optional<c10::Layout>, c10::optional<c10::Device>, c10::optional<bool>

// random operations (out-of-place)
  m.impl(""bernoulli"", unsupportedRandomOp<const Tensor&, optional<Generator>>);
  m.impl(""bernoulli.out"", unsupportedRandomOp_<const Tensor&, optional<Generator>, Tensor&>);
  m.impl(""bernoulli.p"", unsupportedRandomOp<const Tensor&, double, optional<Generator>>);
  m.impl(""bernoulli_.Tensor"", unsupportedRandomOp_<Tensor&, const Tensor&, optional<Generator>>);
m.impl(""bernoulli_.float"", unsupportedRandomOp_<Tensor&, double, optional<Generator>>);
m.impl(""cauchy_"", unsupportedRandomOp_<Tensor&, double, double, optional<Generator>>);
","// However, registering e.g. CppFunction::makeNamedNotSupported() as an implementation
// only works for operators that support boxing.
#define TENSOROPTIONS c10::optional<c10::ScalarType>, c10::optional<c10::Layout>, c10::optional<c10::Device>, c10::optional<bool>
// random operations (out-of-place)
  UNSUPPORTED_RANDOM(bernoulli);
  UNSUPPORTED_RANDOM2(bernoulli, out);
  UNSUPPORTED_RANDOM2(bernoulli, p);
  UNSUPPORTED_RANDOM2(bernoulli_, Tensor);
m.impl(""bernoulli_.float"", unsupportedRandomOp_<Tensor&, double, optional<Generator>>);
m.impl(""cauchy_"", unsupportedRandomOp_<Tensor&, double, double, optional<Generator>>);
"
122,"if (!self_bdim.has_value()) {
return std::make_tuple( Func(self, dims, std::forward<ExtraArgs>(extra_args)...), nullopt );
}
  auto self_dim = self.dim();
// If the dim intlist is empty, that's equivalent to passing in a dim on all dimensions.
if (dims.size() == 0) {
    dims = range(0, self_dim);
}
  if (self_dim == 1 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return std::make_tuple( self.clone(), 0 );
}
auto self_ = moveBatchDimToFront(self, self_bdim);
","if (!self_bdim.has_value()) {
return std::make_tuple( Func(self, dims, std::forward<ExtraArgs>(extra_args)...), nullopt );
}
  auto logical_dim = rankWithoutBatchDim(self, self_bdim);
// If the dim intlist is empty, that's equivalent to passing in a dim on all dimensions.
if (dims.size() == 0) {
    dims = range(0, std::max((int64_t)1, logical_dim));
}
  if (logical_dim == 0 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return std::make_tuple( self.clone(), 0 );
}
auto self_ = moveBatchDimToFront(self, self_bdim);
"
123,"return dims;
}
std::tuple<Tensor,optional<int64_t>> sum_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim, optional<ScalarType> dtype) {
if (!self_bdim.has_value()) {
    return { self.sum(dtype), nullopt };
}
auto self_dim = self.dim();
  if (self_dim == 1) {
return { self.clone(), 0 };
}
auto self_ = moveBatchDimToFront(self, self_bdim);
  auto dims = range(1, self_dim);
  auto result = at::sum(self_, dims, /*keepdim*/false, dtype);
return { result, 0 };
}
bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
  return dim == 0 || dim == -1;
}
std::tuple<Tensor,optional<int64_t>> sum_dim_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, bool keepdim, optional<ScalarType> dtype) {
if (!self_bdim.has_value()) {
    return { at::sum(self, dims, keepdim, dtype), nullopt };
}
auto self_dim = self.dim();
if (self_dim == 1 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return { self.clone(), 0 };
}
","return dims;
}

bool is_allowed_dim_on_scalar_tensor(int64_t dim) {
  return dim == 0 || dim == -1;
}


// Not used right now, since ATEN_OPS isn't landed. Kinda annoying.
// template <typename F, F Func, typename... ExtraArgs>
// std::tuple<Tensor,optional<int64_t>> reduction_dim_batch_rule(
//     const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, ExtraArgs... extra_args) {
//   if (!self_bdim.has_value()) {
//     return { Func(self, dims, std::forward<ExtraArgs>(extra_args)...), nullopt };
//   }
//   auto self_dim = self.dim();

//   if (dims.size() == 0) {
//     dims = range(0, self_dim);
//   }

//   if (self_dim == 1 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
//     return { self.clone(), 0 };
//   }
//   auto self_ = moveBatchDimToFront(self, self_bdim);
//   VmapDimVector new_dims;
//   new_dims.reserve(dims.size());
//   for (auto dim: dims) {
//     new_dims.push_back(getPhysicalDim(self_, self_bdim.has_value(), dim));
//   }
//   auto result = Func(self_, new_dims, std::forward<ExtraArgs>(extra_args)...);
//   return { result, 0 };
// }

std::tuple<Tensor,optional<int64_t>> sum_dim_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, bool keepdim, optional<ScalarType> dtype) {
if (!self_bdim.has_value()) {
    return { at::sum(self, dims, keepdim, dtype), nullopt };
}
auto self_dim = self.dim();

  if (dims.size() == 0) {
    dims = range(0, self_dim);
  }

  if (self_dim == 1 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return { self.clone(), 0 };
}
auto self_ = moveBatchDimToFront(self, self_bdim);
  VmapDimVector new_dims;
  new_dims.reserve(dims.size());
  for (auto dim: dims) {
    new_dims.push_back(getPhysicalDim(self_, self_bdim.has_value(), dim));
  }
  auto result = at::sum(self_, new_dims, keepdim, dtype);
return { result, 0 };
}
std::tuple<Tensor,optional<int64_t>> sum_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim, optional<ScalarType> dtype) {
  return sum_dim_batch_rule(self, self_bdim, range(0, self.dim() - 1), false, dtype);
}
std::tuple<Tensor,optional<int64_t>> var_dim_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, bool unbiased, bool keepdim) {
if (!self_bdim.has_value()) {
    return { at::var(self, dims, unbiased, keepdim), nullopt };
}
auto self_dim = self.dim();
  if (dims.size() == 0) {
    dims = range(0, self_dim);
  }
if (self_dim == 1 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return { self.clone(), 0 };
}
"
124,"primals = _wrap_all_tensors(primals, level)
diff_primals = _create_differentiable(primals, level)
primals_out = f(*diff_primals)
        results = _undo_create_differentiable(primals_out, level)
flat_diff_primals, primals_spec = tree_flatten(diff_primals)
flat_primals_out, primals_out_spec = tree_flatten(primals_out)
","primals = _wrap_all_tensors(primals, level)
diff_primals = _create_differentiable(primals, level)
primals_out = f(*diff_primals)
        results = _undo_create_differentiable(primals_out, level)
flat_diff_primals, primals_spec = tree_flatten(diff_primals)
flat_primals_out, primals_out_spec = tree_flatten(primals_out)
"
125,"// 2. Unwrap all the args in the copy set
// 3. Call the operator
// 4. Wrap the output
  // 5. (!) refreshSizesAndStrides for all the args in the original set
// 6. (!) Pop those args off.
// Step 1 & 2
","// 2. Unwrap all the args in the copy set
// 3. Call the operator
// 4. Wrap the output
  // 5. (!) refreshMetadata for all the args in the original set
// 6. (!) Pop those args off.
// Step 1 & 2
"
126,"// TODO: need to reset sizes/strides on mutation
TORCH_INTERNAL_ASSERT(use_value_sizes_strides);
  refreshSizesAndStrides();
}
// The following are some internal inherited methods that we do not support.
","// TODO: need to reset sizes/strides on mutation
TORCH_INTERNAL_ASSERT(use_value_sizes_strides);
  refreshMetadata();
}
// The following are some internal inherited methods that we do not support.
"
127,"def unwrap_tensors(x):
if isinstance(x, torch.Tensor):
return _unwrap_for_grad(x, level)
        raise AssertionError()
return tree_map(unwrap_tensors, inps)
","def unwrap_tensors(x):
if isinstance(x, torch.Tensor):
return _unwrap_for_grad(x, level)
        # TODO: Remove the following hack for namedtuples
        if isinstance(x, tuple):
            return tree_map(unwrap_tensors, tuple(x))

        raise RuntimeError(""Expected tensors, got unsupported type {type(x)}"")
return tree_map(unwrap_tensors, inps)
"
128,"if (tensor.is_cuda()) {
key_set = key_set.add(DispatchKey::CUDA);
}
return at::detail::make_tensor<BatchedTensorImpl>(key_set, tensor, std::move(bdims));
}
","if (tensor.is_cuda()) {
key_set = key_set.add(DispatchKey::CUDA);
}
  auto* batched = maybeGetBatchedImpl(tensor);
  if (batched) {
    auto requested_level = bdims.back().level();
    auto batched_level = batched->bdims().back().level();
    TORCH_INTERNAL_ASSERT(requested_level > batched_level);
  }
return at::detail::make_tensor<BatchedTensorImpl>(key_set, tensor, std::move(bdims));
}
"
129,"// ""reset exclude set""
// TODO: Still a problem with composabiilty and AutoNonVariableTypeGuard.
// Users cannot do torch.no_grad otherwise there will be problems.
auto keyset = c10::impl::PODLocalDispatchKeySet();
c10::impl::_force_tls_local_dispatch_key_set(keyset);
c10::impl::tls_set_dispatch_key_included(DispatchKey::DynamicLayerFront, true);
","// ""reset exclude set""
// TODO: Still a problem with composabiilty and AutoNonVariableTypeGuard.
// Users cannot do torch.no_grad otherwise there will be problems.
  SaveLocalDispatchKeySet save_guard;
auto keyset = c10::impl::PODLocalDispatchKeySet();
c10::impl::_force_tls_local_dispatch_key_set(keyset);
c10::impl::tls_set_dispatch_key_included(DispatchKey::DynamicLayerFront, true);
"
130,"auto self_sizes = self.sizes();
VmapDimVector expanded_sizes(self_sizes.begin(), self_sizes.end());
expanded_sizes.insert(expanded_sizes.begin() + out_dim, batch_size);
    return self.expand(expanded_sizes);
}
// Must be batched if has_level(self, /*any_level*/)
","auto self_sizes = self.sizes();
VmapDimVector expanded_sizes(self_sizes.begin(), self_sizes.end());
expanded_sizes.insert(expanded_sizes.begin() + out_dim, batch_size);
    auto result = self.expand(expanded_sizes);
    return result;
}
// Must be batched if has_level(self, /*any_level*/)
"
131,"Tensor self_without_bdim;
int64_t newly_exposed_logical_dim;
std::tie(self_without_bdim, newly_exposed_logical_dim) = remove_existing_batch_dim(batched, level);
  return _movedim(self_without_bdim, newly_exposed_logical_dim, out_dim);
}
Tensor _wrap_for_grad(const Tensor& self, int64_t level) {
","Tensor self_without_bdim;
int64_t newly_exposed_logical_dim;
std::tie(self_without_bdim, newly_exposed_logical_dim) = remove_existing_batch_dim(batched, level);
  auto result = _movedim(self_without_bdim, newly_exposed_logical_dim, out_dim);
  return result;
}
Tensor _wrap_for_grad(const Tensor& self, int64_t level) {
"
132,"from collections import defaultdict
import sys
import traceback
import copyreg
def _type(self, dtype=None, non_blocking=False, **kwargs):
","from collections import defaultdict
import sys
import traceback

def _type(self, dtype=None, non_blocking=False, **kwargs):
"
133,"tensor._backward_hooks = backward_hooks
return tensor
# Should not be used, this is kept only for BC of loading old serialized Parameter
def _rebuild_parameter(data, requires_grad, backward_hooks):
param = torch.nn.Parameter(data, requires_grad)
# NB: This line exists only for backwards compatibility; the
","tensor._backward_hooks = backward_hooks
return tensor
def _rebuild_parameter(data, requires_grad, backward_hooks):
param = torch.nn.Parameter(data, requires_grad)
# NB: This line exists only for backwards compatibility; the
"
134,"if getattr(ret.__class__, ""__setstate__"", Tensor.__setstate__) is not Tensor.__setstate__:
ret.__setstate__(state)
else:
        if isinstance(state, tuple):
            if not len(state) == 2:
                raise RuntimeError(f""Invalid serialized state: {state}"")
            dict_state = state[0]
            slots_state = state[1]
        else:
            dict_state = state
            slots_state = None

        for k, v in dict_state.items():
            setattr(ret, k, v)

        if slots_state:
            for k, v in slots_state.items():
                setattr(ret, k, v)
return ret
","if getattr(ret.__class__, ""__setstate__"", Tensor.__setstate__) is not Tensor.__setstate__:
ret.__setstate__(state)
else:
        ret = torch._utils._set_obj_state(ret, state)
return ret
"
135,"return true;
}
#ifdef CAFFE2_USE_MKLDNN
REGISTER_IDEEP_OPERATOR(
BatchPermutation,
IDEEPFallbackOp<BatchPermutationOp<float, CPUContext>>);
","return true;
}
#ifdef USE_MKLDNN
REGISTER_IDEEP_OPERATOR(
BatchPermutation,
IDEEPFallbackOp<BatchPermutationOp<float, CPUContext>>);
"
136,"#include ""caffe2/sgd/iter_op.h""
#ifdef CAFFE2_USE_MKLDNN
#include <caffe2/ideep/operators/operator_fallback_ideep.h>
#include <caffe2/ideep/utils/ideep_operator.h>
#endif
","#include ""caffe2/sgd/iter_op.h""
#ifdef USE_MKLDNN
#include <caffe2/ideep/operators/operator_fallback_ideep.h>
#include <caffe2/ideep/utils/ideep_operator.h>
#endif
"
137,"op_registration_allowlist = None
selector = get_custom_build_selector(
        options.op_registration_allowlist,
options.op_selection_yaml_path,
)
","op_registration_allowlist = None
selector = get_custom_build_selector(
        op_registration_allowlist,
options.op_selection_yaml_path,
)
"
138,"from typing import Any, Dict, List, Optional, Set, Callable, Tuple, Union
import torch
import copy
import warnings
","from typing import Any, Dict, List, Optional, Set, Tuple, Union, Type
from torch.ao.quantization.quant_type import QuantType
import torch
import copy
import warnings
"
139,"from collections import namedtuple
from typing import Optional, Any
import torch
import torch.nn as nn
","from collections import namedtuple
from typing import Optional, Any, Union
import torch
import torch.nn as nn
"
140,"_apply_func_submodules(
_create_swap_params(parameters_and_buffers),
module, name.split("".""), name, (tensor,))
    yield
    for name in parameters_and_buffers:
        _apply_func_submodules(
            _remove_swap,
            module, name.split("".""), name, ())
def _apply_func_submodules(
","_apply_func_submodules(
_create_swap_params(parameters_and_buffers),
module, name.split("".""), name, (tensor,))
    try:
        yield
    finally:
        for name in parameters_and_buffers:
            _apply_func_submodules(
                _remove_swap,
                module, name.split("".""), name, ())
def _apply_func_submodules(
"
141,"// else returns a new negated tensor with neg bit set to 0
Tensor resolve_neg(const Tensor& self) {
if (!self.is_neg()) { return self; }
  // currently a tensor should never have both conj and neg bit set
  // the only way to get an imag bit is complex_tensor.conj().imag but there's
  // no intended designed mechanism to enter the complex world with this imag bit
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!self.is_conj());
// negation is materialized in `copy_()` that clone ultimately calls into
return self.clone();
}
","// else returns a new negated tensor with neg bit set to 0
Tensor resolve_neg(const Tensor& self) {
if (!self.is_neg()) { return self; }
// negation is materialized in `copy_()` that clone ultimately calls into
return self.clone();
}
"
142,"import torch.nn.functional as F
from torch.autograd import Variable
from torch.distributed import ProcessGroup
from torch.distributed._shard.sharded_tensor import (
Shard,
ShardedTensor,
","import torch.nn.functional as F
from torch.autograd import Variable
from torch.distributed import ProcessGroup
from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import _CHECKPOINT_PREFIX
from torch.distributed._shard.sharded_tensor import (
Shard,
ShardedTensor,
"
143,"import torch
from .fake_quantize import default_weight_fake_quant
from .observer import default_weight_observer
from .qconfig import (
default_reuse_input_qconfig,
get_default_qconfig,
","import torch
from .fake_quantize import default_weight_fake_quant
from .observer import (
    _PartialWrapper,
    default_fixed_qparams_range_0to1_observer,
    default_fixed_qparams_range_neg1to1_observer,
    default_weight_observer,
)
from .qconfig import (
default_reuse_input_qconfig,
get_default_qconfig,
"
144,"grad_fn = var.grad_fn
if grad_fn is not None:
for hook in non_full_backward_hooks:
                    wrapper = functools.partial(hook, self)
                    functools.update_wrapper(wrapper, hook)
                    grad_fn.register_hook(wrapper)
self._maybe_warn_non_full_backward_hook(input, result, grad_fn)
return result
","grad_fn = var.grad_fn
if grad_fn is not None:
for hook in non_full_backward_hooks:
                    grad_fn.register_hook(_WrappedHook(hook, self))
self._maybe_warn_non_full_backward_hook(input, result, grad_fn)
return result
"
145,"#if !defined(__powerpc__) && !defined(__s390x__)
if (cpuinfo_initialize()) {
#ifdef HAVE_AVX512_CPU_DEFINITION
// GCC supports some AVX512 intrinsics such as _mm512_set_epi16 only in
// versions 9 & beyond. So, we want to ensure that only releases built with
// supported compilers on supported hardware return CPU Capability AVX512,
","#if !defined(__powerpc__) && !defined(__s390x__)
if (cpuinfo_initialize()) {
    // AVX512 can be slower then AVX2, so lets keep it as opt-in
    // see https://github.com/pytorch/pytorch/issues/80252
#if defined(HAVE_AVX512_CPU_DEFINITION) && false
// GCC supports some AVX512 intrinsics such as _mm512_set_epi16 only in
// versions 9 & beyond. So, we want to ensure that only releases built with
// supported compilers on supported hardware return CPU Capability AVX512,
"
146,"}
void MemoryPlanner::deallocate() {
  deallocateManagedTensors();
for (auto& iv : borrowed_ivalues_needing_incref_) {
auto old = std::move(*iv);
*iv = IValue(old);
","}
void MemoryPlanner::deallocate() {
for (auto& iv : borrowed_ivalues_needing_incref_) {
auto old = std::move(*iv);
*iv = IValue(old);
"
147,"# to filter those out
and torch._C._dispatch_has_kernel(name)
and not torch._C._dispatch_has_kernel_for_dispatch_key(name, 'Meta')
):
meta_lib.impl(op_overload, f)
","# to filter those out
and torch._C._dispatch_has_kernel(name)
and not torch._C._dispatch_has_kernel_for_dispatch_key(name, 'Meta')
                    # Don't register a meta kernel to any operator that has
                    # a CompositeImplicitAutograd kernel in core.
                    # Otherwise we won't be able to run autograd for that operator with the meta backend.
                    and 'CompositeImplicitAutograd' not in torch._C._dispatch_dump(name)
):
meta_lib.impl(op_overload, f)
"
148,"# All-gather full parameters, moving them to compute device if
# necessary.
self._rebuild_full_params()
# Wait for all_gather to finish before computation
torch.cuda.current_stream().wait_stream(self._streams[""all_gather""])
","# All-gather full parameters, moving them to compute device if
# necessary.
self._rebuild_full_params()
                self._pre_backward_hook_full_params_prefetched = False
# Wait for all_gather to finish before computation
torch.cuda.current_stream().wait_stream(self._streams[""all_gather""])
"
149,"#include <ATen/TensorIndexing.h>
#include <ATen/native/TypeProperties.h>
#include <c10/core/QScheme.h>
namespace at {
namespace meta {
","#include <ATen/TensorIndexing.h>
#include <ATen/native/TypeProperties.h>
#include <c10/core/QScheme.h>
#include <ATen/TensorSubclassLikeUtils.h>
namespace at {
namespace meta {
"
150,"paths.append(p)
if not found_one:
        raise RuntimeError(
            ""Didn't find any test reports in s3, there is probably a bug!""
)
return paths
","paths.append(p)
if not found_one:
        print(
            ""::warning title=s3 artifacts not found::""
            ""Didn't find any test reports in s3, there might be a bug!""
)
return paths
"
151,"import argparse
import os
import pathlib
from dataclasses import dataclass
from torchgen.api import cpp
from torchgen.api import unboxing
from torchgen.api.translate import translate
","import argparse
import os
import pathlib
import sys
from dataclasses import dataclass
from typing import Union, Sequence, List

import yaml
from typing_extensions import Literal

from torchgen.api import cpp
from torchgen.api import unboxing
from torchgen.api.translate import translate
"
152,"const auto func_name =
construct_unique_module_name(module_class_name.substr(pos + 1));
  auto func_def_n =
      CreateFunctionDefNode(func_ctx, graph, domain_name, func_name);
// create and insert local function node to graph.
for (const auto& it : func_ctx.scope_ctxs_) {
","const auto func_name =
construct_unique_module_name(module_class_name.substr(pos + 1));
  CreateFunctionDefNode(func_ctx, graph, domain_name, func_name);
// create and insert local function node to graph.
for (const auto& it : func_ctx.scope_ctxs_) {
"
153,""". Please rename the input tensors with `Tensor.rename` to prevent this."");
}
// NOLINTNEXTLINE(clang-diagnostic-unused-function)
static DimnameList batch_dims(DimnameList names) {
  if (names.size() <= 2) {
    return {};
  }
  return DimnameList(names.begin(), names.end() - 2);
}

// NOLINTNEXTLINE(clang-diagnostic-unused-function)
static DimnameList feature_dims(DimnameList names) {
  if (names.size() <= 2) {
    return names;
  }
  return DimnameList(names.end() - 2, 2);
}

// NOLINTNEXTLINE(clang-diagnostic-unused-function)
static bool are_distinct(DimnameList batch_dims, DimnameList feature_dims) {
  for (const auto& target : feature_dims) {
    if (target.isWildcard()) {
      continue;
    }
    if (std::any_of(batch_dims.begin(), batch_dims.end(),
          [&](const Dimname& dim) { return target == dim; })) {
      return false;
    }
  }
  return true;
}

static int64_t num_batch_dims(DimnameList names) {
if (names.size() <= 2) {
return 0;
",""". Please rename the input tensors with `Tensor.rename` to prevent this."");
}
static int64_t num_batch_dims(DimnameList names) {
if (names.size() <= 2) {
return 0;
"
154,"return;
}
void checkCPUTensor(const std::string& fn_name, const Tensor& t) {
  TORCH_CHECK(
      t.device().type() == kCPU, fn_name, "" only supports CPU device type."");
}

void checkFloatTensor(const std::string& fn_name, const Tensor& t) {
TORCH_CHECK(
t.scalar_type() == kFloat, fn_name, "" expects a Float Tensor, got "",
","return;
}
void checkFloatTensor(const std::string& fn_name, const Tensor& t) {
TORCH_CHECK(
t.scalar_type() == kFloat, fn_name, "" expects a Float Tensor, got "",
"
155,"c10::QualifiedName exportName;
};
std::vector<ModuleMethod> getModuleInterfaceExports(
    const Module& module,
    const std::unordered_set<const FunctionSchema*>& schemas) {
  if (schemas.size() == 0) {
    return {};
  }
  std::unordered_set<std::string> names;
  for (auto schema : schemas) {
    names.insert(schema->name());
  }
  std::vector<ModuleMethod> ret;
  for (const auto& submodule : module.modules()) {
    for (const auto& method : submodule.get_methods()) {
      const auto& f = toGraphFunction(method.function());
      if (names.find(f.qualname().name()) != names.end()) {
        ret.emplace_back(submodule, f, f.qualname());
      }
    }
  }
  return ret;
}

bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
","c10::QualifiedName exportName;
};
bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
"
156,"}
}
#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
std::string getTopModuleTypeName(const Module& m) {
std::string name;
if (m._ivalue()->type() && m._ivalue()->type()->name()) {
","}
}
std::string getTopModuleTypeName(const Module& m) {
std::string name;
if (m._ivalue()->type() && m._ivalue()->type()->name()) {
"
157,"}
}
struct ModuleMethod {
ModuleMethod(const Module& m, const GraphFunction& f, c10::QualifiedName n)
: module(m), function(f), exportName(std::move(n)) {}
","}
}
std::unordered_set<const FunctionSchema*> getInterfaceCalls(Graph& graph) {
  std::unordered_set<const FunctionSchema*> ret;
  auto nodes = findAllNodes(graph, c10::prim::CallMethod, true);
  for (Node* node : nodes) {
    if (auto iface = node->input(0)->type()->castRaw<InterfaceType>()) {
      ret.insert(iface->getMethod(node->s(attr::name)));
    }
  }
  return ret;
}

struct ModuleMethod {
ModuleMethod(const Module& m, const GraphFunction& f, c10::QualifiedName n)
: module(m), function(f), exportName(std::move(n)) {}
"
158,"// options.
// TODO: Refactor this so we just pass everything in via options
Tensor new_with_sizes(c10::TensorOptions options, at::ScalarType scalar_type, const optional<Device>& device, IntArrayRef sizes) {
maybe_initialize_cuda(options.device());
pybind11::gil_scoped_release no_gil;
","// options.
// TODO: Refactor this so we just pass everything in via options
Tensor dispatch_ones(c10::TensorOptions options, at::ScalarType scalar_type, const optional<Device>& device, IntArrayRef sizes) {
  maybe_initialize_cuda(options.device());
  pybind11::gil_scoped_release no_gil;
  return torch::ones(sizes, build_options(options, scalar_type, device));
}

Tensor new_with_sizes(c10::TensorOptions options, at::ScalarType scalar_type, const optional<Device>& device, IntArrayRef sizes) {
maybe_initialize_cuda(options.device());
pybind11::gil_scoped_release no_gil;
"
159,"namespace {
// NOLINTNEXTLINE(clang-diagnostic-unused-function)
void adam_ideep_update(
    int N,
    const float* g,
    const float* m,
    const float* v,
    float* ng,
    float* nm,
    float* nv,
    float beta1,
    float beta2,
    float eps_hat,
    float correction,
    const float* lr) {
#ifdef _OPENMP
    #pragma omp parallel for schedule(static)
#endif
  for (auto i = 0; i < N; ++i) {
    float gi = g[i];
    float mi = nm[i] = m[i] * beta1 + gi * (1 - beta1);
    float vi = nv[i] = v[i] * beta2 + gi * gi * (1 - beta2);
    ng[i] = lr[0] * correction * mi / (std::sqrt(vi) + eps_hat);
  }
}

void adam_ideep_compute(
int N,
const float* w,
","namespace {
void adam_ideep_compute(
int N,
const float* w,
"
160,"return;
}
  CAFFE_ENFORCE(op_id >= 0 && op_id < op_end_times_run_.size());
op_end_times_run_[op_id] = timer_.MilliSeconds();
}
","return;
}
  CAFFE_ENFORCE(op_id < op_end_times_run_.size());
op_end_times_run_[op_id] = timer_.MilliSeconds();
}
"
161,"const std::vector<IterDomain*>& new_root_domain,
std::vector<IterDomain*>& rfactor_domain) override {
TORCH_INTERNAL_ASSERT(
        index_ >= 0 && (index_ + 1) < new_root_domain.size(),
""Index: \t"",
index_,
""\t Domain Size:\t"",
","const std::vector<IterDomain*>& new_root_domain,
std::vector<IterDomain*>& rfactor_domain) override {
TORCH_INTERNAL_ASSERT(
        (index_ + 1) < new_root_domain.size(),
""Index: \t"",
index_,
""\t Domain Size:\t"",
"
162,"from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
    parser.add_argument(""--on-mandatory"", action=""store_true"")
parser.add_argument(""--on-green"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
","from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
parser.add_argument(""--on-green"", action=""store_true"")
    parser.add_argument(""--on-mandatory"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
"
163,"return f""https://github.com/{suffix_str}""
def merge_on_green(pr_num: int, repo: GitRepo,
                   dry_run: bool = False,
                   mandatory_only: bool = False,
                   timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
start_time = time.time()
last_exception = ''
elapsed_time = 0.0
while elapsed_time < timeout_minutes * 60:
current_time = time.time()
elapsed_time = current_time - start_time

print(f""Attempting merge of https://github.com/{org}/{project}/pull/{pr_num} ({elapsed_time / 60} minutes elapsed)"")
pr = GitHubPR(org, project, pr_num)
try:
find_matching_merge_rule(pr, repo)
pending = pr_get_pending_checks(pr)
failing = pr_get_failed_checks(pr)
            if not mandatory_only and len(failing) > 0:
raise RuntimeError(f""{len(failing)} additional jobs have failed, first few of them are: "" +
' ,'.join(f""[{x[0]}]({x[1]})"" for x in failing[:5]))
            if not mandatory_only and len(pending) > 0:
raise MandatoryChecksMissingError(f""Still waiting for {len(pending)} additional jobs to finish, "" +
f""first few of them are: {' ,'.join(x[0] for x in pending[:5])}"")
            return pr.merge_into(repo, dry_run=dry_run)
except MandatoryChecksMissingError as ex:
last_exception = str(ex)
print(f""Merge of https://github.com/{org}/{project}/pull/{pr_num} failed due to: {ex}. Retrying in 5 min"")
","return f""https://github.com/{suffix_str}""
def merge(pr_num: int, repo: GitRepo,
          dry_run: bool = False,
          force: bool = False,
          comment_id: Optional[int] = None,
          mandatory_only: bool = False,
          on_green: bool = False,
          timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
    if force:
        pr = GitHubPR(org, project, pr_num)
        pr.merge_into(repo, dry_run=dry_run, force=force, comment_id=comment_id)

start_time = time.time()
last_exception = ''
elapsed_time = 0.0
while elapsed_time < timeout_minutes * 60:
current_time = time.time()
elapsed_time = current_time - start_time
print(f""Attempting merge of https://github.com/{org}/{project}/pull/{pr_num} ({elapsed_time / 60} minutes elapsed)"")
pr = GitHubPR(org, project, pr_num)
try:
find_matching_merge_rule(pr, repo)
pending = pr_get_pending_checks(pr)
failing = pr_get_failed_checks(pr)
            if (not mandatory_only and on_green) and len(failing) > 0:
raise RuntimeError(f""{len(failing)} additional jobs have failed, first few of them are: "" +
' ,'.join(f""[{x[0]}]({x[1]})"" for x in failing[:5]))
            if (not mandatory_only and on_green) and len(pending) > 0:
raise MandatoryChecksMissingError(f""Still waiting for {len(pending)} additional jobs to finish, "" +
f""first few of them are: {' ,'.join(x[0] for x in pending[:5])}"")
            return pr.merge_into(repo, dry_run=dry_run, force=force, comment_id=comment_id)
except MandatoryChecksMissingError as ex:
last_exception = str(ex)
print(f""Merge of https://github.com/{org}/{project}/pull/{pr_num} failed due to: {ex}. Retrying in 5 min"")
"
164,"}
}
// Compute derivatives for targets.
Tensor kl_div_target_backward(Tensor grad_output, Tensor self, Tensor target, int64_t reduction, bool log_target) {
Tensor grad_target;
","}
}
Tensor kl_div_double_backward_grad_output(const Tensor & grad, const Tensor & input, const Tensor & target, int64_t reduction, bool log_target) {
  auto result = kl_div_backward(grad, input, target, at::Reduction::None, log_target);
  if (reduction == at::Reduction::Mean) {
    return result.mean();
  } else if (reduction == at::Reduction::Sum) {
    return result.sum();
  }
  return result;
}

// Compute derivatives for targets.
Tensor kl_div_target_backward(Tensor grad_output, Tensor self, Tensor target, int64_t reduction, bool log_target) {
Tensor grad_target;
"
165,"f""target {node_a.target} is not implemented""
if node_a.target == 'dequantize':
arg_copy = _copy_node_from_a_to_c(
                _get_normalized_nth_input(node_a, gm_a, 0),
gm_a, gm_b, graph_c)  # type: ignore[arg-type]
node_a_copy_name = \
get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
","f""target {node_a.target} is not implemented""
if node_a.target == 'dequantize':
arg_copy = _copy_node_from_a_to_c(
                get_normalized_nth_input(node_a, gm_a, 0),
gm_a, gm_b, graph_c)  # type: ignore[arg-type]
node_a_copy_name = \
get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
"
166,"def meta_dot(self, tensor):
check(
self.dim() == 1 and tensor.dim() == 1,
        f""1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors""
)
return self.new_empty(())
","def meta_dot(self, tensor):
check(
self.dim() == 1 and tensor.dim() == 1,
        lambda: f""1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors""
)
return self.new_empty(())
"
167,"_idx = idx
if _idx < 0 or _idx > _rank:
        msg = ""Received out of bounds index {0} for tensor of rank {1}!"".format(
            idx, rank
)
        raise ValueError(msg)
return _idx
","_idx = idx
if _idx < 0 or _idx > _rank:
        # Same error message as in aten/src/ATen/WrapDimUtils.h:49
        msg = ""Dimension out of range (expected to be in range of [{0}, {1}], but got {2})"".format(
            -rank, rank - 1, idx
)
        raise IndexError(msg)
return _idx
"
168,"@classmethod
def _free_weak_ref(cls, *args, **kwargs):
        return eval(cls.__module__)._UntypedStorage._free_weak_ref(*args, **kwargs)
def _weak_ref(self, *args, **kwargs):
return self._storage._weak_ref(*args, **kwargs)
","@classmethod
def _free_weak_ref(cls, *args, **kwargs):
        return _UntypedStorage._free_weak_ref(*args, **kwargs)
def _weak_ref(self, *args, **kwargs):
return self._storage._weak_ref(*args, **kwargs)
"
169,""""""")
add_docstr_all('as_strided', r""""""
as_strided(size, stride, storage_offset=0) -> Tensor
See :func:`torch.as_strided`
"""""")
",""""""")
add_docstr_all('as_strided', r""""""
as_strided(size, stride, storage_offset=None) -> Tensor
See :func:`torch.as_strided`
"""""")
"
170,"{input}
size (tuple or ints): the shape of the output tensor
stride (tuple or ints): the stride of the output tensor
    storage_offset (int, optional): the offset in the underlying storage of the output tensor
Example::
","{input}
size (tuple or ints): the shape of the output tensor
stride (tuple or ints): the stride of the output tensor
    storage_offset (int, optional): the offset in the underlying storage of the output tensor.
    If ``None``, the storage_offset of the output tensor will match the input tensor.
Example::
"
171,"const Tensor& info,
const c10::string_view api_name,
bool is_matrix) {
if (is_matrix) {
singleCheckErrors(info.item<int64_t>(), api_name);
} else {
","const Tensor& info,
const c10::string_view api_name,
bool is_matrix) {
  if (info.is_meta()) {
    return;
  }
if (is_matrix) {
singleCheckErrors(info.item<int64_t>(), api_name);
} else {
"
172,"prod = _make_reduction_prim(
name=""prod"",
impl_aten=torch.prod,
    doc=_sum_doc,
)
var = _make_var_reduction_prim(
","prod = _make_reduction_prim(
name=""prod"",
impl_aten=torch.prod,
    doc=_prod_doc,
)
var = _make_var_reduction_prim(
"
173,"}
Tensor& _logcumsumexp_out_cuda(const Tensor& self, int64_t dim, Tensor& result) {
result.resize_(self.sizes());
if (self.dim() == 0) {
result.fill_(self);
","}
Tensor& _logcumsumexp_out_cuda(const Tensor& self, int64_t dim, Tensor& result) {
  const auto wrap_dim = maybe_wrap_dim(dim, self.dim());
result.resize_(self.sizes());
if (self.dim() == 0) {
result.fill_(self);
"
174,"auto index_contig = index.contiguous();
if (self.dim() > 1) {
    if (numel == 0 || self.numel() == 0) {
return result;
}
","auto index_contig = index.contiguous();
if (self.dim() > 1) {
    if (numel == 0) {
      return result;
    }
    if (self.numel() == 0) {
      auto src_indexing_axis_dim = self.size(dim);
      TORCH_CHECK(src_indexing_axis_dim > 0,
                  ""index_select(): self indexing axis dim should be positive"");
      AT_DISPATCH_INDEX_TYPES(
      index_contig.scalar_type(), ""index_select_empty_self_bound_check"", [&]() {
        const auto* idxs = index_contig.data_ptr<index_t>();
        check_indexarray_range<index_t>(idxs, numel, src_indexing_axis_dim);
      });
return result;
}
"
175,"if len(approvers_intersection) == 0 and len(rule_approvers_set) > 0:
if reject_reason_score < 10000:
reject_reason_score = 10000
                reject_reason = (f""Matched rule {rule_name}, but it was not reviewed yet by any of:"" +
f""{','.join(list(rule_approvers_set)[:5])}{', ...' if len(rule_approvers_set) > 5 else ''}"")
continue
if rule.mandatory_checks_name is not None:
","if len(approvers_intersection) == 0 and len(rule_approvers_set) > 0:
if reject_reason_score < 10000:
reject_reason_score = 10000
                reject_reason = (f""Matched rule {rule_name}, but PR #{pr.pr_num} was not reviewed yet by any of:"" +
f""{','.join(list(rule_approvers_set)[:5])}{', ...' if len(rule_approvers_set) > 5 else ''}"")
continue
if rule.mandatory_checks_name is not None:
"
176,"func(Callable): Torch function for which we want to provide a sharded
implementation (ex: torch.nn.functional.linear)
""""""
return functools.partial(
_decorator_func,
op=func,
","func(Callable): Torch function for which we want to provide a sharded
implementation (ex: torch.nn.functional.linear)
""""""
    return functools.partial(
        _decorator_func,
        op=func,
        op_table=_CUSTOM_SHARDED_OPS
    )

def _sharded_op_impl(func):
    """"""
    Decorator to register a default sharded op.
    """"""
return functools.partial(
_decorator_func,
op=func,
"
177,"torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
","torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@_sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
"
178,"customized_func=sharded_detach,
)
@sharded_op_impl(torch.Tensor.requires_grad_)
def tensor_requires_grad_set(types, args=(), kwargs=None, pg=None):
self_st = args[0]
requires_grad = args[1]
","customized_func=sharded_detach,
)
@_sharded_op_impl(torch.Tensor.requires_grad_)
def tensor_requires_grad_set(types, args=(), kwargs=None, pg=None):
self_st = args[0]
requires_grad = args[1]
"
179,"cast,
)
import copy
import weakref
import math
import threading
import torch
","cast,
)
import copy
from functools import reduce
import weakref
import threading
import torch
"
180,"if (__builtin_available(macOS 12.3, *) || __builtin_available(macOSApplicationExtension 12.3, *)) {
auto device = device_or_default(device_opt);
TORCH_INTERNAL_ASSERT(device.is_mps());
const DeviceGuard device_guard(device);
auto* allocator = at::mps::GetMPSAllocator();
constexpr c10::DispatchKeySet mps_dks(c10::DispatchKey::MPS);
","if (__builtin_available(macOS 12.3, *) || __builtin_available(macOSApplicationExtension 12.3, *)) {
auto device = device_or_default(device_opt);
TORCH_INTERNAL_ASSERT(device.is_mps());
    TORCH_CHECK_TYPE(dtype != ScalarType::Double, MPS_ERROR_DOUBLE_NOT_SUPPORTED);
const DeviceGuard device_guard(device);
auto* allocator = at::mps::GetMPSAllocator();
constexpr c10::DispatchKeySet mps_dks(c10::DispatchKey::MPS);
"
181,"import torch.distributed.distributed_c10d as distributed_c10d
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
    _sharded_op_impl
)
def _communicate_result(result, pg):
","import torch.distributed.distributed_c10d as distributed_c10d
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
    sharded_op_impl
)
def _communicate_result(result, pg):
"
182,"import torch
import torch.distributed._shard.sharded_tensor as sharded_tensor
from torch.distributed._shard.sharded_tensor import (
    _sharded_op_impl,
)
def validate_param(param, param_name):
if param is None:
raise ValueError(f""param: {param_name} shouldn't be None!"")
@_sharded_op_impl(torch.nn.init.uniform_)
def uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensor in sharded_tensor.local_shards with values drawn from the uniform
","import torch
import torch.distributed._shard.sharded_tensor as sharded_tensor
from torch.distributed._shard.sharded_tensor import (
    sharded_op_impl,
)
def validate_param(param, param_name):
if param is None:
raise ValueError(f""param: {param_name} shouldn't be None!"")
@sharded_op_impl(torch.nn.init.uniform_)
def uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensor in sharded_tensor.local_shards with values drawn from the uniform
"
183,"from torch import Tensor
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
    _sharded_op_impl
)
from torch.distributed._shard.replicated_tensor import ReplicatedTensor
from torch.distributed._shard._utils import narrow_tensor
","from torch import Tensor
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
    sharded_op_impl
)
from torch.distributed._shard.replicated_tensor import ReplicatedTensor
from torch.distributed._shard._utils import narrow_tensor
"
184,"if isinstance(state_dict[key], ShardedTensor):
setattr(submodule, attr_name, state_dict[key])
def sharded_op_impl(func):
""""""
Provides a way for users to write their own custom sharded operator. This
can be used to override existing ShardedTensor operators or write a new
","if isinstance(state_dict[key], ShardedTensor):
setattr(submodule, attr_name, state_dict[key])
def custom_sharded_op_impl(func):
""""""
Provides a way for users to write their own custom sharded operator. This
can be used to override existing ShardedTensor operators or write a new
"
185,"func(Callable): Torch function for which we want to provide a sharded
implementation (ex: torch.nn.functional.linear)
""""""
return functools.partial(
_decorator_func,
op=func,
","func(Callable): Torch function for which we want to provide a sharded
implementation (ex: torch.nn.functional.linear)
""""""
    return functools.partial(
        _decorator_func,
        op=func,
        op_table=_CUSTOM_SHARDED_OPS
    )

def _sharded_op_impl(func):
    """"""
    Decorator to register a default sharded op.
    """"""
return functools.partial(
_decorator_func,
op=func,
"
186,"Example::
>>> op = torch.transpose
        >>> @sharded_op_impl(op)
>>> @_sharded_op_common(op, early_stop_func, extra_check)
>>> def sharded_tensor_op(types, args, kwargs, process_group):
>>>   ....
","Example::
>>> op = torch.transpose
        >>> @_sharded_op_impl(op)
>>> @_sharded_op_common(op, early_stop_func, extra_check)
>>> def sharded_tensor_op(types, args, kwargs, process_group):
>>>   ....
"
187,"encoder_layer: an instance of the TransformerEncoderLayer() class (required).
num_layers: the number of sub-encoder-layers in the encoder (required).
norm: the layer normalization component (optional).
Examples::
>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
","encoder_layer: an instance of the TransformerEncoderLayer() class (required).
num_layers: the number of sub-encoder-layers in the encoder (required).
norm: the layer normalization component (optional).
        enable_nested_tensor: if True, input will automatically convert to nested tensor
            (and convert back on output). This will improve the overall performance of
            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).
Examples::
>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
"
188,"return false;
}
// Make sure reduction axes are consistent through the fusion
auto reduction_ops =
ir_utils::getReductionOps(fusion, false /* ignore_trivial */);
","return false;
}
    if (hasNonUniqueBcast(fusion)) {
      scheduler_debug_utils::canScheduleRejectReason(
          ScheduleHeuristic::Reduction,
          ""Broadcasting dimension might be broadcasting to multiple sizes."");
      return false;
    }

// Make sure reduction axes are consistent through the fusion
auto reduction_ops =
ir_utils::getReductionOps(fusion, false /* ignore_trivial */);
"
189,"offload_to_cpu (bool, optional): If ``True``, full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
                for world_size = 1). It is recommended to use ``offload_to_cpu``
                with ``rank0_only=True`` to avoid redundant copies of model
                parameters being offloaded to the same CPU memory.
""""""
# Note that we specify root_only as FSDP roots will handle summoning
# child FSDP instances based on recurse argument.
","offload_to_cpu (bool, optional): If ``True``, full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
                for world_size = 1 or ``NO_SHARD`` config). It is recommended
                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid
                redundant copies of model parameters being offloaded to the same CPU memory.
""""""
# Note that we specify root_only as FSDP roots will handle summoning
# child FSDP instances based on recurse argument.
"
190,"}
}


TORCH_META_FUNC2(norm, ScalarOpt_dim)
(const Tensor& self, const OptionalScalarRef p, IntArrayRef dim, bool keepdim) {
  check_floating_or_complex_dtype(""norm"", self.scalar_type());
auto out_dtype = get_result_or_self_value_dtype(self, maybe_get_output(), c10::nullopt);
resize_reduction(*this, self, dim, keepdim, out_dtype);
}
","}
}
TORCH_META_FUNC2(norm, ScalarOpt_dim)
(const Tensor& self, const OptionalScalarRef p, IntArrayRef dim, bool keepdim) {
  TORCH_CHECK(
      at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type()),
      ""norm(): input dtype should be either floating point or complex. ""
      ""Got "", self.scalar_type(), "" instead."");

auto out_dtype = get_result_or_self_value_dtype(self, maybe_get_output(), c10::nullopt);
resize_reduction(*this, self, dim, keepdim, out_dtype);
}
"
191,"}
}
#ifndef C10_DISABLE_TENSORIMPL_EXTENSIBILITY

at::IntArrayRef LTCTensorImpl::sizes_custom() const {
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
const_cast<LTCTensorImpl*>(this)->setup_size_properties();
","}
}
at::IntArrayRef LTCTensorImpl::sizes_custom() const {
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
const_cast<LTCTensorImpl*>(this)->setup_size_properties();
"
192,"#include <torch/library.h>
#include <c10/util/irange.h>
namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
","#include <torch/library.h>
#include <c10/util/irange.h>
#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/NativeFunctions.h>
#else
#include <ATen/ops/to_native.h>
#endif

namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
"
193,"DEFINE_DISPATCH(isneginf_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(mode_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_min_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_max_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_min_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_max_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
","DEFINE_DISPATCH(isneginf_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(mode_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_min_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
DEFINE_DISPATCH(clamp_max_scalar_stub); // NOLINT(cppcoreguidelines-avoid-non-const-global-variables)
"
194,"return output;
}
} // namespace native
} // namespace at
","return output;
}
// The default implementation of lift is a no-op.
// If TLS is set appropriately (for wrapper-tensor keys like Functionalize or functorch transforms),
// then we'll dispatch to one of their implementations, which will properly lift the tensor into a wrapper.
at::Tensor lift(const at::Tensor& self) {
    return self;
}

} // namespace native
} // namespace at
"
195,"""copy"",  # only used by the functionalization pass
""fill.Tensor"",  # only used by the functionalization pass
""fill.Scalar"",  # only used by the functionalization pass
]
SKIP_PYTHON_BINDINGS = list(
","""copy"",  # only used by the functionalization pass
""fill.Tensor"",  # only used by the functionalization pass
""fill.Scalar"",  # only used by the functionalization pass
    ""lift"",
]
SKIP_PYTHON_BINDINGS = list(
"
196,"_process_pos_dim_tensor_state,
_unflatten_optim_state,
)
from ._utils import _apply_to_modules, _apply_to_tensors, _replace_by_prefix
from .flatten_params_wrapper import (
FLAT_PARAM,
FPW_MODULE,
FlatParameter,
FlattenParamsWrapper,
)
from .wrap import _recursive_wrap
if TYPE_CHECKING:
from collections import OrderedDict  # noqa: F401
","_process_pos_dim_tensor_state,
_unflatten_optim_state,
)
from ._utils import (
    _apply_to_modules, _apply_to_tensors, _replace_by_prefix,
    _override_batchnorm_mixed_precision, _contains_batchnorm
)
from .flatten_params_wrapper import (
FLAT_PARAM,
FPW_MODULE,
FlatParameter,
FlattenParamsWrapper,
)
from .wrap import _recursive_wrap, _wrap_batchnorm_individually, _or_policy
if TYPE_CHECKING:
from collections import OrderedDict  # noqa: F401
"
197,"# if not recursing, decide whether we should wrap for the leaf node or reminder
return isinstance(module, tuple(transformer_layer_cls))
def size_based_auto_wrap_policy(
module: nn.Module,
","# if not recursing, decide whether we should wrap for the leaf node or reminder
return isinstance(module, tuple(transformer_layer_cls))
def _wrap_batchnorm_individually(
    module: nn.Module,
    recurse: bool,
    *args,
    **kwargs,
) -> bool:
    """"""
    A policy that wraps ``BatchNorm`` instances in their own FSDP unit.
    """"""
    if recurse:
        # always recurse
        return True
    else:
        # if not recursing, decide whether we should wrap based on whether it is a
        # BN layer or not.
        return isinstance(module, _BatchNorm)

def _or_policy(
    module: nn.Module,
    recurse: bool,
    unwrapped_params: int,
    policies,
) -> bool:
    """"""
    A policy that wraps ``module`` if any policy in the passed in iterable of
    ``policies`` returns ``True``.
    """"""
    return any(
        policy(module, recurse, unwrapped_params) for policy in policies
    )

def size_based_auto_wrap_policy(
module: nn.Module,
"
198,"Customizable set of module types to be excluded in wrapping.
""""""
force_leaf_modules = (
        default_auto_wrap_policy.FORCE_LEAF_MODULES  # type: ignore[attr-defined]
if force_leaf_modules is None
else force_leaf_modules
)
exclude_wrap_modules = (
        default_auto_wrap_policy.EXCLUDE_WRAP_MODULES  # type: ignore[attr-defined]
if exclude_wrap_modules is None
else exclude_wrap_modules
)
","Customizable set of module types to be excluded in wrapping.
""""""
force_leaf_modules = (
        size_based_auto_wrap_policy.FORCE_LEAF_MODULES  # type: ignore[attr-defined]
if force_leaf_modules is None
else force_leaf_modules
)
exclude_wrap_modules = (
        size_based_auto_wrap_policy.EXCLUDE_WRAP_MODULES  # type: ignore[attr-defined]
if exclude_wrap_modules is None
else exclude_wrap_modules
)
"
199,"class Library:
""""""
    Class to create linraries that can be used to register new operators or
override operators in existing libraries from Python.
    A user can pass in a dispatch keyname if they only want the library to override kernels corresponding
    to only one specific dispatch key.
Args:
ns: library name
        kind: ""IMPL"" by default
dispatch_key: PyTorch dispatch key (default: """")
""""""
def __init__(self, ns, kind, dispatch_key=""""):
frame = traceback.extract_stack(limit=3)[0]
filename, lineno = frame.filename, frame.lineno
self.m = torch._C._dispatch_library(kind, ns, dispatch_key, filename, lineno)
","class Library:
""""""
    A class to create libraries that can be used to register new operators or
override operators in existing libraries from Python.
    A user can optionally pass in a dispatch keyname if they only want to register
    kernels corresponding to only one specific dispatch key.
Args:
ns: library name
        kind: ""DEF"", ""IMPL"" (default: ""IMPL"")
dispatch_key: PyTorch dispatch key (default: """")
""""""
def __init__(self, ns, kind, dispatch_key=""""):
        if kind != ""IMPL"" and kind != ""DEF"":
            raise ValueError(""Unsupported kind: "", kind)
frame = traceback.extract_stack(limit=3)[0]
filename, lineno = frame.filename, frame.lineno
self.m = torch._C._dispatch_library(kind, ns, dispatch_key, filename, lineno)
"
200,"@register_decomposition(aten.elu_backward)
@cast_for_opmath
def elu_backward(
grad_output: Tensor,
alpha: float,
","@register_decomposition(aten.elu_backward)
@pw_cast_for_opmath
def elu_backward(
grad_output: Tensor,
alpha: float,
"
201,"@register_decomposition(aten.l1_loss_backward)
@cast_for_opmath
def l1_loss_backward(
grad_output: Tensor,
self: Tensor,
","@register_decomposition(aten.l1_loss_backward)
@pw_cast_for_opmath
def l1_loss_backward(
grad_output: Tensor,
self: Tensor,
"
202,"@register_decomposition(aten.mse_loss_backward)
@cast_for_opmath
def mse_loss_backward(
grad_output: Tensor, input: Tensor, target: Tensor, reduction: int
):
","@register_decomposition(aten.mse_loss_backward)
@pw_cast_for_opmath
def mse_loss_backward(
grad_output: Tensor, input: Tensor, target: Tensor, reduction: int
):
"
203,"@register_decomposition(aten.native_dropout)
@cast_for_opmath
def native_dropout_decomposition(input, p, generator=None):
bool_mask = torch.rand_like(input) < p
res = bool_mask * input * float(1.0 / p)
return [res, bool_mask]
@register_decomposition(aten._softmax)
@cast_for_opmath
def _softmax(x: Tensor, dim: int, half_to_float: bool):
x_max = torch.max(x, dim, keepdim=True)[0]
unnormalized = torch.exp(x - x_max)
return unnormalized / torch.sum(unnormalized, dim, keepdim=True)
@register_decomposition(aten._log_softmax)
@cast_for_opmath
def _log_softmax(x: Tensor, dim: int, half_to_float: bool):
x_max = torch.max(x, dim, keepdim=True)[0]
shifted = x - x_max
","@register_decomposition(aten.native_dropout)
@pw_cast_for_opmath
def native_dropout_decomposition(input: Tensor, p: float, train: Optional[bool]):
bool_mask = torch.rand_like(input) < p
res = bool_mask * input * float(1.0 / p)
return [res, bool_mask]
# TODO: Correct the type promotion semantics
@register_decomposition(aten._softmax)
@pw_cast_for_opmath
def _softmax(x: Tensor, dim: int, half_to_float: bool):
x_max = torch.max(x, dim, keepdim=True)[0]
unnormalized = torch.exp(x - x_max)
return unnormalized / torch.sum(unnormalized, dim, keepdim=True)
# TODO: Correct the type promotion semantics
@register_decomposition(aten._log_softmax)
@pw_cast_for_opmath
def _log_softmax(x: Tensor, dim: int, half_to_float: bool):
x_max = torch.max(x, dim, keepdim=True)[0]
shifted = x - x_max
"
204,"return ~self.to(dtype=torch.bool)
# Commented out due to requiring type conversions for correct behavior on OpInfo tests
# @register_decomposition(aten.xlogy)
# def xlogy(self: Tensor, other: Tensor) -> Tensor:
#     return aten.where(aten.isnan(self),
","return ~self.to(dtype=torch.bool)
# Actually, I'm just not sure how to implement this correctly (maybe you need a special case for floating point?)
# @register_decomposition(aten.xlogy)
# def xlogy(self: Tensor, other: Tensor) -> Tensor:
#     return aten.where(aten.isnan(self),
"
205,"public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = at::_isnan<scalar_t>(*src_data) ? *src_data : std::min(*self_data, *src_data);
}
};
static ReduceMinimum reduce_minimum;
","public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = std::min(*self_data, *src_data);
}
};
static ReduceMinimum reduce_minimum;
"
206,"# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
def to_real_dtype(dtype: torch.dtype):
if dtype == torch.complex32:
return torch.float16
","# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
def apply_loss_reduction(loss: Tensor, reduction: int):
    if reduction == Reduction.MEAN.value:
        return torch.mean(loss)
    elif reduction == Reduction.SUM.value:
        return torch.sum(loss)
    else:
        return loss


def to_real_dtype(dtype: torch.dtype):
if dtype == torch.complex32:
return torch.float16
"
207,"HistogramObserver,
MovingAveragePerChannelMinMaxObserver,
FixedQParamsObserver,
    default_affine_fixed_qparams_observer,
    default_symmetric_fixed_qparams_observer,
_with_args,
)
import re
","HistogramObserver,
MovingAveragePerChannelMinMaxObserver,
FixedQParamsObserver,
    default_fixed_qparams_range_0to1_observer,
    default_fixed_qparams_range_neg1to1_observer,
_with_args,
)
import re
"
208,"# mapping from module to output activation post process class
DEFAULT_MODULE_TO_ACT_POST_PROCESS : Dict[Callable, Callable] = {
    nn.Hardsigmoid: default_affine_fixed_qparams_fake_quant,
    nn.Sigmoid: default_affine_fixed_qparams_fake_quant,
    nn.Softmax: default_affine_fixed_qparams_fake_quant,
    nn.Tanh: default_symmetric_fixed_qparams_fake_quant,
}
# Default map for swapping float module to static sparse quantized ones
","# mapping from module to output activation post process class
DEFAULT_MODULE_TO_ACT_POST_PROCESS : Dict[Callable, Callable] = {
    nn.Hardsigmoid: default_fixed_qparams_range_0to1_fake_quant,
    nn.Sigmoid: default_fixed_qparams_range_0to1_fake_quant,
    nn.Softmax: default_fixed_qparams_range_0to1_fake_quant,
    nn.Tanh: default_fixed_qparams_range_neg1to1_fake_quant,
}
# Default map for swapping float module to static sparse quantized ones
"
209,"TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
namespace {

// DO NOT USE THIS -- it's just an implementation detail of wrapped_scalar tensor below.
at::Tensor scalar_to_tensor_default_dtype(
    const Scalar& s,
    const Device device = at::kCPU) {
  if (s.isFloatingPoint()) {
    return at::scalar_tensor(
        s, at::device(device).dtype(at::get_default_dtype()));
  } else if (s.isBoolean()) {
    return at::scalar_tensor(s, at::device(device).dtype(at::kBool));
  } else if (s.isComplex()) {
    return at::scalar_tensor(
        s, at::device(device).dtype(at::get_default_complex_dtype()));
  } else {
    TORCH_INTERNAL_ASSERT(s.isIntegral(false));
    return at::scalar_tensor(s, at::device(device).dtype(at::kLong));
  }
}

// TLDR: Don't call `wrapped_scalar_tensor_default_dtype` -- this function is only necessary to support the partial
// type-promotion that torch.where supports.  Once torch.where fully supports type promotion, we
// won't need this function.
//
// Longer explanation:
// `wrapped_scalar_tensor_default_dtype` is a bit of a hack because torch.where doesn't support type promotion, but
// does support `torch.where(tensor, scalar1, scalar2)` with default scalar types.  The trickiness is we
// usually convert double scalars to doubles, and `set_wrapped_number` defines type promotion priority
// as being below tensor types rather than as the default dtype (perhaps we should?).  This wouldn't matter
// if we just supported type normal type promotion on torch.where, however.
Tensor wrapped_scalar_tensor_default_dtype(
    const Scalar& scalar,
    Device device) {
  at::Tensor tensor;
  tensor = scalar_to_tensor_default_dtype(scalar, device);
  tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
  return tensor;
}

} // anonymous namespace

// Sorting-based algorithm for isin(); used when the number of test elements is large.
static void isin_sorting(
const Tensor& elements,
","TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
// Sorting-based algorithm for isin(); used when the number of test elements is large.
static void isin_sorting(
const Tensor& elements,
"
210,"from abc import ABCMeta, abstractmethod
from collections import OrderedDict
from functools import partial
from typing import Any, List, Tuple, Optional, Dict, Union
import torch
import torch.nn as nn
","from abc import ABCMeta, abstractmethod
from collections import OrderedDict
from functools import partial
from typing import Any, List, Tuple, Optional, Dict
import torch
import torch.nn as nn
"
211,"Iterable,
Iterator,
List,
NamedTuple,
Optional,
Set,
","Iterable,
Iterator,
List,
    Mapping,
NamedTuple,
Optional,
Set,
"
212,"def _full_pre_load_state_dict_hook(
self,
        state_dict: Union[Dict[str, torch.Tensor], ""OrderedDict[str, torch.Tensor]""],
prefix: str,
) -> None:
_replace_by_prefix(state_dict, prefix, prefix + f""{FSDP_WRAPPED_MODULE}."")
def _local_pre_load_state_dict_hook(
self,
        state_dict: Union[Dict[str, torch.Tensor], ""OrderedDict[str, torch.Tensor]""],
prefix: str,
) -> None:
""""""
","def _full_pre_load_state_dict_hook(
self,
        state_dict: Dict[str, Any],
prefix: str,
) -> None:
_replace_by_prefix(state_dict, prefix, prefix + f""{FSDP_WRAPPED_MODULE}."")
def _local_pre_load_state_dict_hook(
self,
        state_dict: Dict[str, Any],
prefix: str,
) -> None:
""""""
"
213,")
@register_decomposition(aten.hardsigmoid_backward)
@cast_for_opmath
def hardsigmoid_backward_decomposition(grad_output: Tensor, self: Tensor):
return torch.where(
(self > -3.0) & (self < 3.0),
grad_output * (1.0 / 6.0),
",")
@register_decomposition(aten.hardsigmoid)
@cast_for_opmath
def hardsigmoid(self: Tensor) -> Tensor:
    return torch.clamp(torch.clamp(self + 3, min=0), max=6) / 6


@register_decomposition(aten.hardsigmoid_backward)
@cast_for_opmath
def hardsigmoid_backward(grad_output: Tensor, self: Tensor):
return torch.where(
(self > -3.0) & (self < 3.0),
grad_output * (1.0 / 6.0),
"
214,")
@register_decomposition(aten.binary_cross_entropy_backward)
@cast_for_opmath
def binary_cross_entropy_backward(
",")
@register_decomposition(aten.nll_loss_backward)
def nll_loss_backward(
    grad_output: Tensor,
    self: Tensor,
    target: Tensor,
    weight: Optional[Tensor],
    reduction: int,
    ignore_index: int,
    total_weight: Tensor,
) -> Tensor:
    assert 0 <= self.dim() <= 2, ""input tensor should be 1D or 2D""
    assert (
        target.dim() <= 1
    ), ""0D or 1D target tensor expected, multi-target not supported""

    no_batch_dim = self.dim() == 1 and target.dim() == 0
    assert no_batch_dim or (
        self.shape[0] == target.shape[0]
    ), f""size mismatch (got input: {self.shape}, target: {target.shape})""
    assert total_weight.numel() == 1, (
        ""expected total_weight to be a single element tensor, got: "",
        f""{total_weight.shape} ({total_weight.numel()} elements)"",
    )

    assert (
        weight is None or weight.numel() == self.shape[-1]
    ), ""weight tensor should be defined either for all or no classes""

    if reduction == Reduction.NONE.value and self.dim() == 2:
        assert grad_output.dim() == 1 and grad_output.shape[0] == self.shape[0], (
            f""Expected a tensor of dimension 1 and tensor.size[0] == {self.shape[0]} but ""
            f""got: dimension {grad_output.dim()} and tensor.size[0] == {grad_output.shape[0]}""
        )
    else:
        assert (
            grad_output.dim() <= 1 and grad_output.numel() == 1
        ), f""Expected a single element grad_output tensor, but got: {grad_output.shape}""

    channel_dim = 0 if self.dim() < 2 else 1
    if reduction == Reduction.MEAN.value:
        grad_output = grad_output / total_weight

    target = target.unsqueeze(channel_dim)
    grad_input = torch.zeros_like(self)
    grad_input = torch.scatter(grad_input, channel_dim, target, -1.0)

    if grad_input.dim() > grad_output.dim() > 0:
        grad_output = grad_output.unsqueeze(channel_dim)

    if weight is not None:
        new_shape = [1 for _ in range(self.dim())]
        new_shape[channel_dim] = weight.shape[0]
        weight.reshape(new_shape)
        grad_output = grad_output * weight

    has_ignore_index = ignore_index >= 0
    if has_ignore_index:
        ignore_index_mask = target != ignore_index
        grad_output = grad_output * ignore_index_mask

    return grad_input * grad_output


@register_decomposition(aten.binary_cross_entropy_backward)
@cast_for_opmath
def binary_cross_entropy_backward(
"
215,"hipify_result = hipify_python.hipify(
project_directory=build_dir,
output_directory=build_dir,
            includes=[os.path.join(os.path.relpath(include_dir, build_dir), '*') for include_dir in include_dirs] if include_dirs else ['*'],
extra_files=[os.path.abspath(s) for s in sources],
show_detailed=True,
is_pytorch_extension=True,
)
hipified_sources = set()
for source in sources:
s_abs = os.path.abspath(source)
            hipified_sources.add(hipify_result[s_abs][""hipified_path""] if s_abs in hipify_result else s_abs)
sources = list(hipified_sources)
","hipify_result = hipify_python.hipify(
project_directory=build_dir,
output_directory=build_dir,
            header_include_dirs=include_dirs,
            includes=[os.path.join(build_dir, '*')],  # limit scope to build_dir only
extra_files=[os.path.abspath(s) for s in sources],
show_detailed=True,
is_pytorch_extension=True,
            hipify_extra_files_only=True,  # don't hipify everything in includes path
)
hipified_sources = set()
for source in sources:
s_abs = os.path.abspath(source)
            hipified_sources.add(hipify_result[s_abs][""hipified_path""] if (s_abs in hipify_result and
                                 hipify_result[s_abs][""hipified_path""] is not None) else s_abs)
sources = list(hipified_sources)
"
216,"-- a/torch/utils/hipify/hipify_python.py
""""""Helper method to see if filename ends with certain extension""""""
return any(filename.endswith(e) for e in extensions)
def matched_files_iter(
root_path: str,
        includes: Iterable = ('*',),
ignores: Iterable = (),
extensions: Iterable = (),
out_of_place_only: bool = False,
is_pytorch_extension: bool = False) -> Iterator[str]:
    def _fnmatch(filepath, patterns):
        return any(fnmatch.fnmatch(filepath, pattern) for pattern in patterns)
exact_matches = set(includes)
","++ b/torch/utils/hipify/hipify_python.py
""""""Helper method to see if filename ends with certain extension""""""
return any(filename.endswith(e) for e in extensions)
def _fnmatch(filepath, patterns):
    return any(fnmatch.fnmatch(filepath, pattern) for pattern in patterns)

def matched_files_iter(
root_path: str,
        includes: Iterable = (),
ignores: Iterable = (),
extensions: Iterable = (),
out_of_place_only: bool = False,
is_pytorch_extension: bool = False) -> Iterator[str]:
exact_matches = set(includes)
"
217,"if ""third_party"" in dirs:
dirs.remove(""third_party"")
for filename in filenames:
            filepath = os.path.join(rel_dirpath, filename)
# We respect extensions, UNLESS you wrote the entire
# filename verbatim, in which case we always accept it
if (
","if ""third_party"" in dirs:
dirs.remove(""third_party"")
for filename in filenames:
            filepath = os.path.join(abs_dirpath, filename)
            rel_filepath = os.path.join(rel_dirpath, filename)
# We respect extensions, UNLESS you wrote the entire
# filename verbatim, in which case we always accept it
if (
"
218,"return os.path.join(dirpath, root + ext)
def is_out_of_place(filepath):
    if filepath.startswith(""torch/""):
return False
    if filepath.startswith(""tools/autograd/templates/""):
return False
return True
# Keep this synchronized with includes/ignores in build_amd.py
def is_pytorch_file(filepath):
    if filepath.startswith(""aten/""):
        if filepath.startswith(""aten/src/ATen/core/""):
return False
return True
    if filepath.startswith(""torch/""):
return True
    if filepath.startswith(""tools/autograd/templates/""):
return True
return False
def is_cusparse_file(filepath):
    if is_pytorch_file(filepath):
        return ""sparse"" in filepath.lower()
return False
def is_caffe2_gpu_file(filepath):
    if filepath.startswith(""c10/cuda""):
return True
    filename = os.path.basename(filepath)
_, ext = os.path.splitext(filename)
return ('gpu' in filename or ext in ['.cu', '.cuh']) and ('cudnn' not in filename)
","return os.path.join(dirpath, root + ext)
def is_out_of_place(rel_filepath):
    assert(not os.path.isabs(rel_filepath))
    if rel_filepath.startswith(""torch/""):
return False
    if rel_filepath.startswith(""tools/autograd/templates/""):
return False
return True
# Keep this synchronized with includes/ignores in build_amd.py
def is_pytorch_file(rel_filepath):
    assert(not os.path.isabs(rel_filepath))
    if rel_filepath.startswith(""aten/""):
        if rel_filepath.startswith(""aten/src/ATen/core/""):
return False
return True
    if rel_filepath.startswith(""torch/""):
return True
    if rel_filepath.startswith(""tools/autograd/templates/""):
return True
return False
def is_cusparse_file(rel_filepath):
    if is_pytorch_file(rel_filepath):
        return ""sparse"" in rel_filepath.lower()
return False
def is_caffe2_gpu_file(rel_filepath):
    assert(not os.path.isabs(rel_filepath))
    if rel_filepath.startswith(""c10/cuda""):
return True
    filename = os.path.basename(rel_filepath)
_, ext = os.path.splitext(filename)
return ('gpu' in filename or ext in ['.cu', '.cuh']) and ('cudnn' not in filename)
"
219,"header_filepath = header_path_to_check
# If not found, look in include dirs one by one and first match wins
if header_filepath is None:
                    for include in includes:
                        header_dir_to_check = os.path.join(output_directory, os.path.dirname(include))
header_path_to_check = os.path.abspath(os.path.join(header_dir_to_check, f))
if os.path.exists(header_path_to_check):
header_dir = header_dir_to_check
","header_filepath = header_path_to_check
# If not found, look in include dirs one by one and first match wins
if header_filepath is None:
                    for header_include_dir in header_include_dirs:
                        header_dir_to_check = os.path.join(output_directory, header_include_dir)
header_path_to_check = os.path.abspath(os.path.join(header_dir_to_check, f))
if os.path.exists(header_path_to_check):
header_dir = header_dir_to_check
"
220,"cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
bool deterministic{true};
bool allow_tf32{false};
setLinearParams(&key.params, input, orig_weight, deterministic, allow_tf32);
","cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are
  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two
  // CacheKey objects have the same user defined parameters, but
  // different padded values, resulting in different hash outputs.
  memset(&key, 0, sizeof(key));
bool deterministic{true};
bool allow_tf32{false};
setLinearParams(&key.params, input, orig_weight, deterministic, allow_tf32);
"
221,"storage_ = functional_storage;
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
}
FunctionalTensorWrapper::FunctionalTensorWrapper(const Tensor& value)
","storage_ = functional_storage;
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
  // All of the keys corresponding to functorch transforms should not be copied over.
  // Functorch transforms all have their own wrapper tensors (e.g. BatchedTensorImpl) which expect
  // to participate in the functorch transforms.
  key_set_ = key_set_ - c10::functorch_transforms_ks;
}
FunctionalTensorWrapper::FunctionalTensorWrapper(const Tensor& value)
"
222,"""MethodOperators_includes"": [],
""MethodOperators_declarations"": list(
mapMaybe(
                    ComputeOperators(Target.DECLARATION, static_dispatch_backend_indices=static_dispatch_idx),
                    method_native_functions
)
),
},
","""MethodOperators_includes"": [],
""MethodOperators_declarations"": list(
mapMaybe(
                    ComputeOperators(
                        Target.DECLARATION,
                        static_dispatch_backend_indices=static_dispatch_idx,
                    ),
                    method_native_functions,
)
),
},
"
223,"""Operators_includes"": [""#include <ATen/MethodOperators.h>""],
""Operators_declarations"": list(
mapMaybe(
                    ComputeOperators(Target.DECLARATION, static_dispatch_backend_indices=static_dispatch_idx),
                    non_method_native_functions
)
),
},
","""Operators_includes"": [""#include <ATen/MethodOperators.h>""],
""Operators_declarations"": list(
mapMaybe(
                    ComputeOperators(
                        Target.DECLARATION,
                        static_dispatch_backend_indices=static_dispatch_idx,
                    ),
                    non_method_native_functions,
)
),
},
"
224,"native_functions,
key_fn=key_func,
env_callable=lambda fn: {
            'operator_headers': [f'#include <ATen/ops/{fn.root_name}.h>'],
            'definitions': [ComputeOperators(Target.DEFINITION,
                                             static_dispatch_backend_indices=static_dispatch_idx)(fn)]},
base_env={
            'static_dispatch_extra_headers': static_dispatch_extra_headers(static_dispatch_idx),
},
num_shards=5,
        sharded_keys={'operator_headers', 'definitions', 'static_dispatch_extra_headers'}
)
cpu_fm.write(""Functions.cpp"", lambda: {})
","native_functions,
key_fn=key_func,
env_callable=lambda fn: {
            ""operator_headers"": [f""#include <ATen/ops/{fn.root_name}.h>""],
            ""definitions"": [
                ComputeOperators(
                    Target.DEFINITION,
                    static_dispatch_backend_indices=static_dispatch_idx,
                )(fn)
            ],
        },
base_env={
            ""static_dispatch_extra_headers"": static_dispatch_extra_headers(
                static_dispatch_idx
            ),
},
num_shards=5,
        sharded_keys={
            ""operator_headers"",
            ""definitions"",
            ""static_dispatch_extra_headers"",
        },
)
cpu_fm.write(""Functions.cpp"", lambda: {})
"
225,"del attrs[""process_group""]
del attrs[""reducer""]
del attrs[""logger""]
        del attrs[""_replicated_tensor_module""]
return attrs
def __setstate__(self, state):
","del attrs[""process_group""]
del attrs[""reducer""]
del attrs[""logger""]
        if self._use_replicated_tensor_module:
            del attrs[""_replicated_tensor_module""]
return attrs
def __setstate__(self, state):
"
226,"}
return outputs;
}
TensorList to_functional_tensor(const TensorList& t_list) {
std::vector<Tensor> outputs(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
outputs[i] = to_functional_tensor(t_list[i]);
","}
return outputs;
}
std::vector<Tensor> to_functional_tensor(const TensorList& t_list) {
std::vector<Tensor> outputs(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
outputs[i] = to_functional_tensor(t_list[i]);
"
227,"}
}
bool isFunctionalTensor(const at::Tensor& tensor) {
return tensor.unsafeGetTensorImpl()->key_set().has(c10::DispatchKey::Functionalize);
}
","}
}
void replace_(const Tensor& functional_tensor, const Tensor& other) {
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(isFunctionalTensor(functional_tensor));
  unsafeGetFunctionalWrapper(functional_tensor)->replace_(other);
}

void replace_(const TensorList functional_tensor, TensorList other) {
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(functional_tensor.size() == other.size());
  for (const auto i : c10::irange(functional_tensor.size())) {
    replace_(functional_tensor[i], other[i]);
  }
}


void commit_update(const Tensor& functional_tensor) {
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(isFunctionalTensor(functional_tensor));
  unsafeGetFunctionalWrapper(functional_tensor)->commit_update();
}

void commit_update(const TensorList functional_tensor) {
  for (const auto i : c10::irange(functional_tensor.size())) {
    commit_update(functional_tensor[i]);
  }
}

bool isFunctionalTensor(const at::Tensor& tensor) {
return tensor.unsafeGetTensorImpl()->key_set().has(c10::DispatchKey::Functionalize);
}
"
228,"scalar_type == at::ScalarType::Float ||
scalar_type == at::ScalarType::Half ||
scalar_type == at::ScalarType::BFloat16) &&
        mat2_sizes[0] > 1 && mat2_sizes[1] > 1;
#endif
if (!useLtInterface) {
self_ = expand_size(self, {mat1_sizes[0], mat2_sizes[1]}, ""addmm"");
","scalar_type == at::ScalarType::Float ||
scalar_type == at::ScalarType::Half ||
scalar_type == at::ScalarType::BFloat16) &&
        mat2_sizes[0] > 1 && mat2_sizes[1] > 1 &&
        mat2_sizes[0] < 65535 && mat2_sizes[1] < 65535 &&
        mat1_sizes[0] < 65535 && mat1_sizes[1] < 65535 &&
        // avoid leaing dim >> rows bugs
        ((mat1.strides()[0]==1 && mat1.strides()[1]==mat1_sizes[0]) || (mat1.strides()[1] == 1 && mat1.strides()[0] == mat1_sizes[1]) || (scalar_type != at::ScalarType::Half && scalar_type != at::ScalarType::BFloat16)) &&
        ((mat2.strides()[0]==1 && mat2.strides()[1]==mat2_sizes[0]) || (mat2.strides()[1] == 1 && mat2.strides()[0] == mat2_sizes[1]) || (scalar_type != at::ScalarType::Half && scalar_type != at::ScalarType::BFloat16));
#endif
if (!useLtInterface) {
self_ = expand_size(self, {mat1_sizes[0], mat2_sizes[1]}, ""addmm"");
"
229,"[2, 4, 6, 8, 10]
""""""
functions: Dict[str, Callable] = {}
    reduce_ex_hook : Optional[Callable] = None
getstate_hook: Optional[Callable] = None
def __getattr__(self, attribute_name):
if attribute_name in IterDataPipe.functions:
","[2, 4, 6, 8, 10]
""""""
functions: Dict[str, Callable] = {}
    reduce_ex_hook: Optional[Callable] = None
getstate_hook: Optional[Callable] = None
    str_hook: Optional[Callable] = None
    repr_hook: Optional[Callable] = None
def __getattr__(self, attribute_name):
if attribute_name in IterDataPipe.functions:
"
230,"index = self._next_index()  # may raise StopIteration
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
if self._pin_memory:
            data = _utils.pin_memory.pin_memory(data)
return data
","index = self._next_index()  # may raise StopIteration
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
if self._pin_memory:
            data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)
return data
"
231,"response = gen.send(request)
except StopIteration as e:
return e.value
namespace['__iter__'] = wrap_generator
else:
","response = gen.send(request)
except StopIteration as e:
return e.value
            except Exception as e:
                # TODO: Simplify the traceback message to skip over `response = gen.send(None)`
                #       Part of https://github.com/pytorch/data/issues/284
                datapipe = args[0]
                msg = ""thrown by __iter__ of""
                full_msg = f""{msg} {datapipe.__class__.__name__}({_generate_input_args_string(datapipe)})""
                if len(e.args) >= 1 and msg not in e.args[0]:
                    e.args = (e.args[0] + f'\nThis exception is {full_msg}',) + e.args[1:]
                raise
namespace['__iter__'] = wrap_generator
else:
"
232,"return SourceRange(self.source_, start, end);
})
.def_property_readonly(""source"", [](const SourceRangeFactory& self) {
        auto text_view = self.source_->text();
        std::string text(text_view.begin(), text_view.end());
        return text;
});
py::class_<TreeView>(m, ""TreeView"")
","return SourceRange(self.source_, start, end);
})
.def_property_readonly(""source"", [](const SourceRangeFactory& self) {
        auto text_view = self.source_->text_str().str();
        return text_view;
});
py::class_<TreeView>(m, ""TreeView"")
"
233,"functional_call_str = \
f""tmp_output = at::_ops::{functional_op.func.name.unambiguous_name()}::call({', '.join(functional_exprs)});""
    mutable_input_post_processing = '\n'.join([
        f""""""
auto {a.name}_functional = at::functionalization::impl::unsafeGetFunctionalWrapper({a.name});
{a.name}_functional->replace_(tmp_output);
{a.name}_functional->commit_update();""""""
        for a in f.func.arguments.flat_non_out
        if a.annotation and a.annotation.is_write and a.type.is_tensor_like()])
return f""""""
{dispatcher_sig.defn(name=wrapper_name(f.func), is_redispatching_fn=True)} {{
","functional_call_str = \
f""tmp_output = at::_ops::{functional_op.func.name.unambiguous_name()}::call({', '.join(functional_exprs)});""
    if f.func.is_out_fn():
        mutable_input_post_processing = '\n'.join([
            f""""""
      auto {a.name}_functional = at::functionalization::impl::unsafeGetFunctionalWrapper({a.name});
      {a.name}_functional->replace_({'std::get<' + str(i) + '>(tmp_output)' if len(f.func.returns) > 1 else 'tmp_output'});
      {a.name}_functional->commit_update();""""""
            for (i, a) in enumerate(f.func.arguments.out) if a.annotation and a.annotation.is_write and a.type.is_tensor_like()])
    else:
        mutable_input_post_processing = '\n'.join([
            f""""""
auto {a.name}_functional = at::functionalization::impl::unsafeGetFunctionalWrapper({a.name});
{a.name}_functional->replace_(tmp_output);
{a.name}_functional->commit_update();""""""
            for a in f.func.arguments.flat_all
            if a.annotation and a.annotation.is_write and a.type.is_tensor_like()])
return f""""""
{dispatcher_sig.defn(name=wrapper_name(f.func), is_redispatching_fn=True)} {{
"
234,"for name in dir(_C._VariableFunctions):
if name.startswith('__') or name in PRIVATE_OPS:
continue
    globals()[name] = getattr(_C._VariableFunctions, name)
__all__.append(name)
################################################################################
","for name in dir(_C._VariableFunctions):
if name.startswith('__') or name in PRIVATE_OPS:
continue
    obj = getattr(_C._VariableFunctions, name)
    obj.__module__ = 'torch'
    globals()[name] = obj
__all__.append(name)
################################################################################
"
235,"new_size[0] = bias_.value().size(0);
broadcasted_bias = bias_.value().reshape(new_size);
broadcasted_bias.value() = broadcasted_bias.value().broadcast_to(quantized_output.sizes());
    broadcasted_bias.value() = broadcasted_bias.value().contiguous(c10::MemoryFormat::ChannelsLast);
bias_multiplier_tensor = at::empty(quantized_output.sizes(), at::device(at::kCUDA).dtype(at::kFloat), at::MemoryFormat::ChannelsLast);
auto bias_multiplier = 1.0 / (act_scale * weight_scale);
bias_multiplier_tensor.value().fill_(bias_multiplier);
","new_size[0] = bias_.value().size(0);
broadcasted_bias = bias_.value().reshape(new_size);
broadcasted_bias.value() = broadcasted_bias.value().broadcast_to(quantized_output.sizes());
    broadcasted_bias.value() = broadcasted_bias.value().to(c10::MemoryFormat::ChannelsLast);
bias_multiplier_tensor = at::empty(quantized_output.sizes(), at::device(at::kCUDA).dtype(at::kFloat), at::MemoryFormat::ChannelsLast);
auto bias_multiplier = 1.0 / (act_scale * weight_scale);
bias_multiplier_tensor.value().fill_(bias_multiplier);
"
236,"auto input = qx;
if (ndim == 4) {
    input = qx.contiguous(MemoryFormat::ChannelsLast);
} else { // 3D
std::vector<int64_t> new_sizes{1, qx.size(0), qx.size(1), qx.size(2)};
input = qx.view(new_sizes);
","auto input = qx;
if (ndim == 4) {
    input = qx.to(MemoryFormat::ChannelsLast);
} else { // 3D
std::vector<int64_t> new_sizes{1, qx.size(0), qx.size(1), qx.size(2)};
input = qx.view(new_sizes);
"
237,"}
auto ret_ptr = c10::make_intrusive<PackedConvWeightCudnn<kSpatialDim>>(
          weight.contiguous(c10::MemoryFormat::ChannelsLast), // TODO: this assumes 2D I think. make it more general?
bias,
stride,
padding,
","}
auto ret_ptr = c10::make_intrusive<PackedConvWeightCudnn<kSpatialDim>>(
          weight.to(c10::MemoryFormat::ChannelsLast), // TODO: this assumes 2D I think. make it more general?
bias,
stride,
padding,
"
238,"uint8_t output_alignment;
// default to -1 when no bias
int8_t bias_alignment;
};
std::unordered_map<CacheKey, cudnn_frontend::ManagedOpaqueDescriptor, at::native::ParamsHash<CacheKey>, at::native::ParamsEqual<CacheKey>> execution_plan_cache;
}
","uint8_t output_alignment;
// default to -1 when no bias
int8_t bias_alignment;
  bool kReluFused;
};
std::unordered_map<CacheKey, cudnn_frontend::ManagedOpaqueDescriptor, at::native::ParamsHash<CacheKey>, at::native::ParamsEqual<CacheKey>> execution_plan_cache;
}
"
239,"} else {
key.bias_alignment = -1;
}
auto run = [&](cudnn_frontend::ManagedOpaqueDescriptor plan_desc) {
auto workspace_size = 0;
","} else {
key.bias_alignment = -1;
}
  key.kReluFused = kReluFused;
auto run = [&](cudnn_frontend::ManagedOpaqueDescriptor plan_desc) {
auto workspace_size = 0;
"
240,"""aten::amax/amin cannot be fused with dynamic keepdim"");
TensorView* out = nullptr;
              if (node->kind() ==
                  c10::Symbol::fromQualString(""aten::amax"")) {
out = max(self->as<TensorView>(), dims, keepdim.value());
              } else if (node->kind() ==
                  c10::Symbol::fromQualString(""aten::amin"")) {
out = min(self->as<TensorView>(), dims, keepdim.value());
} else {
                TORCH_INTERNAL_ASSERT(false, ""unrecognized operation in aten::amax/amin"");
}
value_map.emplace(node->output()->unique(), out);
},
","""aten::amax/amin cannot be fused with dynamic keepdim"");
TensorView* out = nullptr;
              if (node->kind() == c10::Symbol::fromQualString(""aten::amax"")) {
out = max(self->as<TensorView>(), dims, keepdim.value());
              } else if (
                  node->kind() == c10::Symbol::fromQualString(""aten::amin"")) {
out = min(self->as<TensorView>(), dims, keepdim.value());
} else {
                TORCH_INTERNAL_ASSERT(
                    false, ""unrecognized operation in aten::amax/amin"");
}
value_map.emplace(node->output()->unique(), out);
},
"
241,"assert node.op == ""call_module""
assert isinstance(node.target, str)
mod = getattr_from_fqn(gm, node.target)
        if isinstance(mod, (logger_cls, ObserverBase, FakeQuantizeBase)):  # type: ignore[arg-type]
# A logger or observer's input and output type is the output
# type of the preceding node.
first_arg = node.args[0]
","assert node.op == ""call_module""
assert isinstance(node.target, str)
mod = getattr_from_fqn(gm, node.target)
        is_known_fp32_or_int8_input_module = any(
            isinstance(mod, target_type) for target_type in MODS_IO_TYPE_FP32_OR_INT8  # type: ignore[arg-type]
        )
        if (
            isinstance(mod, (logger_cls, ObserverBase, FakeQuantizeBase))  # type: ignore[arg-type]
            or is_known_fp32_or_int8_input_module
        ):
# A logger or observer's input and output type is the output
# type of the preceding node.
first_arg = node.args[0]
"
242,"import torch
import torch.nn.functional as F
from typing import Dict, Any
from torch.overrides import has_torch_function
euler_constant = 0.57721566490153286060  # Euler Mascheroni Constant
","import torch
import torch.nn.functional as F
from typing import Dict, Any
from torch.overrides import is_tensor_like
euler_constant = 0.57721566490153286060  # Euler Mascheroni Constant
"
243,".. _NEP-0018:
https://numpy.org/neps/nep-0018-array-function-protocol.html
""""""
# Runtime is O(num_arguments * num_unique_types)
overloaded_types: Set[Type] = set()
overloaded_args: List[Any] = []
",".. _NEP-0018:
https://numpy.org/neps/nep-0018-array-function-protocol.html
""""""
    # If torch function is not enabled, there are no overloaded types
    if not torch._C._is_torch_function_enabled():
        return []
# Runtime is O(num_arguments * num_unique_types)
overloaded_types: Set[Type] = set()
overloaded_args: List[Any] = []
"
244,"to_i=torch.onnx.TensorProtoDataType.INT32)
return g.op(""prim::TupleConstruct"", q_bias, bias_scale, bias_zero_point)
# ---------------------------------------------------------------------
# ONNX operator version
# ---------------------------------------------------------------------

# READ ME BEFORE EDITING _default_onnx_opset_version:
#
# The variable below controls which ONNX operator set version we are
# targeting. THIS VARIABLE HAS SEMANTIC EFFECT! Say a breaking
# change occurred in version 8. As long as this variable < 8, you can
# export models targeting the old behavior. However, if you bump
# this variable to 8 or later, the breaking change will take into effect:
# you MUST adjust any symbolic affected by breaking changes. The ONNX
# spec publishes a *comprehensive* list of BC-breaking changes for every
# operator revision at:
#
#   https://github.com/onnx/onnx/blob/master/docs/Changelog.md
#
# Please be sure to go through and check all of our implementations here before
# increasing this number. This includes symbolic definitions NOT in this
# file, so grep for ""OpName"" (with quotes)
#
# Besides, opset_version can be specified in the invocation of export()
# and export_to_pretty_string(), and _export_onnx_opset_version will be set
# and the symbolic functions should check it to determine the behavior
# of the exporter.


_default_onnx_opset_version = 9
_onnx_main_opset = 15
_onnx_stable_opsets = [7, 8, 9, 10, 11, 12, 13, 14]
_export_onnx_opset_version = _default_onnx_opset_version
_constant_folding_opset_versions = list(range(9, _onnx_main_opset + 1))
def _set_opset_version(opset_version):
global _export_onnx_opset_version
    if opset_version == _default_onnx_opset_version:
        _export_onnx_opset_version = opset_version
        return
if opset_version in _onnx_stable_opsets + [_onnx_main_opset]:
_export_onnx_opset_version = opset_version
return
","to_i=torch.onnx.TensorProtoDataType.INT32)
return g.op(""prim::TupleConstruct"", q_bias, bias_scale, bias_zero_point)
_default_onnx_opset_version = 13
_onnx_main_opset = 15
_onnx_stable_opsets = list(range(7, _onnx_main_opset))
_export_onnx_opset_version = _default_onnx_opset_version
_constant_folding_opset_versions = list(range(9, _onnx_main_opset + 1))
def _set_opset_version(opset_version):
global _export_onnx_opset_version
if opset_version in _onnx_stable_opsets + [_onnx_main_opset]:
_export_onnx_opset_version = opset_version
return
"
245,"shape_inference_hdr: str = ""torch/csrc/lazy/core/shape_inference.h""
tensor_class: str = ""torch::lazy::LazyTensor""
tensor_class_hdr: str = ""torch/csrc/lazy/core/tensor.h""
    lazy_ir_cls: Type[LazyIR] = TSLazyIR
backend_name: str = ""TorchScript""
def main() -> None:
","shape_inference_hdr: str = ""torch/csrc/lazy/core/shape_inference.h""
tensor_class: str = ""torch::lazy::LazyTensor""
tensor_class_hdr: str = ""torch/csrc/lazy/core/tensor.h""
    lazy_ir_cls: Type[LazyIR] = LazyIR
backend_name: str = ""TorchScript""
def main() -> None:
"
246,"'dtype_class_hints': dtype_class_hints,
'all_directive': all_directive
}
    fm.write_with_template('torch/_C/__init__.pyi', respath('__init__.pyi.in'), lambda: {
'generated_comment': '@' + 'generated from torch/_C/__init__.pyi.in',
**env,
})
    fm.write_with_template('torch/_C/_VariableFunctions.pyi', respath('_VariableFunctions.pyi.in'), lambda: {
'generated_comment': '@' + 'generated from torch/_C/_VariableFunctions.pyi.in',
**env,
})
    fm.write_with_template('torch/_VF.pyi', respath('_VariableFunctions.pyi.in'), lambda: {
'generated_comment': '@' + 'generated from torch/_C/_VariableFunctions.pyi.in',
**env,
})
    fm.write_with_template('torch/return_types.pyi', respath('return_types.pyi.in'), lambda: {
'generated_comment': '@' + 'generated from torch/_C/return_types.pyi',
**env,
})
","'dtype_class_hints': dtype_class_hints,
'all_directive': all_directive
}
    fm.write_with_template('torch/_C/__init__.pyi', 'torch/_C/__init__.pyi.in', lambda: {
'generated_comment': '@' + 'generated from torch/_C/__init__.pyi.in',
**env,
})
    fm.write_with_template('torch/_C/_VariableFunctions.pyi', 'torch/_C/_VariableFunctions.pyi.in', lambda: {
'generated_comment': '@' + 'generated from torch/_C/_VariableFunctions.pyi.in',
**env,
})
    fm.write_with_template('torch/_VF.pyi', 'torch/_C/_VariableFunctions.pyi.in', lambda: {
'generated_comment': '@' + 'generated from torch/_C/_VariableFunctions.pyi.in',
**env,
})
    fm.write_with_template('torch/return_types.pyi', 'torch/_C/return_types.pyi.in', lambda: {
'generated_comment': '@' + 'generated from torch/_C/return_types.pyi',
**env,
})
"
247,"read gen_pyi for the gory details.
""""""
def get_py_torch_functions(
python_funcs: Sequence[PythonSignatureNativeFunctionPair],
method: bool = False,
","read gen_pyi for the gory details.
""""""
def respath(name: str) -> str:
    return parutil.get_file_path(name, pkg=__package__)


def get_py_torch_functions(
python_funcs: Sequence[PythonSignatureNativeFunctionPair],
method: bool = False,
"
248,"c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    C10_CUDA_CHECK(cudaMemGetInfo(&device_free, &device_total));
return {device_free, device_total};
});
}
","c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    cudaMemGetInfo(&device_free, &device_total);
return {device_free, device_total};
});
}
"
249,"s_ipc_event_handle.c_str());
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
cudaEvent_t event;
    AT_CUDA_CHECK(cudaIpcOpenEventHandle(&event, *ipc_event_handle));
AT_CUDA_CHECK(
cudaStreamWaitEvent(c10::cuda::getCurrentCUDAStream(device), event, 0));
}
","s_ipc_event_handle.c_str());
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
cudaEvent_t event;
    cudaIpcOpenEventHandle(&event, *ipc_event_handle);
AT_CUDA_CHECK(
cudaStreamWaitEvent(c10::cuda::getCurrentCUDAStream(device), event, 0));
}
"
250,"void onEachDevice(std::function<void(int)> op) const override {
at::cuda::OptionalCUDAGuard device_guard;
    for(const auto i : c10::irange(at::cuda::device_count())) {
device_guard.set_index(i);
op(i);
}
}
void synchronize() const override {
    TORCH_CUDA_CHECK(cudaDeviceSynchronize());
}
bool enabled() const override {
","void onEachDevice(std::function<void(int)> op) const override {
at::cuda::OptionalCUDAGuard device_guard;
    // NOLINTNEXTLINE(bugprone-signed-char-misuse)
    int count = at::cuda::device_count();
    for(const auto i : c10::irange(count)) {
device_guard.set_index(i);
op(i);
}
}
void synchronize() const override {
    cudaDeviceSynchronize();
}
bool enabled() const override {
"
251,">>> ) -> bool:
>>>     return unwrapped_params >= min_num_params
        backward_prefetch: (Optional[BackwardPrefetch]):
This is an experimental feature that is subject to change in the
the near future. It allows users to enable two different backward_prefetch
algorithms to help backward communication and computation overlapping.
",">>> ) -> bool:
>>>     return unwrapped_params >= min_num_params
        backward_prefetch (Optional[BackwardPrefetch]):
This is an experimental feature that is subject to change in the
the near future. It allows users to enable two different backward_prefetch
algorithms to help backward communication and computation overlapping.
"
252,"r""""""Functional interface""""""
from typing import Callable, List, Optional, Tuple
import math
import warnings
","r""""""Functional interface""""""
from typing import Callable, List, Optional, Tuple, Union
import math
import warnings
"
253,"params and grads to be on same device to work with optimizer. This
API is subject to change. Default is ``None`` in which case there
will be no offloading.
        auto_wrap_policy: (Optional [callable]):
A callable specifying a policy to recursively wrap layers with FSDP.
Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
","params and grads to be on same device to work with optimizer. This
API is subject to change. Default is ``None`` in which case there
will be no offloading.
        auto_wrap_policy (Optional[Callable]):
A callable specifying a policy to recursively wrap layers with FSDP.
Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
"
254,"else:
self.datapipe = datapipe.unbatch(unbatch_level=unbatch_level)
self.buffer_size = buffer_size
        self._shuffle_enabled = default
@staticmethod
def buffer_replace(buffer, x):
","else:
self.datapipe = datapipe.unbatch(unbatch_level=unbatch_level)
self.buffer_size = buffer_size
        self._enabled = True
@staticmethod
def buffer_replace(buffer, x):
"
255,"const std::function<size_t(const void*, size_t)>& writer_func,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info) {
  caffe2::serialize::PyTorchStreamWriter writer(writer_func);
  ScriptModuleSerializer serializer(writer);
  serializer.serialize(
      module, extra_files, bytecode_format, save_mobile_debug_info);
}
namespace {
","const std::function<size_t(const void*, size_t)>& writer_func,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info,
    bool use_flatbuffer) {
  if (use_flatbuffer) {
#if defined(ENABLE_FLATBUFFER)
    save_mobile_module_to(
        module, extra_files, save_mobile_debug_info, writer_func);
#else
    TORCH_CHECK(
        false,
        ""Trying to export as flatbuffer file but the build hasn't enabled flatbuffer"");
#endif
  } else {
    caffe2::serialize::PyTorchStreamWriter writer(writer_func);
    ScriptModuleSerializer serializer(writer);
    serializer.serialize(
        module, extra_files, bytecode_format, save_mobile_debug_info);
  }
}
namespace {
"
256,"reduce_range=reduce_range,
quant_min=quant_min,
quant_max=quant_max,
**kwargs
)
","reduce_range=reduce_range,
quant_min=quant_min,
quant_max=quant_max,
            eps=eps,
**kwargs
)
"
257,"std::shared_ptr<char> data,
size_t,
c10::optional<at::Device>) {
  TORCH_CHECK(
      mobile::serialization::ModuleBufferHasIdentifier(data.get()),
      ""Format error"");
auto* flatbuffer_module = mobile::serialization::GetMutableModule(data.get());
mobile::Module m = FlatbufferLoader().parseModule(flatbuffer_module);
m.set_delete_memory(std::move(data));
","std::shared_ptr<char> data,
size_t,
c10::optional<at::Device>) {
auto* flatbuffer_module = mobile::serialization::GetMutableModule(data.get());
mobile::Module m = FlatbufferLoader().parseModule(flatbuffer_module);
m.set_delete_memory(std::move(data));
"
258,"return storage_context_;
}
#if defined(ENABLE_FLATBUFFER)
void save_mobile_module_to(
    const Module& module,
    const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info,
    const std::function<size_t(const void*, size_t)>& writer_func) {
  CompilationOptions options = getOptionsFromGlobal();
  mobile::Module mod = jitModuleToMobile(module, options);
  auto buffer = save_mobile_module_to_bytes(mod, extra_files);
  writer_func(reinterpret_cast<void*>(buffer.data()), buffer.size());
}
#endif

void ExportModule(
const Module& module,
std::ostream& out,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info,
    bool use_flatbuffer) {
  auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
    out.write(static_cast<const char*>(buf), nbytes);
    return !out ? 0 : nbytes;
  };
  if (use_flatbuffer) {
#if defined(ENABLE_FLATBUFFER)
    save_mobile_module_to(
        module, extra_files, save_mobile_debug_info, writer_func);
#else
    TORCH_CHECK(
        false,
        ""Trying to export as flatbuffer file but the build hasn't enabled flatbuffer"");
#endif
  } else {
    caffe2::serialize::PyTorchStreamWriter writer(writer_func);
    ScriptModuleSerializer serializer(writer);
    serializer.serialize(
        module, extra_files, bytecode_format, save_mobile_debug_info);
  }
}
void ExportModule(
","return storage_context_;
}
void ExportModule(
const Module& module,
std::ostream& out,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info) {
  caffe2::serialize::PyTorchStreamWriter writer(
      [&](const void* buf, size_t nbytes) -> size_t {
        out.write(static_cast<const char*>(buf), nbytes);
        return !out ? 0 : nbytes;
      });
  ScriptModuleSerializer serializer(writer);
  serializer.serialize(
      module, extra_files, bytecode_format, save_mobile_debug_info);
}
void ExportModule(
"
259,"#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
","#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/file_format.h>
#if defined(ENABLE_FLATBUFFER)
#include <torch/csrc/jit/mobile/flatbuffer_loader.h>
#endif
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
"
260,"std::istream& in,
c10::optional<at::Device> device,
ExtraFilesMap& extra_files) {
  std::unique_ptr<IStreamAdapter> rai = std::make_unique<IStreamAdapter>(&in);
  auto module = _load_for_mobile(std::move(rai), device, extra_files);
  return module;
}
mobile::Module _load_for_mobile(
const std::string& filename,
c10::optional<at::Device> device,
ExtraFilesMap& extra_files) {
  std::unique_ptr<FileAdapter> rai = std::make_unique<FileAdapter>(filename);
  auto module = _load_for_mobile(std::move(rai), device, extra_files);
  return module;
}
mobile::Module _load_for_mobile(
","std::istream& in,
c10::optional<at::Device> device,
ExtraFilesMap& extra_files) {
  auto format = getFileFormat(in);
  switch (format) {
    case FileFormat::ZipFileFormat: {
      std::unique_ptr<IStreamAdapter> rai =
          std::make_unique<IStreamAdapter>(&in);
      auto module = _load_for_mobile(std::move(rai), device, extra_files);
      return module;
    }
#if defined(ENABLE_FLATBUFFER)
    case FileFormat::FlatbufferFileFormat: {
      std::shared_ptr<char> data;
      size_t size = 0;
      std::tie(data, size) = get_stream_content(in);
      auto* flatbuffer_module =
          mobile::serialization::GetMutableModule(data.get());
      mobile::Module m = initialize_mobile_module(flatbuffer_module);
      parseExtraFiles(flatbuffer_module, extra_files);
      return m;
    }
#else
    case FileFormat::FlatbufferFileFormat: {
      TORCH_CHECK(
          false,
          ""Flatbuffer input file but the build hasn't enabled flatbuffer"");
    }
#endif
    default: {
      TORCH_CHECK(false, ""Format error"");
    }
  }
}
mobile::Module _load_for_mobile(
const std::string& filename,
c10::optional<at::Device> device,
ExtraFilesMap& extra_files) {
  auto format = getFileFormat(filename);
  switch (format) {
    case FileFormat::ZipFileFormat: {
      std::unique_ptr<FileAdapter> rai =
          std::make_unique<FileAdapter>(filename);
      auto module = _load_for_mobile(std::move(rai), device, extra_files);
      return module;
    }
#if defined(ENABLE_FLATBUFFER)
    case FileFormat::FlatbufferFileFormat: {
      std::shared_ptr<char> data;
      size_t size = 0;
      std::tie(data, size) = get_file_content(filename.c_str());
      auto* flatbuffer_module =
          mobile::serialization::GetMutableModule(data.get());
      mobile::Module m = initialize_mobile_module(flatbuffer_module);
      parseExtraFiles(flatbuffer_module, extra_files);
      return m;
    }
#else
    case FileFormat::FlatbufferFileFormat: {
      TORCH_CHECK(
          false,
          ""Flatbuffer input file but the build hasn't enabled flatbuffer"");
    }
#endif
    default: {
      TORCH_CHECK(false, ""Format error"");
    }
  }
}
mobile::Module _load_for_mobile(
"
261,"""Module did not match against any action pattern. Extern, mock, or intern it.""
)
DENIED = ""Module was denied by a pattern.""
@dataclass
","""Module did not match against any action pattern. Extern, mock, or intern it.""
)
DENIED = ""Module was denied by a pattern.""
    MOCKED_BUT_STILL_USED = (
        ""Module was mocked out, but is still being used in the package. ""
        ""Please intern or extern the mocked modules if objects are supposed to be in ""
        ""the package.""
    )
@dataclass
"
262,"pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()

name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
","pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()
        mocked_modules = defaultdict(list)
name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
"
263,"payloadStart = payloadSection->start;
customLoader = s.customLoader;
size = payloadSection->len;
      TORCH_CHECK(payloadSection.has_value(), ""Missing the payload section"");
break;
}
}
","payloadStart = payloadSection->start;
customLoader = s.customLoader;
size = payloadSection->len;
      MULTIPY_CHECK(payloadSection.has_value(), ""Missing the payload section"");
break;
}
}
"
264,"i = 0;
for (const auto p : c10::irange(params.size())) {
const auto& t = params[p];
    // I'd like to include which process we are in the message,
    // but ProcessGroup::getRank is not public!
for (const auto& sz : t.sizes()) {
      auto msg = c10::str(""params["", p, ""] in this process"",
"" with sizes "",
t.sizes(),
"" appears not to match sizes of the same param in process 0."");
","i = 0;
for (const auto p : c10::irange(params.size())) {
const auto& t = params[p];
for (const auto& sz : t.sizes()) {
      auto msg = c10::str(""["", process_group->getRank(),
                        ""]: params["", p, ""] in this process"",
"" with sizes "",
t.sizes(),
"" appears not to match sizes of the same param in process 0."");
"
265,"params and grads to be on same device to work with optimizer. This
API is subject to change. Default is ``None`` in which case there
will be no offloading.
        fsdp_auto_wrap_policy: (Optional [callable]):
A callable specifying a policy to recursively wrap layers with FSDP.
Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
the returned FSDP root instance.
``default_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is
            an example of ``fsdp_auto_wrap_policy`` callable, this policy wraps layers
with parameter sizes larger than 100M. Users can supply the customized
            ``fsdp_auto_wrap_policy`` callable that should accept following arguments:
``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``,
extra customized arguments could be added to the customized
            ``fsdp_auto_wrap_policy`` callable as well.
Example::
","params and grads to be on same device to work with optimizer. This
API is subject to change. Default is ``None`` in which case there
will be no offloading.
        auto_wrap_policy: (Optional [callable]):
A callable specifying a policy to recursively wrap layers with FSDP.
Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
the returned FSDP root instance.
``default_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is
            an example of ``auto_wrap_policy`` callable, this policy wraps layers
with parameter sizes larger than 100M. Users can supply the customized
            ``auto_wrap_policy`` callable that should accept following arguments:
``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``,
extra customized arguments could be added to the customized
            ``auto_wrap_policy`` callable as well.
Example::
"
266,"from ._symbolic_trace import Tracer
from ._compatibility import compatibility
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
@compatibility(is_backward_compatible=True)
class Interpreter:
","from ._symbolic_trace import Tracer
from ._compatibility import compatibility
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
import inspect
@compatibility(is_backward_compatible=True)
class Interpreter:
"
267,"""""""
Initializes internal Module state, shared by both nn.Module and ScriptModule.
""""""
torch._C._log_api_usage_once(""python.nn_module"")
self.training = True
","""""""
Initializes internal Module state, shared by both nn.Module and ScriptModule.
""""""
        super().__init__()

torch._C._log_api_usage_once(""python.nn_module"")
self.training = True
"
268,"if self._validate_args:
self._validate_sample(value)
diag_elems = value.diagonal(dim1=-1, dim2=-2)[..., 1:]
        order = torch.arange(2, self.dim + 1)
order = 2 * (self.concentration - 1).unsqueeze(-1) + self.dim - order
unnormalized_log_pdf = torch.sum(order * diag_elems.log(), dim=-1)
# Compute normalization constant (page 1999 of [1])
","if self._validate_args:
self._validate_sample(value)
diag_elems = value.diagonal(dim1=-1, dim2=-2)[..., 1:]
        order = torch.arange(2, self.dim + 1, device=self.concentration.device)
order = 2 * (self.concentration - 1).unsqueeze(-1) + self.dim - order
unnormalized_log_pdf = torch.sum(order * diag_elems.log(), dim=-1)
# Compute normalization constant (page 1999 of [1])
"
269,"}
return [](ProcessedNode* p_node) {
const auto inputs = p_node->Input(0).toTensorVector();
const auto dim = p_node->Input(1).toInt();
if (p_node->Output(0).isNone()) {
p_node->Output(0) = at::native::_stack_cpu(inputs, dim);
","}
return [](ProcessedNode* p_node) {
const auto inputs = p_node->Input(0).toTensorVector();
    TORCH_CHECK(inputs.size() > 0, ""stack expects non-empty tensor list"");
const auto dim = p_node->Input(1).toInt();
if (p_node->Output(0).isNone()) {
p_node->Output(0) = at::native::_stack_cpu(inputs, dim);
"
270,"#include <torch/csrc/autograd/python_variable.h>

#include <torch/csrc/THP.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
","#include <torch/csrc/THP.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
"
271,"void BlockRunner::set_inputs(
IValueList&& args,
const std::unordered_map<std::string, c10::IValue>& kwargs) {
  const auto total_num_inputs =
      args.size() + kwargs.size() + first_input_is_self_;
  TORCH_CHECK(total_num_inputs == block_info_.num_inputs());

const auto& schema = static_module_.schema();
if (first_input_is_self_) {
Input(0) = static_module_.module()._ivalue();
","void BlockRunner::set_inputs(
IValueList&& args,
const std::unordered_map<std::string, c10::IValue>& kwargs) {
const auto& schema = static_module_.schema();
if (first_input_is_self_) {
Input(0) = static_module_.module()._ivalue();
"
272,"if (!is_root_block_ || C10_UNLIKELY(!schema)) {
TORCH_CHECK(
kwargs.empty(), ""Schema is not available, but BlockRunner got kwargs."");
for (size_t i = 0; i < args.size(); ++i) {
set_arg(i, std::forward<IValueList>(args));
}
","if (!is_root_block_ || C10_UNLIKELY(!schema)) {
TORCH_CHECK(
kwargs.empty(), ""Schema is not available, but BlockRunner got kwargs."");

    const auto total_num_inputs = args.size() + first_input_is_self_;
    TORCH_CHECK(total_num_inputs == block_info_.num_inputs());

for (size_t i = 0; i < args.size(); ++i) {
set_arg(i, std::forward<IValueList>(args));
}
"
273,"const auto& schema_args = schema->arguments();
size_t consumed_kwargs = 0;
DCHECK(schema_args.size() > 0);

for (size_t i = 0; i < schema_args.size() - 1; ++i) {
// Start at 1 since the schema always contains `self`.
const auto& schema_arg = schema_args[i + 1];
","const auto& schema_args = schema->arguments();
size_t consumed_kwargs = 0;
DCHECK(schema_args.size() > 0);
  TORCH_CHECK(
      args.size() < schema_args.size(),
      ""Static runtime got too many arguments"");
for (size_t i = 0; i < schema_args.size() - 1; ++i) {
// Start at 1 since the schema always contains `self`.
const auto& schema_arg = schema_args[i + 1];
"
274,"import collections.abc
from contextlib import contextmanager
from typing import Optional, List, Sequence
","import collections.abc
import copy
from contextlib import contextmanager
from typing import Optional, List, Sequence
"
275,"END_HANDLE_TH_ERRORS
}
// Makes sure that we don't check for __torch_function__ on basic Python types
static bool is_basic_python_type(PyTypeObject *tp)
{
","END_HANDLE_TH_ERRORS
}
PyObject* THPModule_disable_torch_dispatch(PyObject *self, PyObject *a) {
  HANDLE_TH_ERRORS
  PyObject *func=nullptr, *types=nullptr, *args=nullptr, *kwargs=nullptr;
  if (!PyArg_ParseTuple(a, ""OO|OO"", &func, &types, &args, &kwargs)) {
    return nullptr;
  }
  py::tuple py_args;
  if (args == nullptr) {
    py_args = py::make_tuple();
  }
  else {
    py_args = py::reinterpret_borrow<py::tuple>(args);
  }

  // This implementation is not completely correct.  The moral
  // meaning of this function is that we should do a redispatch
  // ""after"" PythonKey, aka a redispatch() call.  But we don't have a
  // dispatcher call here; we have an opaque Python object.
  //
  // What we have here is a close approximation: instead of redispatch(), we
  // just exclude Python and all the keys before it, so that we will go
  // to the next key after Python.  The difference, however, is we are
  // now PERMANENTLY after Python.  We don't think there are any legitimate
  // cases where we want to go for another round on the entire dispatcher key
  // set, but if there are, then we will have to do something else here.
  c10::impl::ExcludeDispatchKeyGuard guard_(
      // TODO: add constructor for this specifically
      c10::DispatchKeySet(c10::DispatchKeySet::FULL) -
      c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Python)
      // NB: off by one hazard here, but it works out: python key is not
      // included in AFTER, so it is included in the negation (and that's
      // correct: we want to exclude Python key and everything BEFORE it.)
  );
  return PyObject_Call(func, py_args.ptr(), kwargs);
  END_HANDLE_TH_ERRORS
}

// Makes sure that we don't check for __torch_function__ on basic Python types
static bool is_basic_python_type(PyTypeObject *tp)
{
"
276,"namespace {
struct SchemaParser {
  SchemaParser(const std::string& str)
      : L(std::make_shared<SourceView>(c10::string_view(str))),
type_parser(L, /*parse_complete_tensor_types*/ false) {}
either<OperatorName, FunctionSchema> parseDeclaration() {
","namespace {
struct SchemaParser {
  explicit SchemaParser(const std::string& str)
      : L(std::make_shared<Source>(
            c10::string_view(str),
            c10::nullopt,
            0,
            nullptr,
            Source::DONT_COPY)),
type_parser(L, /*parse_complete_tensor_types*/ false) {}
either<OperatorName, FunctionSchema> parseDeclaration() {
"
277,"return;
}
  c10::string_view str = source_view_->text();
if (size() == str.size()) {
// this is just the entire file, not a subset, so print it out.
// primarily used to print out python stack traces
","return;
}
  auto str = source_view_->text_str().str();
if (size() == str.size()) {
// this is just the entire file, not a subset, so print it out.
// primarily used to print out python stack traces
"
278,"return static_cast<int64_t>((*self)->starting_line_no());
})
.def(""text"", [](const c10::intrusive_ptr<SourceRef>& self) {
        return (*self)->text();
});
torch::class_<InstructionStats>(""profiling"", ""InstructionStats"")
","return static_cast<int64_t>((*self)->starting_line_no());
})
.def(""text"", [](const c10::intrusive_ptr<SourceRef>& self) {
        return (*self)->text_str().str();
});
torch::class_<InstructionStats>(""profiling"", ""InstructionStats"")
"
279,"serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<SourceView>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
  if (s == nullptr) {
    serialized = c10::ivalue::Tuple::create({"""", """", 0});
} else {
    serialized = c10::ivalue::Tuple::create(
        {s->text(), s->filename(), (int64_t)s->starting_line_no()});
}
serialized_sources[s] = serialized;
return serialized;
","serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
int64_t SourceRangeSerializer::store_text_and_get_index(
    const std::string& text_view) {
  auto text_iter = text_to_idx_.find(text_view);
  if (text_iter == text_to_idx_.end()) {
    int64_t text_pos = static_cast<int64_t>(texts_.size());
    texts_.emplace_back(text_view);
    text_to_idx_[texts_.back().toStringView()] = text_pos;
    return text_pos;
  } else {
    return text_iter->second;
  }
}

c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<Source>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
  c10::List<int64_t> lines;
  if (should_use_format_with_string_table_) {
    if (s == nullptr) {
      serialized = c10::ivalue::Tuple::create({lines, 0, 0});
    } else {
      for (size_t lineno = 0; lineno < s->num_lines(); lineno++) {
        std::string line_content = s->get_line(lineno).str();
        int64_t text_pos = store_text_and_get_index(line_content);
        lines.push_back(text_pos);
      }

      int64_t fname_pos = 0;
      if (s->filename().has_value()) {
        fname_pos = store_text_and_get_index(*s->filename());
      }
      serialized = c10::ivalue::Tuple::create(
          {lines, fname_pos, (int64_t)s->starting_line_no()});
    }
} else {
    if (s == nullptr) {
      serialized = c10::ivalue::Tuple::create({"""", """", 0});
    } else {
      serialized = c10::ivalue::Tuple::create(
          {s->text_str().str(), s->filename(), (int64_t)s->starting_line_no()});
    }
}
serialized_sources[s] = serialized;
return serialized;
"
280,"return c10::nullopt;
}
} // namespace jit
} // namespace torch
","return c10::nullopt;
}
TORCH_API void setShouldUseFormatWithStringTable(
    bool should_use_format_with_string_table) {
  should_use_format_with_string_table_ = should_use_format_with_string_table;
}

} // namespace jit
} // namespace torch
"
281,"std::vector<at::Tensor> shapeTensors;
shapeTensors.reserve(tensors.size());
for (const auto& tensor : tensors) {
    auto shapesVec = tensor.sizes().vec();
    int64_t shapes_size = shapesVec.size();
    // Need to clone here otherwise the shapesVec.data() memory is not copied
    // and can be released under the hood.
    at::Tensor shapesTensor = at::from_blob(
                                  shapesVec.data(),
                                  {shapes_size},
                                  at::TensorOptions().dtype(at::kLong))
                                  .clone();
shapeTensors.emplace_back(std::move(shapesTensor));
}
return shapeTensors;
","std::vector<at::Tensor> shapeTensors;
shapeTensors.reserve(tensors.size());
for (const auto& tensor : tensors) {
    // Use `at::tensor()` to copy the data underlying `sizes()` since it may be
    // released elsewhere.
    at::Tensor shapesTensor =
        at::tensor(tensor.sizes(), at::TensorOptions().dtype(at::kLong));
shapeTensors.emplace_back(std::move(shapesTensor));
}
return shapeTensors;
"
282,"""""""
raise RuntimeError(
""Reached a code path in Module.get_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md ""
""to report this bug."")
def set_extra_state(self, state: Any):
","""""""
raise RuntimeError(
""Reached a code path in Module.get_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml ""
""to report this bug."")
def set_extra_state(self, state: Any):
"
283,"if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer->deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
","if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer.deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
"
284,"at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalues =
          std::move(*jit::unpickle(
                         reinterpret_cast<const char*>(debug_data.get()),
                         debug_size,
                         nullptr,
                         {},
                         c10::parseType)
                         .toTuple())
              .elements();
      SourceRangeDeserializer deserializer;
      for (auto& val : ivalues) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
","at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalueTuple = jit::unpickle(
          reinterpret_cast<const char*>(debug_data.get()),
          debug_size,
          nullptr,
          {},
          c10::parseType);
      const auto& ivalues = ivalueTuple.toTuple()->elements();
      IValue lines;
      std::unique_ptr<SourceRangeDeserializer> deserializer;
      if (ivalues.size() == 3 && ivalues[0].isString() &&
          kFormatWithStringTable == ivalues[0].toStringRef()) {
        // new format
        deserializer = std::make_unique<SourceRangeDeserializer>(ivalues[1]);
        lines = ivalues[2];
      } else {
        deserializer = std::make_unique<SourceRangeDeserializer>();
        lines = ivalueTuple;
      }

      for (auto& val : lines.toTuple()->elements()) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
"
285,"errors_.clear();
for (::addrinfo* addr = naked_result; addr != nullptr; addr = addr->ai_next) {
      C10D_INFO(""The client socket is attempting to connect to {}."", *addr);
ConnectResult cr = tryConnect(*addr);
if (cr == ConnectResult::Success) {
","errors_.clear();
for (::addrinfo* addr = naked_result; addr != nullptr; addr = addr->ai_next) {
      C10D_TRACE(""The client socket is attempting to connect to {}."", *addr);
ConnectResult cr = tryConnect(*addr);
if (cr == ConnectResult::Success) {
"
286,"self.main_datapipe,
self.num_instances,
self.buffer_size,
            dill_function,
self.drop_none,
) = state
        if DILL_AVAILABLE:
            self.classifier_fn = dill.loads(dill_function)  # type: ignore[assignment]
        else:
            self.classifier_fn = dill_function  # type: ignore[assignment]
self._datapipe_iterator = None
self.current_buffer_usage = 0
self.child_buffers = [deque() for _ in range(self.num_instances)]
","self.main_datapipe,
self.num_instances,
self.buffer_size,
            serialized_fn_with_method,
self.drop_none,
) = state
        self.classifier_fn = deserialize_fn(serialized_fn_with_method)
self._datapipe_iterator = None
self.current_buffer_usage = 0
self.child_buffers = [deque() for _ in range(self.num_instances)]
"
287,") -> None:
super().__init__()
self.datapipe = datapipe
        # Partial object has no attribute '__name__', but can be pickled
        if hasattr(filter_fn, '__name__') and filter_fn.__name__ == '<lambda>' and not DILL_AVAILABLE:
            warnings.warn(""Lambda function is not supported for pickle, please use ""
                          ""regular python function or functools.partial instead."")
self.filter_fn = filter_fn  # type: ignore[assignment]
self.drop_empty_batches = drop_empty_batches
",") -> None:
super().__init__()
self.datapipe = datapipe
        check_lambda_fn(filter_fn)

self.filter_fn = filter_fn  # type: ignore[assignment]
self.drop_empty_batches = drop_empty_batches
"
288,"import warnings
from typing import Callable, TypeVar

from torch.utils.data import MapDataPipe, functional_datapipe
try:
    import dill

    # XXX: By default, dill writes the Pickler dispatch table to inject its
    # own logic there. This globally affects the behavior of the standard library
    # pickler for any user who transitively depends on this module!
    # Undo this extension to avoid altering the behavior of the pickler globally.
    dill.extend(use_dill=False)
    DILL_AVAILABLE = True
except ImportError:
    DILL_AVAILABLE = False

T_co = TypeVar('T_co', covariant=True)
","from torch.utils.data.datapipes.utils.common import check_lambda_fn
from typing import Callable, TypeVar
from torch.utils.data import MapDataPipe, functional_datapipe
T_co = TypeVar('T_co', covariant=True)
"
289,"{},
c10::parseType);
ska::flat_hash_map<int64_t, DebugInfoTuple> callstack_ptrs;
  auto ivalues = std::move(*std::move(ival).toTuple()).elements();
for (auto& val : ivalues) {
const auto& tup_elems = val.toTupleRef().elements();
TORCH_CHECK(
","{},
c10::parseType);
ska::flat_hash_map<int64_t, DebugInfoTuple> callstack_ptrs;
  const auto& ivalues = ival.toTupleRef().elements();
for (auto& val : ivalues) {
const auto& tup_elems = val.toTupleRef().elements();
TORCH_CHECK(
"
290,"const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
  const bool isQAChannelsLast = isNHWC(qa);
  const bool isQBChannelsLast = isNHWC(qb);
auto ResultBuf = (isQAChannelsLast || isQBChannelsLast)
      ? makeQBufHandleNHWC(
""quantized_add"",
outputShape,
Dtype(out_qdtype),
out_qscale,
out_qzero)
      : makeQBufHandleNCHW(
""quantized_add"",
outputShape,
Dtype(out_qdtype),
","const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
  const bool isQAChannelsLast = isChannelsLast(qa);
  const bool isQBChannelsLast = isChannelsLast(qb);
auto ResultBuf = (isQAChannelsLast || isQBChannelsLast)
      ? makeQBufHandleChannelsLast(
""quantized_add"",
outputShape,
Dtype(out_qdtype),
out_qscale,
out_qzero)
      : makeQBufHandleContiguous(
""quantized_add"",
outputShape,
Dtype(out_qdtype),
"
291,"# Sync params and buffers. Ensures all DDP models start off at the same value.
self._sync_params_and_buffers(authoritative_rank=0)
# In debug mode, build a mapping of parameter index -> parameter.
        if dist._get_debug_mode() != dist._DistributedDebugLevel.OFF:
            param_to_name_mapping = self._build_param_to_name_mapping(parameters)
        else:
            param_to_name_mapping = {}
# Builds reducer.
self._ddp_init_helper(parameters, expect_sparse_gradient, param_to_name_mapping)
self._has_rebuilt_buckets = False
","# Sync params and buffers. Ensures all DDP models start off at the same value.
self._sync_params_and_buffers(authoritative_rank=0)
# In debug mode, build a mapping of parameter index -> parameter.
        param_to_name_mapping = self._build_debug_param_to_name_mapping(parameters)
# Builds reducer.
self._ddp_init_helper(parameters, expect_sparse_gradient, param_to_name_mapping)
self._has_rebuilt_buckets = False
"
292,"def __init__(self, *args):
self._importers: List[Importer] = list(args)
    def _check_if_fileless_package(self, module):
if not hasattr(module, ""__path__""):
return False
if not hasattr(module, ""__file__""):
","def __init__(self, *args):
self._importers: List[Importer] = list(args)
    def _is_torchpackage_dummy(self, module):
        """"""Returns true iff this module is an empty PackageNode in a torch.package.

        If you intern `a.b` but never use `a` in your code, then `a` will be an
        empty module with no source. This can break cases where we are trying to
        re-package an object after adding a real dependency on `a`, since
        OrderedImportere will resolve `a` to the dummy package and stop there.

        See: https://github.com/pytorch/pytorch/pull/71520#issuecomment-1029603769
        """"""
        if not getattr(module, ""__torch_package__"", False):
            return False
if not hasattr(module, ""__path__""):
return False
if not hasattr(module, ""__file__""):
"
293,")
try:
module = importer.import_module(module_name)
                if self._check_if_fileless_package(module):
continue
return module
except ModuleNotFoundError as err:
",")
try:
module = importer.import_module(module_name)
                if self._is_torchpackage_dummy(module):
continue
return module
except ModuleNotFoundError as err:
"
294,"} // namespace native
} // namespace at
#else // AT_MKLDNN_EBABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
#include <ATen/native/mkldnn/Utils.h>
","} // namespace native
} // namespace at
#else // AT_MKLDNN_ENABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
#include <ATen/native/mkldnn/Utils.h>
"
295,"} // namespace native
} // namespace at
#else // AT_MKLDNN_EBABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
","} // namespace native
} // namespace at
#else // AT_MKLDNN_ENABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
"
296,"} // namespace native
} // namespace at
#else // AT_MKLDNN_EBABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
","} // namespace native
} // namespace at
#else // AT_MKLDNN_ENABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
"
297,"return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)
if p < 0.0 or p > 1.0:
raise ValueError(""dropout probability has to be between 0 and 1, "" ""but got {}"".format(p))
    return _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
def dropout3d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
","return handle_torch_function(dropout2d, (input,), input, p=p, training=training, inplace=inplace)
if p < 0.0 or p > 1.0:
raise ValueError(""dropout probability has to be between 0 and 1, "" ""but got {}"".format(p))
    inp_dim = input.dim()
    if inp_dim not in (3, 4):
        warn_msg = (f""dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated ""
                    ""and will result in an error in a future release. To retain the behavior ""
                    ""and silence this warning, please use dropout instead. Note that dropout2d ""
                    ""exists to provide channel-wise dropout on inputs with 2 spatial dimensions, ""
                    ""a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs)."")
        warnings.warn(warn_msg)

    is_batched = inp_dim == 4
    if not is_batched:
        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)

    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)

    if not is_batched:
        result = result.squeeze_(0) if inplace else result.squeeze(0)

    return result
def dropout3d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:
"
298,"for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
      if (select_indices_data[i] != padding_idx) {
at::native::cpublas::axpy<float>(
ddim,
1,
            src_data + src_stride0 * select_indices_data[i],
src_stride1,
output_data + output_stride0 * add_indices_data[i],
output_stride1);
","for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
      auto idx = select_indices_data[i];
      TORCH_CHECK(
          idx >= 0 && idx < vocab_size,
          ""embedding_bag: Expected idx >= 0 && idx < num_embeddings but found idx to be "",
          idx);
      if (idx != padding_idx) {
at::native::cpublas::axpy<float>(
ddim,
1,
            src_data + src_stride0 * idx,
src_stride1,
output_data + output_stride0 * add_indices_data[i],
output_stride1);
"
299,"for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
      if (select_indices_data[i] != padding_idx) {
        auto* src_base = src_data + src_stride0 * select_indices_data[i];
auto* output_base = output_data + output_stride0 * add_indices_data[i];
auto scale = scale_data[i * scale_stride];
for (const auto j : c10::irange(ddim)) {
","for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
      auto idx = select_indices_data[i];
      TORCH_CHECK(
          idx >= 0 && idx < vocab_size,
          ""embedding_bag: Expected idx >= 0 && idx < num_embeddings but found idx to be "",
          idx);
      if (idx != padding_idx) {
        auto* src_base = src_data + src_stride0 * idx;
auto* output_base = output_data + output_stride0 * add_indices_data[i];
auto scale = scale_data[i * scale_stride];
for (const auto j : c10::irange(ddim)) {
"
300,"auto size = sizes[stride_index];
auto stride = strides[stride_index];
auto index = absolute_position / ExprHandle(stride);
          auto one = Cast::make(size.dtype(), 1);
          // if the size is one, we don't advance the absolute position
          // which would give 0
          auto non_one_position = absolute_position % ExprHandle(stride);
          absolute_position = CompareSelect::make(
              size, one, absolute_position, non_one_position, kEQ);
new_axes[stride_index] = index;
}
return BufHandle(buf).load(new_axes);
","auto size = sizes[stride_index];
auto stride = strides[stride_index];
auto index = absolute_position / ExprHandle(stride);
          // XXX, in symbolic output ordering, we do not the arbitrary
          // ordering of strides as in usual output ordering, just
          // channels last, so even in the presence of size == 1
          // we produce correct output here
          absolute_position = absolute_position % ExprHandle(stride);
new_axes[stride_index] = index;
}
return BufHandle(buf).load(new_axes);
"
301,"auto zero = LongImm::make(0);
std::vector<ExprPtr> default_strides = make_contiguous_strides(sizes);
// See explanation in convertOutputToCorrectStrides
  return convertOutputToCorrectStrides(
sizes, sorted_stride_indices, strides, buf);
}
","auto zero = LongImm::make(0);
std::vector<ExprPtr> default_strides = make_contiguous_strides(sizes);
// See explanation in convertOutputToCorrectStrides
  return convertSymbolicOutputToCorrectStrides(
sizes, sorted_stride_indices, strides, buf);
}
"
302,"return std::get<0>(at::max_pool1d_with_indices(
self, kernel_size, stride, padding, dilation, ceil_mode));
}

  Tensor result = [&]() {
    NoNamesGuard guard;
    return at::_max_pool1d_cpu_forward(
        self, kernel_size, stride, padding, dilation, ceil_mode);
  }();
  namedinference::propagate_names(result, self);
  return result;
}
} // namespace native
","return std::get<0>(at::max_pool1d_with_indices(
self, kernel_size, stride, padding, dilation, ceil_mode));
}
  return max_pool1d_impl(
      self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // namespace native
"
303,"}
}
void ScriptModuleSerializer::convertNamedType(
const c10::NamedTypePtr& class_type) {
if (converted_types_.count(class_type)) {
","}
}
namespace {

c10::optional<std::string> type_printer(
    const c10::Type& type,
    torch::jit::TypeNameUniquer& type_name_uniquer) {
  if (auto dyn = type.castRaw<c10::DynamicType>()) {
    return dyn->fallback()->annotation_str(
        [&](auto&& t) { return type_printer(t, type_name_uniquer); });
  }
  auto namedType = type.cast<c10::NamedType>();
  if (namedType && namedType->name()) {
    return type_name_uniquer.getUniqueName(namedType).qualifiedName();
  }
  return c10::nullopt;
}

} // namespace

void ScriptModuleSerializer::convertNamedType(
const c10::NamedTypePtr& class_type) {
if (converted_types_.count(class_type)) {
"
304,"#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/mobile/upgrader_mobile.h>
namespace c10 {
TypePtr parseType(const std::string& pythonStr);
} // namespace c10
","/**
 * @generated
 * This is an auto-generated file. Please do not modify it by hand.
 * To re-generate, please run:
 * cd ~/pytorch && python torch/csrc/jit/mobile/upgrader_mobile.cpp
 */

#include <torch/csrc/jit/mobile/upgrader_mobile.h>
#include <ATen/core/ivalue.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/mobile/type_parser.h>

namespace c10 {
TypePtr parseType(const std::string& pythonStr);
} // namespace c10
"
305,"#include <torch/csrc/utils/python_strings.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
#include <exception>
#include <functional>
","#include <torch/csrc/utils/python_strings.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
#include <ATen/FuncTorchTLS.h>
#include <exception>
#include <functional>
"
306,"}
auto ConvParams::use_cudnn(const at::Tensor& input, const at::Tensor& weight) const -> bool {
if (needs_64bit_indexing_no_split(input, weight)) {
return false;
}
","}
auto ConvParams::use_cudnn(const at::Tensor& input, const at::Tensor& weight) const -> bool {

// Note [Mobile check segfaults]
// cudnn and miopen are guaranteed not to be on mobile, and T102591915 / T110194934 suggest
// that maybe the compiledWithCuDNN() check sometimes segfaults (though I can't imagine how)
#if !defined(C10_MOBILE)
if (needs_64bit_indexing_no_split(input, weight)) {
return false;
}
"
307,"}
}
return !is_output_padding_big();
}
auto ConvParams::use_miopen(const at::Tensor& input, const at::Tensor& weight, bool bias_defined) const -> bool {
","}
}
return !is_output_padding_big();
#else
  return false;
#endif
}
auto ConvParams::use_miopen(const at::Tensor& input, const at::Tensor& weight, bool bias_defined) const -> bool {
"
308,"for ref_class in LOWER_MODULE_MAP.keys():
model = _lower_weighted_ref_module(model, ref_class)
model.recompile()
for pattern, replacement in get_fbgemm_patterns_and_replacements():
subgraph_rewriter_FORKED_DO_NOT_USE.replace_pattern(model, pattern, replacement)
model.graph.lint()
return model
","for ref_class in LOWER_MODULE_MAP.keys():
model = _lower_weighted_ref_module(model, ref_class)
model.recompile()

for pattern, replacement in get_fbgemm_patterns_and_replacements():
subgraph_rewriter_FORKED_DO_NOT_USE.replace_pattern(model, pattern, replacement)

    special_pattern_replacement(model)

model.graph.lint()
return model
"
309,"return std::get<0>(at::max_pool1d_with_indices(
self, kernel_size, stride, padding, dilation, ceil_mode));
}
  return max_pool1d_impl(
      self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // namespace native
","return std::get<0>(at::max_pool1d_with_indices(
self, kernel_size, stride, padding, dilation, ceil_mode));
}

  Tensor result = [&]() {
    NoNamesGuard guard;
    return at::_max_pool1d_cpu_forward(
        self, kernel_size, stride, padding, dilation, ceil_mode);
  }();
  namedinference::propagate_names(result, self);
  return result;
}
} // namespace native
"
310,"base_mod_attrs : Dict[str, torch.fx.graph_module.GraphModule] = {}
for node in m.graph.nodes:
if node.op == 'placeholder':
            base_mod_env[node.name] = base_mod_graph.placeholder(node.name)
base_mod_env[node.name].meta = node.meta.copy()
elif node.op == 'get_attr':
base_mod_env[node.name] = base_mod_graph.get_attr(node.target)
","base_mod_attrs : Dict[str, torch.fx.graph_module.GraphModule] = {}
for node in m.graph.nodes:
if node.op == 'placeholder':
            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty
            base_mod_env[node.name] = base_mod_graph.placeholder(
                node.name, type_expr=node.type, default_value=default_value)
base_mod_env[node.name].meta = node.meta.copy()
elif node.op == 'get_attr':
base_mod_env[node.name] = base_mod_graph.get_attr(node.target)
"
311,"device, tensor = packed
return tensor.to(device, non_blocking=pin_memory)
        self.pack_hook = pack_to_cpu
        self.unpack_hook = unpack_from_cpu

    def __enter__(self):
        torch._C._autograd._register_saved_tensors_default_hooks(self.pack_hook, self.unpack_hook)

    def __exit__(self, *args: Any):
        torch._C._autograd._reset_saved_tensors_default_hooks()
","device, tensor = packed
return tensor.to(device, non_blocking=pin_memory)
        super().__init__(pack_to_cpu, unpack_from_cpu)
"
312,"}
}
  void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    PyObject *pack_hook_(nullptr), *unpack_hook_(nullptr);
    std::tie(pack_hook_, unpack_hook_) = at::SavedTensorDefaultHooks::get_hooks();
    TORCH_CHECK(!pack_hook_ && !unpack_hook_,
        ""Setting default hooks but they have already been set. ""
        ""Hint: only one pair of hooks is allowed at a time."");
at::SavedTensorDefaultHooks::enable();
    at::SavedTensorDefaultHooks::set_hooks(pack_hook.release().ptr(), unpack_hook.release().ptr());
}
  void PyDefaultSavedVariableHooks::reset_hooks() {
PyObject *pack_hook(nullptr), *unpack_hook(nullptr);
std::tie(pack_hook, unpack_hook) = at::SavedTensorDefaultHooks::get_hooks();
if (Py_IsInitialized()) {
py::gil_scoped_acquire gil;
Py_XDECREF(pack_hook);
Py_XDECREF(unpack_hook);
}
    at::SavedTensorDefaultHooks::set_hooks(nullptr, nullptr);
}
std::unique_ptr<SavedVariableHooks> PyDefaultSavedVariableHooks::get_hooks() {
","}
}
  void PyDefaultSavedVariableHooks::push_hooks(py::function &pack_hook, py::function &unpack_hook) {
at::SavedTensorDefaultHooks::enable();
    at::SavedTensorDefaultHooks::push_hooks(pack_hook.release().ptr(), unpack_hook.release().ptr());
}
  void PyDefaultSavedVariableHooks::pop_hooks() {
PyObject *pack_hook(nullptr), *unpack_hook(nullptr);
std::tie(pack_hook, unpack_hook) = at::SavedTensorDefaultHooks::get_hooks();
    TORCH_INTERNAL_ASSERT(pack_hook != nullptr && unpack_hook != nullptr);
if (Py_IsInitialized()) {
py::gil_scoped_acquire gil;
Py_XDECREF(pack_hook);
Py_XDECREF(unpack_hook);
}
    at::SavedTensorDefaultHooks::pop_hooks();
}
std::unique_ptr<SavedVariableHooks> PyDefaultSavedVariableHooks::get_hooks() {
"
313,"add_docstr_all('ravel',
r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""")
","add_docstr_all('ravel',
r""""""
ravel() -> Tensor
see :func:`torch.ravel`
"""""")
"
314,"add_docstr_all('tril',
r""""""
tril(k=0) -> Tensor
See :func:`torch.tril`
"""""")
add_docstr_all('tril_',
r""""""
tril_(k=0) -> Tensor
In-place version of :meth:`~Tensor.tril`
"""""")
add_docstr_all('triu',
r""""""
triu(k=0) -> Tensor
See :func:`torch.triu`
"""""")
add_docstr_all('triu_',
r""""""
triu_(k=0) -> Tensor
In-place version of :meth:`~Tensor.triu`
"""""")
","add_docstr_all('tril',
r""""""
tril(diagonal=0) -> Tensor
See :func:`torch.tril`
"""""")
add_docstr_all('tril_',
r""""""
tril_(diagonal=0) -> Tensor
In-place version of :meth:`~Tensor.tril`
"""""")
add_docstr_all('triu',
r""""""
triu(diagonal=0) -> Tensor
See :func:`torch.triu`
"""""")
add_docstr_all('triu_',
r""""""
triu_(diagonal=0) -> Tensor
In-place version of :meth:`~Tensor.triu`
"""""")
"
315,"assert node.outputsSize() == 1
in_id, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
        out_id = self.add_tensor_operand(node.outputsAt(0), in_oper)
for idx, dim in enumerate(in_oper.shape):
if dim == 0:
","assert node.outputsSize() == 1
in_id, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))

        out_oper = in_oper
        if opcode == NNAPI_OperationCode.LOGISTIC:
            # NNAPI docs: For ANEURALNETWORKS_TENSOR_QUANT8_ASYMM, the scale
            # must be 1.f / 256 and the zeroPoint must be 0.
            # https://fburl.com/h52stoog
            if in_oper.op_type == NNAPI_OperandCode.TENSOR_QUANT8_ASYMM:
                out_oper = in_oper._replace(zero_point=0, scale=1.0 / 256)

        out_id = self.add_tensor_operand(node.outputsAt(0), out_oper)
for idx, dim in enumerate(in_oper.shape):
if dim == 0:
"
316,"#include <ATen/NativeFunctions.h>
#include <ATen/AccumulateType.h>
#include <ATen/Parallel.h>
#include <ATen/Dispatch.h>
#include <ATen/native/DispatchStub.h>
#include <ATen/native/TensorIterator.h>
#include <c10/util/irange.h>
#include <cmath>
","#include <ATen/native/RangeFactories.h>
#include <ATen/NativeFunctions.h>
#include <ATen/AccumulateType.h>
#include <ATen/Parallel.h>
#include <ATen/Dispatch.h>
#include <ATen/native/TensorIterator.h>
#include <c10/util/irange.h>
#include <cmath>
"
317,"#ifdef CPU_CAPABILITY_AVX512
// normal_stub isn't being dispatched to AVX512 because it exposes
// flakiness in test_sgd of test/test_optim.py
REGISTER_NO_AVX512_DISPATCH(normal_stub, void(*)(Tensor&, const double, const double, c10::optional<Generator>));
#else
REGISTER_DISPATCH(normal_stub, &normal_kernel);
#endif
","#ifdef CPU_CAPABILITY_AVX512
// normal_stub isn't being dispatched to AVX512 because it exposes
// flakiness in test_sgd of test/test_optim.py
REGISTER_NO_AVX512_DISPATCH(normal_stub);
#else
REGISTER_DISPATCH(normal_stub, &normal_kernel);
#endif
"
318,"#define TORCH_ASSERT_NO_OPERATORS
#include <cmath>
#include <ATen/Config.h>
#include <ATen/Dispatch.h>
","#define TORCH_ASSERT_NO_OPERATORS
#include <ATen/native/RangeFactories.h>
#include <cmath>
#include <ATen/Config.h>
#include <ATen/Dispatch.h>
"
319,"TORCH_CHECK(false, ""mkldnn_convolution_backward: ATen not compiled with MKLDNN support"");
}
REGISTER_NO_CPU_DISPATCH(mkldnn_convolution_backward_stub, mkldnn_convolution_backward_fn);
}}
","TORCH_CHECK(false, ""mkldnn_convolution_backward: ATen not compiled with MKLDNN support"");
}
REGISTER_NO_CPU_DISPATCH(mkldnn_convolution_backward_stub);
}}
"
320,"va_end(fmt_args);
}
void PyWarningHandler::InternalHandler::process(
const c10::SourceLocation& source_location,
const std::string& msg,
","va_end(fmt_args);
}
LinAlgError::LinAlgError(const char* format, ...) {
  va_list fmt_args;
  va_start(fmt_args, format);
  msg = formatMessage(format, fmt_args);
  va_end(fmt_args);
}

void PyWarningHandler::InternalHandler::process(
const c10::SourceLocation& source_location,
const std::string& msg,
"
321,"],
)
@register_acc_op
def quantize_per_tensor(*, input, acc_out_ty):
qparams = acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""qparams"")
dtype = acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""dtype"")
return torch.quantize_per_tensor(
","],
)
@register_acc_op
def quantize_per_tensor(*, input, acc_out_ty=None):
    assert acc_out_ty is not None
qparams = acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""qparams"")
dtype = acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""dtype"")
return torch.quantize_per_tensor(
"
322,"@register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary, AccOpProperty.quantized)
@register_acc_op
def rescale_quantize_per_tensor(*, input, acc_out_ty):
d = dequantize(input=input)
return quantize_per_tensor(input=d, acc_out_ty=acc_out_ty)
@register_acc_op_properties(AccOpProperty.unary, AccOpProperty.quantized)
@register_acc_op
def rescale_quantize_per_channel(*, input, acc_out_ty):
d = dequantize(input=input)
return quantize_per_channel(input=d, acc_out_ty=acc_out_ty)
","@register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary, AccOpProperty.quantized)
@register_acc_op
def rescale_quantize_per_tensor(*, input, acc_out_ty=None):
    assert acc_out_ty is not None
d = dequantize(input=input)
return quantize_per_tensor(input=d, acc_out_ty=acc_out_ty)
@register_acc_op_properties(AccOpProperty.unary, AccOpProperty.quantized)
@register_acc_op
def rescale_quantize_per_channel(*, input, acc_out_ty=None):
    assert acc_out_ty is not None
d = dequantize(input=input)
return quantize_per_channel(input=d, acc_out_ty=acc_out_ty)
"
323,">>> # Assumes backend is not NCCL
>>> device = torch.device(""cpu"")
>>> dist.broadcast_object_list(objects, src=0, device=device)
        >>> broadcast_objects
['foo', 12, {1: 2}]
""""""
if _rank_not_in_group(group):
",">>> # Assumes backend is not NCCL
>>> device = torch.device(""cpu"")
>>> dist.broadcast_object_list(objects, src=0, device=device)
        >>> objects
['foo', 12, {1: 2}]
""""""
if _rank_not_in_group(group):
"
324,"assert type(mod) == cls._FLOAT_MODULE, cls._get_name() + \
'.from_float only works for ' + cls._FLOAT_MODULE.__name__
assert hasattr(mod, 'sparse_params'), \
            'Expecting the Linear to have `sparse_params`.'
sparse_block_shape = mod.sparse_params.get('sparse_block_shape', None)
assert isinstance(sparse_block_shape, (tuple, list))
assert len(sparse_block_shape) == 2
","assert type(mod) == cls._FLOAT_MODULE, cls._get_name() + \
'.from_float only works for ' + cls._FLOAT_MODULE.__name__
assert hasattr(mod, 'sparse_params'), \
            ('Expecting the Linear to have `sparse_params`. Make sure you have provided arguments '
             'in the `sparsifier.squash_mask(params_to_save=(""sparse_block_shape"",))` method.')
sparse_block_shape = mod.sparse_params.get('sparse_block_shape', None)
assert isinstance(sparse_block_shape, (tuple, list))
assert len(sparse_block_shape) == 2
"
325,"}
Tensor _stack(TensorList tensors, int64_t dim) {
  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);
ScalarType high_type = result_type(tensors);
Tensor result = at::empty({0}, tensors[0].options().dtype(high_type));
return at::native::_stack_out(get_stack_inputs(tensors, dim), dim, result);
}
Tensor _stack_cpu(TensorList tensors, int64_t dim) {
  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);
ScalarType high_type = result_type(tensors);
Tensor result = at::empty({0}, tensors[0].options().dtype(high_type));
return at::native::_stack_out_cpu(tensors, dim, result);
","}
Tensor _stack(TensorList tensors, int64_t dim) {
ScalarType high_type = result_type(tensors);
Tensor result = at::empty({0}, tensors[0].options().dtype(high_type));
return at::native::_stack_out(get_stack_inputs(tensors, dim), dim, result);
}
Tensor _stack_cpu(TensorList tensors, int64_t dim) {
ScalarType high_type = result_type(tensors);
Tensor result = at::empty({0}, tensors[0].options().dtype(high_type));
return at::native::_stack_out_cpu(tensors, dim, result);
"
326,"DataLoader,
_DatasetKind,
get_worker_info,
)
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data._decorator import (
","DataLoader,
_DatasetKind,
get_worker_info,
    default_collate,
    default_convert,
)
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data._decorator import (
"
327,"#include <torch/csrc/jit/passes/insert_guards.h>
#include <memory>
#include <unordered_set>
namespace torch {
namespace jit {
void removeProfilingNodes(Block* b) {
  for (auto it = b->nodes().begin(); it != b->nodes().end(); it++) {
    if (it->kind() == prim::profile) {
      it->output()->replaceAllUsesWith(it->input());
      it.destroyCurrent();
    } else {
      for (Block* ib : it->blocks()) {
        removeProfilingNodes(ib);
      }
    }
  }
}

struct GuardInserter {
GuardInserter(std::shared_ptr<Graph> graph) : graph_(std::move(graph)) {}
void run() {
insertGuards(graph_->block());
    removeProfilingNodes(graph_->block());
}
private:
","#include <torch/csrc/jit/passes/insert_guards.h>
#include <torch/csrc/jit/runtime/profiling_record.h>
#include <memory>
#include <unordered_set>
namespace torch {
namespace jit {
struct GuardInserter {
GuardInserter(std::shared_ptr<Graph> graph) : graph_(std::move(graph)) {}
void run() {
insertGuards(graph_->block());
    ProfilingRecord::removeProfilingNodes(graph_->block());
}
private:
"
328,"import collections.abc
from contextlib import contextmanager
from typing import Optional, List, Tuple, Sequence
import torch
from torch.distributed import distributed_c10d
","import collections.abc
from contextlib import contextmanager
from typing import Optional, List, Sequence
import torch
from torch.distributed import distributed_c10d
"
329,"global_size: List[int],
current_rank: int,
pg: distributed_c10d.ProcessGroup
) -> Tuple[ShardedTensorMetadata, torch.device]:
assert len(local_shards) > 0, ""must have local shards!""
local_shard_metadatas: List[ShardMetadata] = []
    local_shards_device = torch.device(""cpu"")
first_shard_dtype = local_shards[0].tensor.dtype
first_shard_layout = local_shards[0].tensor.layout
","global_size: List[int],
current_rank: int,
pg: distributed_c10d.ProcessGroup
) -> ShardedTensorMetadata:
assert len(local_shards) > 0, ""must have local shards!""
local_shard_metadatas: List[ShardMetadata] = []
first_shard_dtype = local_shards[0].tensor.dtype
first_shard_layout = local_shards[0].tensor.layout
"
330,"qconfig = default_qat_qconfig_v2
return qconfig
def assert_valid_qconfig(qconfig: Optional[Union[QConfig, QConfigDynamic]],
mod: torch.nn.Module) -> None:
""""""
","qconfig = default_qat_qconfig_v2
return qconfig
def get_default_qconfig_dict(backend='fbgemm', version=0):
    qconfig = get_default_qconfig(backend)
    return {
        """": qconfig,
        ""object_type"": [(""reshape"", default_reuse_input_qconfig)]
    }

def get_default_qat_qconfig_dict(backend='fbgemm', version=1):
    qconfig = get_default_qat_qconfig(backend, version=version)
    return {
        """": qconfig,
        ""object_type"": [(""reshape"", default_reuse_input_qconfig)]
    }

def assert_valid_qconfig(qconfig: Optional[Union[QConfig, QConfigDynamic]],
mod: torch.nn.Module) -> None:
""""""
"
331,"void StaticRuntime::set_inputs(
std::vector<IValue>&& args,
    const std::unordered_map<std::string, c10::IValue>& kwargs) {
if (!kwargs.empty()) {
// This is not ideal
TORCH_CHECK(
","void StaticRuntime::set_inputs(
std::vector<IValue>&& args,
    const KeywordArgs& kwargs) {
if (!kwargs.empty()) {
// This is not ideal
TORCH_CHECK(
"
332,"template <typename IValueList>
c10::IValue StaticRuntime::run_impl_record_functions(
IValueList&& args,
    const std::unordered_map<std::string, c10::IValue>& kwargs) {
bool pre_sampled = false;
if (C10_UNLIKELY(at::shouldRunRecordFunction(&pre_sampled))) {
at::RecordFunction guard(
","template <typename IValueList>
c10::IValue StaticRuntime::run_impl_record_functions(
IValueList&& args,
    const KeywordArgs& kwargs) {
bool pre_sampled = false;
if (C10_UNLIKELY(at::shouldRunRecordFunction(&pre_sampled))) {
at::RecordFunction guard(
"
333,"# be made configurable later if needed.
for _reverse_fusion_ops, base_op_idx in get_reversed_fusions():
is_match = end_node_matches_reversed_fusion(
                    cur_end_node, _reverse_fusion_ops, self.gm)
if is_match:
# navigate to the base node
for rev_fusion_idx in range(len(_reverse_fusion_ops) - 1):
","# be made configurable later if needed.
for _reverse_fusion_ops, base_op_idx in get_reversed_fusions():
is_match = end_node_matches_reversed_fusion(
                    cur_end_node, _reverse_fusion_ops, self.gm, self.seen_nodes)
if is_match:
# navigate to the base node
for rev_fusion_idx in range(len(_reverse_fusion_ops) - 1):
"
334,"""`quant_min` should be less than or \
equal to `quant_max`."");
  TORCH_CHECK(
      at::min(zero_point).item().toInt() >= quant_min &&
          at::max(zero_point).item().toInt() <= quant_max,
      ""`zero_point` must be between `quant_min` and `quant_max`."");

TORCH_CHECK(
axis >= 0 && axis <= self.dim(),
""`axis` must be between 0 and number of dimensions of input"");
","""`quant_min` should be less than or \
equal to `quant_max`."");
  if(!at::isFloatingType(zero_point.scalar_type())){
      TORCH_CHECK(
          at::min(zero_point).item().toInt() >= quant_min &&
              at::max(zero_point).item().toInt() <= quant_max,
          ""`zero_point` must be between `quant_min` and `quant_max`."");
  }
TORCH_CHECK(
axis >= 0 && axis <= self.dim(),
""`axis` must be between 0 and number of dimensions of input"");
"
335,"elif isinstance(data, string_classes):
return data
elif isinstance(data, collections.abc.Mapping):
        return {k: pin_memory(sample) for k, sample in data.items()}
elif isinstance(data, tuple) and hasattr(data, '_fields'):  # namedtuple
return type(data)(*(pin_memory(sample) for sample in data))
elif isinstance(data, collections.abc.Sequence):
        return [pin_memory(sample) for sample in data]
elif hasattr(data, ""pin_memory""):
return data.pin_memory()
else:
","elif isinstance(data, string_classes):
return data
elif isinstance(data, collections.abc.Mapping):
        try:
            return type(data)({k: pin_memory(sample) for k, sample in data.items()})  # type: ignore[call-arg]
        except TypeError:
            # The mapping type may not support `__init__(iterable)`.
            return {k: pin_memory(sample) for k, sample in data.items()}
elif isinstance(data, tuple) and hasattr(data, '_fields'):  # namedtuple
return type(data)(*(pin_memory(sample) for sample in data))
    elif isinstance(data, tuple):
        return [pin_memory(sample) for sample in data]  # Backwards compatibility.
elif isinstance(data, collections.abc.Sequence):
        try:
            return type(data)([pin_memory(sample) for sample in data])  # type: ignore[call-arg]
        except TypeError:
            # The sequence type may not support `__init__(iterable)` (e.g., `range`).
            return [pin_memory(sample) for sample in data]
elif hasattr(data, ""pin_memory""):
return data.pin_memory()
else:
"
336,"dtype = torch.uint8
storage_numel = cast(Storage, storage).nbytes()
            if storage.data_ptr() in storage_dtypes:
                if storage_dtype != storage_dtypes[storage.data_ptr()]:
                    raise RuntimeError(
                        'Cannot save multiple tensors or storages that '
                        'view the same data as different types')
            else:
                storage_dtypes[storage.data_ptr()] = storage_dtype
view_metadata: Optional[Tuple[str, int, int]]
storage = cast(Storage, storage)
","dtype = torch.uint8
storage_numel = cast(Storage, storage).nbytes()
            # If storage is allocated, ensure that any other saved storages
            # pointing to the same data all have the same dtype. If storage is
            # not allocated, don't perform this check
            if storage.data_ptr() != 0:
                if storage.data_ptr() in storage_dtypes:
                    if storage_dtype != storage_dtypes[storage.data_ptr()]:
                        raise RuntimeError(
                            'Cannot save multiple tensors or storages that '
                            'view the same data as different types')
                else:
                    storage_dtypes[storage.data_ptr()] = storage_dtype
view_metadata: Optional[Tuple[str, int, int]]
storage = cast(Storage, storage)
"
337,"finally:
_CURRENT_PROCESS_GROUP = None
def _parse_and_validate_remote_device(pg, remote_device):
worker_name = remote_device.worker_name()
","finally:
_CURRENT_PROCESS_GROUP = None
def get_current_process_group():
    """"""
    Retrieves the current process group set by ``load_with_process_group``.
    If not set, it just returns the default group.
    """"""
    global _CURRENT_PROCESS_GROUP
    if _CURRENT_PROCESS_GROUP is None:
        return distributed_c10d._get_default_group()
    else:
        return _CURRENT_PROCESS_GROUP

def _parse_and_validate_remote_device(pg, remote_device):
worker_name = remote_device.worker_name()
"
338,"if (!same_storage_values.count(v)) {
same_storage_values[v] = {v};
}
    // skip always alive values (alias inputs/outputs/weights)
    if (value_group.isAlwaysAlive(v)) {
      continue;
    }
    for (const auto& p : same_storage_values) {
      // NB: this means we cannot optimize operations that ""sometimes alias""
      // TODO: add a more robust check of this behavior at runtime
      // FIXME (penguin): this handling makes v and MayAlias(v) share the
      // same storage, which is not correct.
      if (db.mayAlias(p.first, v)) {
        share_storage_fn(v, p.first);
      }
    }
}
// to preserve determinism
","if (!same_storage_values.count(v)) {
same_storage_values[v] = {v};
}
    // NOTE: if we had AliasDb::mustAlias, we could do the following:
    // // skip always alive values (alias inputs/outputs/weights)
    // if (value_group.isAlwaysAlive(v)) {
    //   continue;
    // }
    // for (const auto& p : same_storage_values) {
    //   if (db.mustAlias(p.first, v)) {
    //     share_storage_fn(v, p.first);
    //   }
    // }
    // It also wouldn't matter because ops always create new Tensor
    // objects as aliases; there is no point in trying to reuse their
    // storage.
}
// to preserve determinism
"
339,"reinterpret_cast<uint8_t*>(ncclID) + NCCL_UNIQUE_ID_BYTES);
store_->set(storeKey, vec);
} else {
    auto vec = store_->get(storeKey);
    TORCH_CHECK(vec.size() == NCCL_UNIQUE_ID_BYTES);
    std::memcpy(ncclID, vec.data(), vec.size());
}
}
","reinterpret_cast<uint8_t*>(ncclID) + NCCL_UNIQUE_ID_BYTES);
store_->set(storeKey, vec);
} else {
    try {
      auto vec = store_->get(storeKey);
      TORCH_CHECK(vec.size() == NCCL_UNIQUE_ID_BYTES);
      std::memcpy(ncclID, vec.data(), vec.size());
    } catch (const std::exception& e) {
      std::string exceptionMsg = c10::str(
          ""["",
          rank_,
          ""] is setting up NCCL communicator and ""
          ""retreiving ncclUniqueId from [0] via c10d key-value store by key '"",
          storeKey,
          ""', but store->get('"",
          storeKey,
          ""') got error: "");
      TORCH_CHECK(false, exceptionMsg + e.what());
    } catch (...) {
      TORCH_CHECK(
          false,
          c10::str(
              ""Unknown exception while ["",
              rank_,
              ""] is setting up NCCL communicator and ""
              ""retreiving ncclUniqueId from [0] via c10d key-value store by key '"",
              storeKey,
              ""'""));
    }
}
}
"
340,"# TODO: Once we decide to break serialization FC, this case
# can be deleted
storage = obj._storage
storage_type_str = obj.pickle_storage_type()
storage_type = getattr(torch, storage_type_str)
storage_numel = obj.size()
else:
storage = obj
storage_type = normalize_storage_type(type(obj))
storage_numel = storage.nbytes()
storage = cast(Storage, storage)
storage_key = id_map.setdefault(storage._cdata, str(len(id_map)))
location = location_tag(storage)
serialized_storages[storage_key] = storage
","# TODO: Once we decide to break serialization FC, this case
# can be deleted
storage = obj._storage
                storage_dtype = obj.dtype
storage_type_str = obj.pickle_storage_type()
storage_type = getattr(torch, storage_type_str)
storage_numel = obj.size()
else:
storage = obj
                storage_dtype = storage.dtype
storage_type = normalize_storage_type(type(obj))
storage_numel = storage.nbytes()
storage = cast(Storage, storage)

            if storage.data_ptr() in storage_dtypes:
                if storage_dtype != storage_dtypes[storage.data_ptr()]:
                    raise RuntimeError(
                        'Cannot save multiple tensors or storages that '
                        'view the same data as different types')
            else:
                storage_dtypes[storage.data_ptr()] = storage_dtype

storage_key = id_map.setdefault(storage._cdata, str(len(id_map)))
location = location_tag(storage)
serialized_storages[storage_key] = storage
"
341,"REGISTER_CPU_OPERATOR(
AddFakeFp16,
    BinaryElementwiseBroadcastOp<
TensorTypes<float, int, long>,
CPUContext,
FP16PairWiseCPUFunctor<AddFunctor<CPUContext>>>);
","REGISTER_CPU_OPERATOR(
AddFakeFp16,
    BinaryElementwiseOp<
TensorTypes<float, int, long>,
CPUContext,
FP16PairWiseCPUFunctor<AddFunctor<CPUContext>>>);
"
342,"TanhGradientFunctor<CPUContext>>>);
REGISTER_IDEEP_OPERATOR(
MulGradient,
    IDEEPFallbackOp<BinaryElementwiseGradientBroadcastOp<
NumericTypes,
CPUContext,
MulFunctor<CPUContext>>>);
","TanhGradientFunctor<CPUContext>>>);
REGISTER_IDEEP_OPERATOR(
MulGradient,
    IDEEPFallbackOp<BinaryElementwiseGradientOp<
NumericTypes,
CPUContext,
MulFunctor<CPUContext>>>);
"
343,"# dim, which is a very rare case. For now we just claim not supporting dim=None.
assert dim is not None, ""We don't support dim=None right now.""
    dim = dim % (len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0))
if network.has_implicit_batch_dimension:
assert dim != 0, ""We don't support squeeze batch dim when it's implicit.""
dim -= 1
","# dim, which is a very rare case. For now we just claim not supporting dim=None.
assert dim is not None, ""We don't support dim=None right now.""
    dim = get_positive_dim(dim, len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0))
if network.has_implicit_batch_dimension:
assert dim != 0, ""We don't support squeeze batch dim when it's implicit.""
dim -= 1
"
344,"for idx in self.input_binding_indices_in_order
]
self.input_shapes: Sequence[Sequence[int]] = [
            self.to_tuple(self.engine.get_binding_shape(idx))
for idx in self.input_binding_indices_in_order
]
self.output_dtypes: Sequence[torch.dtype] = [
","for idx in self.input_binding_indices_in_order
]
self.input_shapes: Sequence[Sequence[int]] = [
            tuple(self.engine.get_binding_shape(idx))
for idx in self.input_binding_indices_in_order
]
self.output_dtypes: Sequence[torch.dtype] = [
"
345,"my_rank = get_rank()
_validate_output_list_for_rank(my_rank, dst, object_gather_list)
input_tensor, local_size = _object_to_tensor(obj)
    group_backend = get_backend(group)
current_device = torch.device(""cpu"")
    is_nccl_backend = group_backend == Backend.NCCL
if is_nccl_backend:
current_device = torch.device(""cuda"", torch.cuda.current_device())
input_tensor = input_tensor.to(current_device)
","my_rank = get_rank()
_validate_output_list_for_rank(my_rank, dst, object_gather_list)
input_tensor, local_size = _object_to_tensor(obj)
current_device = torch.device(""cpu"")
    is_nccl_backend = _check_for_nccl_backend(group)

if is_nccl_backend:
current_device = torch.device(""cuda"", torch.cuda.current_device())
input_tensor = input_tensor.to(current_device)
"
346,"for (auto& n : nodes_) {
// LOG(INFO) << ""Running node: "" << PrintNode(n.node());
n.run();
}
if (static_module_.opts().cleanup_activations) {
","for (auto& n : nodes_) {
// LOG(INFO) << ""Running node: "" << PrintNode(n.node());
n.run();
    // Check for incorrect schema alias info.
    verify_and_correct_memory_overlap(n);
}
if (static_module_.opts().cleanup_activations) {
"
347,"}
DCHECK_EQ(managed_tensor_storage_impls_.size(), managed_tensors_.size());
// for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so
// that the objects pointed to by *iv may be reclaimed by reference counting
","}
DCHECK_EQ(managed_tensor_storage_impls_.size(), managed_tensors_.size());
  VLOG(1) << ""managed_bytes: "" << managed_bytes_;
// for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so
// that the objects pointed to by *iv may be reclaimed by reference counting
"
348,"rets = self.func.returns
is_non_mutating_view = len(rets) > 0 and any(r.annotation is not None and not r.annotation.is_write for r in rets)
is_inplace_view = self.tag is not None and self.tag is Tag.inplace_view
        return is_non_mutating_view or is_inplace_view
SchemaKind = Enum('SchemaKind', ('functional', 'inplace', 'out'))
","rets = self.func.returns
is_non_mutating_view = len(rets) > 0 and any(r.annotation is not None and not r.annotation.is_write for r in rets)
is_inplace_view = self.tag is not None and self.tag is Tag.inplace_view
        is_wildcard_view = any(inp.annotation is not None and
                               inp.annotation.alias_set_after != """" for inp in self.func.schema_order_arguments())
        return is_non_mutating_view or is_inplace_view or is_wildcard_view
SchemaKind = Enum('SchemaKind', ('functional', 'inplace', 'out'))
"
349,"def _find_next(self, instance_id: int) -> T_co:
while True:
if self._datapipe_iterator is None:
                raise ValueError(""_datapipe_iterator has not been set, likely because this private method is called directly ""
                                 ""without invoking get_next_element_by_instance() first."")
value = next(self._datapipe_iterator)
classification = self.classifier_fn(value)
if classification is None and self.drop_none:
","def _find_next(self, instance_id: int) -> T_co:
while True:
            if self.main_datapipe_exhausted:
                raise StopIteration
if self._datapipe_iterator is None:
                raise ValueError(
                    ""_datapipe_iterator has not been set, likely because this private method is called directly ""
                    ""without invoking get_next_element_by_instance() first."")
value = next(self._datapipe_iterator)
classification = self.classifier_fn(value)
if classification is None and self.drop_none:
"
350,"# record the execution time in case there were any exceptions during run.
self._total_execution_time = int(time.monotonic() - start_time)
    def get_agent_status_event(self, state: WorkerState) -> Event:
        raw_error = traceback.format_exc() if state == WorkerState.FAILED else None
return self._construct_event(
            state.value, EventSource.AGENT, raw_error=raw_error
)
def _record_worker_events(self, result: RunResult) -> None:
","# record the execution time in case there were any exceptions during run.
self._total_execution_time = int(time.monotonic() - start_time)
    def get_event_failed(self) -> Event:
        return self._construct_event(
            state=""FAILED"",
            source=EventSource.AGENT,
            raw_error=traceback.format_exc(),
        )

    def get_event_succeeded(self) -> Event:
return self._construct_event(
            state=""SUCCEEDED"",
            source=EventSource.AGENT,
)
def _record_worker_events(self, result: RunResult) -> None:
"
351,"import sys
import uuid
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Union, cast, Tuple
import torch.distributed.elastic.rendezvous.registry as rdzv_registry
from torch.distributed.elastic import events, metrics
from torch.distributed.elastic.agent.server.api import WorkerSpec, WorkerState
from torch.distributed.elastic.agent.server.local_elastic_agent import LocalElasticAgent
from torch.distributed.elastic.multiprocessing import Std
from torch.distributed.elastic.multiprocessing.errors import ChildFailedError
from torch.distributed.elastic.rendezvous import RendezvousParameters
from torch.distributed.elastic.rendezvous.utils import parse_rendezvous_endpoint
","import sys
import uuid
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import torch.distributed.elastic.rendezvous.registry as rdzv_registry
from torch.distributed.elastic import events, metrics
from torch.distributed.elastic.agent.server.api import WorkerSpec
from torch.distributed.elastic.agent.server.local_elastic_agent import LocalElasticAgent
from torch.distributed.elastic.multiprocessing import SignalException, Std
from torch.distributed.elastic.multiprocessing.errors import ChildFailedError
from torch.distributed.elastic.rendezvous import RendezvousParameters
from torch.distributed.elastic.rendezvous.utils import parse_rendezvous_endpoint
"
352,"return launch_agent(self._config, self._entrypoint, list(args))
def _construct_event(config: LaunchConfig) -> events.Event:
    metadata = {
        ""rdzv_backend"": config.rdzv_backend,
        ""run_id"": config.run_id,
        ""role"": config.role,
    }
    return events.Event(
        name=""torch.distributed.elastic.launch_agent"",
        source=events.EventSource.AGENT,
        metadata=cast(Dict[str, events.EventMetadataValue], metadata),
    )


def _get_entrypoint_name(
entrypoint: Union[Callable, str, None], args: List[Any]
) -> str:
","return launch_agent(self._config, self._entrypoint, list(args))
def _get_entrypoint_name(
entrypoint: Union[Callable, str, None], args: List[Any]
) -> str:
"
353,"def _autocast_to_full_precision(self, cuda_enabled : bool, cpu_enabled : bool):
self_dtype = self.dtype
def backward(grad_output):
                return grad_output.to(self_dtype)
return torch._autocast_to_full_precision(self, cuda_enabled, cpu_enabled), backward
","def _autocast_to_full_precision(self, cuda_enabled : bool, cpu_enabled : bool):
self_dtype = self.dtype
def backward(grad_output):
                return grad_output.to(self_dtype), None, None
return torch._autocast_to_full_precision(self, cuda_enabled, cpu_enabled), backward
"
354,"at::Tensor input_;
at::Tensor output_;
at::Tensor ref_;
};
} // namespace
","at::Tensor input_;
at::Tensor output_;
at::Tensor ref_;
  at::Tensor input_int_;
  at::Tensor output_int_;
  at::Tensor ref_int_; // no type promotion
};
} // namespace
"
355,"batch_size = inputs[0].shape[0]
contiguous_inputs: List[torch.Tensor] = [i.contiguous() for i in inputs]
bindings: List[Any] = [None] * (
                    len(self.input_names) + len(self.output_names)
)
for i, input_name in enumerate(self.input_names):
","batch_size = inputs[0].shape[0]
contiguous_inputs: List[torch.Tensor] = [i.contiguous() for i in inputs]
bindings: List[Any] = [None] * (
                    len(self.input_names) + len(self.output_names) + len(self.hidden_output_names)
)
for i, input_name in enumerate(self.input_names):
"
356,"// batchCheckErrors(Tensor, char*) calls 'infos = infos.to(kCPU)'
bool vector_case = linalg_solve_is_vector_rhs(input, other);
if (vector_case ? result.dim() > 1 : result.dim() > 2) {
    batchCheckErrors(infos, ""linalg_solve"");
} else {
    singleCheckErrors(infos.item().toInt(), ""linalg_solve"");
}
return result;
","// batchCheckErrors(Tensor, char*) calls 'infos = infos.to(kCPU)'
bool vector_case = linalg_solve_is_vector_rhs(input, other);
if (vector_case ? result.dim() > 1 : result.dim() > 2) {
    batchCheckErrors(infos, ""linalg.solve"");
} else {
    singleCheckErrors(infos.item().toInt(), ""linalg.solve"");
}
return result;
"
357,"}
std::tuple<Tensor&, Tensor&> linalg_cholesky_ex_out(const Tensor& input, bool upper, bool check_errors, Tensor& L, Tensor& info) {
  squareCheckInputs(input);
checkSameDevice(""torch.linalg.cholesky_ex"", L, input, ""L"");
checkLinalgCompatibleDtype(""torch.linalg.cholesky_ex"", L, input, ""L"");
checkSameDevice(""torch.linalg.cholesky_ex"", info, input, ""info"");
","}
std::tuple<Tensor&, Tensor&> linalg_cholesky_ex_out(const Tensor& input, bool upper, bool check_errors, Tensor& L, Tensor& info) {
  squareCheckInputs(input, ""linalg.cholesky_ex"");
checkSameDevice(""torch.linalg.cholesky_ex"", L, input, ""L"");
checkLinalgCompatibleDtype(""torch.linalg.cholesky_ex"", L, input, ""L"");
checkSameDevice(""torch.linalg.cholesky_ex"", info, input, ""info"");
"
358,"}
std::tuple<Tensor&, Tensor&> linalg_eig_out(const Tensor& input, Tensor& values, Tensor& vectors) {
  squareCheckInputs(input);
// unlike NumPy for real-valued inputs the output is always complex-valued
checkLinalgCompatibleDtype(""torch.linalg.eig"", values.scalar_type(), toComplexType(input.scalar_type()), ""eigenvalues"");
","}
std::tuple<Tensor&, Tensor&> linalg_eig_out(const Tensor& input, Tensor& values, Tensor& vectors) {
  squareCheckInputs(input, ""linalg.eig"");
// unlike NumPy for real-valued inputs the output is always complex-valued
checkLinalgCompatibleDtype(""torch.linalg.eig"", values.scalar_type(), toComplexType(input.scalar_type()), ""eigenvalues"");
"
359,"}
Tensor& linalg_eigvals_out(const Tensor& input, Tensor& values) {
  squareCheckInputs(input);
// unlike NumPy for real-valued inputs the output is always complex-valued
checkLinalgCompatibleDtype(""torch.linalg.eigvals"", values.scalar_type(), toComplexType(input.scalar_type()), ""eigenvalues"");
","}
Tensor& linalg_eigvals_out(const Tensor& input, Tensor& values) {
  squareCheckInputs(input, ""linalg.eigvals"");
// unlike NumPy for real-valued inputs the output is always complex-valued
checkLinalgCompatibleDtype(""torch.linalg.eigvals"", values.scalar_type(), toComplexType(input.scalar_type()), ""eigenvalues"");
"
360,"checkSameDevice(""svd"", U, self, ""U"");
checkSameDevice(""svd"", S, self, ""S"");
checkSameDevice(""svd"", Vh, self, ""Vh"");
  checkLinalgCompatibleDtype(""linalg_svd"", U, self, ""U"");
  checkLinalgCompatibleDtype(""linalg_svd"", Vh, self, ""Vh"");
// singular values are always real-valued here
ScalarType real_dtype = toValueType(self.scalar_type());
  checkLinalgCompatibleDtype(""linalg_svd"", S.scalar_type(), real_dtype, ""S"");
Tensor U_tmp, S_tmp, Vh_tmp;
std::tie(U_tmp, S_tmp, Vh_tmp) = at::native::linalg_svd(self, full_matrices);
svd_resize_and_copy(""U"", U_tmp, U);
","checkSameDevice(""svd"", U, self, ""U"");
checkSameDevice(""svd"", S, self, ""S"");
checkSameDevice(""svd"", Vh, self, ""Vh"");
  checkLinalgCompatibleDtype(""linalg.svd"", U, self, ""U"");
  checkLinalgCompatibleDtype(""linalg.svd"", Vh, self, ""Vh"");
// singular values are always real-valued here
ScalarType real_dtype = toValueType(self.scalar_type());
  checkLinalgCompatibleDtype(""linalg.svd"", S.scalar_type(), real_dtype, ""S"");
Tensor U_tmp, S_tmp, Vh_tmp;
std::tie(U_tmp, S_tmp, Vh_tmp) = at::native::linalg_svd(self, full_matrices);
svd_resize_and_copy(""U"", U_tmp, U);
"
361,"for p in group['params']:
state = self.state[p]
state['step'] = 0
                state['sum'] = torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)
def share_memory(self):
for group in self.param_groups:
","for p in group['params']:
state = self.state[p]
state['step'] = 0
                init_value = complex(initial_accumulator_value, initial_accumulator_value) if torch.is_complex(p) \
                    else initial_accumulator_value
                state['sum'] = torch.full_like(p, init_value, memory_format=torch.preserve_format)
def share_memory(self):
for group in self.param_groups:
"
362,"labels={LABEL_CIFLOW_LINUX, LABEL_CIFLOW_CPU},
),
),
    # Build PyTorch with BUILD_CAFFE2=OFF
CIWorkflow(
arch=""linux"",
        build_environment=""puretorch-linux-xenial-py3.6-gcc5.4"",
docker_image_base=f""{DOCKER_REGISTRY}/pytorch/pytorch-linux-xenial-py3.6-gcc5.4"",
test_runner_type=LINUX_CPU_TEST_RUNNER,
exclude_test=True,
","labels={LABEL_CIFLOW_LINUX, LABEL_CIFLOW_CPU},
),
),
    # Build PyTorch with BUILD_CAFFE2=ON
CIWorkflow(
arch=""linux"",
        build_environment=""caffe2-linux-xenial-py3.6-gcc5.4"",
docker_image_base=f""{DOCKER_REGISTRY}/pytorch/pytorch-linux-xenial-py3.6-gcc5.4"",
test_runner_type=LINUX_CPU_TEST_RUNNER,
exclude_test=True,
"
363,"fuser = Fuser()
return fuser.fuse(graph_module, fuse_custom_config_dict)
class Scope(object):
"""""" Scope object that records the module path and the module type
of a module. Scope is used to track the information of the module
","fuser = Fuser()
return fuser.fuse(graph_module, fuse_custom_config_dict)

class Scope(object):
"""""" Scope object that records the module path and the module type
of a module. Scope is used to track the information of the module
"
364,"class QuantizationTracer(Tracer):
def __init__(
            self,
            skipped_module_names: List[str],
            skipped_module_classes: List[Callable]):
super().__init__()
self.skipped_module_names = skipped_module_names
self.skipped_module_classes = skipped_module_classes
","class QuantizationTracer(Tracer):
def __init__(
        self, skipped_module_names: List[str], skipped_module_classes: List[Callable]
    ):
super().__init__()
self.skipped_module_names = skipped_module_names
self.skipped_module_classes = skipped_module_classes
"
365,"""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.fuse_fx"")
    assert not model.training, 'fuse_fx only works on models in eval mode'
check_is_valid_fuse_custom_config_dict(fuse_custom_config_dict)
graph_module = torch.fx.symbolic_trace(model)
preserved_attributes: Set[str] = set()
if fuse_custom_config_dict:
        preserved_attributes = set(fuse_custom_config_dict.get(""preserved_attributes"", []))
for attr_name in preserved_attributes:
setattr(graph_module, attr_name, getattr(model, attr_name))
return _fuse_fx(graph_module, fuse_custom_config_dict)
def prepare_fx(
        model: torch.nn.Module, qconfig_dict: Any,
        prepare_custom_config_dict: Optional[Dict[str, Any]] = None,
        equalization_qconfig_dict: Optional[Dict[str, Any]] = None,
        backend_config_dict: Optional[Dict[str, Any]] = None) -> ObservedGraphModule:
r"""""" Prepare a model for post training static quantization
Args:
","""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.fuse_fx"")
    assert not model.training, ""fuse_fx only works on models in eval mode""
check_is_valid_fuse_custom_config_dict(fuse_custom_config_dict)
graph_module = torch.fx.symbolic_trace(model)
preserved_attributes: Set[str] = set()
if fuse_custom_config_dict:
        preserved_attributes = set(
            fuse_custom_config_dict.get(""preserved_attributes"", [])
        )
for attr_name in preserved_attributes:
setattr(graph_module, attr_name, getattr(model, attr_name))
return _fuse_fx(graph_module, fuse_custom_config_dict)

def prepare_fx(
    model: torch.nn.Module,
    qconfig_dict: Any,
    prepare_custom_config_dict: Optional[Dict[str, Any]] = None,
    equalization_qconfig_dict: Optional[Dict[str, Any]] = None,
    backend_config_dict: Optional[Dict[str, Any]] = None,
) -> ObservedGraphModule:
r"""""" Prepare a model for post training static quantization
Args:
"
366,"case TypeKind::UnionType: {
AliasTypeSet mutable_types;
for (const TypePtr& inner :
             type->expect<UnionType>()->containedTypes()) {
if (auto maybe_inner_types = mapTypeToAliasTypeSet(inner)) {
mutable_types.insert(
mutable_types.end(),
","case TypeKind::UnionType: {
AliasTypeSet mutable_types;
for (const TypePtr& inner :
             type->expectRef<UnionType>().containedTypes()) {
if (auto maybe_inner_types = mapTypeToAliasTypeSet(inner)) {
mutable_types.insert(
mutable_types.end(),
"
367,"def _mkdir_p(d: str) -> None:
try:
        os.makedirs(d)
    except OSError:
        pass
# Ninja
","def _mkdir_p(d: str) -> None:
try:
        os.makedirs(d, exist_ok=True)
    except OSError as e:
        raise RuntimeError(f""Failed to create folder {os.path.abspath(d)}: {e.strerror}"") from e
# Ninja
"
368,"// assert other conditions for cudnnCTCLoss: all label lengths <= 256
// all input lengths = logprob.size(0)
  auto handle = getCudnnHandle();
  cudnnCTCLossAlgo_t algo = (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);
CTCLossDescriptor ctc_loss_desc;
","// assert other conditions for cudnnCTCLoss: all label lengths <= 256
// all input lengths = logprob.size(0)
  const auto handle = getCudnnHandle();
  const cudnnCTCLossAlgo_t algo = (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);
CTCLossDescriptor ctc_loss_desc;
"
369,"def maybe_get_next_module(
node: Node,
modules: Dict[str, nn.Module],
    target_module_type: Type[nn.Module] = None,
target_functional_type: Any = None,
) -> Optional[Node]:
"""""" Gets the next module that matches what is needed in
","def maybe_get_next_module(
node: Node,
modules: Dict[str, nn.Module],
    target_module_type: Optional[Type[nn.Module]] = None,
target_functional_type: Any = None,
) -> Optional[Node]:
"""""" Gets the next module that matches what is needed in
"
370,"if (j != 0 && inner_dim != -1) {
// we are not looking at dim-j, but dim-sorted_index, which
// is the j-th fastest dim;
        // TODO: merge this with above and put a long comment there
        if (t_strides[sorted_index] < t_strides[inner_dim]) {
return false;
}
}
","if (j != 0 && inner_dim != -1) {
// we are not looking at dim-j, but dim-sorted_index, which
// is the j-th fastest dim;
        // Note: we ignore 0-stride dimension, since eager logic on stride
        // indices is ambiguous
        if (t_strides[sorted_index] != 0 && t_strides[inner_dim] != 0 &&
            t_strides[sorted_index] < t_strides[inner_dim]) {
return false;
}
}
"
371,"GlobalRank = int
_FAILURE_FORMAT_TEMPLATE = """"""[${idx}]:
  time: ${time}
  rank: ${rank} (local_rank: ${local_rank})
  exitcode: ${exitcode} (pid: ${pid})
error_file: ${error_file}
  msg: ${message}""""""
# extra new lines before and after are intentional
_MSG_FORMAT_TEMPLATE = """"""
${boarder}
${title}
${section}
Root Cause:
${root_failure}
${section}
Other Failures:
${other_failures}
${boarder}
""""""
class ChildFailedError(Exception):
","GlobalRank = int
_FAILURE_FORMAT_TEMPLATE = """"""[${idx}]:
  time      : ${time}
  host      : ${hostname}
  rank      : ${rank} (local_rank: ${local_rank})
  exitcode  : ${exitcode} (pid: ${pid})
error_file: ${error_file}
  traceback : ${message}""""""
# extra new lines before and after are intentional
_MSG_FORMAT_TEMPLATE = """"""
${boarder}
${title}
${section}
Failures:
${other_failures}
${section}
Root Cause (first observed failure):
${root_failure}
${boarder}""""""
class ChildFailedError(Exception):
"
372,"// make bag_size output deterministic
at::native::zero_(bag_size);
}
    max_indices = bag_size;
} else { // MODE_MAX
AT_DISPATCH_FLOATING_TYPES_AND_HALF(
weight.scalar_type(), ""embedding_bag_cpu_max_out"", [&]() {
","// make bag_size output deterministic
at::native::zero_(bag_size);
}
     max_indices.copy_(bag_size);
} else { // MODE_MAX
AT_DISPATCH_FLOATING_TYPES_AND_HALF(
weight.scalar_type(), ""embedding_bag_cpu_max_out"", [&]() {
"
373,"if batch_size is not None and batch_sampler is None:
# auto_collation without custom batch_sampler
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)  # type: ignore[arg-type]
self.batch_size = batch_size
self.drop_last = drop_last
        self.sampler = sampler  # type: ignore[assignment]
self.batch_sampler = batch_sampler
self.generator = generator
","if batch_size is not None and batch_sampler is None:
# auto_collation without custom batch_sampler
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)
self.batch_size = batch_size
self.drop_last = drop_last
        self.sampler = sampler
self.batch_sampler = batch_sampler
self.generator = generator
"
374,"import torch
from torch import Tensor
from typing import Iterator, Optional, Sequence, List, TypeVar, Generic, Sized
T_co = TypeVar('T_co', covariant=True)
","import torch
from torch import Tensor
from typing import Iterator, Iterable, Optional, Sequence, List, TypeVar, Generic, Sized, Union
T_co = TypeVar('T_co', covariant=True)
"
375,"namespace {
// TODO: Consider representing debug info as a struct instead so you
// don't have to allocate strings all the time
std::string debugString(std::string debug, const char* file, uint32_t line) {
#ifdef STRIP_ERROR_MESSAGES
    return """";
#else
if (debug.empty()) {
      return c10::str(""registered at "", file, "":"", line);
} else {
return debug;
}
","namespace {
// TODO: Consider representing debug info as a struct instead so you
// don't have to allocate strings all the time
  std::string debugString(const char* file, uint32_t line) {
#ifdef STRIP_ERROR_MESSAGES
    return std::string();
#else
    return c10::str(""registered at "", file, "":"", line);
#endif
  }

std::string debugString(std::string debug, const char* file, uint32_t line) {
#ifdef STRIP_ERROR_MESSAGES
    return std::string();
#else
if (debug.empty()) {
      return debugString(file, line);
} else {
return debug;
}
"
376,"self,
module: torch.fx.GraphModule,
sample_input: Tuple[torch.Tensor],
        operator_support: OperatorSupport = None,
settings: splitter_base._SplitterSettingBase = None,
):
if not operator_support:
            operator_support = TRTOperatorSupport()
if not settings:
settings = splitter_base._SplitterSettingBase()
super().__init__(module, sample_input, operator_support, settings)
","self,
module: torch.fx.GraphModule,
sample_input: Tuple[torch.Tensor],
        operator_support: ops.OperatorSupportBase = None,
settings: splitter_base._SplitterSettingBase = None,
):
if not operator_support:
            operator_support = create_trt_operator_support()
if not settings:
settings = splitter_base._SplitterSettingBase()
super().__init__(module, sample_input, operator_support, settings)
"
377,"if not isinstance(node.args[i], torch.fx.Node):
continue
            arg_tensor_meta = node.args[i].meta.get(""tensor_meta"")  # type: ignore[union-attr]
            arg_dtype = arg_tensor_meta.dtype if arg_tensor_meta else None

if arg_dtype not in dtypes:
return False
","if not isinstance(node.args[i], torch.fx.Node):
continue
            arg_dtype = _get_arg_dtype(node.args[i])  # type: ignore[arg-type]
if arg_dtype not in dtypes:
return False
"
378,"from .operator_support import (
get_node_target,
    OperatorSupport,
)
from .graph_drawer import FxGraphDrawer
from .shape_prop import ShapeProp
","from .operator_support import (
get_node_target,
    OperatorSupportBase,
)
from .graph_drawer import FxGraphDrawer
from .shape_prop import ShapeProp
"
379,"def __init__(
self,
module: torch.fx.GraphModule,
        operator_support: OperatorSupport,
allow_non_tensor: bool,
):
self.module = module
","def __init__(
self,
module: torch.fx.GraphModule,
        operator_support: OperatorSupportBase,
allow_non_tensor: bool,
):
self.module = module
"
380,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path, cupti_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
"
381,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path, cupti_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
"
382,"TORCH_INTERNAL_ASSERT(type_);
}
ska::flat_hash_map<std::type_index, c10::ClassTypePtr>& getCustomClassTypeMap() {
static ska::flat_hash_map<std::type_index, c10::ClassTypePtr> tmap;
return tmap;
","TORCH_INTERNAL_ASSERT(type_);
}
WeakTypePtr::WeakTypePtr(
    std::weak_ptr<torch::jit::CompilationUnit> cu,
    TypePtr type) {
  cu_ = std::move(cu);
  type_ = type;
}

WeakTypePtr WeakOrStrongTypePtr::asWeakTypePtr() const {
  if (!holds_strong_ref()) {
    return WeakTypePtr(cu_.getWeakRefOrThrow(), type_);
  } else {
    std::weak_ptr<torch::jit::CompilationUnit> weak_cu =
        cu_.getStrongRefOrThrow();
    return WeakTypePtr(weak_cu, type_);
  }
}


ska::flat_hash_map<std::type_index, c10::ClassTypePtr>& getCustomClassTypeMap() {
static ska::flat_hash_map<std::type_index, c10::ClassTypePtr> tmap;
return tmap;
"
383,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path, cupti_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
"
384,": nullptr,
/*output=*/output_data);
    TORCH_CHECK(
        success,
        ""FBGEMM GenerateEmbeddingSpMDMNBit kernel failed for "",
        bit_width,
        ""-bit input"");
} else {
auto kernel =
fbgemm::GenerateEmbeddingSpMDMNBitRowWiseSparse<IndexType, OffsetType>(
",": nullptr,
/*output=*/output_data);
    if (!success) {
      fbgemm_spmdm_report_error_(
          output_size, index_size, N, offsets_data, indices_data);
    }
} else {
auto kernel =
fbgemm::GenerateEmbeddingSpMDMNBitRowWiseSparse<IndexType, OffsetType>(
"
385,"}
}
} // namespace torch
","}
}
} // namespace torch

python_error::python_error() : type(nullptr), value(nullptr), traceback(nullptr),
    message(""unknown Python error (for more information, try rerunning with TORCH_SHOW_CPP_STACKTRACES=1)"") {
  if (torch::get_cpp_stacktraces_enabled()) {
    // Eagerly populate message
    persist();
    restore();
  }
}
"
386,"*/
// The functions needed for backport model from v5 to v4.
namespace {
void writeArchiveV4(
    PyTorchStreamWriter& writer,
    const std::string& archive_name,
    const c10::IValue& value) {
  std::vector<char> data;

  // Vector to capture the run-time class types during pickling the IValues
  std::vector<c10::ClassTypePtr> memoizedClassTypes;
  Pickler data_pickle(
      [&](const char* buf, size_t size) {
        data.insert(data.end(), buf, buf + size);
      },
      nullptr,
      nullptr,
      &memoizedClassTypes);
  data_pickle.protocol();
  data_pickle.pushIValue(value);
  data_pickle.stop();
  size_t i = 0;
  std::string prefix = archive_name + ""/"";

  for (const auto& td : data_pickle.tensorData()) {
    WriteableTensorData writable_td = getWriteableTensorData(td);
    std::string fname = prefix + c10::to_string(i++);
    writer.writeRecord(fname, writable_td.data(), writable_td.sizeInBytes());
  }
  std::string fname = archive_name + "".pkl"";
  writer.writeRecord(fname, data.data(), data.size());
}

std::stringstream backport_v5_to_v4(std::stringstream& input_model_stream) {
// 1) read from archive `bytecode` archive
PyTorchStreamReader reader(&input_model_stream);
","*/
namespace {
/*
The following functions needed for backport model from v5 to v4.
Backport function bytecode v5 that deduplicate constanst table.
Previously, in v4, constant table will be exported twice, in both archive
bytecode and archive constants, and majority (almost all) are duplicates.
Currently, in v5, JIT and mobile will share archive constants, and all
constant tensors will be exported in this archive. The bump was needed
because the v5 bytecode export the tensor storage path in the schema, since
the runtime code is now able to query which archive this tensor is stored at
and query the correct archive.
For example, Previously, in v4, we deserialize tensor as without archive
path, and mobile will always read tensor from bytecode archive:
(torch._utils._rebuild_tensor_v2(pers.obj(('storage', torch.DoubleStorage,
'0', 'cpu', 8),),
   0,
   (2, 4),
   (4, 1),
   False,
   collections.OrderedDict()),
 1)),
 So, if the program defines: torch.add(x, h, out=x)
Currently, in v5, we deserialize the bytecode with the archive path, and
mobile can read tensor from the given path:
(torch._utils._rebuild_tensor_v2(pers.obj(('storage', torch.DoubleStorage,
'constants/0', 'cpu', 8),),
   0,
   (2, 4),
   (4, 1),
   False,
   collections.OrderedDict()),
 1)),
Thus, the backport is necessary such that the runtime can read tensor from
the correct archive.
*/
std::stringstream backport_v5_to_v4(std::stringstream& input_model_stream) {
// 1) read from archive `bytecode` archive
PyTorchStreamReader reader(&input_model_stream);
"
387,"# get qconfig to determine the eventual dtype of this node
if qconfig is not None:
            if qhandler is not None and qhandler.input_output_observed():
act_dtype, weight_dtype, act_compute_dtype = \
get_qconfig_dtypes(qconfig)
return act_dtype
","# get qconfig to determine the eventual dtype of this node
if qconfig is not None:
            if qhandler is not None and qhandler.input_output_observed() and qhandler.is_output_quantized(qconfig, False):
act_dtype, weight_dtype, act_compute_dtype = \
get_qconfig_dtypes(qconfig)
return act_dtype
"
388,"Runtime* runtime() {
static const std::unique_ptr<Runtime> runtime([]() -> Runtime* {
#ifdef USE_VULKAN_WRAPPER
    if (!InitVulkan()) {
TORCH_WARN(""Vulkan: Failed to initialize Vulkan Wrapper!"");
return nullptr;
}
#endif
try {
return new Runtime(Configuration::kRuntime);
","Runtime* runtime() {
static const std::unique_ptr<Runtime> runtime([]() -> Runtime* {
#ifdef USE_VULKAN_WRAPPER
#ifdef USE_VULKAN_VOLK
    if (VK_SUCCESS != volkInitialize()) {
      TORCH_WARN(""Vulkan: Failed to initialize Volk!"");
      return nullptr;
    }
#else
 if (!InitVulkan()) {
TORCH_WARN(""Vulkan: Failed to initialize Vulkan Wrapper!"");
return nullptr;
}
#endif /* USE_VULKAN_VOLK */
#endif /* USE_VULKAN_WRAPPER */
try {
return new Runtime(Configuration::kRuntime);
"
389,"for input, traced_input in zip(input_states[0], input_states[1]):
if isinstance(input, dict):
if list(input.keys()) != list(traced_input.keys()):
                warning = ""We detected that you are modifying a dictionnary that is an input to your "" \
""model. "" \
""Note that dictionaries are allowed as inputs in ONNX but they should be "" \
""handled with care. "" \
","for input, traced_input in zip(input_states[0], input_states[1]):
if isinstance(input, dict):
if list(input.keys()) != list(traced_input.keys()):
                warning = ""We detected that you are modifying a dictionary that is an input to your "" \
""model. "" \
""Note that dictionaries are allowed as inputs in ONNX but they should be "" \
""handled with care. "" \
"
390,"num_dims = len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0)
k = kwargs[""k""]
    dim = (kwargs[""dim""] if kwargs[""dim""] else -1) % num_dims
operation = trt.TopKOperation.MAX if kwargs[""largest""] else trt.TopKOperation.MIN
layer = network.add_topk(
input_val, operation, k, get_axes_for_reduce_op(dim, network.has_implicit_batch_dimension)
","num_dims = len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0)
k = kwargs[""k""]
    dim = (kwargs[""dim""] if kwargs[""dim""] is not None else -1) % num_dims
operation = trt.TopKOperation.MAX if kwargs[""largest""] else trt.TopKOperation.MIN
layer = network.add_topk(
input_val, operation, k, get_axes_for_reduce_op(dim, network.has_implicit_batch_dimension)
"
391,"#  rank of the worker among all the workers with the same role
#  across all ``agent`` instances.
        #  Global rank is not stable between re-rendezvous.
self.role_rank: int = role_rank
# total number of workers (globally). Due to elasticity
","#  rank of the worker among all the workers with the same role
#  across all ``agent`` instances.
        #  Role rank is not stable between re-rendezvous.
self.role_rank: int = role_rank
# total number of workers (globally). Due to elasticity
"
392,"// If the inputs to `cat` are of different types, then the implementation
// of `cat` is expected to promote type.
bool doesCatPromoteTypes(Node* node) {
  TORCH_INTERNAL_ASSERT(node->kind() == aten::cat);
  TORCH_INTERNAL_ASSERT(node->input(0)->node()->kind() == prim::ListConstruct);
auto inputs = node->input(0)->node()->inputs();
  TORCH_INTERNAL_ASSERT(!inputs.empty());
auto scalar_type =
inputs.front()->type()->cast<c10::TensorType>()->scalarType();
for (size_t i = 1; i < inputs.size(); ++i) {
","// If the inputs to `cat` are of different types, then the implementation
// of `cat` is expected to promote type.
bool doesCatPromoteTypes(Node* node) {
  TORCH_INTERNAL_ASSERT(
      node->kind() == aten::cat,
      buildErrorMessage(""Graph node is not aten::cat.""));
  TORCH_INTERNAL_ASSERT(
      node->input(0)->node()->kind() == prim::ListConstruct,
      buildErrorMessage(""aten::cat inputs are not expected.""));
auto inputs = node->input(0)->node()->inputs();
  TORCH_INTERNAL_ASSERT(
      !inputs.empty(), buildErrorMessage(""Empty inputs of ListConstruct""));
auto scalar_type =
inputs.front()->type()->cast<c10::TensorType>()->scalarType();
for (size_t i = 1; i < inputs.size(); ++i) {
"
393,"// We intend to promote Integers to floating-point types
TORCH_INTERNAL_ASSERT(
      !c10::isIntegralType(defaultType, /*includeBool*/ true));
return Cast::make(
Dtype(
","// We intend to promote Integers to floating-point types
TORCH_INTERNAL_ASSERT(
      !c10::isIntegralType(defaultType, /*includeBool*/ true),
      buildErrorMessage(""Non-integer type""));
return Cast::make(
Dtype(
"
394,"Since :func:`~torch.stft` discards elements at the end of the signal if they do not fit in a frame,
``istft`` may return a shorter signal than the original signal (can occur if :attr:`center` is False
    since the signal isn't padded).
If :attr:`center` is ``True``, then there will be padding e.g. ``'constant'``, ``'reflect'``, etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
","Since :func:`~torch.stft` discards elements at the end of the signal if they do not fit in a frame,
``istft`` may return a shorter signal than the original signal (can occur if :attr:`center` is False
    since the signal isn't padded). If `length` is given in the arguments and is longer than expected,
    ``istft`` will pad zeros to the end of the returned signal.
If :attr:`center` is ``True``, then there will be padding e.g. ``'constant'``, ``'reflect'``, etc.
Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
"
395,"%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %r = aten::hardtanh_(%conv2d_res, %output_min, %output_max)
        return (%r) )"";
rewriter.RegisterRewritePattern(
      linear_prepack_run_hardtanh_inplace, linear_prepack_run_hardtanh_fused);
rewriter.RegisterRewritePattern(
      conv2d_prepack_run_hardtanh_inplace, conv2d_prepack_run_hardtanh_fused);
rewriter.runOnGraph(graph, torch::jit::graph_rewrite_helper::isClampFusable);
}
","%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %res = aten::hardtanh_(%conv2d_res, %output_min, %output_max)
        return (%res) )"";

  value_mappings = {
      {""packed_weight_bias"", ""packed_weight_bias""}, {""res"", ""res""}};
rewriter.RegisterRewritePattern(
      linear_prepack_run_hardtanh_inplace,
      linear_prepack_run_hardtanh_fused,
      value_mappings);

  value_mappings = {
      {""packed_weight_bias"", ""packed_weight_bias""}, {""res"", ""res""}};

rewriter.RegisterRewritePattern(
      conv2d_prepack_run_hardtanh_inplace,
      conv2d_prepack_run_hardtanh_fused,
      value_mappings);
rewriter.runOnGraph(graph, torch::jit::graph_rewrite_helper::isClampFusable);
}
"
396,"namespace tensorexpr {
std::string buildErrorMessage(const std::string& s) {
  // TODO: Update this generic error message to include details regarding
  // turning off the fuser.
  static const std::string generic_error_message = """";
  return s + "" "" + generic_error_message;
}
static int te_cuda_pointwise_loop_levels = -1;
","namespace tensorexpr {
std::string buildErrorMessage(const std::string& s) {
  static const std::string generic_error_message =
      ""This error occured in the fuser. You can turn off the fuser with ""
      ""torch._C._jit_override_can_fuse_on_cpu(False)"";
  if (s.empty()) {
    return generic_error_message;
  }
  if (s.back() == '.') {
    return s + "" "" + generic_error_message;
  }
  return s + "". "" + generic_error_message;
}
static int te_cuda_pointwise_loop_levels = -1;
"
397,"// (since we don't support in-place writes). Resolves issue 52581.
TORCH_INTERNAL_ASSERT(
*intValue(i) == 0,
            ""Constant index impression should always be zero"");
producer_index_vars_.push_back(nullptr);
} else {
throw std::logic_error(""cannot inline Buf with compound indices"");
","// (since we don't support in-place writes). Resolves issue 52581.
TORCH_INTERNAL_ASSERT(
*intValue(i) == 0,
            buildErrorMessage(
                ""Unexpected non-zero constant index in inlined buffer in the fuser.""));
producer_index_vars_.push_back(nullptr);
} else {
throw std::logic_error(""cannot inline Buf with compound indices"");
"
398,"std::vector<ForPtr> LoopNest::distributeLoop(
ForPtr loop,
const std::unordered_set<StmtPtr>& pivots) {
  TORCH_INTERNAL_ASSERT(loop);
auto root = loop->get_parent();
if (root == nullptr) {
throw malformed_input(""Loop without parent: "", loop);
","std::vector<ForPtr> LoopNest::distributeLoop(
ForPtr loop,
const std::unordered_set<StmtPtr>& pivots) {
  TORCH_INTERNAL_ASSERT(
      loop,
      buildErrorMessage(
          ""Expected non-null loop in distributeLoop in the fuser.""));
auto root = loop->get_parent();
if (root == nullptr) {
throw malformed_input(""Loop without parent: "", loop);
"
399,"return {nullptr, nullptr};
}
  TORCH_INTERNAL_ASSERT(bounds_it->second.size() == 1);
TensorAccessBoundsInfo& info = bounds_it->second[0];
bool hasReads = info.kind == kLoad || info.kind == kMutate;
bool hasWrites = info.kind == kStore || info.kind == kMutate;
","return {nullptr, nullptr};
}
  TORCH_INTERNAL_ASSERT(
      bounds_it->second.size() == 1,
      buildErrorMessage(
          ""Unexpected number of bound info entries in cacheAccesses in the fuser.""));
TensorAccessBoundsInfo& info = bounds_it->second[0];
bool hasReads = info.kind == kLoad || info.kind == kMutate;
bool hasWrites = info.kind == kStore || info.kind == kMutate;
"
400,"if (aStrides.empty() || oStrides.empty()) {
return false;
}
  TORCH_INTERNAL_ASSERT(info->bounds().size() == other->bounds().size());
for (size_t b = 0; b < info->bounds().size(); ++b) {
ExprPtr aIndexStride = aStrides[b];
ExprPtr oIndexStride = oStrides[b];
","if (aStrides.empty() || oStrides.empty()) {
return false;
}
  TORCH_INTERNAL_ASSERT(
      info->bounds().size() == other->bounds().size(),
      buildErrorMessage(
          ""Dimension mismatch for two accesses in mem dep checker in the fuser.""));
for (size_t b = 0; b < info->bounds().size(); ++b) {
ExprPtr aIndexStride = aStrides[b];
ExprPtr oIndexStride = oStrides[b];
"
401,"}
// assume JIT not supporting complex and qint yet
  TORCH_INTERNAL_ASSERT((typeConstraints & (kQintTypes | kComplexTypes)) == 0);
return false;
}
","}
// assume JIT not supporting complex and qint yet
  TORCH_INTERNAL_ASSERT(
      (typeConstraints & (kQintTypes | kComplexTypes)) == 0,
      buildErrorMessage(
          ""Qint and Complex types are not supported in the fuser.""));
return false;
}
"
402,"&& input.suggest_memory_format() == grad_out_.suggest_memory_format();
if (all_contiguous) {
batch_norm_cpu_backward_stub(kCPU, grad_input, grad_weight, grad_bias,
grad_out_, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps);
return std::make_tuple(grad_input, grad_weight, grad_bias);
","&& input.suggest_memory_format() == grad_out_.suggest_memory_format();
if (all_contiguous) {
    if (grad_input_mask[0]) {
      grad_input = at::empty_like(input, suggest_memory_format_contig(input));
    }
batch_norm_cpu_backward_stub(kCPU, grad_input, grad_weight, grad_bias,
grad_out_, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps);
return std::make_tuple(grad_input, grad_weight, grad_bias);
"
403,"return self.tensor_split(sections, dim);
} else {
auto indices_data = tensor_indices_or_sections.data_ptr<int64_t>();
    std::vector<int64_t> indices(indices_data, indices_data + tensor_indices_or_sections.numel());
return self.tensor_split(indices, dim);
}
}
","return self.tensor_split(sections, dim);
} else {
auto indices_data = tensor_indices_or_sections.data_ptr<int64_t>();
    auto stride = tensor_indices_or_sections.stride(0);
    auto numel = tensor_indices_or_sections.numel();
    std::vector<int64_t> indices(numel);
    for (size_t offset = 0; offset < numel; offset++) {
      // indices tensor could be non-contiguous
      indices[offset] = *(indices_data + offset * stride);
    }
return self.tensor_split(indices, dim);
}
}
"
404,"const std::string& name = buf_->name_hint();
VarPtr new_var = alloc<Var>(name, v->dtype());
random_bindings_[alloc<Let>(new_var, v)] = index_vars_;
return new_var;
}
","const std::string& name = buf_->name_hint();
VarPtr new_var = alloc<Var>(name, v->dtype());
random_bindings_[alloc<Let>(new_var, v)] = index_vars_;
    GRAPH_DEBUG(
        ""ComputeInline: created random bindings for "", std::to_string(new_var));
return new_var;
}
"
405,"}
struct CudaIPCGlobalEntities {
std::mutex ref_counters_mutex_;
std::atomic<int64_t> sync_events_used_{0};
std::map<std::string, std::shared_ptr<CudaIPCRefCountersFile>>
ref_counters_files_;
std::shared_ptr<CudaIPCRefCountersFile> next_available_ref_counters_file_;
CudaIPCSentDataLimbo CudaIPCSentDataLimbo_;
  CudaIPCGlobalEntities()  = default;
~CudaIPCGlobalEntities() {
CudaIPCSentDataLimbo_.collect();
    // Clear shared blocks to avoid releasing shared blocks after
    // ~CudaIPCGlobalEntities is done since circular references causes the
    // destructor of ~CudaIPCSentData to access the cuda_ipc_global_entities
    // again.
    CudaIPCSentDataLimbo_.clear_shared_blocks();
safe_clean_current_file();
if (next_available_ref_counters_file_) {
warnProducerTerminatedBeforeSharedTensorsReleased();
}
}
void safe_clean_current_file() {
std::lock_guard<std::mutex> lock(ref_counters_mutex_);
","}
struct CudaIPCGlobalEntities {
  // This class is used as a singleton (see cuda_ipc_global_entities)
  // This variable is used to track its lifetime to avoid accessing it
  // after it was destroyed which would lead to segmentation faults
  // Note that a trvial type is used which doesn't suffer from construction
  // and destruction order issues
  static bool alive;

std::mutex ref_counters_mutex_;
std::atomic<int64_t> sync_events_used_{0};
std::map<std::string, std::shared_ptr<CudaIPCRefCountersFile>>
ref_counters_files_;
std::shared_ptr<CudaIPCRefCountersFile> next_available_ref_counters_file_;
CudaIPCSentDataLimbo CudaIPCSentDataLimbo_;
  CudaIPCGlobalEntities() { alive = true; }
~CudaIPCGlobalEntities() {
CudaIPCSentDataLimbo_.collect();
safe_clean_current_file();
if (next_available_ref_counters_file_) {
warnProducerTerminatedBeforeSharedTensorsReleased();
}
    alive = false;
}
void safe_clean_current_file() {
std::lock_guard<std::mutex> lock(ref_counters_mutex_);
"
406,"}
void ReturnRefCounter(const std::string& handle, uint64_t offset /* unused */) {
std::lock_guard<std::mutex> lock(
cuda_ipc_global_entities.ref_counters_mutex_);
auto& map = cuda_ipc_global_entities.ref_counters_files_;
","}
void ReturnRefCounter(const std::string& handle, uint64_t offset /* unused */) {
  if(!CudaIPCGlobalEntities::alive) {
    return;
  }
std::lock_guard<std::mutex> lock(
cuda_ipc_global_entities.ref_counters_mutex_);
auto& map = cuda_ipc_global_entities.ref_counters_files_;
"
407,"load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
        dtypes = get_qconfig_dtypes(qconfig)
        if dtypes == (torch.float16, torch.float16, None):
            op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
            return quantized_graph.create_node(
                ""call_method"", ""to"", (op_out, torch.float16,), {}
            )
else:
            return quantized_graph.node_copy(node, load_arg(quantized=None))
@register_quant_pattern(torch.nn.AdaptiveAvgPool1d)
@register_quant_pattern(torch.nn.AdaptiveAvgPool2d)
","load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
        if not is_reference:
            dtypes = get_qconfig_dtypes(qconfig)
            if dtypes == (torch.float16, torch.float16, None):
                op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
                return quantized_graph.create_node(
                    ""call_method"", ""to"", (op_out, torch.float16,), {}
                )
            else:
                return quantized_graph.node_copy(node, load_arg(quantized=None))
else:
            act_dtype = activation_dtype(qconfig)
            if act_dtype == torch.float:
                op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
                return op_out
            else:
                activation_post_process = \
                    self._maybe_get_last_node_only_observer(modules)
                assert activation_post_process is not None
                # make sure the input is quantized to act_dtype
                load_arg(quantized={0: act_dtype})(node.args)
                args = load_arg(quantized=torch.float)(node.args)
                kwargs = load_arg(quantized=torch.float)(node.kwargs)
                op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
                return quantize_node(
                    op_out, activation_post_process,
                    node, modules, quantized_graph, node_name_to_scope, is_input=False)
@register_quant_pattern(torch.nn.AdaptiveAvgPool1d)
@register_quant_pattern(torch.nn.AdaptiveAvgPool2d)
"
408,"namespace {
constexpr int64_t cufft_max_ndim = 3;
// Execute a general fft operation (can be c2c, onesided r2c or onesided c2r)
static const Tensor& _exec_fft(Tensor& out, const Tensor& self, IntArrayRef out_sizes,
IntArrayRef dim, bool forward) {
","namespace {
constexpr int64_t cufft_max_ndim = 3;
// ""Large"" here means a prime factor not special-cased by cuFFT
// Ref: https://docs.nvidia.com/cuda/cufft/index.html#accuracy-and-performance
bool has_large_prime_factor(int64_t n) {
  constexpr int64_t first_large_prime = 11;
  const std::array<int64_t, 4> prime_radices{{2, 3, 5, 7}};
  for (auto prime : prime_radices) {
    if (n < first_large_prime) {
        return false;
    }

    while (n % prime == 0) {
      n /= prime;
    }
  }
  return n != 1;
}

// Execute a general fft operation (can be c2c, onesided r2c or onesided c2r)
static const Tensor& _exec_fft(Tensor& out, const Tensor& self, IntArrayRef out_sizes,
IntArrayRef dim, bool forward) {
"
409,".sequenceNr(ctx->sequenceNr)
.fwdThreadId(ctx->fwdThreadId)
.scope(ctx->recFunScope)
          .setAsync(fn.isAsync());
if (ctx->shapes && !ctx->shapes->empty()) {
kineto_events_.back().shapes(*ctx->shapes);
}
",".sequenceNr(ctx->sequenceNr)
.fwdThreadId(ctx->fwdThreadId)
.scope(ctx->recFunScope)
          .setAsync(fn.isAsync())
          .debugHandle(ctx->debug_handle);
if (ctx->shapes && !ctx->shapes->empty()) {
kineto_events_.back().shapes(*ctx->shapes);
}
"
410,"bucket_index = bucket.index()
# Proceed as normal until the DDP buckets have been rebuilt
        if not ddp._has_rebuilt_buckets:
assert overlap_info.status == _OverlapStatus.UNINITIALIZED
return fut
if overlap_info.status == _OverlapStatus.UNINITIALIZED:
overlap_info.status = _OverlapStatus.DDP_HAS_REBUILT_BUCKETS

rank = zero.global_rank
        assigned_rank = zero._ddp_bucket_index_to_rank(bucket_index)
# Once DDP buckets have been rebuilt but ZeRO has not been
# properly initialized yet, collect the information needed
if overlap_info.status == _OverlapStatus.DDP_HAS_REBUILT_BUCKETS:
            _collect_ddp_bucket_info(bucket, zero, rank, assigned_rank)
return fut
assert overlap_info.status == _OverlapStatus.INITIALIZED
# Save the bucket reference and all-reduce future for the final bucket
        if assigned_rank == rank:
overlap_info.bucket_index_to_bucket[bucket_index] = bucket
overlap_info.bucket_index_to_future[bucket_index] = fut
","bucket_index = bucket.index()
# Proceed as normal until the DDP buckets have been rebuilt
        if not ddp_ref()._has_rebuilt_buckets:  # type: ignore[union-attr]
assert overlap_info.status == _OverlapStatus.UNINITIALIZED
return fut
if overlap_info.status == _OverlapStatus.UNINITIALIZED:
overlap_info.status = _OverlapStatus.DDP_HAS_REBUILT_BUCKETS
rank = zero.global_rank
# Once DDP buckets have been rebuilt but ZeRO has not been
# properly initialized yet, collect the information needed
if overlap_info.status == _OverlapStatus.DDP_HAS_REBUILT_BUCKETS:
            _save_ddp_bucket_info(bucket, zero)
return fut
assert overlap_info.status == _OverlapStatus.INITIALIZED
        assert len(overlap_info.assigned_ranks_per_bucket) > bucket_index, \
            ""`assigned_ranks_per_bucket` is not fully constructed""
        assigned_to_bucket = rank in overlap_info.assigned_ranks_per_bucket[bucket_index]
# Save the bucket reference and all-reduce future for the final bucket
        if assigned_to_bucket:
overlap_info.bucket_index_to_bucket[bucket_index] = bucket
overlap_info.bucket_index_to_future[bucket_index] = fut
"
411,"fut = hook(state, bucket)
# Proceed as normal until the DDP buckets have been rebuilt
        if not ddp._has_rebuilt_buckets:
assert zero._overlap_info.status == _OverlapStatus.UNINITIALIZED
return fut
","fut = hook(state, bucket)
# Proceed as normal until the DDP buckets have been rebuilt
        if not ddp_ref()._has_rebuilt_buckets:  # type: ignore[union-attr]
assert zero._overlap_info.status == _OverlapStatus.UNINITIALIZED
return fut
"
412,"bucket_indices_seen (List[int]): :class:`list` of the bucket indices
seen on this iteration.
""""""
    def __init__(self) -> None:
self.status: _OverlapStatus = _OverlapStatus.UNINITIALIZED
# Modified per bucket reconstruction
self.params_per_bucket: List[List[torch.Tensor]] = []
self.params_per_rank: List[List[torch.Tensor]] = \
            [[] for _ in range(dist.get_world_size())]
self.offsets: Dict[int, int] = {}
# Modified per iteration
self.broadcast_handles: List[Any] = []
","bucket_indices_seen (List[int]): :class:`list` of the bucket indices
seen on this iteration.
""""""
    def __init__(self, world_size) -> None:
self.status: _OverlapStatus = _OverlapStatus.UNINITIALIZED
        self.shard_buckets: bool = False
# Modified per bucket reconstruction
self.params_per_bucket: List[List[torch.Tensor]] = []
self.params_per_rank: List[List[torch.Tensor]] = \
            [[] for _ in range(world_size)]
self.offsets: Dict[int, int] = {}
        self.assigned_ranks_per_bucket: List[Set[int]] = []
        self.num_bucket_assignments: int = 0
        self.total_size: Optional[int] = None
# Modified per iteration
self.broadcast_handles: List[Any] = []
"
413,"if not overlap_with_ddp:
self._init_local_optimizer()
else:
            self._overlap_info: _OverlapInfo = _OverlapInfo()
if parameters_as_bucket_view:
logging.warning(
""`parameters_as_bucket_view=True` will be ignored since ""
","if not overlap_with_ddp:
self._init_local_optimizer()
else:
            self._overlap_info: _OverlapInfo = _OverlapInfo(self.world_size)
if parameters_as_bucket_view:
logging.warning(
""`parameters_as_bucket_view=True` will be ignored since ""
"
414,"#endif
#include <ATen/record_function.h>
namespace at {
","#endif
#include <ATen/record_function.h>
#include <ATen/SavedTensorHooks.h>
namespace at {
"
415,"############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(
        u""""""
Welcome to the developer reference for the PyTorch C++ API.
    """"""
    ),
}
# Tell sphinx what the primary language being documented is.
primary_domain = ""cpp""
# Tell sphinx what the pygments highlight language should be.
highlight_language = ""cpp""
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
","############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(u'''
Welcome to the developer reference for the PyTorch C++ API.
    '''),
}
# Tell sphinx what the primary language being documented is.
primary_domain = 'cpp'
# Tell sphinx what the pygments highlight language should be.
highlight_language = 'cpp'
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
"
416,"#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = ""master ("" + torch_version + "" )""
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = ""master""
# Customized html_title here.
# Default is "" "".join(project, release, ""documentation"") if not set
if RELEASE:
# remove hash (start with 'a') from version number if any
    version_end = torch_version.find(""a"")
if version_end == -1:
html_title = "" "".join((project, torch_version, ""documentation""))
version = torch_version
","#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = 'master (' + torch_version + ' )'
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = 'master'
# Customized html_title here.
# Default is "" "".join(project, release, ""documentation"") if not set
if RELEASE:
# remove hash (start with 'a') from version number if any
    version_end = torch_version.find('a')
if version_end == -1:
html_title = "" "".join((project, torch_version, ""documentation""))
version = torch_version
"
417,"autodoc_inherit_docstrings = False
# Disable displaying type annotations, these can be very verbose
autodoc_typehints = ""none""
# Enable overriding of function signatures in the first line of the docstring.
autodoc_docstring_signature = True
","autodoc_inherit_docstrings = False
# Disable displaying type annotations, these can be very verbose
autodoc_typehints = 'none'
# Enable overriding of function signatures in the first line of the docstring.
autodoc_docstring_signature = True
"
418,"# documentation.
html_theme_options = {
    ""pytorch_project"": ""docs"",
    ""canonical_url"": ""https://pytorch.org/docs/stable/"",
    ""collapse_navigation"": False,
    ""display_version"": True,
    ""logo_only"": True,
    ""analytics_id"": ""UA-117752657-2"",
}
html_logo = ""_static/img/pytorch-logo-dark-unstable.png""
if RELEASE:
    html_logo = ""_static/img/pytorch-logo-dark.svg""
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = [""_static""]
html_css_files = [
    ""css/jit.css"",
]
","# documentation.
html_theme_options = {
    'pytorch_project': 'docs',
    'canonical_url': 'https://pytorch.org/docs/stable/',
    'collapse_navigation': False,
    'display_version': True,
    'logo_only': True,
    'analytics_id': 'UA-117752657-2',
}
html_logo = '_static/img/pytorch-logo-dark-unstable.png'
if RELEASE:
    html_logo = '_static/img/pytorch-logo-dark.svg'
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['_static']
html_css_files = [
    'css/jit.css',
]
"
419,"i += 1
setuptools.command.build_ext.build_ext.build_extensions(self)
def get_outputs(self):
outputs = setuptools.command.build_ext.build_ext.get_outputs(self)
outputs.append(os.path.join(self.build_lib, ""caffe2""))
","i += 1
setuptools.command.build_ext.build_ext.build_extensions(self)

def get_outputs(self):
outputs = setuptools.command.build_ext.build_ext.get_outputs(self)
outputs.append(os.path.join(self.build_lib, ""caffe2""))
"
420,"is a single license file in the sdist and wheels with all of the necessary
licensing info.
""""""

def __init__(self):
        self.f1 = ""LICENSE""
        self.f2 = ""third_party/LICENSES_BUNDLED.txt""
def __enter__(self):
""""""Concatenate files""""""
        with open(self.f1, ""r"") as f1:
self.bsd_text = f1.read()
        with open(self.f1, ""a"") as f1:
            with open(self.f2, ""r"") as f2:
self.bundled_text = f2.read()
                f1.write(""\n\n"")
f1.write(self.bundled_text)
def __exit__(self, exception_type, exception_value, traceback):
""""""Restore content of f1""""""
        with open(self.f1, ""w"") as f:
f.write(self.bsd_text)
","is a single license file in the sdist and wheels with all of the necessary
licensing info.
""""""
def __init__(self):
        self.f1 = 'LICENSE'
        self.f2 = 'third_party/LICENSES_BUNDLED.txt'
def __enter__(self):
""""""Concatenate files""""""
        with open(self.f1, 'r') as f1:
self.bsd_text = f1.read()
        with open(self.f1, 'a') as f1:
            with open(self.f2, 'r') as f2:
self.bundled_text = f2.read()
                f1.write('\n\n')
f1.write(self.bundled_text)
def __exit__(self, exception_type, exception_value, traceback):
""""""Restore content of f1""""""
        with open(self.f1, 'w') as f:
f.write(self.bsd_text)
"
421,"raise err
try:
        ctypes.CDLL(""vcruntime140.dll"")
        ctypes.CDLL(""msvcp140.dll"")
        if cuda_version not in (""9.2"", ""10.0""):
            ctypes.CDLL(""vcruntime140_1.dll"")
except OSError:
        print(
            """"""Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.
                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe""""""
        )
    dlls = glob.glob(os.path.join(th_dll_path, ""*.dll""))
path_patched = False
for dll in dlls:
is_loaded = False
","raise err
try:
        ctypes.CDLL('vcruntime140.dll')
        ctypes.CDLL('msvcp140.dll')
        if cuda_version not in ('9.2', '10.0'):
            ctypes.CDLL('vcruntime140_1.dll')
except OSError:
        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.
                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')
    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))
path_patched = False
for dll in dlls:
is_loaded = False
"
422,"class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
pass

class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
pass

class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
pass

class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
pass

class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
pass

class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
pass

class QUInt4x2Storage(_C.QUInt4x2StorageBase, _StorageBase):
pass

_storage_classes = {
    DoubleStorage,
    FloatStorage,
    LongStorage,
    IntStorage,
    ShortStorage,
    CharStorage,
    ByteStorage,
    HalfStorage,
    BoolStorage,
    QUInt8Storage,
    QInt8Storage,
    QInt32Storage,
    BFloat16Storage,
    ComplexFloatStorage,
    ComplexDoubleStorage,
    QUInt4x2Storage,
}
# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()
_tensor_classes: Set[Type] = set()
from ._tensor_str import set_printoptions

# If you edit these imports, please update torch/__init__.py.in as well
from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed
from .serialization import save, load
################################################################################
# Initialize extension
################################################################################

def manager_path():
    if platform.system() == ""Windows"" or sys.executable == ""torch_deploy"":
return b""""
    path = get_file_path(""torch"", ""bin"", ""torch_shm_manager"")
    prepare_multiprocessing_environment(get_file_path(""torch""))
if not os.path.exists(path):
raise RuntimeError(""Unable to find torch_shm_manager at "" + path)
    return path.encode(""utf-8"")
# Shared memory manager needs to know the exact location of manager executable
","class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
pass
class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
pass
class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
pass
class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
pass
class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
pass
class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
pass
class QUInt4x2Storage(_C.QUInt4x2StorageBase, _StorageBase):
pass
_storage_classes = {
    DoubleStorage, FloatStorage, LongStorage, IntStorage, ShortStorage,
    CharStorage, ByteStorage, HalfStorage, BoolStorage, QUInt8Storage, QInt8Storage,
    QInt32Storage, BFloat16Storage, ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage
}
# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()
_tensor_classes: Set[Type] = set()
# If you edit these imports, please update torch/__init__.py.in as well
from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed
from .serialization import save, load
from ._tensor_str import set_printoptions
################################################################################
# Initialize extension
################################################################################
def manager_path():
    if platform.system() == 'Windows' or sys.executable == 'torch_deploy':
return b""""
    path = get_file_path('torch', 'bin', 'torch_shm_manager')
    prepare_multiprocessing_environment(get_file_path('torch'))
if not os.path.exists(path):
raise RuntimeError(""Unable to find torch_shm_manager at "" + path)
    return path.encode('utf-8')
# Shared memory manager needs to know the exact location of manager executable
"
423,"class AppDirs(object):
""""""Convenience wrapper for getting application dirs.""""""

    def __init__(
        self, appname=None, appauthor=None, version=None, roaming=False, multipath=False
    ):
self.appname = appname
self.appauthor = appauthor
self.version = version
","class AppDirs(object):
""""""Convenience wrapper for getting application dirs.""""""
    def __init__(self, appname=None, appauthor=None, version=None,
            roaming=False, multipath=False):
self.appname = appname
self.appauthor = appauthor
self.version = version
"
424,"pickler.persistent_id = persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()
    return (
        data_value,
        serialized_storages,
        serialized_dtypes,
        importer.zip_reader if importer else None,
    )

def _load_storages(id, zip_reader, obj_bytes, serialized_storages):
def persistent_load(saved_id):
assert isinstance(saved_id, tuple)
typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        if typename == ""storage"":
return serialized_storages[data[0]]
        if typename == ""reduce_deploy"":
reduce_id, func, args = data
if reduce_id not in _loaded_reduces:
_loaded_reduces[reduce_id] = func(_raw_packages[zip_reader], *args)
","pickler.persistent_id = persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()
    return data_value, serialized_storages, serialized_dtypes, importer.zip_reader if importer else None
def _load_storages(id, zip_reader, obj_bytes, serialized_storages):

def persistent_load(saved_id):
assert isinstance(saved_id, tuple)
typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        if typename == 'storage':
return serialized_storages[data[0]]
        if typename == 'reduce_deploy':
reduce_id, func, args = data
if reduce_id not in _loaded_reduces:
_loaded_reduces[reduce_id] = func(_raw_packages[zip_reader], *args)
"
425,"LockType: Type
try:
import _thread

LockType = _thread.LockType
except ImportError:
import _dummy_thread

LockType = _dummy_thread.LockType
# Wrapper functions that can call either of 2 functions depending on a boolean
# argument
boolean_dispatched: ""weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]"" = (
    weakref.WeakKeyDictionary()
)  # noqa: T484
def createResolutionCallbackFromEnv(lookup_base):
","LockType: Type
try:
import _thread
LockType = _thread.LockType
except ImportError:
import _dummy_thread
LockType = _dummy_thread.LockType
# Wrapper functions that can call either of 2 functions depending on a boolean
# argument
boolean_dispatched: 'weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]' = weakref.WeakKeyDictionary()  # noqa: T484
def createResolutionCallbackFromEnv(lookup_base):
"
426,"""if_false"": if_false,
""index"": arg_index,
""default"": default,
        ""arg_name"": arg_name,
}
return fn
","""if_false"": if_false,
""index"": arg_index,
""default"": default,
        ""arg_name"": arg_name
}
return fn
"
427,"line_no = current_frame.f_code.co_firstlineno
return class_name, line_no

# At the the point the decorator is applied to class methods the method
# has no reference to its owning class. _qualified_name would not include
# the class it is defined in, so any methods with the same name in the same file
","line_no = current_frame.f_code.co_firstlineno
return class_name, line_no
# At the the point the decorator is applied to class methods the method
# has no reference to its owning class. _qualified_name would not include
# the class it is defined in, so any methods with the same name in the same file
"
428,"return False
return issubclass(the_type, super_type)
    if not hasattr(ann, ""__module__""):
return False
union_optional = False
    if ann.__module__ == ""typing"" and (getattr(ann, ""__origin__"", None) is Union):
        args = getattr(ann, ""__args__"", ())
if len(args) == 2:
            union_optional = (
                safe_is_subclass(args[1], type(None))
                and not safe_is_subclass(args[0], type(None))
            ) or (
                safe_is_subclass(args[0], type(None))
                and not safe_is_subclass(args[1], type(None))
            )
    optional = ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is Optional
    )
return optional or union_optional

def is_future(ann) -> bool:
if ann is Future:
raise RuntimeError(
","return False
return issubclass(the_type, super_type)
    if not hasattr(ann, '__module__'):
return False
union_optional = False
    if ann.__module__ == 'typing' and \
       (getattr(ann, '__origin__', None) is Union):
        args = getattr(ann, '__args__', ())
if len(args) == 2:
            union_optional = (safe_is_subclass(args[1], type(None)) and not safe_is_subclass(args[0], type(None))) \
                or (safe_is_subclass(args[0], type(None)) and not safe_is_subclass(args[1], type(None)))
    optional = ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is Optional)
return optional or union_optional
def is_future(ann) -> bool:
if ann is Future:
raise RuntimeError(
"
429,"def __exit__(self, *args) -> None:
torch._C._jit_set_emit_hooks(self.hooks[0], self.hooks[1])

def _is_exception(obj) -> bool:
if not inspect.isclass(obj):
return False
return issubclass(obj, Exception)

def raise_error_container_parameter_missing(target_type) -> None:
    if target_type == ""Dict"":
raise RuntimeError(
""Attempted to use Dict without ""
""contained types. Please add contained type, e.g. ""
","def __exit__(self, *args) -> None:
torch._C._jit_set_emit_hooks(self.hooks[0], self.hooks[1])
def _is_exception(obj) -> bool:
if not inspect.isclass(obj):
return False
return issubclass(obj, Exception)
def raise_error_container_parameter_missing(target_type) -> None:
    if target_type == 'Dict':
raise RuntimeError(
""Attempted to use Dict without ""
""contained types. Please add contained type, e.g. ""
"
430,")
# compute the gradient part in span(U)
    res = _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)
# incorporate the Sylvester equation solution into the full gradient
# it resides in span(U_ortho)
res -= U_ortho.matmul(
        chr_poly_D_at_A_to_U_ortho_sign
        * torch.cholesky_solve(
            U_ortho_t.matmul(series_acc), chr_poly_D_at_A_to_U_ortho_L
)
).matmul(Ut)
return res

def _symeig_backward(D_grad, U_grad, A, D, U, largest):
# if `U` is square, then the columns of `U` is a complete eigenspace
if U.size(-1) == U.size(-2):
        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)
else:
        return _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest)

class LOBPCGAutogradFunction(torch.autograd.Function):
@staticmethod
    def forward(
        ctx,  # type: ignore[override]
        A: Tensor,
        k: Optional[int] = None,
        B: Optional[Tensor] = None,
        X: Optional[Tensor] = None,
        n: Optional[int] = None,
        iK: Optional[Tensor] = None,
        niter: Optional[int] = None,
        tol: Optional[float] = None,
        largest: Optional[bool] = None,
        method: Optional[str] = None,
        tracker: None = None,
        ortho_iparams: Optional[Dict[str, int]] = None,
        ortho_fparams: Optional[Dict[str, float]] = None,
        ortho_bparams: Optional[Dict[str, bool]] = None,
    ) -> Tuple[Tensor, Tensor]:
# makes sure that input is contiguous for efficiency.
# Note: autograd does not support dense gradients for sparse input yet.
",")
# compute the gradient part in span(U)
    res = _symeig_backward_complete_eigenspace(
        D_grad, U_grad, A, D, U
    )
# incorporate the Sylvester equation solution into the full gradient
# it resides in span(U_ortho)
res -= U_ortho.matmul(
        chr_poly_D_at_A_to_U_ortho_sign * torch.cholesky_solve(
            U_ortho_t.matmul(series_acc),
            chr_poly_D_at_A_to_U_ortho_L
)
).matmul(Ut)
return res
def _symeig_backward(D_grad, U_grad, A, D, U, largest):
# if `U` is square, then the columns of `U` is a complete eigenspace
if U.size(-1) == U.size(-2):
        return _symeig_backward_complete_eigenspace(
            D_grad, U_grad, A, D, U
        )
else:
        return _symeig_backward_partial_eigenspace(
            D_grad, U_grad, A, D, U, largest
        )
class LOBPCGAutogradFunction(torch.autograd.Function):

@staticmethod
    def forward(ctx,  # type: ignore[override]
                A: Tensor,
                k: Optional[int] = None,
                B: Optional[Tensor] = None,
                X: Optional[Tensor] = None,
                n: Optional[int] = None,
                iK: Optional[Tensor] = None,
                niter: Optional[int] = None,
                tol: Optional[float] = None,
                largest: Optional[bool] = None,
                method: Optional[str] = None,
                tracker: None = None,
                ortho_iparams: Optional[Dict[str, int]] = None,
                ortho_fparams: Optional[Dict[str, float]] = None,
                ortho_bparams: Optional[Dict[str, bool]] = None
                ) -> Tuple[Tensor, Tensor]:
# makes sure that input is contiguous for efficiency.
# Note: autograd does not support dense gradients for sparse input yet.
"
431,"self.S[..., :n] = self.X
W = _utils.matmul(self.iK, self.R)
            self.ivars[""converged_end""] = ns = n + np + W.shape[-1]
            self.S[:, n + np : ns] = W
else:
S_ = self.S[:, nc:ns]
Ri = self._get_rayleigh_ritz_transform(S_)
M = _utils.qform(_utils.qform(self.A, S_), Ri)
E_, Z = _utils.symeig(M, largest)
            self.X[:, nc:] = mm(S_, mm(Ri, Z[:, : n - nc]))
            self.E[nc:] = E_[: n - nc]
            P = mm(S_, mm(Ri, Z[:, n : 2 * n - nc]))
np = P.shape[-1]
self.update_residual()
nc = self.update_converged_count()
self.S[..., :n] = self.X
            self.S[:, n : n + np] = P
W = _utils.matmul(self.iK, self.R[:, nc:])
            self.ivars[""converged_end""] = ns = n + np + W.shape[-1]
            self.S[:, n + np : ns] = W
def _update_ortho(self):
""""""
Update or initialize iteration variables when `method == ""ortho""`.
""""""
mm = torch.matmul
        ns = self.ivars[""converged_end""]
        nc = self.ivars[""converged_count""]
        n = self.iparams[""n""]
        largest = self.bparams[""largest""]
        if self.ivars[""istep""] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
","self.S[..., :n] = self.X
W = _utils.matmul(self.iK, self.R)
            self.ivars['converged_end'] = ns = n + np + W.shape[-1]
            self.S[:, n + np:ns] = W
else:
S_ = self.S[:, nc:ns]
Ri = self._get_rayleigh_ritz_transform(S_)
M = _utils.qform(_utils.qform(self.A, S_), Ri)
E_, Z = _utils.symeig(M, largest)
            self.X[:, nc:] = mm(S_, mm(Ri, Z[:, :n - nc]))
            self.E[nc:] = E_[:n - nc]
            P = mm(S_, mm(Ri, Z[:, n:2 * n - nc]))
np = P.shape[-1]
self.update_residual()
nc = self.update_converged_count()
self.S[..., :n] = self.X
            self.S[:, n:n + np] = P
W = _utils.matmul(self.iK, self.R[:, nc:])
            self.ivars['converged_end'] = ns = n + np + W.shape[-1]
            self.S[:, n + np:ns] = W
def _update_ortho(self):
""""""
Update or initialize iteration variables when `method == ""ortho""`.
""""""
mm = torch.matmul
        ns = self.ivars['converged_end']
        nc = self.ivars['converged_count']
        n = self.iparams['n']
        largest = self.bparams['largest']
        if self.ivars['istep'] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
"
432,"# update S
self.S[:, :n] = self.X
            self.S[:, n : n + np] = P
            W = self._get_ortho(self.R[:, nc:], self.S[:, : n + np])
            ns = self.ivars[""converged_end""] = n + np + W.shape[-1]
            self.S[:, n + np : ns] = W
def _get_rayleigh_ritz_transform(self, S):
""""""Return a transformation matrix that is used in Rayleigh-Ritz
","# update S
self.S[:, :n] = self.X
            self.S[:, n:n + np] = P
            W = self._get_ortho(self.R[:, nc:], self.S[:, :n + np])
            ns = self.ivars['converged_end'] = n + np + W.shape[-1]
            self.S[:, n + np:ns] = W
def _get_rayleigh_ritz_transform(self, S):
""""""Return a transformation matrix that is used in Rayleigh-Ritz
"
433,"ctypes.CDLL(path)
self.loaded_libraries.add(path)

# The ops ""namespace""
ops = _Ops()
","ctypes.CDLL(path)
self.loaded_libraries.add(path)
# The ops ""namespace""
ops = _Ops()
"
434,"self.stride(),
quantizer_params,
self.requires_grad,
                        self._backward_hooks,
                    )
else:
new_tensor = self.new_empty([])
                    new_tensor.set_(
                        new_storage, self.storage_offset(), self.size(), self.stride()
                    )
if self.is_conj():
new_tensor = new_tensor.conj_physical()
if self.is_neg():
","self.stride(),
quantizer_params,
self.requires_grad,
                        self._backward_hooks)
else:
new_tensor = self.new_empty([])
                    new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())
if self.is_conj():
new_tensor = new_tensor.conj_physical()
if self.is_neg():
"
435,"# Warning: this method is NOT called when you torch.load() a tensor;
# that is managed by _rebuild_tensor_v2
if not self.is_leaf:
            raise RuntimeError(""__setstate__ can be only called on leaf Tensors"")
if len(state) == 4:
# legacy serialization of Tensor
self.set_(*state)
","# Warning: this method is NOT called when you torch.load() a tensor;
# that is managed by _rebuild_tensor_v2
if not self.is_leaf:
            raise RuntimeError('__setstate__ can be only called on leaf Tensors')
if len(state) == 4:
# legacy serialization of Tensor
self.set_(*state)
"
436,"gradient=gradient,
retain_graph=retain_graph,
create_graph=create_graph,
                inputs=inputs,
            )
        torch.autograd.backward(
            self, gradient, retain_graph, create_graph, inputs=inputs
        )
def register_hook(self, hook):
r""""""Registers a backward hook.
","gradient=gradient,
retain_graph=retain_graph,
create_graph=create_graph,
                inputs=inputs)
        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
def register_hook(self, hook):
r""""""Registers a backward hook.
"
437,"def norm(self, p=""fro"", dim=None, keepdim=False, dtype=None):
r""""""See :func:`torch.norm`""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype
            )
return torch.norm(self, p, dim, keepdim, dtype=dtype)
def lu(self, pivot=True, get_infos=False):
r""""""See :func:`torch.lu`""""""
# If get_infos is True, then we don't need to check for errors and vice versa
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.lu, (self,), self, pivot=pivot, get_infos=get_infos
            )
if not torch._jit_internal.is_scripting():
if self.requires_grad:
                if not (
                    self.size(-2) == self.size(-1)
                    and (self.dtype.is_floating_point)
                    or self.is_complex
                ):
raise ValueError(
                        ""lu.backward works only with batches of squared full-rank matrices""
                        "" of floating or complex types.""
)
from torch._autograd_functions import _LU

LU, pivots, infos = _LU.apply(self, pivot, get_infos)
if get_infos:
return LU, pivots, infos
","def norm(self, p=""fro"", dim=None, keepdim=False, dtype=None):
r""""""See :func:`torch.norm`""""""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype)
return torch.norm(self, p, dim, keepdim, dtype=dtype)
def lu(self, pivot=True, get_infos=False):
r""""""See :func:`torch.lu`""""""
# If get_infos is True, then we don't need to check for errors and vice versa
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.lu, (self,), self, pivot=pivot, get_infos=get_infos)
if not torch._jit_internal.is_scripting():
if self.requires_grad:
                if not (self.size(-2) == self.size(-1) and (self.dtype.is_floating_point) or self.is_complex):
raise ValueError(
                        'lu.backward works only with batches of squared full-rank matrices'
                        ' of floating or complex types.'
)
from torch._autograd_functions import _LU
LU, pivots, infos = _LU.apply(self, pivot, get_infos)
if get_infos:
return LU, pivots, infos
"
438,"raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (
            isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))
        ):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)
def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.rename_, (self,), self, *names, **rename_map
            )
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
","raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)

def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.rename_, (self,), self, *names, **rename_map)
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
"
439,"Args:
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.contiguous_format``.
"""""",
)
add_docstr_all(
    ""copy_"",
    r""""""
copy_(src, non_blocking=False) -> Tensor
Copies the elements from :attr:`src` into :attr:`self` tensor and returns
","Args:
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.contiguous_format``.
"""""")
add_docstr_all('copy_',
               r""""""
copy_(src, non_blocking=False) -> Tensor
Copies the elements from :attr:`src` into :attr:`self` tensor and returns
"
440,"the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""xpu"",
    r""""""
xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor
Returns a copy of this object in XPU memory.
","the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(**common_args))

add_docstr_all('xpu',
               r""""""
xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor
Returns a copy of this object in XPU memory.
"
441,"do not have a shared-storage narrow method.  Calling ``narrow_copy``
with ``dimemsion > self.sparse_dim()`` will return a copy with the
relevant dense dimension narrowed, and ``self.shape`` updated accordingly.
"""""",
)
add_docstr_all(
    ""ndimension"",
    r""""""
ndimension() -> int
Alias for :meth:`~Tensor.dim()`
"""""",
)
add_docstr_all(
    ""nan_to_num"",
    r""""""
nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor
See :func:`torch.nan_to_num`.
"""""",
)
add_docstr_all(
    ""nan_to_num_"",
    r""""""
nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor
In-place version of :meth:`~Tensor.nan_to_num`.
"""""",
)
add_docstr_all(
    ""ne"",
    r""""""
ne(other) -> Tensor
See :func:`torch.ne`.
"""""",
)
add_docstr_all(
    ""ne_"",
    r""""""
ne_(other) -> Tensor
In-place version of :meth:`~Tensor.ne`.
"""""",
)
add_docstr_all(
    ""not_equal"",
    r""""""
not_equal(other) -> Tensor
See :func:`torch.not_equal`.
"""""",
)
add_docstr_all(
    ""not_equal_"",
    r""""""
not_equal_(other) -> Tensor
In-place version of :meth:`~Tensor.not_equal`.
"""""",
)
add_docstr_all(
    ""neg"",
    r""""""
neg() -> Tensor
See :func:`torch.neg`
"""""",
)
add_docstr_all(
    ""negative"",
    r""""""
negative() -> Tensor
See :func:`torch.negative`
"""""",
)
add_docstr_all(
    ""neg_"",
    r""""""
neg_() -> Tensor
In-place version of :meth:`~Tensor.neg`
"""""",
)
add_docstr_all(
    ""negative_"",
    r""""""
negative_() -> Tensor
In-place version of :meth:`~Tensor.negative`
"""""",
)
add_docstr_all(
    ""nelement"",
    r""""""
nelement() -> int
Alias for :meth:`~Tensor.numel`
"""""",
)
add_docstr_all(
    ""nextafter"",
    r""""""
nextafter(other) -> Tensor
See :func:`torch.nextafter`
"""""",
)
add_docstr_all(
    ""nextafter_"",
    r""""""
nextafter_(other) -> Tensor
In-place version of :meth:`~Tensor.nextafter`
"""""",
)
add_docstr_all(
    ""nonzero"",
    r""""""
nonzero() -> LongTensor
See :func:`torch.nonzero`
"""""",
)
add_docstr_all(
    ""norm"",
    r""""""
norm(p=2, dim=None, keepdim=False) -> Tensor
See :func:`torch.norm`
"""""",
)
add_docstr_all(
    ""normal_"",
    r""""""
normal_(mean=0, std=1, *, generator=None) -> Tensor
Fills :attr:`self` tensor with elements samples from the normal distribution
parameterized by :attr:`mean` and :attr:`std`.
"""""",
)
add_docstr_all(
    ""numel"",
    r""""""
numel() -> int
See :func:`torch.numel`
"""""",
)
add_docstr_all(
    ""numpy"",
    r""""""
numpy() -> numpy.ndarray
Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the
returned :class:`ndarray` share the same underlying storage. Changes to
:attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.
"""""",
)
add_docstr_all(
    ""orgqr"",
    r""""""
orgqr(input2) -> Tensor
See :func:`torch.orgqr`
"""""",
)
add_docstr_all(
    ""ormqr"",
    r""""""
ormqr(input2, input3, left=True, transpose=False) -> Tensor
See :func:`torch.ormqr`
"""""",
)
add_docstr_all(
    ""permute"",
    r""""""
permute(*dims) -> Tensor
See :func:`torch.permute`
"""""",
)
add_docstr_all(
    ""polygamma"",
    r""""""
polygamma(n) -> Tensor
See :func:`torch.polygamma`
"""""",
)
add_docstr_all(
    ""polygamma_"",
    r""""""
polygamma_(n) -> Tensor
In-place version of :meth:`~Tensor.polygamma`
"""""",
)
add_docstr_all(
    ""positive"",
    r""""""
positive() -> Tensor
See :func:`torch.positive`
"""""",
)
add_docstr_all(
    ""pow"",
    r""""""
pow(exponent) -> Tensor
See :func:`torch.pow`
"""""",
)
add_docstr_all(
    ""pow_"",
    r""""""
pow_(exponent) -> Tensor
In-place version of :meth:`~Tensor.pow`
"""""",
)
add_docstr_all(
    ""float_power"",
    r""""""
float_power(exponent) -> Tensor
See :func:`torch.float_power`
"""""",
)
add_docstr_all(
    ""float_power_"",
    r""""""
float_power_(exponent) -> Tensor
In-place version of :meth:`~Tensor.float_power`
"""""",
)
add_docstr_all(
    ""prod"",
    r""""""
prod(dim=None, keepdim=False, dtype=None) -> Tensor
See :func:`torch.prod`
"""""",
)
add_docstr_all(
    ""put_"",
    r""""""
put_(index, source, accumulate=False) -> Tensor
Copies the elements from :attr:`source` into the positions specified by
","do not have a shared-storage narrow method.  Calling ``narrow_copy``
with ``dimemsion > self.sparse_dim()`` will return a copy with the
relevant dense dimension narrowed, and ``self.shape`` updated accordingly.
"""""")
add_docstr_all('ndimension',
               r""""""
ndimension() -> int
Alias for :meth:`~Tensor.dim()`
"""""")
add_docstr_all('nan_to_num', r""""""
nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor
See :func:`torch.nan_to_num`.
"""""")
add_docstr_all('nan_to_num_', r""""""
nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor
In-place version of :meth:`~Tensor.nan_to_num`.
"""""")
add_docstr_all('ne', r""""""
ne(other) -> Tensor
See :func:`torch.ne`.
"""""")
add_docstr_all('ne_', r""""""
ne_(other) -> Tensor
In-place version of :meth:`~Tensor.ne`.
"""""")
add_docstr_all('not_equal', r""""""
not_equal(other) -> Tensor
See :func:`torch.not_equal`.
"""""")
add_docstr_all('not_equal_', r""""""
not_equal_(other) -> Tensor
In-place version of :meth:`~Tensor.not_equal`.
"""""")
add_docstr_all('neg',
               r""""""
neg() -> Tensor
See :func:`torch.neg`
"""""")
add_docstr_all('negative',
               r""""""
negative() -> Tensor
See :func:`torch.negative`
"""""")
add_docstr_all('neg_',
               r""""""
neg_() -> Tensor
In-place version of :meth:`~Tensor.neg`
"""""")
add_docstr_all('negative_',
               r""""""
negative_() -> Tensor
In-place version of :meth:`~Tensor.negative`
"""""")
add_docstr_all('nelement',
               r""""""
nelement() -> int
Alias for :meth:`~Tensor.numel`
"""""")
add_docstr_all('nextafter',
               r""""""
nextafter(other) -> Tensor
See :func:`torch.nextafter`
"""""")
add_docstr_all('nextafter_',
               r""""""
nextafter_(other) -> Tensor
In-place version of :meth:`~Tensor.nextafter`
"""""")
add_docstr_all('nonzero',
               r""""""
nonzero() -> LongTensor
See :func:`torch.nonzero`
"""""")
add_docstr_all('norm',
               r""""""
norm(p=2, dim=None, keepdim=False) -> Tensor
See :func:`torch.norm`
"""""")
add_docstr_all('normal_',
               r""""""
normal_(mean=0, std=1, *, generator=None) -> Tensor
Fills :attr:`self` tensor with elements samples from the normal distribution
parameterized by :attr:`mean` and :attr:`std`.
"""""")
add_docstr_all('numel',
               r""""""
numel() -> int
See :func:`torch.numel`
"""""")
add_docstr_all('numpy',
               r""""""
numpy() -> numpy.ndarray
Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the
returned :class:`ndarray` share the same underlying storage. Changes to
:attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.
"""""")
add_docstr_all('orgqr',
               r""""""
orgqr(input2) -> Tensor
See :func:`torch.orgqr`
"""""")
add_docstr_all('ormqr',
               r""""""
ormqr(input2, input3, left=True, transpose=False) -> Tensor
See :func:`torch.ormqr`
"""""")
add_docstr_all('permute',
               r""""""
permute(*dims) -> Tensor
See :func:`torch.permute`
"""""")
add_docstr_all('polygamma',
               r""""""
polygamma(n) -> Tensor
See :func:`torch.polygamma`
"""""")
add_docstr_all('polygamma_',
               r""""""
polygamma_(n) -> Tensor
In-place version of :meth:`~Tensor.polygamma`
"""""")
add_docstr_all('positive',
               r""""""
positive() -> Tensor
See :func:`torch.positive`
"""""")
add_docstr_all('pow',
               r""""""
pow(exponent) -> Tensor
See :func:`torch.pow`
"""""")
add_docstr_all('pow_',
               r""""""
pow_(exponent) -> Tensor
In-place version of :meth:`~Tensor.pow`
"""""")
add_docstr_all('float_power',
               r""""""
float_power(exponent) -> Tensor
See :func:`torch.float_power`
"""""")
add_docstr_all('float_power_',
               r""""""
float_power_(exponent) -> Tensor
In-place version of :meth:`~Tensor.float_power`
"""""")
add_docstr_all('prod',
               r""""""
prod(dim=None, keepdim=False, dtype=None) -> Tensor
See :func:`torch.prod`
"""""")
add_docstr_all('put_',
               r""""""
put_(index, source, accumulate=False) -> Tensor
Copies the elements from :attr:`source` into the positions specified by
"
442,"real_str = formatter1.format(val.real)
imag_str = (formatter2.format(val.imag) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
            if imag_str[0] == ""+"" or imag_str[0] == ""-"":
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
","real_str = formatter1.format(val.real)
imag_str = (formatter2.format(val.imag) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
            if imag_str[0] == '+' or imag_str[0] == '-':
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
"
443,"return _vector_str(self, indent, summarize, formatter1, formatter2)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        slices = (
            [
                _tensor_str_with_formatter(
                    self[i], indent + 1, summarize, formatter1, formatter2
                )
                for i in range(0, PRINT_OPTS.edgeitems)
            ]
            + [""...""]
            + [
                _tensor_str_with_formatter(
                    self[i], indent + 1, summarize, formatter1, formatter2
                )
                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))
            ]
        )
else:
        slices = [
            _tensor_str_with_formatter(
                self[i], indent + 1, summarize, formatter1, formatter2
            )
            for i in range(0, self.size(0))
        ]

    tensor_str = ("","" + ""\n"" * (dim - 1) + "" "" * (indent + 1)).join(slices)
    return ""["" + tensor_str + ""]""
def _tensor_str(self, indent):
if self.numel() == 0:
        return ""[]""
if self.has_names():
# There are two main codepaths (possibly more) that tensor printing goes through:
","return _vector_str(self, indent, summarize, formatter1, formatter2)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        slices = ([_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                   for i in range(0, PRINT_OPTS.edgeitems)] +
                  ['...'] +
                  [_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                   for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])
else:
        slices = [_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                  for i in range(0, self.size(0))]
    tensor_str = (',' + '\n' * (dim - 1) + ' ' * (indent + 1)).join(slices)
    return '[' + tensor_str + ']'
def _tensor_str(self, indent):
if self.numel() == 0:
        return '[]'
if self.has_names():
# There are two main codepaths (possibly more) that tensor printing goes through:
"
444,"[-18.6971, -18.0736, -17.0994, -17.3216],
[ -6.7845,  -6.1610,  -5.1868,  -5.4090],
[ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.addbmm,
    r""""""
addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor
Performs a batch matrix-matrix product of matrices stored
","[-18.6971, -18.0736, -17.0994, -17.3216],
[ -6.7845,  -6.1610,  -5.1868,  -5.4090],
[ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
"""""".format(**common_args))

add_docstr(torch.addbmm,
           r""""""
addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor
Performs a batch matrix-matrix product of matrices stored
"
445,">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.sspaddmm,
    r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
",">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.sspaddmm,
           r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
"
446,">>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
[1.0795, 2.1939]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.as_tensor,
    r""""""
as_tensor(data, dtype=None, device=None) -> Tensor
Convert the data into a `torch.Tensor`. If the data is already a `Tensor` with the same `dtype` and `device`,
",">>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
[1.0795, 2.1939]])
"""""".format(**common_args))

add_docstr(torch.as_tensor,
           r""""""
as_tensor(data, dtype=None, device=None) -> Tensor
Convert the data into a `torch.Tensor`. If the data is already a `Tensor` with the same `dtype` and `device`,
"
447,"tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
>>> torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.arctan,
    r""""""
arctan(input, *, out=None) -> Tensor
Alias for :func:`torch.atan`.
"""""",
)
add_docstr(
    torch.atan2,
    r""""""
atan2(input, other, *, out=None) -> Tensor
Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}`
","tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
>>> torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
"""""".format(**common_args))

add_docstr(torch.arctan, r""""""
arctan(input, *, out=None) -> Tensor
Alias for :func:`torch.atan`.
"""""")
add_docstr(torch.atan2,
           r""""""
atan2(input, other, *, out=None) -> Tensor
Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}`
"
448,">>> batch2 = torch.randn(10, 4, 5)
>>> torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.bernoulli,
    r""""""
bernoulli(input, *, generator=None, out=None) -> Tensor
Draws binary random numbers (0 or 1) from a Bernoulli distribution.
",">>> batch2 = torch.randn(10, 4, 5)
>>> torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.bernoulli,
           r""""""
bernoulli(input, *, generator=None, out=None) -> Tensor
Draws binary random numbers (0 or 1) from a Bernoulli distribution.
"
449,".. math::
\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})
""""""
    + r""""""
The returned :attr:`out` tensor only has values 0 or 1 and is of the same
shape as :attr:`input`.
",".. math::
\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})
"""""" + r""""""
The returned :attr:`out` tensor only has values 0 or 1 and is of the same
shape as :attr:`input`.
"
450,".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
""""""
    + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
"""""" + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
"
451,">>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.bitwise_and,
    r""""""
bitwise_and(input, other, *, out=None) -> Tensor
Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
",">>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.bitwise_and,
           r""""""
bitwise_and(input, other, *, out=None) -> Tensor
Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
"
452,"[ 8,  9, 10, 11, 12]]),
tensor([[ 6],
[13]]))
"""""",
)
add_docstr(
    torch.chunk,
    r""""""
chunk(input, chunks, dim=0) -> List of Tensors
Attempts to split a tensor into the specified number of chunks. Each chunk is a view of
","[ 8,  9, 10, 11, 12]]),
tensor([[ 6],
[13]]))
"""""")
add_docstr(torch.chunk,
           r""""""
chunk(input, chunks, dim=0) -> List of Tensors
Attempts to split a tensor into the specified number of chunks. Each chunk is a view of
"
453,"responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
"""""",
)
add_docstr(
    torch.unsafe_split,
    r""""""
unsafe_split(tensor, split_size_or_sections, dim=0) -> List of Tensors
Works like :func:`torch.split` but without enforcing the autograd restrictions
","responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
"""""")
add_docstr(torch.unsafe_split,
           r""""""
unsafe_split(tensor, split_size_or_sections, dim=0) -> List of Tensors
Works like :func:`torch.split` but without enforcing the autograd restrictions
"
454,"1.0969, -0.4614],
[-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
0.5790,  0.1497]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.ceil,
    r""""""
ceil(input, *, out=None) -> Tensor
Returns a new tensor with the ceil of the elements of :attr:`input`,
","[-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
"""""".format(**common_args))

add_docstr(torch.ceil,
           r""""""
ceil(input, *, out=None) -> Tensor
Returns a new tensor with the ceil of the elements of :attr:`input`,
"
455,">>> z = torch.matmul(l, l.transpose(-1, -2))
>>> torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)
"""""",
)
add_docstr(
    torch.cholesky_solve,
    r""""""
cholesky_solve(input, input2, upper=False, *, out=None) -> Tensor
Solves a linear system of equations with a positive semidefinite
",">>> z = torch.matmul(l, l.transpose(-1, -2))
>>> torch.max(torch.abs(z - a)) # Max non-zero
tensor(2.3842e-07)
"""""")
add_docstr(torch.cholesky_solve, r""""""
cholesky_solve(input, input2, upper=False, *, out=None) -> Tensor
Solves a linear system of equations with a positive semidefinite
"
456,"[-2.4490, -1.5687,  1.9792],
[-0.8304, -1.3037,  0.5650],
[-1.2329,  1.9883,  1.0551]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.logcumsumexp,
    r""""""
logcumsumexp(input, dim, *, out=None) -> Tensor
Returns the logarithm of the cumulative summation of the exponentiation of
elements of :attr:`input` in the dimension :attr:`dim`.
","[-2.4490, -1.5687,  1.9792],
[-0.8304, -1.3037,  0.5650],
[-1.2329,  1.9883,  1.0551]])
"""""".format(**common_args))

add_docstr(torch.logcumsumexp,
           r""""""
logcumsumexp(input, dim, *, out=None) -> Tensor
Returns the logarithm of the cumulative summation of the exponentiation of
elements of :attr:`input` in the dimension :attr:`dim`.
"
457,"Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""",
)
add_docstr(
    torch.diag,
    r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
 If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
","Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""")
add_docstr(torch.diag,
           r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
"
458,">>> c = torch.tensor([3])
>>> torch.gcd(a, c)
tensor([1, 1, 3])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.ge,
    r""""""
ge(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} \geq \text{other}` element-wise.
""""""
    + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
",">>> c = torch.tensor([3])
>>> torch.gcd(a, c)
tensor([1, 1, 3])
"""""".format(**common_args))

add_docstr(torch.ge, r""""""
ge(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} \geq \text{other}` element-wise.
"""""" + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
"
459,"tensor(1.00000e-06 *
3.6386)
"""""",
)
add_docstr(
    torch.get_default_dtype,
    r""""""
get_default_dtype() -> torch.dtype
Get the current default floating point :class:`torch.dtype`.
","tensor(1.00000e-06 *
3.6386)
"""""")
add_docstr(torch.get_default_dtype,
           r""""""
get_default_dtype() -> torch.dtype
Get the current default floating point :class:`torch.dtype`.
"
460,"The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
""""""
    + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
","The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
"""""" + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
"
461,"[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.lcm,
    r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
","[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(**single_dim_common))

add_docstr(torch.lcm,
           r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
"
462,".. math::
y_{i} = \log_{10} (x_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
y_{i} = \log_{10} (x_{i})
"""""" + r""""""
Args:
{input}
"
463,"tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
>>> torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.log2,
    r""""""
log2(input, *, out=None) -> Tensor
Returns a new tensor with the logarithm to the base 2 of the elements
","tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])
>>> torch.log1p(a)
tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])
"""""".format(**common_args))

add_docstr(torch.log2,
           r""""""
log2(input, *, out=None) -> Tensor
Returns a new tensor with the logarithm to the base 2 of the elements
"
464,">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.lu_unpack,
    r""""""
lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -> (Tensor, Tensor, Tensor)
Unpacks the data and pivots from a LU factorization of a tensor into tensors ``L`` and ``U`` and a permutation tensor ``P``
",">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
"""""".format(**common_args))

add_docstr(torch.lu_unpack, r""""""
lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -> (Tensor, Tensor, Tensor)
Unpacks the data and pivots from a LU factorization of a tensor into tensors ``L`` and ``U`` and a permutation tensor ``P``
"
465,".. math::
\mathrm{e}^\text{input} = \sum_{k=0}^\infty \text{input}^k / k!
""""""
    + r""""""
The implementation is based on:
Bader, P.; Blanes, S.; Casas, F.
",".. math::
\mathrm{e}^\text{input} = \sum_{k=0}^\infty \text{input}^k / k!
"""""" + r""""""
The implementation is based on:
Bader, P.; Blanes, S.; Casas, F.
"
466,"[-1.6092,  0.5419, -0.2993,  0.3195]])
>>> torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.mean,
    r""""""
mean(input) -> Tensor
Returns the mean value of all elements in the :attr:`input` tensor.
","[-1.6092,  0.5419, -0.2993,  0.3195]])
>>> torch.argmax(a, dim=1)
tensor([ 0,  2,  0,  1])
"""""".format(**single_dim_common))

add_docstr(torch.mean,
           r""""""
mean(input) -> Tensor
Returns the mean value of all elements in the :attr:`input` tensor.
"
467,">>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
"""""".format(
        **common_args, **tf32_notes
    ),
)
add_docstr(
    torch.mode,
    r""""""
mode(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor)
Returns a namedtuple ``(values, indices)`` where ``values`` is the mode
",">>> torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
"""""".format(**common_args, **tf32_notes))
add_docstr(torch.mode,
           r""""""
mode(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor)
Returns a namedtuple ``(values, indices)`` where ``values`` is the mode
"
468,".. math::
\text{{out}}_i = \text{{input}}_i \times \text{{other}}_i
"""""".format(
        **common_args
    )
    + r""""""
Args:
input (Tensor): the first multiplicand tensor
",".. math::
\text{{out}}_i = \text{{input}}_i \times \text{{other}}_i
"""""".format(**common_args) + r""""""
Args:
input (Tensor): the first multiplicand tensor
"
469,">>> vec = torch.randn(3)
>>> torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.mvlgamma,
    r""""""
mvlgamma(input, p, *, out=None) -> Tensor
Alias for :func:`torch.special.multigammaln`.
"""""",
)
add_docstr(
    torch.movedim,
    r""""""
movedim(input, source, destination) -> Tensor
Moves the dimension(s) of :attr:`input` at the position(s) in :attr:`source`
",">>> vec = torch.randn(3)
>>> torch.mv(mat, vec)
tensor([ 1.0404, -0.6361])
"""""".format(**common_args))

add_docstr(torch.mvlgamma,
           r""""""
mvlgamma(input, p, *, out=None) -> Tensor
Alias for :func:`torch.special.multigammaln`.
"""""")
add_docstr(torch.movedim, r""""""
movedim(input, source, destination) -> Tensor
Moves the dimension(s) of :attr:`input` at the position(s) in :attr:`source`
"
470,"[[1, 5],
[3, 7]]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.swapaxes,
    r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
","[[1, 5],
[3, 7]]])
"""""".format(**common_args))

add_docstr(torch.swapaxes, r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
"
471,"tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""",
)
add_docstr(
    torch.nan_to_num,
    r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
","tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""")
add_docstr(torch.nan_to_num,
           r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
"
472,"tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
>>> torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.negative,
    r""""""
negative(input, *, out=None) -> Tensor
Alias for :func:`torch.neg`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.nextafter,
    r""""""
nextafter(input, other, *, out=None) -> Tensor
Return the next floating-point value after :attr:`input` towards :attr:`other`, elementwise.
","tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
>>> torch.neg(a)
tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
"""""".format(**common_args))

add_docstr(torch.negative,
           r""""""
negative(input, *, out=None) -> Tensor
Alias for :func:`torch.neg`
"""""".format(**common_args))

add_docstr(torch.nextafter,
           r""""""
nextafter(input, other, *, out=None) -> Tensor
Return the next floating-point value after :attr:`input` towards :attr:`other`, elementwise.
"
473,">>> torch.numel(a)
16
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.ones,
    r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `1`, with the shape defined
",">>> torch.numel(a)
16
"""""".format(**common_args))
add_docstr(torch.ones,
           r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `1`, with the shape defined
"
474,"[6, 7]])
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.randint_like,
    """"""
randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
","[6, 7]])
"""""".format(**factory_common_args))
add_docstr(torch.randint_like,
           """"""
randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
"
475,">>> torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
[ 1.0000], dtype=torch.float64)
"""""",
)
add_docstr(
    torch.set_num_threads,
    r""""""
set_num_threads(int)
Sets the number of threads used for intraop parallelism on CPU.
",">>> torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
[ 1.0000], dtype=torch.float64)
"""""")
add_docstr(torch.set_num_threads, r""""""
set_num_threads(int)
Sets the number of threads used for intraop parallelism on CPU.
"
476,"[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.msort,
    r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
","[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(**common_args))

add_docstr(torch.msort,
           r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
"
477,"[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(
        **common_args
    ),
)
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(
    torch.triu_indices,
    r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
","[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(**common_args))
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(torch.triu_indices,
           r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
"
478,">>> torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
[ 3.1416,  3.1416,  3.1416]])
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.full_like,
    """"""
full_like(input, fill_value, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
",">>> torch.full((2, 3), 3.141592)
tensor([[ 3.1416,  3.1416,  3.1416],
[ 3.1416,  3.1416,  3.1416]])
"""""".format(**factory_common_args))
add_docstr(torch.full_like,
           """"""
full_like(input, fill_value, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
"
479,".. note::
If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.
""""""
    + r""""""
Arguments:
window_length (int): the size of returned window
periodic (bool, optional): If True, returns a window to be used as periodic
",".. note::
If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.
"""""" + r""""""
Arguments:
window_length (int): the size of returned window
periodic (bool, optional): If True, returns a window to be used as periodic
"
480,"{device}
{requires_grad}
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.vander,
    """"""
vander(x, N=None, increasing=False) -> Tensor
""""""
    + r""""""
Generates a Vandermonde matrix.
The columns of the output matrix are elementwise powers of the input vector :math:`x^{{(N-1)}}, x^{{(N-2)}}, ..., x^0`.
","{device}
{requires_grad}
"""""".format(**factory_common_args))
add_docstr(torch.vander,
           """"""
vander(x, N=None, increasing=False) -> Tensor
"""""" + r""""""
Generates a Vandermonde matrix.
The columns of the output matrix are elementwise powers of the input vector :math:`x^{{(N-1)}}, x^{{(N-2)}}, ..., x^0`.
"
481,"quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
tensor([ 0, 10, 20, 30], dtype=torch.uint8)
"""""",
)
add_docstr(
    torch.quantize_per_channel,
    r""""""
quantize_per_channel(input, scales, zero_points, axis, dtype) -> Tensor
Converts a float tensor to a per-channel quantized tensor with given scales and zero points.
","quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)
>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()
tensor([ 0, 10, 20, 30], dtype=torch.uint8)
"""""")
add_docstr(torch.quantize_per_channel,
           r""""""
quantize_per_channel(input, scales, zero_points, axis, dtype) -> Tensor
Converts a float tensor to a per-channel quantized tensor with given scales and zero points.
"
482,"**kwargs: For compatibility, may contain the key ``async`` in place of
the ``non_blocking`` argument.
""""""
    non_blocking = _get_async_or_non_blocking(""cuda"", non_blocking, kwargs)
if self.is_cuda:
if device is None:
device = torch.cuda.current_device()
","**kwargs: For compatibility, may contain the key ``async`` in place of
the ``non_blocking`` argument.
""""""
    non_blocking = _get_async_or_non_blocking('cuda', non_blocking, kwargs)
if self.is_cuda:
if device is None:
device = torch.cuda.current_device()
"
483,"A tuple of two contiguous 1D buffers, one containing input tensors'
indices and the other containing the values.
""""""
    flat_indices = torch._C._nn.flatten_dense_tensors(
        [torch.Tensor._indices(t) for t in tensors]
    )
    flat_values = torch._C._nn.flatten_dense_tensors(
        [torch.Tensor._values(t) for t in tensors]
    )
return flat_indices, flat_values
","A tuple of two contiguous 1D buffers, one containing input tensors'
indices and the other containing the values.
""""""
    flat_indices = torch._C._nn.flatten_dense_tensors([torch.Tensor._indices(t) for t in tensors])
    flat_values = torch._C._nn.flatten_dense_tensors([torch.Tensor._values(t) for t in tensors])
return flat_indices, flat_values
"
484,"flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(
        flat_indices, [torch.Tensor._indices(t) for t in tensors]
    )
    values = torch._C._nn.unflatten_dense_tensors(
        flat_values, [torch.Tensor._values(t) for t in tensors]
    )
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
","flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(flat_indices, [torch.Tensor._indices(t) for t in tensors])
    values = torch._C._nn.unflatten_dense_tensors(flat_values, [torch.Tensor._values(t) for t in tensors])
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
"
485,"# use is the FB build environment, where this source file is replaced
# by an equivalent.
if sys.executable == ""torch_deploy"":
# __file__ is meaningless in the context of frozen torch used in torch deploy.
# setting empty torch_parent should allow below functions to operate without crashing,
# but it's unclear if there is a valid use case for them in the context of deploy.
torch_parent = """"
else:
    if os.path.basename(os.path.dirname(__file__)) == ""shared"":
torch_parent = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
else:
torch_parent = os.path.dirname(os.path.dirname(__file__))

def get_file_path(*path_components: str) -> str:
return os.path.join(torch_parent, *path_components)
","# use is the FB build environment, where this source file is replaced
# by an equivalent.
if sys.executable == 'torch_deploy':
# __file__ is meaningless in the context of frozen torch used in torch deploy.
# setting empty torch_parent should allow below functions to operate without crashing,
# but it's unclear if there is a valid use case for them in the context of deploy.
torch_parent = """"
else:
    if os.path.basename(os.path.dirname(__file__)) == 'shared':
torch_parent = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
else:
torch_parent = os.path.dirname(os.path.dirname(__file__))
def get_file_path(*path_components: str) -> str:
return os.path.join(torch_parent, *path_components)
"
486,"if isinstance(batched_outputs, Tensor):
out_dim = out_dims_as_tuple[0]
return torch._remove_batch_dim(batched_outputs, vmap_level, batch_size, out_dim)  # type: ignore[return-value]
    return tuple(
        torch._remove_batch_dim(out, vmap_level, batch_size, out_dim)
        for out, out_dim in zip(batched_outputs, out_dims_as_tuple)
    )

# Checks that `fn` returned one or more Tensors and nothing else.
# NB: A python function that return multiple arguments returns a single tuple,
","if isinstance(batched_outputs, Tensor):
out_dim = out_dims_as_tuple[0]
return torch._remove_batch_dim(batched_outputs, vmap_level, batch_size, out_dim)  # type: ignore[return-value]
    return tuple(torch._remove_batch_dim(out, vmap_level, batch_size, out_dim)
                 for out, out_dim in zip(batched_outputs, out_dims_as_tuple))
# Checks that `fn` returned one or more Tensors and nothing else.
# NB: A python function that return multiple arguments returns a single tuple,
"
487,"if isinstance(outputs, Tensor):
return
if not isinstance(outputs, tuple):
        raise ValueError(
            f""vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return ""
            f""Tensors, got type {type(outputs)} as the return.""
        )
for idx, output in enumerate(outputs):
if isinstance(output, Tensor):
continue
        raise ValueError(
            f""vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return ""
            f""Tensors, got type {type(output)} for return {idx}.""
        )

def _check_out_dims_is_int_or_int_tuple(out_dims: out_dims_t, func: Callable) -> None:
if isinstance(out_dims, int):
return
    if not isinstance(out_dims, tuple) or not all(
        [isinstance(out_dim, int) for out_dim in out_dims]
    ):
raise ValueError(
            f""vmap({_get_name(func)}, ..., out_dims={out_dims}): `out_dims` must be ""
            f""an int or a tuple of int representing where in the outputs the ""
            f""vmapped dimension should appear.""
        )

def _get_name(func: Callable):
    if hasattr(func, ""__name__""):
return func.__name__
# Not all callables have __name__, in fact, only static functions/methods do.
","if isinstance(outputs, Tensor):
return
if not isinstance(outputs, tuple):
        raise ValueError(f'vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return '
                         f'Tensors, got type {type(outputs)} as the return.')
for idx, output in enumerate(outputs):
if isinstance(output, Tensor):
continue
        raise ValueError(f'vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return '
                         f'Tensors, got type {type(output)} for return {idx}.')
def _check_out_dims_is_int_or_int_tuple(out_dims: out_dims_t, func: Callable) -> None:
if isinstance(out_dims, int):
return
    if not isinstance(out_dims, tuple) or \
            not all([isinstance(out_dim, int) for out_dim in out_dims]):
raise ValueError(
            f'vmap({_get_name(func)}, ..., out_dims={out_dims}): `out_dims` must be '
            f'an int or a tuple of int representing where in the outputs the '
            f'vmapped dimension should appear.')
def _get_name(func: Callable):
    if hasattr(func, '__name__'):
return func.__name__
# Not all callables have __name__, in fact, only static functions/methods do.
"
488,"# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
    from tensorflow.core.framework import graph_pb2
from tensorflow.core.util import event_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(
        ""TensorBoard visualization of GraphExecutors requires having ""
        ""TensorFlow installed""
    ) from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(
            wall_time=time.time(), graph_def=pb_graph.SerializeToString()
        )
w.add_event(evt)
def visualize(graph, name_prefix="""", pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(
            graph, name_prefix, pb_graph, partial(visualize, pb_graph=pb_graph)
        )
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op=""input"", name=name_prefix + ""input"")
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + ""input:"" + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op=""output"", name=name_prefix + ""output"")
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
","# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
from tensorflow.core.util import event_pb2
    from tensorflow.core.framework import graph_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(""TensorBoard visualization of GraphExecutors requires having ""
                      ""TensorFlow installed"") from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
w.add_event(evt)
def visualize(graph, name_prefix='', pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(graph, name_prefix, pb_graph,
                                 partial(visualize, pb_graph=pb_graph))
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op='input', name=name_prefix + 'input')
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + 'input:' + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op='output', name=name_prefix + 'output')
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
"
489,">>> torch.fft.fftfreq(4)
tensor([ 0.0000,  0.2500, -0.5000, -0.2500])
"""""".format(
        **factory_common_args
    ),
)
rfftfreq = _add_docstr(
    _fft.fft_rfftfreq,
    r""""""
rfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Computes the sample frequencies for :func:`~torch.fft.rfft` with a signal of size :attr:`n`.
",">>> torch.fft.fftfreq(4)
tensor([ 0.0000,  0.2500, -0.5000, -0.2500])
"""""".format(**factory_common_args))
rfftfreq = _add_docstr(_fft.fft_rfftfreq, r""""""
rfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Computes the sample frequencies for :func:`~torch.fft.rfft` with a signal of size :attr:`n`.
"
490,"return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(
    input: Tensor,
    n_fft: int,
    hop_length: Optional[int] = None,
    win_length: Optional[int] = None,
    window: Optional[Tensor] = None,
    center: bool = True,
    pad_mode: str = ""reflect"",
    normalized: bool = False,
    onesided: Optional[bool] = None,
    return_complex: Optional[bool] = None,
) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
","return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,
         win_length: Optional[int] = None, window: Optional[Tensor] = None,
         center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,
         onesided: Optional[bool] = None,
         return_complex: Optional[bool] = None) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
"
491,"pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
","pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
"
492,"# Workaround for https://github.com/python/typing/issues/449 in Python 3.6
from typing import GenericMeta
    class _PyFutureMeta(type(torch._C.Future), GenericMeta):  # type: ignore[misc]
pass


else:

class _PyFutureMeta(type(torch._C.Future), type(Generic)):  # type: ignore[misc, no-redef]
pass

class Future(torch._C.Future, Generic[T], metaclass=_PyFutureMeta):
r""""""
Wrapper around a ``torch._C.Future`` which encapsulates an asynchronous
","# Workaround for https://github.com/python/typing/issues/449 in Python 3.6
from typing import GenericMeta
    class _PyFutureMeta(type(torch._C.Future), GenericMeta):   # type: ignore[misc]
pass
else:
class _PyFutureMeta(type(torch._C.Future), type(Generic)):  # type: ignore[misc, no-redef]
pass
class Future(torch._C.Future, Generic[T], metaclass=_PyFutureMeta):
r""""""
Wrapper around a ``torch._C.Future`` which encapsulates an asynchronous
"
493,"if self.total is None:
sys.stderr.write(""\r{0:.1f} bytes"".format(self.n))
else:
                    sys.stderr.write(
                        ""\r{0:.1f}%"".format(100 * self.n / float(self.total))
                    )
sys.stderr.flush()
def close(self):
","if self.total is None:
sys.stderr.write(""\r{0:.1f} bytes"".format(self.n))
else:
                    sys.stderr.write(""\r{0:.1f}%"".format(100 * self.n / float(self.total)))
sys.stderr.flush()
def close(self):
"
494,"variable is not set.
""""""
# Issue warning to move data if old env is set
    if os.getenv(""TORCH_HUB""):
        warnings.warn(""TORCH_HUB is deprecated, please use env TORCH_HOME instead"")
if _hub_dir is not None:
return _hub_dir
    return os.path.join(_get_torch_home(), ""hub"")
def set_dir(d):
","variable is not set.
""""""
# Issue warning to move data if old env is set
    if os.getenv('TORCH_HUB'):
        warnings.warn('TORCH_HUB is deprecated, please use env TORCH_HOME instead')
if _hub_dir is not None:
return _hub_dir
    return os.path.join(_get_torch_home(), 'hub')
def set_dir(d):
"
495,">>> info
tensor(0, dtype=torch.int32)
"""""",
)
inv = _add_docstr(
    _linalg.linalg_inv,
    r""""""
linalg.inv(A, *, out=None) -> Tensor
Computes the inverse of a square matrix if it exists.
",">>> info
tensor(0, dtype=torch.int32)
"""""")
inv = _add_docstr(_linalg.linalg_inv, r""""""
linalg.inv(A, *, out=None) -> Tensor
Computes the inverse of a square matrix if it exists.
"
496,"https://pytorch.org/docs/master/linalg.html#torch.linalg.cond
.. _full description of these drivers:
https://www.netlib.org/lapack/lug/node27.html
"""""",
)
matrix_power = _add_docstr(
    _linalg.linalg_matrix_power,
    r""""""
matrix_power(A, n, *, out=None) -> Tensor
Computes the `n`-th power of a square matrix for an integer `n`.
","https://pytorch.org/docs/master/linalg.html#torch.linalg.cond
.. _full description of these drivers:
https://www.netlib.org/lapack/lug/node27.html
"""""")
matrix_power = _add_docstr(_linalg.linalg_matrix_power, r""""""
matrix_power(A, n, *, out=None) -> Tensor
Computes the `n`-th power of a square matrix for an integer `n`.
"
497,"tensor([ 3.7417, 11.2250])
>>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""",
)
vector_norm = _add_docstr(
    _linalg.linalg_vector_norm,
    r""""""
linalg.vector_norm(A, ord=2, dim=None, keepdim=False, *, dtype=None, out=None) -> Tensor
Computes a vector norm.
","tensor([ 3.7417, 11.2250])
>>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""")
vector_norm = _add_docstr(_linalg.linalg_vector_norm, r""""""
linalg.vector_norm(A, ord=2, dim=None, keepdim=False, *, dtype=None, out=None) -> Tensor
Computes a vector norm.
"
498,"torch.bucketize: lambda input, boundaries, out_int32=False, right=False, out=None: -1,
torch.cartesian_prod: lambda *tensors: -1,
torch.cat: lambda tensors, dim=0, out=None: -1,
        torch.cdist: lambda x1, x2, p=2.0, compute_mode=""use_mm_for_euclid_dist_if_necessary"": -1,
torch.ceil: lambda input, out=None: -1,
        torch.celu: lambda input, alhpa=1.0, inplace=False: -1,
torch.chain_matmul: lambda *matrices, out=None: -1,
        torch.channel_shuffle: lambda input, groups: -1,
torch.cholesky: lambda input, upper=False, out=None: -1,
torch.linalg.cholesky: lambda input, out=None: -1,
torch.linalg.cholesky_ex: lambda input, check_errors=False, out=None: -1,
","torch.bucketize: lambda input, boundaries, out_int32=False, right=False, out=None: -1,
torch.cartesian_prod: lambda *tensors: -1,
torch.cat: lambda tensors, dim=0, out=None: -1,
        torch.cdist: lambda x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary': -1,
torch.ceil: lambda input, out=None: -1,
        torch.celu: lambda input, alhpa=1., inplace=False: -1,
torch.chain_matmul: lambda *matrices, out=None: -1,
        torch.channel_shuffle: lambda input, groups : -1,
torch.cholesky: lambda input, upper=False, out=None: -1,
torch.linalg.cholesky: lambda input, out=None: -1,
torch.linalg.cholesky_ex: lambda input, check_errors=False, out=None: -1,
"
499,"torch.linalg.eigh: lambda input, UPLO=""L"", out=None: -1,
torch.linalg.eigvalsh: lambda input, UPLO=""L"", out=None: -1,
torch.einsum: lambda equation, *operands: -1,
        torch.embedding: (
            lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False: -1
        ),
        torch.embedding_bag: (
            lambda input, weight, offsets, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode=""mean"", sparse=False, per_sample_weights=None, padding_idx=None: -1
        ),
torch.empty_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
torch.eq: lambda input, other, out=None: -1,
torch.equal: lambda input, other: -1,
","torch.linalg.eigh: lambda input, UPLO=""L"", out=None: -1,
torch.linalg.eigvalsh: lambda input, UPLO=""L"", out=None: -1,
torch.einsum: lambda equation, *operands: -1,
        torch.embedding: (lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,
                          sparse=False: -1),
        torch.embedding_bag: (lambda input, weight, offsets, max_norm=None, norm_type=2, scale_grad_by_freq=False,
                              mode='mean', sparse=False, per_sample_weights=None, padding_idx=None: -1),
torch.empty_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
torch.eq: lambda input, other, out=None: -1,
torch.equal: lambda input, other: -1,
"
500,"call:
`has_torch_function_unary(t)`
which skips unnecessary packing and unpacking work.
    """""",
)
has_torch_function_variadic = _add_docstr(
","call:
`has_torch_function_unary(t)`
which skips unnecessary packing and unpacking work.
    """"""
)
has_torch_function_variadic = _add_docstr(
"
501,"# cannot be overriden by __torch_function__
if func in get_ignored_functions():
                msg = (
                    ""{}.{} is in the tuple returned by torch._overrides.get_ignored_functions ""
                    ""but still has an explicit override""
                )
                assert func not in get_testing_overrides(), msg.format(
                    namespace, func.__name__
                )
continue
overridable_funcs[namespace].append(func)
return overridable_funcs

@functools.lru_cache(None)
def _get_tensor_methods() -> Set[Callable]:
    """"""Returns a set of the overridable methods on ``torch.Tensor``""""""
overridable_funcs = get_overridable_functions()
methods = set(overridable_funcs[torch.Tensor])
return methods

def is_tensor_method_or_property(func: Callable) -> bool:
""""""
Returns True if the function passed in is a handler for a
","# cannot be overriden by __torch_function__
if func in get_ignored_functions():
                msg = (""{}.{} is in the tuple returned by torch._overrides.get_ignored_functions ""
                       ""but still has an explicit override"")
                assert func not in get_testing_overrides(), msg.format(namespace, func.__name__)
continue
overridable_funcs[namespace].append(func)
return overridable_funcs
@functools.lru_cache(None)
def _get_tensor_methods() -> Set[Callable]:
    """""" Returns a set of the overridable methods on ``torch.Tensor`` """"""
overridable_funcs = get_overridable_functions()
methods = set(overridable_funcs[torch.Tensor])
return methods
def is_tensor_method_or_property(func: Callable) -> bool:
""""""
Returns True if the function passed in is a handler for a
"
502,"r""""""
PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.
Profiler's context manager API can be used to better understand what model operators are the most expensive,
examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
","r'''
PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.
Profiler's context manager API can be used to better understand what model operators are the most expensive,
examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
"
503,"""""""
Profiler actions that can be taken at the specified intervals
""""""

NONE = 0
WARMUP = 1
RECORD = 2
RECORD_AND_SAVE = 3
def schedule(
    *, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0
) -> Callable:
""""""
Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip
the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,
","""""""
Profiler actions that can be taken at the specified intervals
""""""
NONE = 0
WARMUP = 1
RECORD = 2
RECORD_AND_SAVE = 3
def schedule(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -> Callable:
""""""
Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip
the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,
"
504,"if self.current_action == ProfilerAction.WARMUP:
self._start_trace()
self._stop_trace()
        elif self.current_action in [
            ProfilerAction.RECORD,
            ProfilerAction.RECORD_AND_SAVE,
        ]:
self._stop_trace()
if self.on_trace_ready:
self.on_trace_ready(self)
","if self.current_action == ProfilerAction.WARMUP:
self._start_trace()
self._stop_trace()
        elif self.current_action in \
                [ProfilerAction.RECORD, ProfilerAction.RECORD_AND_SAVE]:
self._stop_trace()
if self.on_trace_ready:
self.on_trace_ready(self)
"
505,"torch._sobol_engine_scramble_(self.sobolstate, ltm, self.dimension)
def __repr__(self):
        fmt_string = [f""dimension={self.dimension}""]
if self.scramble:
            fmt_string += [""scramble=True""]
if self.seed is not None:
            fmt_string += [f""seed={self.seed}""]
        return self.__class__.__name__ + ""("" + "", "".join(fmt_string) + "")""
","torch._sobol_engine_scramble_(self.sobolstate, ltm, self.dimension)
def __repr__(self):
        fmt_string = [f'dimension={self.dimension}']
if self.scramble:
            fmt_string += ['scramble=True']
if self.seed is not None:
            fmt_string += [f'seed={self.seed}']
        return self.__class__.__name__ + '(' + ', '.join(fmt_string) + ')'
"
506,"else:
view_metadata = None
            return (
                ""storage"",
                storage_type,
                obj_key,
                location,
                obj.size(),
                view_metadata,
            )
return None
sys_info = dict(
protocol_version=PROTOCOL_VERSION,
        little_endian=sys.byteorder == ""little"",
type_sizes=dict(
short=SHORT_SIZE,
int=INT_SIZE,
","else:
view_metadata = None
            return ('storage',
                    storage_type,
                    obj_key,
                    location,
                    obj.size(),
                    view_metadata)
return None
sys_info = dict(
protocol_version=PROTOCOL_VERSION,
        little_endian=sys.byteorder == 'little',
type_sizes=dict(
short=SHORT_SIZE,
int=INT_SIZE,
"
507,"""""""
_check_dill_version(pickle_module)
    if ""encoding"" not in pickle_load_args.keys():
        pickle_load_args[""encoding""] = ""utf-8""
    with _open_file_like(f, ""rb"") as opened_file:
if _is_zipfile(opened_file):
# The zipfile reader is going to advance the current file position.
# If we want to actually tail call to torch.jit.load, we need to
","""""""
_check_dill_version(pickle_module)
    if 'encoding' not in pickle_load_args.keys():
        pickle_load_args['encoding'] = 'utf-8'
    with _open_file_like(f, 'rb') as opened_file:
if _is_zipfile(opened_file):
# The zipfile reader is going to advance the current file position.
# If we want to actually tail call to torch.jit.load, we need to
"
508,"tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])
"""""".format(
        **common_args
    ),
)

logsumexp = _add_docstr(
    _special.special_logsumexp,
    r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Alias for :func:`torch.logsumexp`.
"""""".format(
        **multi_dim_common
    ),
)

expit = _add_docstr(
    _special.special_expit,
    r""""""
expit(input, *, out=None) -> Tensor
Computes the expit (also known as the logistic sigmoid function) of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}
""""""
    + r""""""
Args:
{input}
","tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])
"""""".format(**common_args))

logsumexp = _add_docstr(_special.special_logsumexp,
                        r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Alias for :func:`torch.logsumexp`.
"""""".format(**multi_dim_common))

expit = _add_docstr(_special.special_expit,
                    r""""""
expit(input, *, out=None) -> Tensor
Computes the expit (also known as the logistic sigmoid function) of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}
"""""" + r""""""
Args:
{input}
"
509,".. math::
y_{i} = 2^{x_{i}}
""""""
    + r""""""
Args:
{input}
",".. math::
y_{i} = 2^{x_{i}}
"""""" + r""""""
Args:
{input}
"
510,"tensor([1.3863, 2.7726, 4.1589])
>>> torch.special.xlogy(2, y)
tensor([2.1972, 1.3863, 0.0000])
"""""".format(
        **common_args
    ),
)

i0 = _add_docstr(
    _special.special_i0,
    r""""""
i0(input, *, out=None) -> Tensor
Computes the zeroth order modified Bessel function of the first kind for each element of :attr:`input`.
","tensor([1.3863, 2.7726, 4.1589])
>>> torch.special.xlogy(2, y)
tensor([2.1972, 1.3863, 0.0000])
"""""".format(**common_args))

i0 = _add_docstr(_special.special_i0,
                 r""""""
i0(input, *, out=None) -> Tensor
Computes the zeroth order modified Bessel function of the first kind for each element of :attr:`input`.
"
511,"import argparse
import ast
from caffe2.python.model_helper import ModelHelper
from caffe2.python.predictor import mobile_exporter
from caffe2.python import workspace, brew
def parse_kwarg(kwarg_str):
    key, value = kwarg_str.split('=')
try:
value = ast.literal_eval(value)
except ValueError:
","import argparse
import ast
from caffe2.python import workspace, brew
from caffe2.python.model_helper import ModelHelper
from caffe2.python.predictor import mobile_exporter
def parse_kwarg(kwarg_str):
    key, value = kwarg_str.split(""="")
try:
value = ast.literal_eval(value)
except ValueError:
"
512,"for op in predict_net.op:
print("" "", op.type, op.input, ""-->"", op.output)
    with open(args.predict_net, 'wb') as f:
f.write(predict_net.SerializeToString())
    with open(args.init_net, 'wb') as f:
f.write(init_net.SerializeToString())
if __name__ == ""__main__"":
parser = argparse.ArgumentParser(
        description=""Utilitity to generate Caffe2 benchmark models."")
parser.add_argument(""operator"", help=""Caffe2 operator to benchmark."")
    parser.add_argument(""-b"", ""--blob"",
                        help=""Instantiate a blob --blob name=dim1,dim2,dim3"",
                        action='append')
parser.add_argument(""--context"", help=""Context to run on."", default=""CPU"")
    parser.add_argument(""--kwargs"", help=""kwargs to pass to operator."",
                        nargs=""*"", type=parse_kwarg, default=[])
    parser.add_argument(""--init_net"", help=""Output initialization net."",
                        default=""init_net.pb"")
    parser.add_argument(""--predict_net"", help=""Output prediction net."",
                        default=""predict_net.pb"")
    parser.add_argument(""--benchmark_name"",
                        help=""Name of the benchmark network"",
                        default=""benchmark"")
    parser.add_argument(""--input_name"", help=""Name of the input blob."",
                        default=""data"")
    parser.add_argument(""--output_name"", help=""Name of the output blob."",
                        default=""output"")
    parser.add_argument(""--instances"",
                        help=""Number of instances to run the operator."",
                        default=""1"")
    parser.add_argument(""-d"", ""--debug"", help=""Print debug information."",
                        action='store_true')
    parser.add_argument(""-c"", ""--chain"",
                        help=""Chain ops together (create data dependencies)"",
                        action='store_true')
args = parser.parse_args()
main(args)
","for op in predict_net.op:
print("" "", op.type, op.input, ""-->"", op.output)
    with open(args.predict_net, ""wb"") as f:
f.write(predict_net.SerializeToString())
    with open(args.init_net, ""wb"") as f:
f.write(init_net.SerializeToString())
if __name__ == ""__main__"":
parser = argparse.ArgumentParser(
        description=""Utilitity to generate Caffe2 benchmark models.""
    )
parser.add_argument(""operator"", help=""Caffe2 operator to benchmark."")
    parser.add_argument(
        ""-b"",
        ""--blob"",
        help=""Instantiate a blob --blob name=dim1,dim2,dim3"",
        action=""append"",
    )
parser.add_argument(""--context"", help=""Context to run on."", default=""CPU"")
    parser.add_argument(
        ""--kwargs"",
        help=""kwargs to pass to operator."",
        nargs=""*"",
        type=parse_kwarg,
        default=[],
    )
    parser.add_argument(
        ""--init_net"", help=""Output initialization net."", default=""init_net.pb""
    )
    parser.add_argument(
        ""--predict_net"", help=""Output prediction net."", default=""predict_net.pb""
    )
    parser.add_argument(
        ""--benchmark_name"", help=""Name of the benchmark network"", default=""benchmark""
    )
    parser.add_argument(""--input_name"", help=""Name of the input blob."", default=""data"")
    parser.add_argument(
        ""--output_name"", help=""Name of the output blob."", default=""output""
    )
    parser.add_argument(
        ""--instances"", help=""Number of instances to run the operator."", default=""1""
    )
    parser.add_argument(
        ""-d"", ""--debug"", help=""Print debug information."", action=""store_true""
    )
    parser.add_argument(
        ""-c"",
        ""--chain"",
        help=""Chain ops together (create data dependencies)"",
        action=""store_true"",
    )
args = parser.parse_args()
main(args)
"
513,"#!/usr/bin/env python3




import argparse
from textwrap import dedent
from subprocess import call
def parse_lines(lines):
","#!/usr/bin/env python3
import argparse
from subprocess import call
from textwrap import dedent
def parse_lines(lines):
"
514,"# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'
# The master toctree document.
master_doc = 'index'
# General information about the project.
project = 'PyTorch'
copyright = '2019, Torch Contributors'
author = 'Torch Contributors'
# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
","# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = "".rst""
# The master toctree document.
master_doc = ""index""
# General information about the project.
project = ""PyTorch""
copyright = ""2019, Torch Contributors""
author = ""Torch Contributors""
# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
"
515,"#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = 'master'
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = 'master'
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
","#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = ""master""
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = ""master""
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
"
516,"# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = '1.6'
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.napoleon',
    'sphinx.ext.viewcode',
    'sphinxcontrib.katex',
    'sphinx.ext.autosectionlabel',
]
# build the templated autosummary files
","# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = ""1.6""
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    ""sphinx.ext.autodoc"",
    ""sphinx.ext.autosummary"",
    ""sphinx.ext.doctest"",
    ""sphinx.ext.intersphinx"",
    ""sphinx.ext.todo"",
    ""sphinx.ext.coverage"",
    ""sphinx.ext.napoleon"",
    ""sphinx.ext.viewcode"",
    ""sphinxcontrib.katex"",
    ""sphinx.ext.autosectionlabel"",
]
# build the templated autosummary files
"
517,"def get_correct_mypy_version():
# there's probably a more elegant way to do this
    match, = re.finditer(
        r'mypy==(\d+(?:\.\d+)*)',
        Path('.circleci/docker/common/install_conda.sh').read_text(),
)
    version, = match.groups()
return version
def plugin(version: str):
correct_version = get_correct_mypy_version()
if version != correct_version:
        print(f'''\
You are using mypy version {version}, which is not supported
in the PyTorch repo. Please switch to mypy version {correct_version}.
","def get_correct_mypy_version():
# there's probably a more elegant way to do this
    (match,) = re.finditer(
        r""mypy==(\d+(?:\.\d+)*)"",
        Path("".circleci/docker/common/install_conda.sh"").read_text(),
)
    (version,) = match.groups()
return version
def plugin(version: str):
correct_version = get_correct_mypy_version()
if version != correct_version:
        print(
            f""""""\
You are using mypy version {version}, which is not supported
in the PyTorch repo. Please switch to mypy version {correct_version}.
"
518,"def _embed_libiomp(self):
if not IS_DARWIN:
return
        lib_dir = os.path.join(self.build_lib, 'torch', 'lib')
        libtorch_cpu_path = os.path.join(lib_dir, 'libtorch_cpu.dylib')
if not os.path.exists(libtorch_cpu_path):
return
# Parse libtorch_cpu load commands
        otool_cmds = subprocess.check_output(['otool', '-l', libtorch_cpu_path]).decode('utf-8').split('\n')
rpaths, libs = [], []
for idx, line in enumerate(otool_cmds):
            if line.strip() == 'cmd LC_LOAD_DYLIB':
lib_name = otool_cmds[idx + 2].strip()
                assert lib_name.startswith('name ')
                libs.append(lib_name.split(' ', 1)[1].rsplit('(', 1)[0][:-1])
            if line.strip() == 'cmd LC_RPATH':
rpath = otool_cmds[idx + 2].strip()
                assert rpath.startswith('path ')
                rpaths.append(rpath.split(' ', 1)[1].rsplit('(', 1)[0][:-1])
        omp_lib_name = 'libiomp5.dylib'
        if os.path.join('@rpath', omp_lib_name) not in libs:
return
# Copy libiomp5 from rpath locations
","def _embed_libiomp(self):
if not IS_DARWIN:
return
        lib_dir = os.path.join(self.build_lib, ""torch"", ""lib"")
        libtorch_cpu_path = os.path.join(lib_dir, ""libtorch_cpu.dylib"")
if not os.path.exists(libtorch_cpu_path):
return
# Parse libtorch_cpu load commands
        otool_cmds = (
            subprocess.check_output([""otool"", ""-l"", libtorch_cpu_path])
            .decode(""utf-8"")
            .split(""\n"")
        )
rpaths, libs = [], []
for idx, line in enumerate(otool_cmds):
            if line.strip() == ""cmd LC_LOAD_DYLIB"":
lib_name = otool_cmds[idx + 2].strip()
                assert lib_name.startswith(""name "")
                libs.append(lib_name.split("" "", 1)[1].rsplit(""("", 1)[0][:-1])
            if line.strip() == ""cmd LC_RPATH"":
rpath = otool_cmds[idx + 2].strip()
                assert rpath.startswith(""path "")
                rpaths.append(rpath.split("" "", 1)[1].rsplit(""("", 1)[0][:-1])
        omp_lib_name = ""libiomp5.dylib""
        if os.path.join(""@rpath"", omp_lib_name) not in libs:
return
# Copy libiomp5 from rpath locations
"
519,"# ""install"" command by default.
# We only make this copy for Caffe2's pybind extensions
caffe2_pybind_exts = [
            'caffe2.python.caffe2_pybind11_state',
            'caffe2.python.caffe2_pybind11_state_gpu',
            'caffe2.python.caffe2_pybind11_state_hip',
]
i = 0
while i < len(self.extensions):
","# ""install"" command by default.
# We only make this copy for Caffe2's pybind extensions
caffe2_pybind_exts = [
            ""caffe2.python.caffe2_pybind11_state"",
            ""caffe2.python.caffe2_pybind11_state_gpu"",
            ""caffe2.python.caffe2_pybind11_state_hip"",
]
i = 0
while i < len(self.extensions):
"
520,"""""""
return torch._C._show_config()
# TODO: In principle, we could provide more structured version/config
# information here. For now only CXX_FLAGS is exposed, as Timer
# uses them.
","""""""
return torch._C._show_config()

# TODO: In principle, we could provide more structured version/config
# information here. For now only CXX_FLAGS is exposed, as Timer
# uses them.
"
521,"old_flags = sys.getdlopenflags()
sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_LAZY)
from torch._C import *  # noqa: F403
sys.setdlopenflags(old_flags)
del old_flags
del _dl_flags
","old_flags = sys.getdlopenflags()
sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_LAZY)
from torch._C import *  # noqa: F403

sys.setdlopenflags(old_flags)
del old_flags
del _dl_flags
"
522,"has_high_char = True
break
if has_high_char:
        buf = array.zeros('c', buf_size)
kernel = win32.Kernel32.INSTANCE
if kernel.GetShortPathName(dir, buf, buf_size):
dir = jna.Native.toString(buf.tostring()).rstrip(""\0"")
return dir
if system == ""win32"":
try:
import win32com.shell
_get_win_folder = _get_win_folder_with_pywin32
except ImportError:
try:
from ctypes import windll
_get_win_folder = _get_win_folder_with_ctypes
except ImportError:
try:
import com.sun.jna
_get_win_folder = _get_win_folder_with_jna
except ImportError:
_get_win_folder = _get_win_folder_from_registry
#---- self test code
if __name__ == ""__main__"":
appname = ""MyApp""
appauthor = ""MyCompany""
    props = (""user_data_dir"",
             ""user_config_dir"",
             ""user_cache_dir"",
             ""user_state_dir"",
             ""user_log_dir"",
             ""site_data_dir"",
             ""site_config_dir"")
print(""-- app dirs %s --"" % __version__)
","has_high_char = True
break
if has_high_char:
        buf = array.zeros(""c"", buf_size)
kernel = win32.Kernel32.INSTANCE
if kernel.GetShortPathName(dir, buf, buf_size):
dir = jna.Native.toString(buf.tostring()).rstrip(""\0"")
return dir

if system == ""win32"":
try:
import win32com.shell

_get_win_folder = _get_win_folder_with_pywin32
except ImportError:
try:
from ctypes import windll

_get_win_folder = _get_win_folder_with_ctypes
except ImportError:
try:
import com.sun.jna

_get_win_folder = _get_win_folder_with_jna
except ImportError:
_get_win_folder = _get_win_folder_from_registry
# ---- self test code
if __name__ == ""__main__"":
appname = ""MyApp""
appauthor = ""MyCompany""
    props = (
        ""user_data_dir"",
        ""user_config_dir"",
        ""user_cache_dir"",
        ""user_state_dir"",
        ""user_log_dir"",
        ""site_data_dir"",
        ""site_config_dir"",
    )
print(""-- app dirs %s --"" % __version__)
"
523,"import torch
class _LU(torch.autograd.Function):
@staticmethod
def forward(ctx, self, pivot=True, get_infos=False):
        LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))
ctx.save_for_backward(LU, pivots)
ctx.mark_non_differentiable(pivots, infos)
return LU, pivots, infos
","import torch

class _LU(torch.autograd.Function):
@staticmethod
def forward(ctx, self, pivot=True, get_infos=False):
        LU, pivots, infos = torch._lu_with_info(
            self, pivot=pivot, check_errors=(not get_infos)
        )
ctx.save_for_backward(LU, pivots)
ctx.mark_non_differentiable(pivots, infos)
return LU, pivots, infos
"
524,"import io
import torch
from torch.package._package_pickler import create_pickler
from torch.package._package_unpickler import PackageUnpickler
from torch.package import sys_importer, OrderedImporter, PackageImporter, Importer
from torch.serialization import _maybe_decode_ascii
def _save_storages(importer, obj):
serialized_storages = []
serialized_dtypes = []
","import io

import torch
from torch.package import sys_importer, OrderedImporter, PackageImporter, Importer
from torch.package._package_pickler import create_pickler
from torch.package._package_unpickler import PackageUnpickler
from torch.serialization import _maybe_decode_ascii

def _save_storages(importer, obj):
serialized_storages = []
serialized_dtypes = []
"
525,"name_to_type = {
name: parameter.annotation
for name, parameter in signature.parameters.items()
        if parameter.annotation is not inspect.Parameter.empty and not isinstance(parameter.annotation, str)
}
# Then, get the literal type annotations from the function declaration
","name_to_type = {
name: parameter.annotation
for name, parameter in signature.parameters.items()
        if parameter.annotation is not inspect.Parameter.empty
        and not isinstance(parameter.annotation, str)
}
# Then, get the literal type annotations from the function declaration
"
526,"""""""
if isinstance(fn, property):
prop = fn
        setattr(prop.fget, ""_torchscript_modifier"", FunctionModifiers.UNUSED)  # noqa: B010
if prop.fset:
            setattr(prop.fset, ""_torchscript_modifier"", FunctionModifiers.UNUSED)  # noqa: B010
return prop
fn._torchscript_modifier = FunctionModifiers.UNUSED
return fn
# No op context manager from python side
class _IgnoreContextManager(contextlib.AbstractContextManager):
def __init__(self, **kwargs):
","""""""
if isinstance(fn, property):
prop = fn
        setattr(
            prop.fget, ""_torchscript_modifier"", FunctionModifiers.UNUSED
        )  # noqa: B010
if prop.fset:
            setattr(
                prop.fset, ""_torchscript_modifier"", FunctionModifiers.UNUSED
            )  # noqa: B010
return prop
fn._torchscript_modifier = FunctionModifiers.UNUSED
return fn

# No op context manager from python side
class _IgnoreContextManager(contextlib.AbstractContextManager):
def __init__(self, **kwargs):
"
527,"return fn
if not isinstance(drop, bool):
        raise RuntimeError(""Argument to @torch.jit.ignore must be a bool or ""
                           f""a function but got {drop}"")
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
drop = drop_on_export
elif drop:
        warnings.warn(""ignore(True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
def decorator(fn):
if drop:
","return fn
if not isinstance(drop, bool):
        raise RuntimeError(
            ""Argument to @torch.jit.ignore must be a bool or ""
            f""a function but got {drop}""
        )
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(
            ""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
drop = drop_on_export
elif drop:
        warnings.warn(
            ""ignore(True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
def decorator(fn):
if drop:
"
528,"else:
fn._torchscript_modifier = FunctionModifiers.IGNORE
return fn
return decorator
","else:
fn._torchscript_modifier = FunctionModifiers.IGNORE
return fn

return decorator
"
529,"raise_error_container_parameter_missing(""Tuple"")
# For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is Tuple or
            getattr(ann, '__origin__', None) is tuple)
def is_list(ann) -> bool:
if ann is List:
raise_error_container_parameter_missing(""List"")
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is List or
            getattr(ann, '__origin__', None) is list)
def is_dict(ann) -> bool:
if ann is Dict:
raise_error_container_parameter_missing(""Dict"")
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is Dict or
            getattr(ann, '__origin__', None) is dict)
def is_optional(ann) -> bool:
if ann is Optional:
","raise_error_container_parameter_missing(""Tuple"")
# For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is Tuple
        or getattr(ann, ""__origin__"", None) is tuple
    )

def is_list(ann) -> bool:
if ann is List:
raise_error_container_parameter_missing(""List"")
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is List
        or getattr(ann, ""__origin__"", None) is list
    )

def is_dict(ann) -> bool:
if ann is Dict:
raise_error_container_parameter_missing(""Dict"")
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is Dict
        or getattr(ann, ""__origin__"", None) is dict
    )

def is_optional(ann) -> bool:
if ann is Optional:
"
530,"# its qualname so it appears correctly in the TorchScript system. This,
# we set '_jit_override_qualname' with the original traced module's
# qualified name, which is picked up here
    if hasattr(obj, '_jit_override_qualname'):
return obj._jit_override_qualname
# short-circuit in cases where the object already has a known qualified name
if isinstance(obj, torch._C.ScriptFunction):
","# its qualname so it appears correctly in the TorchScript system. This,
# we set '_jit_override_qualname' with the original traced module's
# qualified name, which is picked up here
    if hasattr(obj, ""_jit_override_qualname""):
return obj._jit_override_qualname
# short-circuit in cases where the object already has a known qualified name
if isinstance(obj, torch._C.ScriptFunction):
"
531,"import torch
from torch import Tensor
from . import _linalg_utils as _utils
from .overrides import has_torch_function, handle_torch_function
__all__ = ['lobpcg']
def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):
# compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0
F = D.unsqueeze(-2) - D.unsqueeze(-1)
    F.diagonal(dim1=-2, dim2=-1).fill_(float('inf'))
F.pow_(-1)
# A.grad = U (D.grad + (U^T U.grad * F)) U^T
Ut = U.transpose(-1, -2).contiguous()
res = torch.matmul(
        U,
        torch.matmul(
            torch.diag_embed(D_grad) + torch.matmul(Ut, U_grad) * F,
            Ut
        )
)
return res
","import torch
from torch import Tensor

from . import _linalg_utils as _utils
from .overrides import has_torch_function, handle_torch_function
__all__ = [""lobpcg""]

def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):
# compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0
F = D.unsqueeze(-2) - D.unsqueeze(-1)
    F.diagonal(dim1=-2, dim2=-1).fill_(float(""inf""))
F.pow_(-1)
# A.grad = U (D.grad + (U^T U.grad * F)) U^T
Ut = U.transpose(-1, -2).contiguous()
res = torch.matmul(
        U, torch.matmul(torch.diag_embed(D_grad) + torch.matmul(Ut, U_grad) * F, Ut)
)
return res
"
532,"if not torch.jit.is_scripting():
tensor_ops = (A, B, X, iK)
        if (not set(map(type, tensor_ops)).issubset((torch.Tensor, type(None))) and has_torch_function(tensor_ops)):
return handle_torch_function(
                lobpcg, tensor_ops, A, k=k,
                B=B, X=X, n=n, iK=iK, niter=niter, tol=tol,
                largest=largest, method=method, tracker=tracker,
ortho_iparams=ortho_iparams,
ortho_fparams=ortho_fparams,
                ortho_bparams=ortho_bparams)
if not torch._jit_internal.is_scripting():
if A.requires_grad or (B is not None and B.requires_grad):
","if not torch.jit.is_scripting():
tensor_ops = (A, B, X, iK)
        if not set(map(type, tensor_ops)).issubset(
            (torch.Tensor, type(None))
        ) and has_torch_function(tensor_ops):
return handle_torch_function(
                lobpcg,
                tensor_ops,
                A,
                k=k,
                B=B,
                X=X,
                n=n,
                iK=iK,
                niter=niter,
                tol=tol,
                largest=largest,
                method=method,
                tracker=tracker,
ortho_iparams=ortho_iparams,
ortho_fparams=ortho_fparams,
                ortho_bparams=ortho_bparams,
            )
if not torch._jit_internal.is_scripting():
if A.requires_grad or (B is not None and B.requires_grad):
"
533,"Update or initialize iteration variables when `method == ""basic""`.
""""""
mm = torch.matmul
        ns = self.ivars['converged_end']
        nc = self.ivars['converged_count']
        n = self.iparams['n']
        largest = self.bparams['largest']
        if self.ivars['istep'] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
","Update or initialize iteration variables when `method == ""basic""`.
""""""
mm = torch.matmul
        ns = self.ivars[""converged_end""]
        nc = self.ivars[""converged_count""]
        n = self.iparams[""n""]
        largest = self.bparams[""largest""]
        if self.ivars[""istep""] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
"
534,"if not torch.jit.is_scripting():
if type(A) is not torch.Tensor and has_torch_function((A,)):
            return handle_torch_function(pca_lowrank, (A,), A, q=q, center=center, niter=niter)
(m, n) = A.shape[-2:]
if q is None:
q = min(6, m, n)
elif not (q >= 0 and q <= min(m, n)):
        raise ValueError('q(={}) must be non-negative integer'
                         ' and not greater than min(m, n)={}'
                         .format(q, min(m, n)))
if not (niter >= 0):
        raise ValueError('niter(={}) must be non-negative integer'
                         .format(niter))
dtype = _utils.get_floating_dtype(A)
","if not torch.jit.is_scripting():
if type(A) is not torch.Tensor and has_torch_function((A,)):
            return handle_torch_function(
                pca_lowrank, (A,), A, q=q, center=center, niter=niter
            )
(m, n) = A.shape[-2:]
if q is None:
q = min(6, m, n)
elif not (q >= 0 and q <= min(m, n)):
        raise ValueError(
            ""q(={}) must be non-negative integer""
            "" and not greater than min(m, n)={}"".format(q, min(m, n))
        )
if not (niter >= 0):
        raise ValueError(""niter(={}) must be non-negative integer"".format(niter))
dtype = _utils.get_floating_dtype(A)
"
535,"return tensor._update_names(None, inplace)
return tensor._update_names(
        resolve_ellipsis(names, tensor.names, namer_api_name(inplace)), inplace)
def update_names_with_mapping(tensor, rename_map, inplace):
","return tensor._update_names(None, inplace)
return tensor._update_names(
        resolve_ellipsis(names, tensor.names, namer_api_name(inplace)), inplace
    )
def update_names_with_mapping(tensor, rename_map, inplace):
"
536,"this PythonDispatcher.E.g. for CPU key, a kernel(e.g fn_CPU for CPU) is
automatically generated and registered.
""""""
def register(self, dispatchKeys):
# Overriden is not supported and triggers a warning in C++ dispatcher.
if len(set(dispatchKeys)) != len(dispatchKeys):
            raise RuntimeError(f""Overriden is not allowed but found duplicates in {dispatchKeys}."")
# We currently forbid this in codegen instead of C++ dispatcher.
        if 'CompositeImplicitAutograd' in dispatchKeys and 'CompositeExplicitAutograd' in dispatchKeys:
            raise RuntimeError(""Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed."")
for key in dispatchKeys:
if key not in self.supported_keys:
                raise RuntimeError(f""{key} is not supported, please select a dispatch key in {self.supported_keys}."")
self.ref.impl_t_t(""foo"", dispatch=key, debug=""fn_"" + key)
""""""
Helper function to format (key, kernel).
""""""
def _format_line(self, key, kernel):
return ""{:<15} {}\n"".format(key, kernel)
""""""
Helper function to print a table header.
""""""
def _format_header(self, header):
s = f""""""
{header}
","this PythonDispatcher.E.g. for CPU key, a kernel(e.g fn_CPU for CPU) is
automatically generated and registered.
""""""

def register(self, dispatchKeys):
# Overriden is not supported and triggers a warning in C++ dispatcher.
if len(set(dispatchKeys)) != len(dispatchKeys):
            raise RuntimeError(
                f""Overriden is not allowed but found duplicates in {dispatchKeys}.""
            )
# We currently forbid this in codegen instead of C++ dispatcher.
        if (
            ""CompositeImplicitAutograd"" in dispatchKeys
            and ""CompositeExplicitAutograd"" in dispatchKeys
        ):
            raise RuntimeError(
                ""Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed.""
            )
for key in dispatchKeys:
if key not in self.supported_keys:
                raise RuntimeError(
                    f""{key} is not supported, please select a dispatch key in {self.supported_keys}.""
                )
self.ref.impl_t_t(""foo"", dispatch=key, debug=""fn_"" + key)
""""""
Helper function to format (key, kernel).
""""""

def _format_line(self, key, kernel):
return ""{:<15} {}\n"".format(key, kernel)
""""""
Helper function to print a table header.
""""""

def _format_header(self, header):
s = f""""""
{header}
"
537,"Returns a table(str) including all the registrations from users.
Note this includes registrations to both runtime keys and alias keys.
""""""
def registrations(self):
output = self._format_header(""Registered Kernels"")
state = self.rawRegistrations()
        state_entries = state.split('\n')
for line in state_entries:
first = line.split("":"")[0]
if any(first.startswith(k) for k in self.supported_keys):
","Returns a table(str) including all the registrations from users.
Note this includes registrations to both runtime keys and alias keys.
""""""

def registrations(self):
output = self._format_header(""Registered Kernels"")
state = self.rawRegistrations()
        state_entries = state.split(""\n"")
for line in state_entries:
first = line.split("":"")[0]
if any(first.startswith(k) for k in self.supported_keys):
"
538,"whitespace = fn_def.split(""def"")[0]
# Add this leading whitespace to all lines before and after the `def`
    aligned_prefix = [whitespace + remove_prefix(s, whitespace) for s in sourcelines[:idx]]
    aligned_suffix = [whitespace + remove_prefix(s, whitespace) for s in sourcelines[idx + 1:]]
# Put it together again
aligned_prefix.append(fn_def)
","whitespace = fn_def.split(""def"")[0]
# Add this leading whitespace to all lines before and after the `def`
    aligned_prefix = [
        whitespace + remove_prefix(s, whitespace) for s in sourcelines[:idx]
    ]
    aligned_suffix = [
        whitespace + remove_prefix(s, whitespace) for s in sourcelines[idx + 1 :]
    ]
# Put it together again
aligned_prefix.append(fn_def)
"
539,"""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, 'align_to')
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)],
            ellipsis_idx)
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
","""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, ""align_to"")
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)], ellipsis_idx
        )
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
"
540,"floating point ``dtype``, and the result will have the same ``dtype``.
See :func:`torch.bernoulli`
"""""")
add_docstr_all('bernoulli_',
               r""""""
bernoulli_(p=0.5, *, generator=None) -> Tensor
Fills each location of :attr:`self` with an independent sample from
","floating point ``dtype``, and the result will have the same ``dtype``.
See :func:`torch.bernoulli`
"""""",
)
add_docstr_all(
    ""bernoulli_"",
    r""""""
bernoulli_(p=0.5, *, generator=None) -> Tensor
Fills each location of :attr:`self` with an independent sample from
"
541,"[0., 5., 0.],
[0., 0., 5.]])
"""""")
add_docstr_all('floor_divide',
               r""""""
floor_divide(value) -> Tensor
See :func:`torch.floor_divide`
"""""")
add_docstr_all('floor_divide_',
               r""""""
floor_divide_(value) -> Tensor
In-place version of :meth:`~Tensor.floor_divide`
"""""")
add_docstr_all('diff',
               r""""""
diff(n=1, dim=-1, prepend=None, append=None) -> Tensor
See :func:`torch.diff`
"""""")
add_docstr_all('digamma',
               r""""""
digamma() -> Tensor
See :func:`torch.digamma`
"""""")
add_docstr_all('digamma_',
               r""""""
digamma_() -> Tensor
In-place version of :meth:`~Tensor.digamma`
"""""")
add_docstr_all('dim',
               r""""""
dim() -> int
Returns the number of dimensions of :attr:`self` tensor.
"""""")
add_docstr_all('dist',
               r""""""
dist(other, p=2) -> Tensor
See :func:`torch.dist`
"""""")
add_docstr_all('div', r""""""
div(value, *, rounding_mode=None) -> Tensor
See :func:`torch.div`
"""""")
add_docstr_all('div_', r""""""
div_(value, *, rounding_mode=None) -> Tensor
In-place version of :meth:`~Tensor.div`
"""""")
add_docstr_all('divide', r""""""
divide(value, *, rounding_mode=None) -> Tensor
See :func:`torch.divide`
"""""")
add_docstr_all('divide_', r""""""
divide_(value, *, rounding_mode=None) -> Tensor
In-place version of :meth:`~Tensor.divide`
"""""")
add_docstr_all('dot',
               r""""""
dot(other) -> Tensor
See :func:`torch.dot`
"""""")
add_docstr_all('eig',
               r""""""
eig(eigenvectors=False) -> (Tensor, Tensor)
See :func:`torch.eig`
"""""")
add_docstr_all('element_size',
               r""""""
element_size() -> int
Returns the size in bytes of an individual element.
","[0., 5., 0.],
[0., 0., 5.]])
"""""",
)
add_docstr_all(
    ""floor_divide"",
    r""""""
floor_divide(value) -> Tensor
See :func:`torch.floor_divide`
"""""",
)
add_docstr_all(
    ""floor_divide_"",
    r""""""
floor_divide_(value) -> Tensor
In-place version of :meth:`~Tensor.floor_divide`
"""""",
)
add_docstr_all(
    ""diff"",
    r""""""
diff(n=1, dim=-1, prepend=None, append=None) -> Tensor
See :func:`torch.diff`
"""""",
)
add_docstr_all(
    ""digamma"",
    r""""""
digamma() -> Tensor
See :func:`torch.digamma`
"""""",
)
add_docstr_all(
    ""digamma_"",
    r""""""
digamma_() -> Tensor
In-place version of :meth:`~Tensor.digamma`
"""""",
)
add_docstr_all(
    ""dim"",
    r""""""
dim() -> int
Returns the number of dimensions of :attr:`self` tensor.
"""""",
)
add_docstr_all(
    ""dist"",
    r""""""
dist(other, p=2) -> Tensor
See :func:`torch.dist`
"""""",
)
add_docstr_all(
    ""div"",
    r""""""
div(value, *, rounding_mode=None) -> Tensor
See :func:`torch.div`
"""""",
)
add_docstr_all(
    ""div_"",
    r""""""
div_(value, *, rounding_mode=None) -> Tensor
In-place version of :meth:`~Tensor.div`
"""""",
)
add_docstr_all(
    ""divide"",
    r""""""
divide(value, *, rounding_mode=None) -> Tensor
See :func:`torch.divide`
"""""",
)
add_docstr_all(
    ""divide_"",
    r""""""
divide_(value, *, rounding_mode=None) -> Tensor
In-place version of :meth:`~Tensor.divide`
"""""",
)
add_docstr_all(
    ""dot"",
    r""""""
dot(other) -> Tensor
See :func:`torch.dot`
"""""",
)
add_docstr_all(
    ""eig"",
    r""""""
eig(eigenvectors=False) -> (Tensor, Tensor)
See :func:`torch.eig`
"""""",
)
add_docstr_all(
    ""element_size"",
    r""""""
element_size() -> int
Returns the size in bytes of an individual element.
"
542,"Args:
other (:class:`torch.Tensor`): The result tensor has the same shape
as :attr:`other`.
"""""")
add_docstr_all('resize_',
               r""""""
resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor
Resizes :attr:`self` tensor to the specified size. If the number of elements is
","Args:
other (:class:`torch.Tensor`): The result tensor has the same shape
as :attr:`other`.
"""""",
)
add_docstr_all(
    ""resize_"",
    r""""""
resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor
Resizes :attr:`self` tensor to the specified size. If the number of elements is
"
543,">>> x.resize_(2, 2)
tensor([[ 1,  2],
[ 3,  4]])
"""""")
add_docstr_all('resize_as_',
               r""""""
resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor
Resizes the :attr:`self` tensor to be the same size as the specified
",">>> x.resize_(2, 2)
tensor([[ 1,  2],
[ 3,  4]])
"""""",
)
add_docstr_all(
    ""resize_as_"",
    r""""""
resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor
Resizes the :attr:`self` tensor to be the same size as the specified
"
544,">>> tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
[ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
"""""".format(**common_args))

add_docstr_all('byte',
               r""""""
byte(memory_format=torch.preserve_format) -> Tensor
``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('bool',
               r""""""
bool(memory_format=torch.preserve_format) -> Tensor
``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('char',
               r""""""
char(memory_format=torch.preserve_format) -> Tensor
``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('bfloat16',
               r""""""
bfloat16(memory_format=torch.preserve_format) -> Tensor
``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('double',
               r""""""
double(memory_format=torch.preserve_format) -> Tensor
``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('float',
               r""""""
float(memory_format=torch.preserve_format) -> Tensor
``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('cdouble',
               r""""""
cdouble(memory_format=torch.preserve_format) -> Tensor
``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('cfloat',
               r""""""
cfloat(memory_format=torch.preserve_format) -> Tensor
``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('half',
               r""""""
half(memory_format=torch.preserve_format) -> Tensor
``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('int',
               r""""""
int(memory_format=torch.preserve_format) -> Tensor
``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('int_repr',
               r""""""
int_repr() -> Tensor
Given a quantized Tensor,
``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.
"""""")
add_docstr_all('long',
               r""""""
long(memory_format=torch.preserve_format) -> Tensor
``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('short',
               r""""""
short(memory_format=torch.preserve_format) -> Tensor
``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('take',
               r""""""
take(indices) -> Tensor
See :func:`torch.take`
"""""")
add_docstr_all('take_along_dim',
               r""""""
take_along_dim(indices, dim) -> Tensor
See :func:`torch.take_along_dim`
"""""")
add_docstr_all('tan',
               r""""""
tan() -> Tensor
See :func:`torch.tan`
"""""")
add_docstr_all('tan_',
               r""""""
tan_() -> Tensor
In-place version of :meth:`~Tensor.tan`
"""""")
add_docstr_all('tanh',
               r""""""
tanh() -> Tensor
See :func:`torch.tanh`
"""""")
add_docstr_all('tanh_',
               r""""""
tanh_() -> Tensor
In-place version of :meth:`~Tensor.tanh`
"""""")
add_docstr_all('tolist',
               r""""""
tolist() -> list or number
Returns the tensor as a (nested) list. For scalars, a standard
",">>> tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
[ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""byte"",
    r""""""
byte(memory_format=torch.preserve_format) -> Tensor
``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""bool"",
    r""""""
bool(memory_format=torch.preserve_format) -> Tensor
``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""char"",
    r""""""
char(memory_format=torch.preserve_format) -> Tensor
``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""bfloat16"",
    r""""""
bfloat16(memory_format=torch.preserve_format) -> Tensor
``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""double"",
    r""""""
double(memory_format=torch.preserve_format) -> Tensor
``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""float"",
    r""""""
float(memory_format=torch.preserve_format) -> Tensor
``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""cdouble"",
    r""""""
cdouble(memory_format=torch.preserve_format) -> Tensor
``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""cfloat"",
    r""""""
cfloat(memory_format=torch.preserve_format) -> Tensor
``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""half"",
    r""""""
half(memory_format=torch.preserve_format) -> Tensor
``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""int"",
    r""""""
int(memory_format=torch.preserve_format) -> Tensor
``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""int_repr"",
    r""""""
int_repr() -> Tensor
Given a quantized Tensor,
``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.
"""""",
)
add_docstr_all(
    ""long"",
    r""""""
long(memory_format=torch.preserve_format) -> Tensor
``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""short"",
    r""""""
short(memory_format=torch.preserve_format) -> Tensor
``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""take"",
    r""""""
take(indices) -> Tensor
See :func:`torch.take`
"""""",
)
add_docstr_all(
    ""take_along_dim"",
    r""""""
take_along_dim(indices, dim) -> Tensor
See :func:`torch.take_along_dim`
"""""",
)
add_docstr_all(
    ""tan"",
    r""""""
tan() -> Tensor
See :func:`torch.tan`
"""""",
)
add_docstr_all(
    ""tan_"",
    r""""""
tan_() -> Tensor
In-place version of :meth:`~Tensor.tan`
"""""",
)
add_docstr_all(
    ""tanh"",
    r""""""
tanh() -> Tensor
See :func:`torch.tanh`
"""""",
)
add_docstr_all(
    ""tanh_"",
    r""""""
tanh_() -> Tensor
In-place version of :meth:`~Tensor.tanh`
"""""",
)
add_docstr_all(
    ""tolist"",
    r""""""
tolist() -> list or number
Returns the tensor as a (nested) list. For scalars, a standard
"
545,":noindex:
See :func:`torch.var`
"""""")
add_docstr_all('vdot',
               r""""""
vdot(other) -> Tensor
See :func:`torch.vdot`
"""""")
add_docstr_all('view',
               r""""""
view(*shape) -> Tensor
Returns a new tensor with the same data as the :attr:`self` tensor but of a
",":noindex:
See :func:`torch.var`
"""""",
)
add_docstr_all(
    ""vdot"",
    r""""""
vdot(other) -> Tensor
See :func:`torch.vdot`
"""""",
)
add_docstr_all(
    ""view"",
    r""""""
view(*shape) -> Tensor
Returns a new tensor with the same data as the :attr:`self` tensor but of a
"
546,"# width for imag_formatter + an extra j for complex
element_length += formatter2.width() + 1
    elements_per_line = max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / (element_length))))
char_per_line = element_length * elements_per_line
def _val_formatter(val, formatter1=formatter1, formatter2=formatter2):
","# width for imag_formatter + an extra j for complex
element_length += formatter2.width() + 1
    elements_per_line = max(
        1, int(math.floor((PRINT_OPTS.linewidth - indent) / (element_length)))
    )
char_per_line = element_length * elements_per_line
def _val_formatter(val, formatter1=formatter1, formatter2=formatter2):
"
547,">>> torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
"""""".format(**common_args))

add_docstr(torch.absolute,
           r""""""
absolute(input, *, out=None) -> Tensor
Alias for :func:`torch.abs`
"""""".format(**common_args))

add_docstr(torch.acos, r""""""
acos(input, *, out=None) -> Tensor
Computes the inverse cosine of each element in :attr:`input`.
.. math::
\text{out}_{i} = \cos^{-1}(\text{input}_{i})
"""""" + r""""""
Args:
{input}
",">>> torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.absolute,
    r""""""
absolute(input, *, out=None) -> Tensor
Alias for :func:`torch.abs`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.acos,
    r""""""
acos(input, *, out=None) -> Tensor
Computes the inverse cosine of each element in :attr:`input`.
.. math::
\text{out}_{i} = \cos^{-1}(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
"
548,"Args:
input (Tensor): a sparse matrix to be matrix multiplied
mat (Tensor): a dense matrix to be matrix multiplied
"""""")
add_docstr(torch.addmv,
           r""""""
addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`mat` and
","Args:
input (Tensor): a sparse matrix to be matrix multiplied
mat (Tensor): a dense matrix to be matrix multiplied
"""""",
)
add_docstr(
    torch.addmv,
    r""""""
addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`mat` and
"
549,"tensor([ True, False,  True,  True], dtype=torch.bool)
>>> torch.all(a, dim=0)
tensor([ True, False], dtype=torch.bool)
"""""".format(**single_dim_common))

add_docstr(torch.any,
           r""""""
any(input) -> Tensor
Args:
","tensor([ True, False,  True,  True], dtype=torch.bool)
>>> torch.all(a, dim=0)
tensor([ True, False], dtype=torch.bool)
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.any,
    r""""""
any(input) -> Tensor
Args:
"
550,"If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
"""""" + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers.
","If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
""""""
    + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers.
"
551,"tensor([-0.6341, -1.4208, -1.0900,  0.5826])
>>> torch.ceil(a)
tensor([-0., -1., -1.,  1.])
"""""".format(**common_args))

add_docstr(torch.real,
           r""""""
real(input) -> Tensor
Returns a new tensor containing real values of the :attr:`self` tensor.
","tensor([-0.6341, -1.4208, -1.0900,  0.5826])
>>> torch.ceil(a)
tensor([-0., -1., -1.,  1.])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.real,
    r""""""
real(input) -> Tensor
Returns a new tensor containing real values of the :attr:`self` tensor.
"
552,"tensor([-0.4595, -2.1219, -1.4314,  0.7298])
>>> torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])
"""""".format(**common_args))

add_docstr(torch.cholesky, r""""""
cholesky(input, upper=False, *, out=None) -> Tensor
Computes the Cholesky decomposition of a symmetric positive-definite
","tensor([-0.4595, -2.1219, -1.4314,  0.7298])
>>> torch.reciprocal(a)
tensor([-2.1763, -0.4713, -0.6986,  1.3702])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.cholesky,
    r""""""
cholesky(input, upper=False, *, out=None) -> Tensor
Computes the Cholesky decomposition of a symmetric positive-definite
"
553,"tensor([[ -8.1626,  19.6097],
[ -5.8398,  14.2387],
[ -4.3771,  10.4173]])
"""""")
add_docstr(torch.cholesky_inverse, r""""""
cholesky_inverse(input, upper=False, *, out=None) -> Tensor
Computes the inverse of a symmetric positive-definite matrix :math:`A` using its
","tensor([[ -8.1626,  19.6097],
[ -5.8398,  14.2387],
[ -4.3771,  10.4173]])
"""""",
)
add_docstr(
    torch.cholesky_inverse,
    r""""""
cholesky_inverse(input, upper=False, *, out=None) -> Tensor
Computes the inverse of a symmetric positive-definite matrix :math:`A` using its
"
554,"|\text{input}_{i}| & \text{if} \text{other}_{i} \leq -0.0 \\
|\text{input}_{i}| & \text{if} \text{other}_{i} \geq 0.0 \\
\end{cases}
"""""" + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
and integer and float inputs.
","|\text{input}_{i}| & \text{if} \text{other}_{i} \geq 0.0 \\
\end{cases}
""""""
    + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
and integer and float inputs.
"
555,"[[ 0.0000,  0.0000,  0.0000,  0.0000],
[ 0.0000,  0.0000,  0.0000,  0.0000]]])
"""""".format(**common_args))
add_docstr(torch.diagflat,
           r""""""
diagflat(input, offset=0) -> Tensor
 If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
","[[ 0.0000,  0.0000,  0.0000,  0.0000],
[ 0.0000,  0.0000,  0.0000,  0.0000]]])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.diagflat,
    r""""""
diagflat(input, offset=0) -> Tensor
"
556,"tensor([16.+1.j])
>>> torch.vdot(b, a)
tensor([16.-1.j])
"""""".format(**common_args))

add_docstr(torch.eig,
           r""""""
eig(input, eigenvectors=False, *, out=None) -> (Tensor, Tensor)
Computes the eigenvalues and eigenvectors of a real square matrix.
","tensor([16.+1.j])
>>> torch.vdot(b, a)
tensor([16.-1.j])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.eig,
    r""""""
eig(input, eigenvectors=False, *, out=None) -> (Tensor, Tensor)
Computes the eigenvalues and eigenvectors of a real square matrix.
"
557,">>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])
"""""".format(**common_args))

add_docstr(torch.greater_equal, r""""""
greater_equal(input, other, *, out=None) -> Tensor
Alias for :func:`torch.ge`.
"""""")
add_docstr(torch.gradient,
           r""""""
gradient(input, *, spacing=1, dim=None, edge_order=1) -> List of Tensors
Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in
",">>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[True, True], [False, True]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.greater_equal,
    r""""""
greater_equal(input, other, *, out=None) -> Tensor
Alias for :func:`torch.ge`.
"""""",
)
add_docstr(
    torch.gradient,
    r""""""
gradient(input, *, spacing=1, dim=None, edge_order=1) -> List of Tensors
Estimates the gradient of a function :math:`g : \mathbb{R}^n \rightarrow \mathbb{R}` in
"
558,">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
"""""".format(**common_args))

add_docstr(torch.histogram,
           r""""""
histogram(input, bins, *, range=None, weight=None, density=False, out=None) -> (Tensor, Tensor)
Computes a histogram of the values in a tensor.
",">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.histogram,
    r""""""
histogram(input, bins, *, range=None, weight=None, density=False, out=None) -> (Tensor, Tensor)
Computes a histogram of the values in a tensor.
"
559,"tensor([[ 0.1427, -0.5414],
[-0.4664, -0.1228],
[-1.1734,  0.7230]])
"""""".format(**common_args))

add_docstr(torch.inverse, r""""""
inverse(input, *, out=None) -> Tensor
Alias for :func:`torch.linalg.inv`
"""""".format(**common_args))

add_docstr(torch.isin, r""""""
isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor
Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns
","tensor([[ 0.1427, -0.5414],
[-0.4664, -0.1228],
[-1.1734,  0.7230]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.inverse,
    r""""""
inverse(input, *, out=None) -> Tensor
Alias for :func:`torch.linalg.inv`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isin,
    r""""""
isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor
Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns
"
560,"tensor([ True, False, False])
>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)
tensor([True, True])
"""""")
add_docstr(torch.isfinite, r""""""
isfinite(input) -> Tensor
Returns a new tensor with boolean elements representing if each element is `finite` or not.
","tensor([ True, False, False])
>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)
tensor([True, True])
"""""",
)
add_docstr(
    torch.isfinite,
    r""""""
isfinite(input) -> Tensor
Returns a new tensor with boolean elements representing if each element is `finite` or not.
"
561,">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))
tensor([True, False, True])
"""""".format(**common_args))

add_docstr(torch.is_floating_point, r""""""
is_floating_point(input) -> (bool)
Returns True if the data type of :attr:`input` is a floating point data type i.e.,
",">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))
tensor([True, False, True])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.is_floating_point,
    r""""""
is_floating_point(input) -> (bool)
Returns True if the data type of :attr:`input` is a floating point data type i.e.,
"
562,"[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(**single_dim_common))

add_docstr(torch.lcm,
           r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
","[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.lcm,
    r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
"
563,".. math::
\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)
"""""" + r""""""
The shapes of :attr:`start` and :attr:`end` must be
:ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then
the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)
""""""
    + r""""""
The shapes of :attr:`start` and :attr:`end` must be
:ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then
the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.
"
564,".. math::
y_{i} = \log_{e} (x_{i})
"""""" + r""""""
Args:
{input}
",".. math::
y_{i} = \log_{e} (x_{i})
""""""
    + r""""""
Args:
{input}
"
565,".. math::
y_{i} = \log_{10} (x_{i})
"""""" + r""""""
Args:
{input}
",".. math::
y_{i} = \log_{10} (x_{i})
""""""
    + r""""""
Args:
{input}
"
566,">>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
>>> torch.norm(A_ - A)
tensor(2.9802e-08)
"""""".format(**common_args))

add_docstr(torch.less, r""""""
less(input, other, *, out=None) -> Tensor
Alias for :func:`torch.lt`.
"""""")
add_docstr(torch.lu_solve,
           r""""""
lu_solve(b, LU_data, LU_pivots, *, out=None) -> Tensor
Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted
",">>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
>>> torch.norm(A_ - A)
tensor(2.9802e-08)
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.less,
    r""""""
less(input, other, *, out=None) -> Tensor
Alias for :func:`torch.lt`.
"""""",
)
add_docstr(
    torch.lu_solve,
    r""""""
lu_solve(b, LU_data, LU_pivots, *, out=None) -> Tensor
Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted
"
567,">>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])
tensor([True, True])
"""""".format(**common_args))
add_docstr(torch.nonzero,
           r""""""
nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors
.. note::
",">>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])
tensor([True, True])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.nonzero,
    r""""""
nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors
.. note::
"
568,">>> torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])
"""""".format(**factory_common_args))
add_docstr(torch.ones_like,
           r""""""
ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `1`, with the same size as
",">>> torch.ones(5)
tensor([ 1.,  1.,  1.,  1.,  1.])
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.ones_like,
    r""""""
ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `1`, with the same size as
"
569,"torch.Size([2, 3, 5])
>>> torch.permute(x, (2, 0, 1)).size()
torch.Size([5, 2, 3])
"""""".format(**common_args))

add_docstr(torch.poisson,
           r""""""
poisson(input, generator=None) -> Tensor
Returns a tensor of the same size as :attr:`input` with each element
","torch.Size([2, 3, 5])
>>> torch.permute(x, (2, 0, 1)).size()
torch.Size([5, 2, 3])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.poisson,
    r""""""
poisson(input, generator=None) -> Tensor
Returns a tensor of the same size as :attr:`input` with each element
"
570,"True
>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
"""""")
add_docstr(torch.rad2deg,
           r""""""
rad2deg(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
","True
>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
"""""",
)
add_docstr(
    torch.rad2deg,
    r""""""
rad2deg(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
"
571,"values, & \text{if input == 0}\\
1, & \text{if input > 0}
\end{cases}
"""""" + r""""""
Args:
{input}
","values, & \text{if input == 0}\\
1, & \text{if input > 0}
\end{cases}
""""""
    + r""""""
Args:
{input}
"
572,"...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
"""""".format(**common_args))

add_docstr(torch.remainder,
           r""""""
remainder(input, other, *, out=None) -> Tensor
Like :func:`torch.fmod` this applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
","...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.remainder,
    r""""""
remainder(input, other, *, out=None) -> Tensor
Like :func:`torch.fmod` this applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
"
573,"\frac{{\text{{input}}_i}}{|{\text{{input}}_i}|} & \text{otherwise}
\end{cases}
"""""" + r""""""
Args:
{input}
","\frac{{\text{{input}}_i}}{|{\text{{input}}_i}|} & \text{otherwise}
\end{cases}
""""""
    + r""""""
Args:
{input}
"
574,"size=(1, 2), nnz=0, layout=torch.sparse_coo)
.. _torch.sparse: https://pytorch.org/docs/stable/sparse.html
"""""".format(**factory_common_args))

add_docstr(torch.sqrt,
           r""""""
sqrt(input, *, out=None) -> Tensor
Returns a new tensor with the square-root of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sqrt{\text{input}_{i}}
"""""" + r""""""
Args:
{input}
","size=(1, 2), nnz=0, layout=torch.sparse_coo)
.. _torch.sparse: https://pytorch.org/docs/stable/sparse.html
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.sqrt,
    r""""""
sqrt(input, *, out=None) -> Tensor
Returns a new tensor with the square-root of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sqrt{\text{input}_{i}}
""""""
    + r""""""
Args:
{input}
"
575,">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(**common_args))

add_docstr(torch.roll,
           r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
",">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.roll,
    r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
"
576,"tensor([[ 1.0028, -0.1669],
[-0.9893,  0.7299],
[ 0.5809,  0.4942]])
"""""".format(**common_args))

add_docstr(torch.triangular_solve,
           r""""""
triangular_solve(b, A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)
Solves a system of equations with a triangular coefficient matrix :math:`A`
","tensor([[ 1.0028, -0.1669],
[-0.9893,  0.7299],
[ 0.5809,  0.4942]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.triangular_solve,
    r""""""
triangular_solve(b, A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)
Solves a system of equations with a triangular coefficient matrix :math:`A`
"
577,"[[0.0000, 1.6134],
[0.6323, 0.0000]]])
"""""")
add_docstr(torch.fix,
           r""""""
fix(input, *, out=None) -> Tensor
Alias for :func:`torch.trunc`
"""""".format(**common_args))

add_docstr(torch.unsqueeze,
           r""""""
unsqueeze(input, dim) -> Tensor
Returns a new tensor with a dimension of size one inserted at the
","[[0.0000, 1.6134],
[0.6323, 0.0000]]])
"""""",
)
add_docstr(
    torch.fix,
    r""""""
fix(input, *, out=None) -> Tensor
Alias for :func:`torch.trunc`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.unsqueeze,
    r""""""
unsqueeze(input, dim) -> Tensor
Returns a new tensor with a dimension of size one inserted at the
"
578,"[ 1,  3,  9],
[ 1,  5, 25]])
"""""".format(**factory_common_args))
add_docstr(torch.unbind,
           r""""""
unbind(input, dim=0) -> seq
Removes a tensor dimension.
","[ 1,  3,  9],
[ 1,  5, 25]])
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.unbind,
    r""""""
unbind(input, dim=0) -> seq
Removes a tensor dimension.
"
579,">>> g_cpu = torch.Generator()
>>> g_cpu.device
device(type='cpu')
"""""")
add_docstr(torch._assert_async,
           r""""""
_assert_async(tensor) -> void
Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
",">>> g_cpu = torch.Generator()
>>> g_cpu.device
device(type='cpu')
"""""",
)
add_docstr(
    torch._assert_async,
    r""""""
_assert_async(tensor) -> void
Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
"
580,">>> torch.searchsorted(sorted_sequence_1d, values)
tensor([[1, 3, 4],
[1, 3, 4]])
"""""")
add_docstr(torch.bucketize,
           r""""""
bucketize(input, boundaries, *, out_int32=False, right=False, out=None) -> Tensor
Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the
",">>> torch.searchsorted(sorted_sequence_1d, values)
tensor([[1, 3, 4],
[1, 3, 4]])
"""""",
)
add_docstr(
    torch.bucketize,
    r""""""
bucketize(input, boundaries, *, out_int32=False, right=False, out=None) -> Tensor
Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the
"
581,"import torch
from typing import Optional, List, DefaultDict, Any
import warnings
from collections import defaultdict
import sys
import traceback
def _type(self, dtype=None, non_blocking=False, **kwargs):
","import sys
import traceback
import warnings
from collections import defaultdict
from typing import Optional, List, DefaultDict, Any
import torch
def _type(self, dtype=None, non_blocking=False, **kwargs):
"
582,"scales = torch.tensor(scales, dtype=torch.float)
zero_points = torch.tensor(zero_points, dtype=torch.float)
tensor = torch._empty_per_channel_affine_quantized(
            size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)
else:
        raise RuntimeError(""Can't deserialize quantized tensor with qscheme {}"".format(qscheme))
tensor.set_(storage, storage_offset, size, stride)
tensor.requires_grad = requires_grad
# NB: This line exists only for backwards compatibility; the
","scales = torch.tensor(scales, dtype=torch.float)
zero_points = torch.tensor(zero_points, dtype=torch.float)
tensor = torch._empty_per_channel_affine_quantized(
            size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype
        )
else:
        raise RuntimeError(
            ""Can't deserialize quantized tensor with qscheme {}"".format(qscheme)
        )
tensor.set_(storage, storage_offset, size, stride)
tensor.requires_grad = requires_grad
# NB: This line exists only for backwards compatibility; the
"
583,"return tempfile.mkdtemp(suffix=os.path.basename(path))

def prepare_multiprocessing_environment(path: str) -> None:
pass
","return tempfile.mkdtemp(suffix=os.path.basename(path))
def prepare_multiprocessing_environment(path: str) -> None:
pass
"
584,"import torch
import functools
from torch import Tensor
from typing import Any, Callable, Optional, Tuple, Union, List
from torch.utils._pytree import tree_flatten, tree_unflatten, _broadcast_to_and_flatten
import warnings
in_dims_t = Union[int, Tuple]
out_dims_t = Union[int, Tuple[int, ...]]
# Checks that all args-to-be-batched have the same batch dim size
def _validate_and_get_batch_size(
        flat_in_dims: List[Optional[int]],
        flat_args: List) -> int:
    batch_sizes = [arg.size(in_dim) for in_dim, arg in zip(flat_in_dims, flat_args)
                   if in_dim is not None]
if batch_sizes and any([size != batch_sizes[0] for size in batch_sizes]):
raise ValueError(
            f'vmap: Expected all tensors to have the same size in the mapped '
            f'dimension, got sizes {batch_sizes} for the mapped dimension')
return batch_sizes[0]
def _num_outputs(batched_outputs: Union[Tensor, Tuple[Tensor, ...]]) -> int:
if isinstance(batched_outputs, tuple):
return len(batched_outputs)
return 1
# If value is a tuple, check it has length `num_elements`.
# If value is not a tuple, make a tuple with `value` repeated `num_elements` times
def _as_tuple(value: Any, num_elements: int, error_message_lambda: Callable[[], str]) -> Tuple:
if not isinstance(value, tuple):
return (value,) * num_elements
if len(value) != num_elements:
raise ValueError(error_message_lambda())
return value
# Creates BatchedTensors for every Tensor in arg that should be batched.
# Returns the (potentially) batched arguments and the batch_size.
def _create_batched_inputs(
        in_dims: in_dims_t, args: Tuple, vmap_level: int, func: Callable) -> Tuple[Tuple, int]:
if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
raise ValueError(
            f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): '
            f'expected `in_dims` to be int or a (potentially nested) tuple '
            f'matching the structure of inputs, got: {type(in_dims)}.')
if len(args) == 0:
raise ValueError(
            f'vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add '
            f'inputs, or you are trying to vmap over a function with no inputs. '
            f'The latter is unsupported.')
flat_args, args_spec = tree_flatten(args)
flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
if flat_in_dims is None:
raise ValueError(
            f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): '
            f'in_dims is not compatible with the structure of `inputs`. '
            f'in_dims has structure {tree_flatten(in_dims)[1]} but inputs '
            f'has structure {args_spec}.')
for arg, in_dim in zip(flat_args, flat_in_dims):
if not isinstance(in_dim, int) and in_dim is not None:
raise ValueError(
                f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): '
                f'Got in_dim={in_dim} for an input but in_dim must be either '
                f'an integer dimension or None.')
if isinstance(in_dim, int) and not isinstance(arg, Tensor):
raise ValueError(
                f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): '
                f'Got in_dim={in_dim} for an input but the input is of type '
                f'{type(arg)}. We cannot vmap over non-Tensor arguments, '
                f'please use None as the respective in_dim')
if in_dim is not None and (in_dim < 0 or in_dim >= arg.dim()):
raise ValueError(
                f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): '
                f'Got in_dim={in_dim} for some input, but that input is a Tensor '
                f'of dimensionality {arg.dim()} so expected in_dim to satisfy '
                f'0 <= in_dim < {arg.dim()}.')
batch_size = _validate_and_get_batch_size(flat_in_dims, flat_args)
# See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
    batched_inputs = [arg if in_dim is None else
                      torch._add_batch_dim(arg, in_dim, vmap_level)
                      for in_dim, arg in zip(flat_in_dims, flat_args)]
return tree_unflatten(batched_inputs, args_spec), batch_size
# Undos the batching (and any batch dimensions) associated with the `vmap_level`.
def _unwrap_batched(
        batched_outputs: Union[Tensor, Tuple[Tensor, ...]],
        out_dims: out_dims_t,
        vmap_level: int, batch_size: int, func: Callable) -> Tuple:
num_outputs = _num_outputs(batched_outputs)
out_dims_as_tuple = _as_tuple(
        out_dims, num_outputs,
        lambda: f'vmap({_get_name(func)}, ..., out_dims={out_dims}): `out_dims` must '
                f'have one dim per output (got {num_outputs} outputs) of {_get_name(func)}.')
# NOTE [Ignored _remove_batch_dim, _add_batch_dim]
# There is something wrong with our type bindings for functions that begin
","import functools
import warnings
from typing import Any, Callable, Optional, Tuple, Union, List

import torch
from torch import Tensor
from torch.utils._pytree import tree_flatten, tree_unflatten, _broadcast_to_and_flatten
in_dims_t = Union[int, Tuple]
out_dims_t = Union[int, Tuple[int, ...]]
# Checks that all args-to-be-batched have the same batch dim size
def _validate_and_get_batch_size(
    flat_in_dims: List[Optional[int]], flat_args: List
) -> int:
    batch_sizes = [
        arg.size(in_dim)
        for in_dim, arg in zip(flat_in_dims, flat_args)
        if in_dim is not None
    ]
if batch_sizes and any([size != batch_sizes[0] for size in batch_sizes]):
raise ValueError(
            f""vmap: Expected all tensors to have the same size in the mapped ""
            f""dimension, got sizes {batch_sizes} for the mapped dimension""
        )
return batch_sizes[0]

def _num_outputs(batched_outputs: Union[Tensor, Tuple[Tensor, ...]]) -> int:
if isinstance(batched_outputs, tuple):
return len(batched_outputs)
return 1

# If value is a tuple, check it has length `num_elements`.
# If value is not a tuple, make a tuple with `value` repeated `num_elements` times
def _as_tuple(
    value: Any, num_elements: int, error_message_lambda: Callable[[], str]
) -> Tuple:
if not isinstance(value, tuple):
return (value,) * num_elements
if len(value) != num_elements:
raise ValueError(error_message_lambda())
return value

# Creates BatchedTensors for every Tensor in arg that should be batched.
# Returns the (potentially) batched arguments and the batch_size.
def _create_batched_inputs(
    in_dims: in_dims_t, args: Tuple, vmap_level: int, func: Callable
) -> Tuple[Tuple, int]:
if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
raise ValueError(
            f""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): ""
            f""expected `in_dims` to be int or a (potentially nested) tuple ""
            f""matching the structure of inputs, got: {type(in_dims)}.""
        )
if len(args) == 0:
raise ValueError(
            f""vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add ""
            f""inputs, or you are trying to vmap over a function with no inputs. ""
            f""The latter is unsupported.""
        )
flat_args, args_spec = tree_flatten(args)
flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
if flat_in_dims is None:
raise ValueError(
            f""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): ""
            f""in_dims is not compatible with the structure of `inputs`. ""
            f""in_dims has structure {tree_flatten(in_dims)[1]} but inputs ""
            f""has structure {args_spec}.""
        )
for arg, in_dim in zip(flat_args, flat_in_dims):
if not isinstance(in_dim, int) and in_dim is not None:
raise ValueError(
                f""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): ""
                f""Got in_dim={in_dim} for an input but in_dim must be either ""
                f""an integer dimension or None.""
            )
if isinstance(in_dim, int) and not isinstance(arg, Tensor):
raise ValueError(
                f""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): ""
                f""Got in_dim={in_dim} for an input but the input is of type ""
                f""{type(arg)}. We cannot vmap over non-Tensor arguments, ""
                f""please use None as the respective in_dim""
            )
if in_dim is not None and (in_dim < 0 or in_dim >= arg.dim()):
raise ValueError(
                f""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): ""
                f""Got in_dim={in_dim} for some input, but that input is a Tensor ""
                f""of dimensionality {arg.dim()} so expected in_dim to satisfy ""
                f""0 <= in_dim < {arg.dim()}.""
            )
batch_size = _validate_and_get_batch_size(flat_in_dims, flat_args)
# See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
    batched_inputs = [
        arg if in_dim is None else torch._add_batch_dim(arg, in_dim, vmap_level)
        for in_dim, arg in zip(flat_in_dims, flat_args)
    ]
return tree_unflatten(batched_inputs, args_spec), batch_size

# Undos the batching (and any batch dimensions) associated with the `vmap_level`.
def _unwrap_batched(
    batched_outputs: Union[Tensor, Tuple[Tensor, ...]],
    out_dims: out_dims_t,
    vmap_level: int,
    batch_size: int,
    func: Callable,
) -> Tuple:
num_outputs = _num_outputs(batched_outputs)
out_dims_as_tuple = _as_tuple(
        out_dims,
        num_outputs,
        lambda: f""vmap({_get_name(func)}, ..., out_dims={out_dims}): `out_dims` must ""
        f""have one dim per output (got {num_outputs} outputs) of {_get_name(func)}."",
    )
# NOTE [Ignored _remove_batch_dim, _add_batch_dim]
# There is something wrong with our type bindings for functions that begin
"
585,"def decorate_autocast(*args, **kwargs):
with self:
return func(*args, **kwargs)
return decorate_autocast
","def decorate_autocast(*args, **kwargs):
with self:
return func(*args, **kwargs)

return decorate_autocast
"
586,"# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
    from tensorflow.core.util import event_pb2
from tensorflow.core.framework import graph_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(""TensorBoard visualization of GraphExecutors requires having ""
                      ""TensorFlow installed"") from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
w.add_event(evt)
def visualize(graph, name_prefix='', pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(graph, name_prefix, pb_graph,
                                 partial(visualize, pb_graph=pb_graph))
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op='input', name=name_prefix + 'input')
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + 'input:' + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op='output', name=name_prefix + 'output')
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
","# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
from tensorflow.core.framework import graph_pb2
    from tensorflow.core.util import event_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(
        ""TensorBoard visualization of GraphExecutors requires having ""
        ""TensorFlow installed""
    ) from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(
            wall_time=time.time(), graph_def=pb_graph.SerializeToString()
        )
w.add_event(evt)
def visualize(graph, name_prefix="""", pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(
            graph, name_prefix, pb_graph, partial(visualize, pb_graph=pb_graph)
        )
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op=""input"", name=name_prefix + ""input"")
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + ""input:"" + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op=""output"", name=name_prefix + ""output"")
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
"
587,"import torch
class autocast(torch.autocast_mode.autocast):
r""""""
See :class:`torch.autocast`.
``torch.cpu.amp.autocast(args...)`` is equivalent to ``torch.autocast(""cpu"", args...)``
""""""
def __init__(self, enabled=True, fast_dtype=torch.float16):
super().__init__(""cpu"", enabled=enabled, fast_dtype=fast_dtype)
","import torch

class autocast(torch.autocast_mode.autocast):
r""""""
See :class:`torch.autocast`.
``torch.cpu.amp.autocast(args...)`` is equivalent to ``torch.autocast(""cpu"", args...)``
""""""

def __init__(self, enabled=True, fast_dtype=torch.float16):
super().__init__(""cpu"", enabled=enabled, fast_dtype=fast_dtype)
"
588,">>> torch.fft.fftfreq(4)
tensor([ 0.0000,  0.2500, -0.5000, -0.2500])
"""""".format(**factory_common_args))
fftshift = _add_docstr(_fft.fft_fftshift, r""""""
fftshift(input, dim=None) -> Tensor
Reorders n-dimensional FFT data, as provided by :func:`~torch.fft.fftn`, to have
",">>> torch.fft.fftfreq(4)
tensor([ 0.0000,  0.2500, -0.5000, -0.2500])
"""""".format(
        **factory_common_args
    ),
)
fftshift = _add_docstr(
    _fft.fft_fftshift,
    r""""""
fftshift(input, dim=None) -> Tensor
Reorders n-dimensional FFT data, as provided by :func:`~torch.fft.fftn`, to have
"
589,">>> torch.testing.assert_close(x_centered.to(torch.complex64), x_centered_2, check_stride=False)
"""""")
ifftshift = _add_docstr(_fft.fft_ifftshift, r""""""
ifftshift(input, dim=None) -> Tensor
Inverse of :func:`~torch.fft.fftshift`.
",">>> torch.testing.assert_close(x_centered.to(torch.complex64), x_centered_2, check_stride=False)
"""""",
)
ifftshift = _add_docstr(
    _fft.fft_ifftshift,
    r""""""
ifftshift(input, dim=None) -> Tensor
Inverse of :func:`~torch.fft.fftshift`.
"
590,">>> torch.fft.ifftshift(shifted)
tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])
"""""")
",">>> torch.fft.ifftshift(shifted)
tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])
"""""",
)
"
591,"# input operands into a tensorlist (List[Tensor]).
def parse_subscript(n: int) -> str:
if n == Ellipsis:
                return '...'
if n >= 0 and n < 26:
                return chr(ord('A') + n)
if n >= 26 and n < 52:
                return chr(ord('a') + n - 26)
            raise ValueError('einsum(): subscript in subscript list is not within the valid range [0, 52)')
# Parse subscripts for input operands
        equation = ','.join(''.join(parse_subscript(s) for s in l) for l in args[1::2])
# Parse optional output subscripts (provided when the number of arguments is odd)
if len(args) % 2 == 1:
            equation += '->' + ''.join(parse_subscript(s) for s in args[-1])
operands = args[:-1:2]
else:
operands = args[::2]
","# input operands into a tensorlist (List[Tensor]).
def parse_subscript(n: int) -> str:
if n == Ellipsis:
                return ""...""
if n >= 0 and n < 26:
                return chr(ord(""A"") + n)
if n >= 26 and n < 52:
                return chr(ord(""a"") + n - 26)
            raise ValueError(
                ""einsum(): subscript in subscript list is not within the valid range [0, 52)""
            )
# Parse subscripts for input operands
        equation = "","".join("""".join(parse_subscript(s) for s in l) for l in args[1::2])
# Parse optional output subscripts (provided when the number of arguments is odd)
if len(args) % 2 == 1:
            equation += ""->"" + """".join(parse_subscript(s) for s in args[-1])
operands = args[:-1:2]
else:
operands = args[::2]
"
592,"""""""
if has_torch_function_unary(input):
return handle_torch_function(
            istft, (input,), input, n_fft, hop_length=hop_length, win_length=win_length,
            window=window, center=center, normalized=normalized, onesided=onesided,
            length=length, return_complex=return_complex)
    return _VF.istft(input, n_fft, hop_length, win_length, window, center,  # type: ignore[attr-defined]
                     normalized, onesided, length, return_complex)
del torch.unique_dim
","""""""
if has_torch_function_unary(input):
return handle_torch_function(
            istft,
            (input,),
            input,
            n_fft,
            hop_length=hop_length,
            win_length=win_length,
            window=window,
            center=center,
            normalized=normalized,
            onesided=onesided,
            length=length,
            return_complex=return_complex,
        )
    return _VF.istft(
        input,
        n_fft,
        hop_length,
        win_length,
        window,
        center,  # type: ignore[attr-defined]
        normalized,
        onesided,
        length,
        return_complex,
    )
del torch.unique_dim
"
593,"def _git_archive_link(repo_owner, repo_name, branch):
    return 'https://github.com/{}/{}/archive/{}.zip'.format(repo_owner, repo_name, branch)
def _load_attr_from_module(module, func_name):
","def _git_archive_link(repo_owner, repo_name, branch):
    return ""https://github.com/{}/{}/archive/{}.zip"".format(
        repo_owner, repo_name, branch
    )
def _load_attr_from_module(module, func_name):
"
594,">>> path = '/some/local/path/pytorch/vision'
>>> model = torch.hub.load(path, 'resnet50', pretrained=True)
""""""
    source = kwargs.pop('source', 'github').lower()
    force_reload = kwargs.pop('force_reload', False)
    verbose = kwargs.pop('verbose', True)
    skip_validation = kwargs.pop('skip_validation', False)
    if source not in ('github', 'local'):
raise ValueError(
            f'Unknown source: ""{source}"". Allowed values: ""github"" | ""local"".')
    if source == 'github':
        repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, verbose, skip_validation)
model = _load_local(repo_or_dir, model, *args, **kwargs)
return model
",">>> path = '/some/local/path/pytorch/vision'
>>> model = torch.hub.load(path, 'resnet50', pretrained=True)
""""""
    source = kwargs.pop(""source"", ""github"").lower()
    force_reload = kwargs.pop(""force_reload"", False)
    verbose = kwargs.pop(""verbose"", True)
    skip_validation = kwargs.pop(""skip_validation"", False)
    if source not in (""github"", ""local""):
raise ValueError(
            f'Unknown source: ""{source}"". Allowed values: ""github"" | ""local"".'
        )
    if source == ""github"":
        repo_or_dir = _get_cache_or_reload(
            repo_or_dir, force_reload, verbose, skip_validation
        )
model = _load_local(repo_or_dir, model, *args, **kwargs)
return model
"
595,"Also supports batches of matrices, and if :attr:`A` is a batch of matrices
then the output has the same batch dimensions.
"""""" + fr""""""
.. note:: {common_notes[""sync_note""]}
"""""" + r""""""
.. note::
Consider using :func:`torch.linalg.solve` if possible for multiplying a matrix on the left by
","Also supports batches of matrices, and if :attr:`A` is a batch of matrices
then the output has the same batch dimensions.
""""""
    + fr""""""
.. note:: {common_notes[""sync_note""]}
""""""
    + r""""""
.. note::
Consider using :func:`torch.linalg.solve` if possible for multiplying a matrix on the left by
"
596,"""wrap_torch_function"",
]
@functools.lru_cache(None)
def get_ignored_functions() -> Set[Callable]:
""""""
","""wrap_torch_function"",
]

@functools.lru_cache(None)
def get_ignored_functions() -> Set[Callable]:
""""""
"
597,"torch.batch_norm_update_stats: lambda input, running_mean, running_var, momentum: -1,
torch.bernoulli: lambda input, generator=None, out=None: -1,
torch.bilinear: lambda input1, input2, weight, bias: -1,
        torch.binary_cross_entropy_with_logits: (lambda input, target, weight=None, size_average=None, reduce=None,
                                                 reduction='mean', pos_weight=None: -1),
torch.bincount: lambda input, weights=None, minlength=0: -1,
torch.binomial: lambda count, prob, generator=None: -1,
torch.bitwise_and: lambda input, other, out=None: -1,
","torch.batch_norm_update_stats: lambda input, running_mean, running_var, momentum: -1,
torch.bernoulli: lambda input, generator=None, out=None: -1,
torch.bilinear: lambda input1, input2, weight, bias: -1,
        torch.binary_cross_entropy_with_logits: (
            lambda input, target, weight=None, size_average=None, reduce=None, reduction=""mean"", pos_weight=None: -1
        ),
torch.bincount: lambda input, weights=None, minlength=0: -1,
torch.binomial: lambda count, prob, generator=None: -1,
torch.bitwise_and: lambda input, other, out=None: -1,
"
598,"torch.stack: lambda tensors, dim=0, out=None: -1,
torch.std: lambda input, dim=None: -1,
torch.std_mean: lambda input, dim=None: -1,
        torch.stft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
                     pad_mode='reflect', normalized=False, onesided=True, return_complex=None: -1),
torch.sub: lambda input, other, out=None: -1,
torch.subtract: lambda input, other, out=None: -1,
torch.sum: lambda input, dim=None: -1,
","torch.stack: lambda tensors, dim=0, out=None: -1,
torch.std: lambda input, dim=None: -1,
torch.std_mean: lambda input, dim=None: -1,
        torch.stft: (
            lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode=""reflect"", normalized=False, onesided=True, return_complex=None: -1
        ),
torch.sub: lambda input, other, out=None: -1,
torch.subtract: lambda input, other, out=None: -1,
torch.sum: lambda input, dim=None: -1,
"
599,">>> def func(a): # This will make func dispatchable by __torch_function__
...     return a + 0
""""""
def inner(func):
@functools.wraps(func)
def wrapped(*args, **kwargs):
",">>> def func(a): # This will make func dispatchable by __torch_function__
...     return a + 0
""""""

def inner(func):
@functools.wraps(func)
def wrapped(*args, **kwargs):
"
600,"# We only collect arguments if they have a unique type, which ensures
# reasonable performance even with a long list of possibly overloaded
# arguments.
        if (arg_type not in overloaded_types and hasattr(arg_type, '__torch_function__')):
# Create lists explicitly for the first type (usually the only one
# done) to avoid setting up the iterator for overloaded_args.
if overloaded_types:
","# We only collect arguments if they have a unique type, which ensures
# reasonable performance even with a long list of possibly overloaded
# arguments.
        if arg_type not in overloaded_types and hasattr(arg_type, ""__torch_function__""):
# Create lists explicitly for the first type (usually the only one
# done) to avoid setting up the iterator for overloaded_args.
if overloaded_types:
"
601,"def start(self):
self._enter_actions()
if self.record_steps:
            self.step_rec_fn = prof.record_function(""ProfilerStep#"" + str(self.step_num))
self.step_rec_fn.__enter__()
def stop(self):
","def start(self):
self._enter_actions()
if self.record_steps:
            self.step_rec_fn = prof.record_function(
                ""ProfilerStep#"" + str(self.step_num)
            )
self.step_rec_fn.__enter__()
def stop(self):
"
602,"import torch
from typing import Optional
class SobolEngine(object):
r""""""
","from typing import Optional
import torch

class SobolEngine(object):
r""""""
"
603,"cpu = torch.device(""cpu"")
# Generate shift vector
        shift_ints = torch.randint(2, (self.dimension, self.MAXBIT), device=cpu, generator=g)
        self.shift = torch.mv(shift_ints, torch.pow(2, torch.arange(0, self.MAXBIT, device=cpu)))
# Generate lower triangular matrices (stacked across dimensions)
ltm_dims = (self.dimension, self.MAXBIT, self.MAXBIT)
","cpu = torch.device(""cpu"")
# Generate shift vector
        shift_ints = torch.randint(
            2, (self.dimension, self.MAXBIT), device=cpu, generator=g
        )
        self.shift = torch.mv(
            shift_ints, torch.pow(2, torch.arange(0, self.MAXBIT, device=cpu))
        )
# Generate lower triangular matrices (stacked across dimensions)
ltm_dims = (self.dimension, self.MAXBIT, self.MAXBIT)
"
604,"location = location_tag(obj)
serialized_storages[obj_key] = obj
            return ('storage',
                    storage_type,
                    obj_key,
                    location,
                    obj.size())
return None
# Write the pickle data for `obj`
","location = location_tag(obj)
serialized_storages[obj_key] = obj
            return (""storage"", storage_type, obj_key, location, obj.size())
return None
# Write the pickle data for `obj`
"
605,"\infty & x < 0
\end{cases}
\end{align}
"""""" + """"""
Args:
input (Tensor): the input tensor.
","\end{cases}
\end{align}
""""""
    + """"""
Args:
input (Tensor): the input tensor.
"
606,"tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742])
"""""".format(**common_args))

erf = _add_docstr(_special.special_erf,
                  r""""""
erf(input, *, out=None) -> Tensor
Computes the error function of :attr:`input`. The error function is defined as follows:
.. math::
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
"""""" + r""""""
Args:
{input}
","tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742])
"""""".format(
        **common_args
    ),
)

erf = _add_docstr(
    _special.special_erf,
    r""""""
erf(input, *, out=None) -> Tensor
Computes the error function of :attr:`input`. The error function is defined as follows:
.. math::
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
""""""
    + r""""""
Args:
{input}
"
607,".. math::
\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
"""""" + r""""""
Args:
{input}
",".. math::
\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
""""""
    + r""""""
Args:
{input}
"
608,"tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])
"""""".format(**common_args))

logsumexp = _add_docstr(_special.special_logsumexp,
                        r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Alias for :func:`torch.logsumexp`.
"""""".format(**multi_dim_common))

expit = _add_docstr(_special.special_expit,
                    r""""""
expit(input, *, out=None) -> Tensor
Computes the expit (also known as the logistic sigmoid function) of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}
"""""" + r""""""
Args:
{input}
","tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])
>>> torch.special.logit(a, eps=1e-6)
tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])
"""""".format(
        **common_args
    ),
)

logsumexp = _add_docstr(
    _special.special_logsumexp,
    r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Alias for :func:`torch.logsumexp`.
"""""".format(
        **multi_dim_common
    ),
)

expit = _add_docstr(
    _special.special_expit,
    r""""""
expit(input, *, out=None) -> Tensor
Computes the expit (also known as the logistic sigmoid function) of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}
""""""
    + r""""""
Args:
{input}
"
609,">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))
tensor([ 1.,  2.,  8., 16.])
"""""".format(**common_args))

expm1 = _add_docstr(_special.special_expm1,
                    r""""""
expm1(input, *, out=None) -> Tensor
Computes the exponential of the elements minus 1
",">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))
tensor([ 1.,  2.,  8., 16.])
"""""".format(
        **common_args
    ),
)

expm1 = _add_docstr(
    _special.special_expm1,
    r""""""
expm1(input, *, out=None) -> Tensor
Computes the exponential of the elements minus 1
"
610,">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(**common_args))
i0e = _add_docstr(_special.special_i0e,
                  r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
",">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(
        **common_args
    ),
)
i0e = _add_docstr(
    _special.special_i0e,
    r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
"
611,".. math::
\text{out}_{i} = \exp(-|x|) * i0(x) = \exp(-|x|) * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}
"""""" + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \exp(-|x|) * i0(x) = \exp(-|x|) * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}
""""""
    + r""""""
Args:
{input}
"
612,"Example::
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])
"""""".format(**common_args))

ndtr = _add_docstr(_special.special_ndtr,
                   r""""""
ndtr(input, *, out=None) -> Tensor
Computes the area under the standard Gaussian probability density function,
integrated from minus infinity to :attr:`input`, elementwise.
","Example::
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])
"""""".format(
        **common_args
    ),
)

ndtr = _add_docstr(
    _special.special_ndtr,
    r""""""
ndtr(input, *, out=None) -> Tensor
Computes the area under the standard Gaussian probability density function,
integrated from minus infinity to :attr:`input`, elementwise.
"
613,".. math::
\text{log\_softmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
"""""" + r""""""
Args:
input (Tensor): input
",".. math::
\text{log\_softmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
""""""
    + r""""""
Args:
input (Tensor): input
"
614,"return (_load_from_bytes, (b.getvalue(),))
def __sizeof__(self):
        return super(_StorageBase, self).__sizeof__() + self.element_size() * self.size()
def clone(self):
""""""Returns a copy of this storage""""""
","return (_load_from_bytes, (b.getvalue(),))
def __sizeof__(self):
        return (
            super(_StorageBase, self).__sizeof__() + self.element_size() * self.size()
        )
def clone(self):
""""""Returns a copy of this storage""""""
"
615,"qconfig = default_qconfig
return qconfig
def get_default_qat_qconfig(backend='fbgemm', version=None):
# Histogram observer is too slow for quantization aware training
if version is None:
if backend == 'fbgemm':
","qconfig = default_qconfig
return qconfig
def get_default_qat_qconfig(backend='fbgemm', version=1):
# Histogram observer is too slow for quantization aware training
if version is None:
if backend == 'fbgemm':
"
616,"default_placeholder_observer, default_weight_observer)
from .fake_quantize import (FakeQuantize, default_fake_quant,
default_per_channel_weight_fake_quant,
                            default_weight_fake_quant)
import torch
import torch.nn as nn
","default_placeholder_observer, default_weight_observer)
from .fake_quantize import (FakeQuantize, default_fake_quant,
default_per_channel_weight_fake_quant,
                            default_weight_fake_quant, default_fused_act_fake_quant, default_fused_wt_fake_quant,
                            FusedMovingAvgObsFakeQuantize, default_fused_per_channel_wt_fake_quant)
import torch
import torch.nn as nn
"
617,"return ret->accept_mutator(this);
}
const Expr* lhs_new = lhs->accept_mutator(this);
const Expr* rhs_new = rhs->accept_mutator(this);
if (lhs == lhs_new && rhs == rhs_new) {
","return ret->accept_mutator(this);
}
  // i % N -> i if the range of i's values is a subset of [0, N)
  // where N is an integer constant
  auto* lhsVar = dynamic_cast<const Var*>(lhs);
  const Expr* rhsScalar = rhs->isConstant() ? rhs : nullptr;
  if (lhsVar && rhsScalar && !rhsScalar->dtype().is_floating_point()) {
    auto got = var_bound_info_.find(lhsVar);
    if (got != var_bound_info_.end()) {
      auto start = got->second.first;
      auto end = got->second.second;
      const Expr* check_start =
          IRSimplifier::simplify(new CompareSelect(start, new IntImm(0), kGE));
      const Expr* check_end =
          IRSimplifier::simplify(new CompareSelect(end, rhsScalar, kLE));
      if (check_start->isConstant() && check_end->isConstant() &&
          immediateEquals(check_start, 1) && immediateEquals(check_end, 1)) {
        return lhsVar;
      }
    }
  }

const Expr* lhs_new = lhs->accept_mutator(this);
const Expr* rhs_new = rhs->accept_mutator(this);
if (lhs == lhs_new && rhs == rhs_new) {
"
618,"#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <ATen/Tensor.h>
","#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/engine.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <torch/csrc/autograd/variable.h>
#include <ATen/Tensor.h>
"
619,"}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
  if (!hooks) {
    return;
  }
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
620,"# @register_quant_pattern(torch.nn.functional.gelu)
# @register_quant_pattern(torch.nn.functional.softmax)
class DefaultNodeQuantizeHandler(QuantizeHandler):
    ''' Common quantized op, first input and first output will be quantized
    '''

def __init__(
self,
node: Node,
","# @register_quant_pattern(torch.nn.functional.gelu)
# @register_quant_pattern(torch.nn.functional.softmax)
class DefaultNodeQuantizeHandler(QuantizeHandler):
    """""" Common quantized op, first input and first output will be quantized
    """"""
def __init__(
self,
node: Node,
"
621,"// Converts model to Android NNAPI backend and serializes it for mobile
// Returns a dictionary with preprocessed items:
//    ""shape_compute_module"": torch::jit::Module,
//    ""ser_model"": torch::Tensor,
//    ""weights"": List[torch.Tensor],
//    ""inp_mem_fmts"": List[int],
//    ""out_mem_fmts"": List[int]
","// Converts model to Android NNAPI backend and serializes it for mobile
// Returns a dictionary with preprocessed items:
//    ""shape_compute_module"": torch::jit::Module,
//    ""ser_model"": at::Tensor,
//    ""weights"": List[torch.Tensor],
//    ""inp_mem_fmts"": List[int],
//    ""out_mem_fmts"": List[int]
"
622,"}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
  if (!hooks) {
    return;
  }
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
623,"}
}
  std::mutex PyDefaultSavedVariableHooks::mutex_;
  PyObject* PyDefaultSavedVariableHooks::pack_hook_(nullptr);
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_(nullptr);
void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    std::lock_guard<std::mutex> lock(mutex_);
TORCH_CHECK(!pack_hook_ && !unpack_hook_,
""Setting default hooks but they have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
}
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::pack_hook_ = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_ = nullptr;
void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
TORCH_CHECK(!pack_hook_ && !unpack_hook_,
""Setting default hooks but they have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
624,"# Inputs for which the primal is required for this formula
required_inputs_primal: Optional[Tuple[str, ...]]
# Represents differentiability info for a NativeFunction.
@dataclass(frozen=True)
class DifferentiabilityInfo:
","# Inputs for which the primal is required for this formula
required_inputs_primal: Optional[Tuple[str, ...]]
    # Flag to specify if this formula requires the original value of self
    # This is only used by inplace operations
    required_original_self_value: bool

    # If this formula is specified in derivatives.yaml or if we are re-using the
    # out of place formula for inplace
    is_reusing_outplace_formula: bool

# Represents differentiability info for a NativeFunction.
@dataclass(frozen=True)
class DifferentiabilityInfo:
"
625,"m.def(""_clear_callbacks"", []() {
at::clearCallbacks();
});
py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
","m.def(""_clear_callbacks"", []() {
at::clearCallbacks();
});
  m.def(""_register_default_hooks"", [](py::function &pack_hook, py::function &unpack_hook) {
    torch::autograd::PyDefaultSavedVariableHooks::set_hooks(pack_hook, unpack_hook);
  });
  m.def(""_reset_default_hooks"", []() {
    torch::autograd::PyDefaultSavedVariableHooks::reset_hooks();
  });
py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
"
626,"weight = weight_extraction_fn(mod)
return {
'type': res_type,
                    'values': weight,
'prev_node_name': node.name,
'prev_node_target_type': str(type(mod)),
'ref_node_name': node.name,
","weight = weight_extraction_fn(mod)
return {
'type': res_type,
                    'values': [weight],
'prev_node_name': node.name,
'prev_node_target_type': str(type(mod)),
'ref_node_name': node.name,
"
627,"""""""
return _node_list(self)
    def graph_copy(self, g : 'Graph', val_map : Dict[Node, Node]) -> 'Optional[Argument]':
""""""
Copy all nodes from a given graph into ``self``.
","""""""
return _node_list(self)
    def graph_copy(self, g : 'Graph', val_map : Dict[Node, Node], return_output_node=False) -> 'Optional[Argument]':
""""""
Copy all nodes from a given graph into ``self``.
"
628,"return unsqueeze_node;
}
} // namespace jit
} // namespace torch
","return unsqueeze_node;
}
bool isValidToTransformToONNXConcatNode(Node* lc_node) {
  return !lc_node->inputs().empty();
}

Node* transformToONNXConcatNode(
    Graph* g,
    Node* lc_node,
    bool need_new_input,
    int opset_version) {
  // ListConstruct Int[] output case, we need to transform to ONNX
  // Concat to ensure the output is a single tensor(dynamic) type in
  // order to be consumed as inputs
  std::vector<Value*> unsqueezed;
  auto new_node = need_new_input ? g->return_node() : lc_node;

  for (auto* input : lc_node->inputs()) {
    auto new_input =
        need_new_input ? g->addInput()->copyMetadata(input) : input;

    Node* unsqueezed_node =
        createONNXUnsqueeze(g, new_node, new_input, 0, opset_version);
    unsqueezed.emplace_back(unsqueezed_node->output());
  }

  Node* concat_node = need_new_input
      ? g->insertNode(g->create(onnx::Concat, 1))
      : g->create(onnx::Concat, 1)->insertBefore(lc_node);
  concat_node->i_(attr::axis, 0);
  for (auto v : unsqueezed) {
    concat_node->addInput(v);
  }

  return concat_node;
}

} // namespace jit
} // namespace torch
"
629,"auto* lc_node = input->node();
TypePtr elem =
lc_node->output()->type()->castRaw<ListType>()->getElementType();
      if (elem->cast<IntType>()) {
        // ListConstruct Int[] output case, we need to transform to ONNX
        // Concat to ensure the output is a single tensor(dynamic) type in
        // order to be consumed as inputs
        std::vector<Value*> unsqueezed;
        Graph* g = block->owningGraph();
        for (auto* input : lc_node->inputs()) {
          Node* unsqueezed_node =
              createONNXUnsqueeze(g, lc_node, input, 0, opset_version);
          unsqueezed.emplace_back(unsqueezed_node->output());
        }
        Node* concat_node = g->create(onnx::Concat, 1);
        concat_node->i_(attr::axis, 0);
        for (auto v : unsqueezed) {
          concat_node->addInput(v);
        }
        concat_node->insertBefore(lc_node);

// make concat node output as new input, then ListConstruct should
// become dead
replacements.emplace_back(
i, std::vector<Value*>({concat_node->output()}));

} else {
if (opset_version >= OPSET_VERSION_11) {
c10::Symbol seq_node_kind = lc_node->inputs().size() > 0
","auto* lc_node = input->node();
TypePtr elem =
lc_node->output()->type()->castRaw<ListType>()->getElementType();
      if (elem->cast<IntType>() &&
          isValidToTransformToONNXConcatNode(lc_node)) {
        auto concat_node = transformToONNXConcatNode(
            block->owningGraph(), input->node(), false, opset_version);
// make concat node output as new input, then ListConstruct should
// become dead
replacements.emplace_back(
i, std::vector<Value*>({concat_node->output()}));
} else {
if (opset_version >= OPSET_VERSION_11) {
c10::Symbol seq_node_kind = lc_node->inputs().size() > 0
"
630,"for (const auto i : c10::irange(future_result.size())) {
auto& replica = bucket.replicas[i];
if (bucket.expect_sparse_gradient) {
        // If no DDP comm hook is registered,
        // the allreduce only sums up the value, and a separate division is required.
        if (comm_hook_ == nullptr) {
          future_result[i].div_(div_factor_);
        }
replica.contents.copy_(future_result[i]);
} else {
// Reinitialize only `bucket_views_out` with the future_result by
","for (const auto i : c10::irange(future_result.size())) {
auto& replica = bucket.replicas[i];
if (bucket.expect_sparse_gradient) {
replica.contents.copy_(future_result[i]);
} else {
// Reinitialize only `bucket_views_out` with the future_result by
"
631,"#include <ATen/WrapDimUtils.h>
#include <ATen/core/DimVector.h>
#include <c10/util/Exception.h>
namespace at {
namespace native {
","#include <ATen/WrapDimUtils.h>
#include <ATen/core/DimVector.h>
#include <c10/util/Exception.h>
#include <c10/core/ScalarType.h>
namespace at {
namespace native {
"
632,"#include <torch/csrc/python_headers.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <stdarg.h>
#include <string>
#include <torch/csrc/cuda/THCP.h>
","#include <torch/csrc/python_headers.h>
#include <cstdarg>
#include <string>
#include <torch/csrc/cuda/THCP.h>
"
633,"#include <sys/mman.h>
#include <poll.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <errno.h>
#include <unistd.h>
#include <fcntl.h>
#include <vector>
#include <set>
#include <algorithm>
#include <memory>
#include <unordered_map>
#include <c10/util/tempfile.h>
","#include <algorithm>
#include <cerrno>
#include <fcntl.h>
#include <memory>
#include <poll.h>
#include <set>
#include <sys/mman.h>
#include <unistd.h>
#include <unordered_map>
#include <vector>
#include <c10/util/tempfile.h>
"
634,">>> a = torch.arange(9, dtype=torch.float) - 4
>>> a
tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])
    >>> b = a.reshape((3, 3))
    >>> b
tensor([[-4., -3., -2.],
[-1.,  0.,  1.],
[ 2.,  3.,  4.]])
>>> LA.norm(a)
tensor(7.7460)
    >>> LA.norm(b)
tensor(7.7460)
    >>> LA.norm(b, 'fro')
tensor(7.7460)
>>> LA.norm(a, float('inf'))
tensor(4.)
    >>> LA.norm(b, float('inf'))
tensor(9.)
>>> LA.norm(a, -float('inf'))
tensor(0.)
    >>> LA.norm(b, -float('inf'))
tensor(2.)
>>> LA.norm(a, 1)
tensor(20.)
    >>> LA.norm(b, 1)
tensor(7.)
>>> LA.norm(a, -1)
tensor(0.)
    >>> LA.norm(b, -1)
tensor(6.)
>>> LA.norm(a, 2)
tensor(7.7460)
    >>> LA.norm(b, 2)
tensor(7.3485)
>>> LA.norm(a, -2)
tensor(0.)
    >>> LA.norm(b.double(), -2)
tensor(1.8570e-16, dtype=torch.float64)
>>> LA.norm(a, 3)
tensor(5.8480)
",">>> a = torch.arange(9, dtype=torch.float) - 4
>>> a
tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])
    >>> B = a.reshape((3, 3))
    >>> B
tensor([[-4., -3., -2.],
[-1.,  0.,  1.],
[ 2.,  3.,  4.]])
>>> LA.norm(a)
tensor(7.7460)
    >>> LA.norm(B)
tensor(7.7460)
    >>> LA.norm(B, 'fro')
tensor(7.7460)
>>> LA.norm(a, float('inf'))
tensor(4.)
    >>> LA.norm(B, float('inf'))
tensor(9.)
>>> LA.norm(a, -float('inf'))
tensor(0.)
    >>> LA.norm(B, -float('inf'))
tensor(2.)
>>> LA.norm(a, 1)
tensor(20.)
    >>> LA.norm(B, 1)
tensor(7.)
>>> LA.norm(a, -1)
tensor(0.)
    >>> LA.norm(B, -1)
tensor(6.)
>>> LA.norm(a, 2)
tensor(7.7460)
    >>> LA.norm(B, 2)
tensor(7.3485)
>>> LA.norm(a, -2)
tensor(0.)
    >>> LA.norm(B.double(), -2)
tensor(1.8570e-16, dtype=torch.float64)
>>> LA.norm(a, 3)
tensor(5.8480)
"
635,"Examples::
    >>> a = torch.eye(4 * 6).reshape((4, 6, 8, 3))
    >>> ainv = torch.linalg.tensorinv(a, ind=2)
    >>> ainv.shape
torch.Size([8, 3, 4, 6])
    >>> b = torch.randn(4, 6)
    >>> torch.allclose(torch.tensordot(ainv, b), torch.linalg.tensorsolve(a, b))
True
    >>> a = torch.randn(4, 4)
    >>> a_tensorinv = torch.linalg.tensorinv(a, ind=1)
    >>> a_inv = torch.inverse(a)
    >>> torch.allclose(a_tensorinv, a_inv)
True
"""""")
","Examples::
    >>> A = torch.eye(4 * 6).reshape((4, 6, 8, 3))
    >>> Ainv = torch.linalg.tensorinv(A, ind=2)
    >>> Ainv.shape
torch.Size([8, 3, 4, 6])
    >>> B = torch.randn(4, 6)
    >>> torch.allclose(torch.tensordot(Ainv, B), torch.linalg.tensorsolve(A, B))
True
    >>> A = torch.randn(4, 4)
    >>> Atensorinv = torch.linalg.tensorinv(A, ind=1)
    >>> Ainv = torch.linalg.inverse(A)
    >>> torch.allclose(Atensorinv, Ainv)
True
"""""")
"
636,"#include <torch/csrc/jit/api/object.h>
#include <torch/csrc/jit/python/script_init.h>
","#include <pybind11/detail/common.h>
#include <torch/csrc/jit/api/object.h>
#include <torch/csrc/jit/python/script_init.h>
"
637,"# compiled -- translation units are, of which there is one per implementation
# (c/cc/cpp) file.
DEFAULT_FILE_PATTERN = re.compile(r""^.*\.c(c|pp)?$"")
CLANG_WARNING_PATTERN = re.compile(r""([^:]+):(\d+):\d+:\s+warning:.*\[([^\]]+)\]"")
# Set from command line arguments in main().
VERBOSE = False
# Functions for correct handling of ""ATen/native/cpu"" mapping
","# compiled -- translation units are, of which there is one per implementation
# (c/cc/cpp) file.
DEFAULT_FILE_PATTERN = re.compile(r""^.*\.c(c|pp)?$"")
CLANG_WARNING_PATTERN = re.compile(
    r""([^:]+):(\d+):\d+:\s+(warning|error):.*\[([^\]]+)\]""
)
# Set from command line arguments in main().
VERBOSE = False
QUIET = False
def log(*args: Any, **kwargs: Any) -> None:
    if not QUIET:
        print(*args, **kwargs)

class CommandResult:
    def __init__(self, returncode: int, stdout: str, stderr: str):
        self.returncode = returncode
        self.stdout = stdout.strip()
        self.stderr = stderr.strip()

    def failed(self) -> bool:
        return self.returncode != 0

    def __add__(self, other: ""CommandResult"") -> ""CommandResult"":
        return CommandResult(
            self.returncode + other.returncode,
            f""{self.stdout}\n{other.stdout}"",
            f""{self.stderr}\n{other.stderr}"",
        )

    def __str__(self) -> str:
        return f""{self.stdout}""

    def __repr__(self) -> str:
        return (
            f""returncode: {self.returncode}\n""
            + f""stdout: {self.stdout}\n""
            + f""stderr: {self.stderr}""
        )


class ProgressMeter:
    def __init__(
        self, num_items: int, start_msg: str = """", disable_progress_bar: bool = False
    ) -> None:
        self.num_items = num_items
        self.num_processed = 0
        self.width = 80
        self.disable_progress_bar = disable_progress_bar

        # helper escape sequences
        self._clear_to_end = ""\x1b[2K""
        self._move_to_previous_line = ""\x1b[F""
        self._move_to_start_of_line = ""\r""
        self._move_to_next_line = ""\n""

        if self.disable_progress_bar:
            log(start_msg)
        else:
            self._write(
                start_msg
                + self._move_to_next_line
                + ""[>""
                + (self.width * "" "")
                + ""]""
                + self._move_to_start_of_line
            )
            self._flush()

    def _write(self, s: str) -> None:
        sys.stderr.write(s)

    def _flush(self) -> None:
        sys.stderr.flush()

    def update(self, msg: str) -> None:
        if self.disable_progress_bar:
            return

        # Once we've processed all items, clear the progress bar
        if self.num_processed == self.num_items - 1:
            self._write(self._clear_to_end)
            return

        # NOP if we've already processed all items
        if self.num_processed > self.num_items:
            return

        self.num_processed += 1

        self._write(
            self._move_to_previous_line
            + self._clear_to_end
            + msg
            + self._move_to_next_line
        )

        progress = int((self.num_processed / self.num_items) * self.width)
        padding = self.width - progress
        self._write(
            self._move_to_start_of_line
            + self._clear_to_end
            + f""({self.num_processed} of {self.num_items}) ""
            + f""[{progress*'='}>{padding*' '}]""
            + self._move_to_start_of_line
        )
        self._flush()

    def print(self, msg: str) -> None:
        if QUIET:
            return
        elif self.disable_progress_bar:
            print(msg)
        else:
            self._write(
                self._clear_to_end
                + self._move_to_previous_line
                + self._clear_to_end
                + msg
                + self._move_to_next_line
                + self._move_to_next_line
            )
            self._flush()


class ClangTidyWarning:
    def __init__(self, name: str, occurrences: List[Tuple[str, int]]):
        self.name = name
        self.occurrences = occurrences

    def __str__(self) -> str:
        base = f""[{self.name}] occurred {len(self.occurrences)} times\n""
        for occ in self.occurrences:
            base += f""    {occ[0]}:{occ[1]}\n""
        return base


async def run_shell_command(
    cmd: List[str], on_completed: Any = None, *args: Any
) -> CommandResult:
    """"""Executes a shell command and runs an optional callback when complete""""""
    if VERBOSE:
        log(""Running: "", "" "".join(cmd))

    proc = await asyncio.create_subprocess_shell(
        "" "".join(shlex.quote(x) for x in cmd),  # type: ignore[attr-defined]
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    output = await proc.communicate()
    result = CommandResult(
        returncode=proc.returncode if proc.returncode is not None else -1,
        stdout=output[0].decode(""utf-8"").strip(),
        stderr=output[1].decode(""utf-8"").strip(),
    )

    if on_completed:
        on_completed(result, *args)

    return result


async def _run_clang_tidy_in_parallel(
    commands: List[Tuple[List[str], str]], disable_progress_bar: bool
) -> CommandResult:
    progress_meter = ProgressMeter(
        len(commands),
        f""Processing {len(commands)} clang-tidy jobs"",
        disable_progress_bar=disable_progress_bar,
    )

    async def gather_with_concurrency(n: int, tasks: List[Any]) -> Any:
        semaphore = asyncio.Semaphore(n)

        async def sem_task(task: Any) -> Any:
            async with semaphore:
                return await task

        return await asyncio.gather(
            *(sem_task(task) for task in tasks), return_exceptions=True
        )

    async def helper() -> Any:
        def on_completed(result: CommandResult, filename: str) -> None:
            if result.failed():
                msg = str(result) if not VERBOSE else repr(result)
                progress_meter.print(msg)
            progress_meter.update(f""Processed {filename}"")

        coros = [
            run_shell_command(cmd, on_completed, filename)
            for (cmd, filename) in commands
        ]
        return await gather_with_concurrency(multiprocessing.cpu_count(), coros)

    results = await helper()
    return sum(results, CommandResult(0, """", """"))


async def _run_clang_tidy(
    options: Any, line_filters: List[Dict[str, Any]], files: Iterable[str]
) -> CommandResult:
    """"""Executes the actual clang-tidy command in the shell.""""""

    base = [options.clang_tidy_exe]

    # Apply common options
    base += [""-p"", options.compile_commands_dir]
    if not options.config_file and os.path.exists("".clang-tidy""):
        options.config_file = "".clang-tidy""
    if options.config_file:
        import yaml

        with open(options.config_file) as config:
            # Here we convert the YAML config file to a JSON blob.
            base += [
                ""-config"",
                json.dumps(yaml.load(config, Loader=yaml.SafeLoader)),
            ]
    if options.print_include_paths:
        base += [""--extra-arg"", ""-v""]
    if options.include_dir:
        for dir in options.include_dir:
            base += [""--extra-arg"", f""-I{dir}""]
    base += options.extra_args
    if line_filters:
        base += [""-line-filter"", json.dumps(line_filters)]

    # Apply per-file options
    commands = []
    for f in files:
        command = list(base) + [map_filename(options.compile_commands_dir, f)]
        commands.append((command, f))

    if options.dry_run:
        return CommandResult(0, str([c for c, _ in commands]), """")

    return await _run_clang_tidy_in_parallel(commands, options.disable_progress_bar)


def extract_warnings(
    output: str, base_dir: str = "".""
) -> Tuple[Dict[str, Dict[int, Set[str]]], List[ClangTidyWarning]]:
    warn2occ: Dict[str, List[Tuple[str, int]]] = {}
    fixes: Dict[str, Dict[int, Set[str]]] = {}
    for line in output.splitlines():
        p = CLANG_WARNING_PATTERN.match(line)
        if p is None:
            continue
        if os.path.isabs(p.group(1)):
            path = os.path.abspath(p.group(1))
        else:
            path = os.path.abspath(os.path.join(base_dir, p.group(1)))
        line_no = int(p.group(2))

        # Filter out any options (which start with '-')
        warning_names = set([w for w in p.group(4).split("","") if not w.startswith(""-"")])

        for name in warning_names:
            if name not in warn2occ:
                warn2occ[name] = []
            warn2occ[name].append((path, line_no))

        if path not in fixes:
            fixes[path] = {}
        if line_no not in fixes[path]:
            fixes[path][line_no] = set()
        fixes[path][line_no].update(warning_names)

    warnings = [ClangTidyWarning(name, sorted(occ)) for name, occ in warn2occ.items()]

    return fixes, warnings


def apply_nolint(fname: str, warnings: Dict[int, Set[str]]) -> None:
    with open(fname, encoding=""utf-8"") as f:
        lines = f.readlines()

    line_offset = -1  # As in .cpp files lines are numbered starting from 1
    for line_no in sorted(warnings.keys()):
        nolint_diagnostics = "","".join(warnings[line_no])
        line_no += line_offset
        indent = "" "" * (len(lines[line_no]) - len(lines[line_no].lstrip("" "")))
        lines.insert(line_no, f""{indent}// NOLINTNEXTLINE({nolint_diagnostics})\n"")
        line_offset += 1

    with open(fname, mode=""w"") as f:
        f.write("""".join(lines))
# Functions for correct handling of ""ATen/native/cpu"" mapping
"
638,"parser.add_argument(
""-c"",
""--compile-commands-dir"",
        default=""build"",
help=""Path to the folder containing compile_commands.json"",
)
parser.add_argument(
","parser.add_argument(
""-c"",
""--compile-commands-dir"",
        default=DEFAULTS[""compile-commands-dir""],
help=""Path to the folder containing compile_commands.json"",
)
parser.add_argument(
"
639,"}
OUTPUT_DIR = os.path.join(PYTORCH_ROOT, "".clang-tidy-bin"")
if __name__ == ""__main__"":
ok = download(""clang-tidy"", OUTPUT_DIR, PLATFORM_TO_URL, PLATFORM_TO_HASH)
","}
OUTPUT_DIR = os.path.join(PYTORCH_ROOT, "".clang-tidy-bin"")
INSTALLATION_PATH = os.path.join(OUTPUT_DIR, ""clang-tidy"")
if __name__ == ""__main__"":
ok = download(""clang-tidy"", OUTPUT_DIR, PLATFORM_TO_URL, PLATFORM_TO_HASH)
"
640,"// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
PyTypeObject THPFInfoType = {
PyVarObject_HEAD_INIT(nullptr, 0) ""torch.finfo"", /* tp_name */
    sizeof(THPFInfo), /* tp_basicsize */
    0, /* tp_itemsize */
    nullptr, /* tp_dealloc */
    // NOLINTNEXTLINE(modernize-use-nullptr)
    0, /* tp_vectorcall_offset */
    nullptr, /* tp_getattr */
    nullptr, /* tp_setattr */
    nullptr, /* tp_reserved */
    (reprfunc)THPFInfo_str, /* tp_repr */
    nullptr, /* tp_as_number */
    nullptr, /* tp_as_sequence */
    nullptr, /* tp_as_mapping */
    nullptr, /* tp_hash  */
    nullptr, /* tp_call */
    (reprfunc)THPFInfo_str, /* tp_str */
    nullptr, /* tp_getattro */
    nullptr, /* tp_setattro */
    nullptr, /* tp_as_buffer */
    Py_TPFLAGS_DEFAULT, /* tp_flags */
    nullptr, /* tp_doc */
    nullptr, /* tp_traverse */
    nullptr, /* tp_clear */
    (richcmpfunc)THPDTypeInfo_compare, /* tp_richcompare */
    0, /* tp_weaklistoffset */
    nullptr, /* tp_iter */
    nullptr, /* tp_iternext */
    THPFInfo_methods, /* tp_methods */
    nullptr, /* tp_members */
    THPFInfo_properties, /* tp_getset */
    nullptr, /* tp_base */
    nullptr, /* tp_dict */
    nullptr, /* tp_descr_get */
    nullptr, /* tp_descr_set */
    0, /* tp_dictoffset */
    nullptr, /* tp_init */
    nullptr, /* tp_alloc */
    THPFInfo_pynew, /* tp_new */
};
// NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,cppcoreguidelines-avoid-c-arrays)
","// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
PyTypeObject THPFInfoType = {
PyVarObject_HEAD_INIT(nullptr, 0) ""torch.finfo"", /* tp_name */
    sizeof(THPFInfo),                                /* tp_basicsize */
    0,                                               /* tp_itemsize */
    nullptr,                                         /* tp_dealloc */
    0,                                               /* tp_vectorcall_offset */
    nullptr,                                         /* tp_getattr */
    nullptr,                                         /* tp_setattr */
    nullptr,                                         /* tp_reserved */
    (reprfunc)THPFInfo_str,                          /* tp_repr */
    nullptr,                                         /* tp_as_number */
    nullptr,                                         /* tp_as_sequence */
    nullptr,                                         /* tp_as_mapping */
    nullptr,                                         /* tp_hash  */
    nullptr,                                         /* tp_call */
    (reprfunc)THPFInfo_str,                          /* tp_str */
    nullptr,                                         /* tp_getattro */
    nullptr,                                         /* tp_setattro */
    nullptr,                                         /* tp_as_buffer */
    Py_TPFLAGS_DEFAULT,                              /* tp_flags */
    nullptr,                                         /* tp_doc */
    nullptr,                                         /* tp_traverse */
    nullptr,                                         /* tp_clear */
    (richcmpfunc)THPDTypeInfo_compare,               /* tp_richcompare */
    0,                                               /* tp_weaklistoffset */
    nullptr,                                         /* tp_iter */
    nullptr,                                         /* tp_iternext */
    THPFInfo_methods,                                /* tp_methods */
    nullptr,                                         /* tp_members */
    THPFInfo_properties,                             /* tp_getset */
    nullptr,                                         /* tp_base */
    nullptr,                                         /* tp_dict */
    nullptr,                                         /* tp_descr_get */
    nullptr,                                         /* tp_descr_set */
    0,                                               /* tp_dictoffset */
    nullptr,                                         /* tp_init */
    nullptr,                                         /* tp_alloc */
    THPFInfo_pynew,                                  /* tp_new */
};
// NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables,cppcoreguidelines-avoid-c-arrays)
"
641,"sizeof(THPIInfo), /* tp_basicsize */
0, /* tp_itemsize */
nullptr, /* tp_dealloc */
    // NOLINTNEXTLINE(modernize-use-nullptr)
0, /* tp_vectorcall_offset */
nullptr, /* tp_getattr */
nullptr, /* tp_setattr */
","sizeof(THPIInfo), /* tp_basicsize */
0, /* tp_itemsize */
nullptr, /* tp_dealloc */
0, /* tp_vectorcall_offset */
nullptr, /* tp_getattr */
nullptr, /* tp_setattr */
"
642,"sizeof(THPFunction),                         /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPFunction_dealloc,             /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","sizeof(THPFunction),                         /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPFunction_dealloc,             /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
643,"// directly.  Subclasses will have their tp_dealloc set appropriately
// by the metaclass
nullptr, /* tp_dealloc */
    // NOLINTNEXTLINE(modernize-use-nullptr)
0, /* tp_vectorcall_offset */
nullptr, /* tp_getattr */
nullptr, /* tp_setattr */
","// directly.  Subclasses will have their tp_dealloc set appropriately
// by the metaclass
nullptr, /* tp_dealloc */
0, /* tp_vectorcall_offset */
nullptr, /* tp_getattr */
nullptr, /* tp_setattr */
"
644,"// All Python defined classes have __dict__
if (C10_LIKELY(type->tp_dictoffset)) {
PyObject** dictptr = _PyObject_GetDictPtr(self);
    // NOLINTNEXTLINE(modernize-use-nullptr)
    if (dictptr != NULL) {
PyObject* dict = *dictptr;
      // NOLINTNEXTLINE(modernize-use-nullptr)
      if (dict != NULL) {
Py_DECREF(dict);
        // NOLINTNEXTLINE(modernize-use-nullptr)
        *dictptr = NULL;
}
}
}
","// All Python defined classes have __dict__
if (C10_LIKELY(type->tp_dictoffset)) {
PyObject** dictptr = _PyObject_GetDictPtr(self);
    if (dictptr != nullptr) {
PyObject* dict = *dictptr;
      if (dict != nullptr) {
Py_DECREF(dict);
        *dictptr = nullptr;
}
}
}
"
645,"If ``allow_empty=True``, no such exception is thrown.
""""""
        if version_info < (3, 7):
            raise RuntimeError(
                ""Python 3.7 or higher is required to mock out modules. ""
                ""Please upgrade your Python version.""
            )
self.patterns[GlobGroup(include, exclude=exclude)] = _PatternInfo(
_ModuleProviderAction.MOCK, allow_empty
)
","If ``allow_empty=True``, no such exception is thrown.
""""""
self.patterns[GlobGroup(include, exclude=exclude)] = _PatternInfo(
_ModuleProviderAction.MOCK, allow_empty
)
"
646,"from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import (
Any,
BinaryIO,
","from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from sys import version_info
from typing import (
Any,
BinaryIO,
"
647,"def visit_Call(self, node):
# __import__ calls aren't routed to the visit_Import/From nodes
if hasattr(node.func, ""id"") and node.func.id == ""__import__"":
            if type(node.args[0]) not in [ast.Constant, ast.Str]:
                # We don't want to parse dynamic uses of __import__
return
            name = self._grab_node_str(node.args[0])
            fromlist = []
            level = 0
            if len(node.args) > 3:
                for v in node.args[3].elts:
                    fromlist.append(self._grab_node_str(v))
            elif hasattr(node, ""keywords""):
                for keyword in node.keywords:
                    if keyword.arg == ""fromlist"":
                        for v in keyword.value.elts:
                            fromlist.append(self._grab_node_str(v))
            if len(node.args) > 4:
                level = self._grab_node_int(node.args[4])
            elif hasattr(node, ""keywords""):
                for keyword in node.keywords:
                    if keyword.arg == ""level"":
                        level = self._grab_node_int(keyword.value)
            if fromlist == []:
                # the top-level package (the name up till the first dot) is returned
                # when the fromlist argument is empty in normal import system,
                # we need to include top level package to match this behavior and last
                # level package to capture the intended dependency of user
                self.references[(name, None)] = True
                top_name = name.rsplit(""."", maxsplit=1)[0]
                if top_name != name:
                    top_name = self._absmodule(top_name, level)
                    self.references[(top_name, None)] = True
            else:
                name = self._absmodule(name, level)
                for alias in fromlist:
                    # fromlist args may be submodules, so we have to add the fromlist args
                    # to the list of potential references. If import of an arg fails we
                    # will ignore it, similar to visit_ImportFrom
                    if alias != ""*"":
                        self.references[(name, alias)] = True
                    else:
                        self.references[(name, None)] = True

find_files_source_depends_on = _ExtractModuleReferences.run
","def visit_Call(self, node):
# __import__ calls aren't routed to the visit_Import/From nodes
if hasattr(node.func, ""id"") and node.func.id == ""__import__"":
            try:
                name = self._grab_node_str(node.args[0])
                fromlist = []
                level = 0
                if len(node.args) > 3:
                    for v in node.args[3].elts:
                        fromlist.append(self._grab_node_str(v))
                elif hasattr(node, ""keywords""):
                    for keyword in node.keywords:
                        if keyword.arg == ""fromlist"":
                            for v in keyword.value.elts:
                                fromlist.append(self._grab_node_str(v))
                if len(node.args) > 4:
                    level = self._grab_node_int(node.args[4])
                elif hasattr(node, ""keywords""):
                    for keyword in node.keywords:
                        if keyword.arg == ""level"":
                            level = self._grab_node_int(keyword.value)
                if fromlist == []:
                    # the top-level package (the name up till the first dot) is returned
                    # when the fromlist argument is empty in normal import system,
                    # we need to include top level package to match this behavior and last
                    # level package to capture the intended dependency of user
                    self.references[(name, None)] = True
                    top_name = name.rsplit(""."", maxsplit=1)[0]
                    if top_name != name:
                        top_name = self._absmodule(top_name, level)
                        self.references[(top_name, None)] = True
                else:
                    name = self._absmodule(name, level)
                    for alias in fromlist:
                        # fromlist args may be submodules, so we have to add the fromlist args
                        # to the list of potential references. If import of an arg fails we
                        # will ignore it, similar to visit_ImportFrom
                        if alias != ""*"":
                            self.references[(name, alias)] = True
                        else:
                            self.references[(name, None)] = True
            except Exception as e:
return
find_files_source_depends_on = _ExtractModuleReferences.run
"
648,"THPObjectPtr value(THPVariable_Wrap(values.at(value_idx)));
if (!value) throw python_error();
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  PyObject *key, *hook;
  Py_ssize_t pos = 0;
  while (PyDict_Next(dict, &pos, &key, &hook)) {
THPObjectPtr res(PyObject_CallFunctionObjArgs(hook, value.get(), nullptr));
if (!res) throw python_error();
if (res == Py_None) continue;
","THPObjectPtr value(THPVariable_Wrap(values.at(value_idx)));
if (!value) throw python_error();
  // Note: [Extend Hook Lifetime]
  // Hold a reference to hooks till we iterate over them.
  // This is to handle the case when hook calls `handle.remove` inside it
  // and it's refcount goes to `0`, Python is free to GC it.
  // We hold onto a stale pointer and subsequent call to
  // `check_single_result`, which tries to fetch the `hook`'s name segfaults.
  // So, we use `PyDict_Values` which returns a new reference to the values
  // i.e. we hold the reference to the hooks till we have iterated over them.
  // Reference: https://github.com/pytorch/pytorch/issues/58354
  auto hooks = THPObjectPtr{PyDict_Values(dict)};
  const auto len = PyList_Size(hooks);
  for (Py_ssize_t idx = 0; idx < len; ++idx) {
    const auto hook = PyList_GetItem(hooks, idx);
THPObjectPtr res(PyObject_CallFunctionObjArgs(hook, value.get(), nullptr));
if (!res) throw python_error();
if (res == Py_None) continue;
"
649,"Target.REGISTRATION,
selector,
rocm=options.rocm,
                    cpp_namespace='at::native'),
grouped_native_functions
)),
})
","Target.REGISTRATION,
selector,
rocm=options.rocm,
                    cpp_namespace='at::native',
                    class_method_name=None),
grouped_native_functions
)),
})
"
650,"Target.NAMESPACED_DECLARATION,
selector,
rocm=options.rocm,
                        cpp_namespace='at::native'),
grouped_native_functions
)),
})
","Target.NAMESPACED_DECLARATION,
selector,
rocm=options.rocm,
                        cpp_namespace='at::native',
                        class_method_name=None),
grouped_native_functions
)),
})
"
651,"Target.NAMESPACED_DEFINITION,
selector,
rocm=False,
                        cpp_namespace=cpp_namespace),
grouped_native_functions
)),
'dispatch_anonymous_definitions': list(concatMap(
","Target.NAMESPACED_DEFINITION,
selector,
rocm=False,
                        cpp_namespace=cpp_namespace,
                        class_method_name=f'{backend_dispatch_key}NativeFunctions'),
grouped_native_functions
)),
'dispatch_anonymous_definitions': list(concatMap(
"
652,"Target.ANONYMOUS_DEFINITION,
selector,
rocm=False,
                        cpp_namespace=cpp_namespace),
grouped_native_functions
)),
'dispatch_registrations': list(concatMap(
","Target.ANONYMOUS_DEFINITION,
selector,
rocm=False,
                        cpp_namespace=cpp_namespace,
                        class_method_name=f'{backend_dispatch_key}NativeFunctions'),
grouped_native_functions
)),
'dispatch_registrations': list(concatMap(
"
653,"if (opts_.debug) {
WriteProtoToTextFile(*pred_net, ""debug_full_opt_net.pb_txt"", false);
}
}
} // namespace caffe2
","if (opts_.debug) {
WriteProtoToTextFile(*pred_net, ""debug_full_opt_net.pb_txt"", false);
}
  if (opts_.verify_only_single_subnet && cutResult.numberOfSubnets > 1) {
    CAFFE_THROW(""Multiple Onnxifi ops were created: "", cutResult.numberOfSubnets, "" subnets found"");
  }
}
} // namespace caffe2
"
654,"std::vector<at::Tensor> tensor_args;
std::vector<int> tensor_args_indices;
// Step 1: Convert all non-CPU tensor inputs into CPU tensors
// and put them on the stack at the correct indices.
for (int64_t idx = 0; idx < arguments.size(); ++idx) {
","std::vector<at::Tensor> tensor_args;
std::vector<int> tensor_args_indices;
  std::vector<c10::List<at::Tensor>> tensorlist_args;

// Step 1: Convert all non-CPU tensor inputs into CPU tensors
// and put them on the stack at the correct indices.
for (int64_t idx = 0; idx < arguments.size(); ++idx) {
"
655,"doctest_test_doctest_blocks = ''
doctest_default_flags = sphinx.ext.doctest.doctest.ELLIPSIS
doctest_global_setup = '''
try:
import torchvision
except ImportError:
","doctest_test_doctest_blocks = ''
doctest_default_flags = sphinx.ext.doctest.doctest.ELLIPSIS
doctest_global_setup = '''
import torch
try:
import torchvision
except ImportError:
"
656,"in Futures.
Example::
        >>> import torch
        >>>
>>> fut0 = torch.futures.Future()
>>> fut1 = torch.futures.Future()
        >>>
>>> fut = torch.futures.collect_all([fut0, fut1])
        >>>
>>> fut0.set_result(0)
>>> fut1.set_result(1)
        >>>
>>> fut_list = fut.wait()
>>> print(f""fut0 result = {fut_list[0].wait()}"")
>>> print(f""fut1 result = {fut_list[1].wait()}"")
        >>> # outputs:
        >>> # fut0 result = 0
        >>> # fut1 result = 1
""""""
return cast(Future[List[Future]], torch._C._collect_all(cast(List[torch._C.Future], futures)))
","in Futures.
Example::
>>> fut0 = torch.futures.Future()
>>> fut1 = torch.futures.Future()
>>> fut = torch.futures.collect_all([fut0, fut1])
>>> fut0.set_result(0)
>>> fut1.set_result(1)
>>> fut_list = fut.wait()
>>> print(f""fut0 result = {fut_list[0].wait()}"")
        fut0 result = 0
>>> print(f""fut1 result = {fut_list[1].wait()}"")
        fut1 result = 1
""""""
return cast(Future[List[Future]], torch._C._collect_all(cast(List[torch._C.Future], futures)))
"
657,"import math

import torch
import torch.jit
from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import broadcast_all, lazy_property


def _eval_poly(y, coef):
    coef = list(coef)
    result = coef.pop()
    while coef:
        result = coef.pop() + y * result
    return result


_I0_COEF_SMALL = [1.0, 3.5156229, 3.0899424, 1.2067492, 0.2659732, 0.360768e-1, 0.45813e-2]
_I0_COEF_LARGE = [0.39894228, 0.1328592e-1, 0.225319e-2, -0.157565e-2, 0.916281e-2,
                  -0.2057706e-1, 0.2635537e-1, -0.1647633e-1, 0.392377e-2]
_I1_COEF_SMALL = [0.5, 0.87890594, 0.51498869, 0.15084934, 0.2658733e-1, 0.301532e-2, 0.32411e-3]
_I1_COEF_LARGE = [0.39894228, -0.3988024e-1, -0.362018e-2, 0.163801e-2, -0.1031555e-1,
                  0.2282967e-1, -0.2895312e-1, 0.1787654e-1, -0.420059e-2]

_COEF_SMALL = [_I0_COEF_SMALL, _I1_COEF_SMALL]
_COEF_LARGE = [_I0_COEF_LARGE, _I1_COEF_LARGE]


def _log_modified_bessel_fn(x, order=0):
    """"""
    Returns ``log(I_order(x))`` for ``x > 0``,
    where `order` is either 0 or 1.
    """"""
    assert order == 0 or order == 1

    # compute small solution
    y = (x / 3.75)
    y = y * y
    small = _eval_poly(y, _COEF_SMALL[order])
    if order == 1:
        small = x.abs() * small
    small = small.log()

    # compute large solution
    y = 3.75 / x
    large = x - 0.5 * x.log() + _eval_poly(y, _COEF_LARGE[order]).log()

    result = torch.where(x < 3.75, small, large)
    return result


@torch.jit.script_if_tracing
def _rejection_sample(loc, concentration, proposal_r, x):
    done = torch.zeros(x.shape, dtype=torch.bool, device=loc.device)
    while not done.all():
        u = torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device)
        u1, u2, u3 = u.unbind()
        z = torch.cos(math.pi * u1)
        f = (1 + proposal_r * z) / (proposal_r + z)
        c = concentration * (proposal_r - f)
        accept = ((c * (2 - c) - u2) > 0) | ((c / u2).log() + 1 - c >= 0)
        if accept.any():
            x = torch.where(accept, (u3 - 0.5).sign() * f.acos(), x)
            done = done | accept
    return (x + math.pi + loc) % (2 * math.pi) - math.pi


class VonMises(Distribution):
    """"""
    A circular von Mises distribution.

    This implementation uses polar coordinates. The ``loc`` and ``value`` args
    can be any real number (to facilitate unconstrained optimization), but are
    interpreted as angles modulo 2 pi.

    Example::
        >>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample() # von Mises distributed with loc=1 and concentration=1
        tensor([1.9777])

    :param torch.Tensor loc: an angle in radians.
    :param torch.Tensor concentration: concentration parameter
    """"""
    arg_constraints = {'loc': constraints.real, 'concentration': constraints.positive}
    support = constraints.real
    has_rsample = False

    def __init__(self, loc, concentration, validate_args=None):
        self.loc, self.concentration = broadcast_all(loc, concentration)
        batch_shape = self.loc.shape
        event_shape = torch.Size()

        # Parameters for sampling
        tau = 1 + (1 + 4 * self.concentration ** 2).sqrt()
        rho = (tau - (2 * tau).sqrt()) / (2 * self.concentration)
        self._proposal_r = (1 + rho ** 2) / (2 * rho)

        super(VonMises, self).__init__(batch_shape, event_shape, validate_args)

    def log_prob(self, value):
        log_prob = self.concentration * torch.cos(value - self.loc)
        log_prob = log_prob - math.log(2 * math.pi) - _log_modified_bessel_fn(self.concentration, order=0)
        return log_prob

    @torch.no_grad()
    def sample(self, sample_shape=torch.Size()):
        """"""
        The sampling algorithm for the von Mises distribution is based on the following paper:
        Best, D. J., and Nicholas I. Fisher.
        ""Efficient simulation of the von Mises distribution."" Applied Statistics (1979): 152-157.
        """"""
        shape = self._extended_shape(sample_shape)
        x = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)
        return _rejection_sample(self.loc, self.concentration, self._proposal_r, x)

    def expand(self, batch_shape):
        try:
            return super(VonMises, self).expand(batch_shape)
        except NotImplementedError:
            validate_args = self.__dict__.get('_validate_args')
            loc = self.loc.expand(batch_shape)
            concentration = self.concentration.expand(batch_shape)
            return type(self)(loc, concentration, validate_args=validate_args)

    @property
    def mean(self):
        """"""
        The provided mean is the circular one.
        """"""
        return self.loc

    @lazy_property
    def variance(self):
        """"""
        The provided variance is the circular one.
        """"""
        return 1 - (_log_modified_bessel_fn(self.concentration, order=1) -
                    _log_modified_bessel_fn(self.concentration, order=0)).exp()
","import math

import torch
import torch.jit
from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import broadcast_all, lazy_property


def _eval_poly(y, coef):
    coef = list(coef)
    result = coef.pop()
    while coef:
        result = coef.pop() + y * result
    return result


_I0_COEF_SMALL = [1.0, 3.5156229, 3.0899424, 1.2067492, 0.2659732, 0.360768e-1, 0.45813e-2]
_I0_COEF_LARGE = [0.39894228, 0.1328592e-1, 0.225319e-2, -0.157565e-2, 0.916281e-2,
                  -0.2057706e-1, 0.2635537e-1, -0.1647633e-1, 0.392377e-2]
_I1_COEF_SMALL = [0.5, 0.87890594, 0.51498869, 0.15084934, 0.2658733e-1, 0.301532e-2, 0.32411e-3]
_I1_COEF_LARGE = [0.39894228, -0.3988024e-1, -0.362018e-2, 0.163801e-2, -0.1031555e-1,
                  0.2282967e-1, -0.2895312e-1, 0.1787654e-1, -0.420059e-2]

_COEF_SMALL = [_I0_COEF_SMALL, _I1_COEF_SMALL]
_COEF_LARGE = [_I0_COEF_LARGE, _I1_COEF_LARGE]


def _log_modified_bessel_fn(x, order=0):
    """"""
    Returns ``log(I_order(x))`` for ``x > 0``,
    where `order` is either 0 or 1.
    """"""
    assert order == 0 or order == 1

    # compute small solution
    y = (x / 3.75)
    y = y * y
    small = _eval_poly(y, _COEF_SMALL[order])
    if order == 1:
        small = x.abs() * small
    small = small.log()

    # compute large solution
    y = 3.75 / x
    large = x - 0.5 * x.log() + _eval_poly(y, _COEF_LARGE[order]).log()

    result = torch.where(x < 3.75, small, large)
    return result


@torch.jit.script_if_tracing
def _rejection_sample(loc, concentration, proposal_r, x):
    done = torch.zeros(x.shape, dtype=torch.bool, device=loc.device)
    while not done.all():
        u = torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device)
        u1, u2, u3 = u.unbind()
        z = torch.cos(math.pi * u1)
        f = (1 + proposal_r * z) / (proposal_r + z)
        c = concentration * (proposal_r - f)
        accept = ((c * (2 - c) - u2) > 0) | ((c / u2).log() + 1 - c >= 0)
        if accept.any():
            x = torch.where(accept, (u3 - 0.5).sign() * f.acos(), x)
            done = done | accept
    return (x + math.pi + loc) % (2 * math.pi) - math.pi


class VonMises(Distribution):
    """"""
    A circular von Mises distribution.

    This implementation uses polar coordinates. The ``loc`` and ``value`` args
    can be any real number (to facilitate unconstrained optimization), but are
    interpreted as angles modulo 2 pi.

    Example::
        >>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample() # von Mises distributed with loc=1 and concentration=1
        tensor([1.9777])

    :param torch.Tensor loc: an angle in radians.
    :param torch.Tensor concentration: concentration parameter
    """"""
    arg_constraints = {'loc': constraints.real, 'concentration': constraints.positive}
    support = constraints.real
    has_rsample = False

    def __init__(self, loc, concentration, validate_args=None):
        self.loc, self.concentration = broadcast_all(loc, concentration)
        batch_shape = self.loc.shape
        event_shape = torch.Size()

        # Parameters for sampling
        tau = 1 + (1 + 4 * self.concentration ** 2).sqrt()
        rho = (tau - (2 * tau).sqrt()) / (2 * self.concentration)
        self._proposal_r = (1 + rho ** 2) / (2 * rho)

        super(VonMises, self).__init__(batch_shape, event_shape, validate_args)

    def log_prob(self, value):
        if self._validate_args:
            self._validate_sample(value)
        log_prob = self.concentration * torch.cos(value - self.loc)
        log_prob = log_prob - math.log(2 * math.pi) - _log_modified_bessel_fn(self.concentration, order=0)
        return log_prob

    @torch.no_grad()
    def sample(self, sample_shape=torch.Size()):
        """"""
        The sampling algorithm for the von Mises distribution is based on the following paper:
        Best, D. J., and Nicholas I. Fisher.
        ""Efficient simulation of the von Mises distribution."" Applied Statistics (1979): 152-157.
        """"""
        shape = self._extended_shape(sample_shape)
        x = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)
        return _rejection_sample(self.loc, self.concentration, self._proposal_r, x)

    def expand(self, batch_shape):
        try:
            return super(VonMises, self).expand(batch_shape)
        except NotImplementedError:
            validate_args = self.__dict__.get('_validate_args')
            loc = self.loc.expand(batch_shape)
            concentration = self.concentration.expand(batch_shape)
            return type(self)(loc, concentration, validate_args=validate_args)

    @property
    def mean(self):
        """"""
        The provided mean is the circular one.
        """"""
        return self.loc

    @lazy_property
    def variance(self):
        """"""
        The provided variance is the circular one.
        """"""
        return 1 - (_log_modified_bessel_fn(self.concentration, order=1) -
                    _log_modified_bessel_fn(self.concentration, order=0)).exp()
"
658,"return rtensor;
}
// NOLINTNEXTLINE(modernize-use-equals-default)
Quantizer::~Quantizer() {}
C10_EXPORT void set_quantizer_(const Tensor& self, ConstQuantizerPtr quantizer) {
get_qtensorimpl(self)->set_quantizer_(quantizer);
","return rtensor;
}
Quantizer::~Quantizer() = default;
C10_EXPORT void set_quantizer_(const Tensor& self, ConstQuantizerPtr quantizer) {
get_qtensorimpl(self)->set_quantizer_(quantizer);
"
659,"}
struct C10_API DefaultCPUAllocator final : at::Allocator {
  // NOLINTNEXTLINE(modernize-use-equals-default)
  DefaultCPUAllocator() {}
  // NOLINTNEXTLINE(modernize-use-equals-default)
  ~DefaultCPUAllocator() override {}
at::DataPtr allocate(size_t nbytes) const override {
void* data = alloc_cpu(nbytes);
profiledCPUMemoryReporter().New(data, nbytes);
","}
struct C10_API DefaultCPUAllocator final : at::Allocator {
  DefaultCPUAllocator() = default;
at::DataPtr allocate(size_t nbytes) const override {
void* data = alloc_cpu(nbytes);
profiledCPUMemoryReporter().New(data, nbytes);
"
660,"} else {
auto stream = c10::cuda::getCurrentCUDAStream(device.index());
C10_CUDA_CHECK(cudaStreamSynchronize(stream));
event_sync_required_ = false;
}
#else
","} else {
auto stream = c10::cuda::getCurrentCUDAStream(device.index());
C10_CUDA_CHECK(cudaStreamSynchronize(stream));
    event_ = nullptr;
event_sync_required_ = false;
}
#else
"
661,"streams_(streams),
unsqueeze_scalars_(unsqueeze_scalars) {}
// NOLINTNEXTLINE(modernize-use-equals-default)
Scatter::~Scatter() {}
variable_list Scatter::apply(variable_list&& inputs) {
AT_ASSERT(inputs.size() == 1);
","streams_(streams),
unsqueeze_scalars_(unsqueeze_scalars) {}
Scatter::~Scatter() = default;
variable_list Scatter::apply(variable_list&& inputs) {
AT_ASSERT(inputs.size() == 1);
"
662,"Gather::Gather(const at::Device& destination_device, int64_t dim)
: destination_device_(destination_device), dim_(dim) {}
// NOLINTNEXTLINE(modernize-use-equals-default)
Gather::~Gather() {}
variable_list Gather::apply(variable_list&& inputs) {
bool all_are_zero_dim = true;
","Gather::Gather(const at::Device& destination_device, int64_t dim)
: destination_device_(destination_device), dim_(dim) {}
Gather::~Gather() = default;
variable_list Gather::apply(variable_list&& inputs) {
bool all_are_zero_dim = true;
"
663,"at::cuda::set_device(prior_device);
}
// NOLINTNEXTLINE(modernize-use-equals-default)
FusedKernelCUDA::~FusedKernelCUDA() {
nvrtc().cuModuleUnload(module_);
}
","at::cuda::set_device(prior_device);
}
FusedKernelCUDA::~FusedKernelCUDA() {
nvrtc().cuModuleUnload(module_);
}
"
664,"#include ""caffe2/utils/math/reduce.h""
#ifdef CAFFE2_USE_ACCELERATE
#include <Accelerate/Accelerate.h>
#endif // CAFFE2_USE_ACCELERATE
","#include ""caffe2/utils/math/reduce.h""
#include <algorithm>
#include <cstring>
#include <functional>
#include <numeric>
#include <vector>

#ifdef CAFFE2_USE_ACCELERATE
#include <Accelerate/Accelerate.h>
#endif // CAFFE2_USE_ACCELERATE
"
665,"ConstEigenArrayMap<T> X_arr(X, cols, rows);
EigenVectorArrayMap<T> mean_arr(mean, cols);
EigenVectorArrayMap<T> var_arr(var, cols);
  mean_arr = X_arr.col(0);
  var_arr = X_arr.col(0).square();
  for (int i = 1; i < rows; ++i) {
    mean_arr += X_arr.col(i);
    var_arr += X_arr.col(i).square();
}
  const T scale = T(1) / static_cast<T>(rows);
  mean_arr *= scale;
  var_arr = var_arr * scale - mean_arr.square();
}
template <typename T>
","ConstEigenArrayMap<T> X_arr(X, cols, rows);
EigenVectorArrayMap<T> mean_arr(mean, cols);
EigenVectorArrayMap<T> var_arr(var, cols);
  EArrXt<T> delta_arr(cols);
  mean_arr.setZero();
  var_arr.setZero();
  for (int i = 0; i < rows; ++i) {
    delta_arr = X_arr.col(i) - mean_arr;
    mean_arr += delta_arr / static_cast<T>(i + 1);
    var_arr += delta_arr * (X_arr.col(i) - mean_arr);
}
  var_arr /= static_cast<T>(rows);
}
template <typename T>
"
666,"from typing import List, Optional, Union
from setuptools.command.build_ext import build_ext
from pkg_resources import packaging  # type: ignore[attr-defined]
IS_WINDOWS = sys.platform == 'win32'
LIB_EXT = '.pyd' if IS_WINDOWS else '.so'
","from typing import List, Optional, Union
from setuptools.command.build_ext import build_ext
from pkg_resources import packaging, parse_version  # type: ignore[attr-defined]
IS_WINDOWS = sys.platform == 'win32'
LIB_EXT = '.pyd' if IS_WINDOWS else '.so'
"
667,"def build_extensions(self) -> None:
self._check_abi()
for extension in self.extensions:
# Ensure at least an empty list of flags for 'cxx' and 'nvcc' when
# extra_compile_args is a dict. Otherwise, default torch flags do
","def build_extensions(self) -> None:
self._check_abi()

        cuda_ext = False
        extension_iter = iter(self.extensions)
        extension = next(extension_iter, None)
        while not cuda_ext and extension:
            for source in extension.sources:
                _, ext = os.path.splitext(source)
                if ext == '.cu':
                    cuda_ext = True
                    break
            extension = next(extension_iter, None)

        if cuda_ext and not IS_HIP_EXTENSION:
            self._check_cuda_version()

for extension in self.extensions:
# Ensure at least an empty list of flags for 'cxx' and 'nvcc' when
# extra_compile_args is a dict. Otherwise, default torch flags do
"
668,"object_type_dict[all_qat_mappings[k]] = v
return qconfig_dict
def insert_observer(
node: Node,
observer: torch.quantization.ObserverBase,
","object_type_dict[all_qat_mappings[k]] = v
return qconfig_dict
def update_qconfig_for_fusion(
    model: GraphModule,
    qconfig_dict: Any,
) -> Any:
    """"""
    Update the qconfig_dict to account for fused modules such as LinearReLU.
    """"""
    object_type_dict = qconfig_dict.get(""object_type"", None)
    if object_type_dict is None:
        return qconfig_dict

    modules = dict(model.named_modules())

    for node in model.graph.nodes:
        if node.op == 'call_module':
            module_type = type(modules[str(node.target)])
            if module_type not in list(DEFAULT_OP_LIST_TO_FUSER_METHOD.values()):
                continue

            for ops, fuser in DEFAULT_OP_LIST_TO_FUSER_METHOD.items():
                if module_type == fuser:
                    fused_qconfig = object_type_dict.get(ops[0], None)

                    # Raise an error if the modules in the fused module have
                    # different qconfigs specified in the qconfig_dict
                    for op in ops:
                        if object_type_dict.get(op, None) != fused_qconfig:
                            raise LookupError(""During fusion, we need to specify the same "" +
                                              f""qconfigs for both modules in {module_type}."")

                    if fused_qconfig is not None:
                        object_type_dict[module_type] = fused_qconfig

    return qconfig_dict

def insert_observer(
node: Node,
observer: torch.quantization.ObserverBase,
"
669,"}
// 4. Default to error
  return {missingKernel_, ""missing""};
}
// synchronizes the dispatch table entry for a given dispatch key
","}
// 4. Default to error
  return {missingKernel(), ""missing""};
}
// synchronizes the dispatch table entry for a given dispatch key
"
670,"""static_runtime::to_copy.prim_dtype(Tensor self, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor"");
m.def(
""static_runtime::to_copy.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor"");
}
bool HasInplaceOp(std::shared_ptr<Graph>& graph, const AliasDb& alias_db) {
","""static_runtime::to_copy.prim_dtype(Tensor self, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor"");
m.def(
""static_runtime::to_copy.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor"");
  m.def(
      ""static_runtime::to_copy.other(Tensor self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor"");
}
bool HasInplaceOp(std::shared_ptr<Graph>& graph, const AliasDb& alias_db) {
"
671,"return result;
}
bool LoopNest::areLoopsPerfectlyNested(const std::vector<For*>& loops) {
if (loops.size() < 2) {
return true;
","return result;
}
For* LoopNest::tile(For* x, For* y, int x_factor, int y_factor) {
  auto parent = dynamic_cast<Block*>(x->get_parent());
  if (parent == nullptr) {
    throw malformed_input(""parent of the loops must be a Block"");
  }
  if (!areLoopsPerfectlyNested({x, y})) {
    throw malformed_input(""two loops must be perfectly nested"");
  }

  // Split x, y axes by x_factor and y_factor
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  For *yi, *ytail;
  splitWithTail(y, y_factor, &yi, &ytail);
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  For *xi, *xtail;
  splitWithTail(x, x_factor, &xi, &xtail);

  // Distribute xi over yo and ytail so we can manipulate the loop order of {xo,
  // xi, yo, yi}
  auto loops = distributeLoop(xi);

  // For {xi, yo, yi}, reorder the axes to be yo, xi, yi
  xi = loops.front();
  For* yo = dynamic_cast<For*>(xi->body()->stmts().front());
  CHECK(yo);
  reorder({xi, yo}, {1, 0});

  // For {xi, ytail}, reorder the axes to be ytail, xi
  if (loops.size() == 2) {
    xi = loops.back();
    ytail = dynamic_cast<For*>(xi->body()->stmts().front());
    CHECK(ytail);
    reorder({xi, ytail}, {1, 0});
  }

  return xtail;
}

bool LoopNest::areLoopsPerfectlyNested(const std::vector<For*>& loops) {
if (loops.size() < 2) {
return true;
"
672,"add_docstr_all('index_put',
r""""""
index_put(tensor1, indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
`tensor1` corresponds to `self` in :meth:`torch.Tensor.index_put_`.
"""""")
add_docstr_all('index_select',
","add_docstr_all('index_put',
r""""""
index_put(indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
"""""")
add_docstr_all('index_select',
"
673,"return result;
}
// LLVM_DEBUG needs opt to be built with debug support.
template<
typename T,
","return result;
}
inline bool _isCallSite(Value* V) {
#if LLVM_VERSION_MAJOR >= 8
  return isa<CallBase>(V);
#else
  return !!CallSite(V);
#endif
}

inline Function* _getCalledFunction(Value* V) {
#if LLVM_VERSION_MAJOR >= 8
  return dyn_cast<CallBase>(V)->getCalledFunction();
#else
  return CallSite(V).getCalledFunction();
#endif
}

// LLVM_DEBUG needs opt to be built with debug support.
template<
typename T,
"
674,"if (F.hasDefaultVisibility()) {
visibleFuncs->insert(&F);
}
      std::string caller = F.getName();
std::string callerDemangled = _demangle(caller);
for (BasicBlock& BB : F) {
for (Instruction& I : BB) {
scanReferredFunctions(I, [&](Function* func) -> void {
            std::string callee = func->getName();
std::string calleeDemangled = _demangle(callee);
(*deps)[caller].insert(callee);
if (Verbose > 1) {
","if (F.hasDefaultVisibility()) {
visibleFuncs->insert(&F);
}
      std::string caller = _name(&F);
std::string callerDemangled = _demangle(caller);
for (BasicBlock& BB : F) {
for (Instruction& I : BB) {
scanReferredFunctions(I, [&](Function* func) -> void {
            std::string callee = _name(func);
std::string calleeDemangled = _demangle(callee);
(*deps)[caller].insert(callee);
if (Verbose > 1) {
"
675,"std::set<libkineto::ActivityType> cpuTypes = {
libkineto::ActivityType::CPU_OP,
#ifdef USE_KINETO_UPDATED
libkineto::ActivityType::USER_ANNOTATION,
#endif
","std::set<libkineto::ActivityType> cpuTypes = {
libkineto::ActivityType::CPU_OP,
    libkineto::ActivityType::CPU_INSTANT_EVENT,
#ifdef USE_KINETO_UPDATED
libkineto::ActivityType::USER_ANNOTATION,
#endif
"
676,"if n == Ellipsis:
return '...'
if n >= 0 and n < 26:
                return chr(n + ord('a'))
if n >= 26 and n < 52:
                return chr(n - 26 + ord('A'))
raise ValueError('einsum(): subscript in subscript list is not within the valid range [0, 52)')
# Parse subscripts for input operands
","if n == Ellipsis:
return '...'
if n >= 0 and n < 26:
                return chr(ord('A') + n)
if n >= 26 and n < 52:
                return chr(ord('a') + n - 26)
raise ValueError('einsum(): subscript in subscript list is not within the valid range [0, 52)')
# Parse subscripts for input operands
"
677,"if (type->cast<TensorType>()) {
return;
}
  // NOLINTNEXTLINE(performance-for-range-copy)
  for (auto t : type->containedTypes()) {
TORCH_INTERNAL_ASSERT(!t->cast<TensorType>());
}
}
","if (type->cast<TensorType>()) {
return;
}
  for (const auto& t : type->containedTypes()) {
TORCH_INTERNAL_ASSERT(!t->cast<TensorType>());
}
}
"
678,"onnx_shape_inference);
}
} else if (PyList_Check(output_obj)) {
    size_t list_len = PyList_GET_SIZE(output_obj);
if (HasSequenceTypeOutput(graph->outputs().at(outputs_index)->node())) {
auto output_type = graph->outputs().at(outputs_index)->type();
TORCH_CHECK(
","onnx_shape_inference);
}
} else if (PyList_Check(output_obj)) {
    const auto list_len = PyList_GET_SIZE(output_obj);
if (HasSequenceTypeOutput(graph->outputs().at(outputs_index)->node())) {
auto output_type = graph->outputs().at(outputs_index)->type();
TORCH_CHECK(
"
679,"}
int ProcessGroup::Work::sourceRank() const {
  throw std::runtime_error(
""sourceRank() may only be called on work objects ""
""that correspond to a recv or recv-from-any call."");
}
std::vector<at::Tensor> ProcessGroup::Work::result() {
  throw std::runtime_error(""result() not implemented."");
}
void ProcessGroup::Work::synchronize() {}
","}
int ProcessGroup::Work::sourceRank() const {
  TORCH_CHECK(false,
""sourceRank() may only be called on work objects ""
""that correspond to a recv or recv-from-any call."");
}
std::vector<at::Tensor> ProcessGroup::Work::result() {
  TORCH_CHECK(false, ""result() not implemented."");
}
void ProcessGroup::Work::synchronize() {}
"
680,"const auto device_id = tensorgroup[0].device().index();
for (const auto& tensor : tensorgroup) {
if (tensor.device().index() != device_id) {
        throw std::runtime_error(
""tensors in the nested tensor vectors need to ""
""be on the same device"");
}
","const auto device_id = tensorgroup[0].device().index();
for (const auto& tensor : tensorgroup) {
if (tensor.device().index() != device_id) {
        TORCH_CHECK(false,
""tensors in the nested tensor vectors need to ""
""be on the same device"");
}
"
681,"MPI_CHECK(MPI_Init_thread(
nullptr, nullptr, MPI_THREAD_SERIALIZED, &mpiThreadSupport_));
if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {
      throw std::runtime_error(
""Used MPI implementation doesn't have the ""
""minimum level of threading support: ""
""MPI_THREAD_SERIALIZED. This is required by ""
""c10d package"");
}
if (std::atexit(ProcessGroupMPI::mpiExit)) {
      throw std::runtime_error(""Fail to register the MPI exit handler"");
}
});
}
","MPI_CHECK(MPI_Init_thread(
nullptr, nullptr, MPI_THREAD_SERIALIZED, &mpiThreadSupport_));
if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {
      TORCH_CHECK(false,
""Used MPI implementation doesn't have the ""
""minimum level of threading support: ""
""MPI_THREAD_SERIALIZED. This is required by ""
""c10d package"");
}
if (std::atexit(ProcessGroupMPI::mpiExit)) {
      TORCH_CHECK(false, ""Fail to register the MPI exit handler"");
}
});
}
"
682,""" ran for "",
timeElapsed.count(),
"" milliseconds before timing out."");
        throw std::runtime_error(exceptionMsg);
}
// Check for errors and throw appropriate exception.
checkAndThrowException();
",""" ran for "",
timeElapsed.count(),
"" milliseconds before timing out."");
        TORCH_CHECK(false, exceptionMsg);
}
// Check for errors and throw appropriate exception.
checkAndThrowException();
"
683,"std::vector<at::Tensor>& /* unused */,
int /* unused */,
int /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports recv for NCCL lib version >= 2.7.0"");
}
#endif
","std::vector<at::Tensor>& /* unused */,
int /* unused */,
int /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports recv for NCCL lib version >= 2.7.0"");
}
#endif
"
684,"#else
void BackgroundThread::initStopSignal() {
if (pipe(controlPipeFd_.data()) == -1) {
    throw std::runtime_error(
""Failed to create the control pipe to start the ""
""BackgroundThread run"");
}
","#else
void BackgroundThread::initStopSignal() {
if (pipe(controlPipeFd_.data()) == -1) {
    TORCH_CHECK(false,
""Failed to create the control pipe to start the ""
""BackgroundThread run"");
}
"
685,"if (timeout != kNoTimeout) {
const auto elapsed = std::chrono::high_resolution_clock::now() - start;
if (elapsed > timeout) {
        throw std::runtime_error(kConnectTimeoutMsg);
}
}
std::this_thread::sleep_for(std::chrono::seconds(1));
","if (timeout != kNoTimeout) {
const auto elapsed = std::chrono::high_resolution_clock::now() - start;
if (elapsed > timeout) {
        TORCH_CHECK(false, kConnectTimeoutMsg);
}
}
std::this_thread::sleep_for(std::chrono::seconds(1));
"
686,"MPI_CHECK(MPI_Init_thread(
nullptr, nullptr, MPI_THREAD_SERIALIZED, &mpiThreadSupport_));
if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {
      TORCH_CHECK(false,
""Used MPI implementation doesn't have the ""
""minimum level of threading support: ""
""MPI_THREAD_SERIALIZED. This is required by ""
""c10d package"");
}
if (std::atexit(ProcessGroupMPI::mpiExit)) {
      TORCH_CHECK(false, ""Fail to register the MPI exit handler"");
}
});
}
","MPI_CHECK(MPI_Init_thread(
nullptr, nullptr, MPI_THREAD_SERIALIZED, &mpiThreadSupport_));
if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {
      throw std::runtime_error(
""Used MPI implementation doesn't have the ""
""minimum level of threading support: ""
""MPI_THREAD_SERIALIZED. This is required by ""
""c10d package"");
}
if (std::atexit(ProcessGroupMPI::mpiExit)) {
      throw std::runtime_error(""Fail to register the MPI exit handler"");
}
});
}
"
687,"// Check validity of tensor
void check_gpu_single_tensor(const at::Tensor& tensor) {
if (!tensor.is_cuda() || tensor.is_sparse()) {
    TORCH_CHECK(false, ""Tensors must be CUDA and dense"");
}
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""Tensors must be contiguous"");
}
}
","// Check validity of tensor
void check_gpu_single_tensor(const at::Tensor& tensor) {
if (!tensor.is_cuda() || tensor.is_sparse()) {
    throw std::runtime_error(""Tensors must be CUDA and dense"");
}
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""Tensors must be contiguous"");
}
}
"
688,"void setMemoryFraction(double fraction, int device) {
TORCH_INTERNAL_ASSERT(
        0 <= device && device < device_allocator.size(),
""Allocator not initialized for device "",
device,
"": did you call init?"");
","void setMemoryFraction(double fraction, int device) {
TORCH_INTERNAL_ASSERT(
        0 <= device && static_cast<size_t>(device) < device_allocator.size(),
""Allocator not initialized for device "",
device,
"": did you call init?"");
"
689,"const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout != kNoTimeout && elapsed > timeout) {
      throw std::runtime_error(""Wait timeout"");
}
/* sleep override */
","const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout != kNoTimeout && elapsed > timeout) {
      TORCH_CHECK(false, ""Wait timeout"");
}
/* sleep override */
"
690,"}
int ProcessGroup::Work::sourceRank() const {
  throw std::runtime_error(
""sourceRank() may only be called on work objects ""
""that correspond to a recv or recv-from-any call."");
}
std::vector<at::Tensor> ProcessGroup::Work::result() {
  throw std::runtime_error(""result() not implemented."");
}
void ProcessGroup::Work::synchronize() {}
","}
int ProcessGroup::Work::sourceRank() const {
  TORCH_CHECK(false,
""sourceRank() may only be called on work objects ""
""that correspond to a recv or recv-from-any call."");
}
std::vector<at::Tensor> ProcessGroup::Work::result() {
  TORCH_CHECK(false, ""result() not implemented."");
}
void ProcessGroup::Work::synchronize() {}
"
691,"func<int64_t>(args);                             \
break;                                           \
default:                                           \
      throw std::runtime_error(""Invalid scalar type""); \
}
#endif
","func<int64_t>(args);                             \
break;                                           \
default:                                           \
      TORCH_CHECK(false, ""Invalid scalar type""); \
}
#endif
"
692,"break;
}
  throw std::runtime_error(""Unhandled ReduceOp"");
}
template <typename T, typename O>
","break;
}
  TORCH_CHECK(false, ""Unhandled ReduceOp"");
}
template <typename T, typename O>
"
693,"// Check validity of tensor
void check_gpu_single_tensor(const at::Tensor& tensor) {
if (!tensor.is_cuda() || tensor.is_sparse()) {
    throw std::runtime_error(""Tensors must be CUDA and dense"");
}
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""Tensors must be contiguous"");
}
}
","// Check validity of tensor
void check_gpu_single_tensor(const at::Tensor& tensor) {
if (!tensor.is_cuda() || tensor.is_sparse()) {
    TORCH_CHECK(false, ""Tensors must be CUDA and dense"");
}
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""Tensors must be contiguous"");
}
}
"
694,"std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL does not support allgather_coalesced"");
}
","std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL does not support allgather_coalesced"");
}
"
695,"std::vector<int64_t>& /* unused */,
std::vector<int64_t>& /* unused */,
const AllToAllOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
","std::vector<int64_t>& /* unused */,
std::vector<int64_t>& /* unused */,
const AllToAllOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
"
696,"check_gpu_single_tensor(output_tensor);
if (input_tensor.dtype() != output_tensor.dtype()) {
    throw std::runtime_error(""output tensor must have the same type as input tensor"");
}
if (input_tensor.numel() * size_ != output_tensor.numel()) {
    throw std::runtime_error(""output tensor size must be equal to world_size times input tensor size"");
}
// just a wrapper to fit the collective interface
","check_gpu_single_tensor(output_tensor);
if (input_tensor.dtype() != output_tensor.dtype()) {
    TORCH_CHECK(false, ""output tensor must have the same type as input tensor"");
}
if (input_tensor.numel() * size_ != output_tensor.numel()) {
    TORCH_CHECK(false, ""output tensor size must be equal to world_size times input tensor size"");
}
// just a wrapper to fit the collective interface
"
697,"""tried to send() a message of type "",
requestMessage->type(),
"" but RPC is no longer running on this node."");
    throw std::runtime_error(err);
}
const auto& url = findWorkerURL(toWorkerInfo);
","""tried to send() a message of type "",
requestMessage->type(),
"" but RPC is no longer running on this node."");
    TORCH_CHECK(false, err);
}
const auto& url = findWorkerURL(toWorkerInfo);
"
698,"'.*_backward', '.*_backward_(out|input|weight|bias)', '.*_forward',
'.*_forward_out', '_unsafe_view', 'tensor', '_?sparse_coo_tensor.*',
'_?sparse_csr_tensor.*',
    '_arange.*', '_range.*', '_linspace.*', '_logspace.*',
'_sparse_add_out', '_sparse_div.*', '_sparse_mul.*', '_sparse_sub.*', '_sparse_dense_add_out',
'index', 'unique_dim_consecutive',
'_cumsum.*', '_cumprod.*', '_sum.*', '_prod.*',
","'.*_backward', '.*_backward_(out|input|weight|bias)', '.*_forward',
'.*_forward_out', '_unsafe_view', 'tensor', '_?sparse_coo_tensor.*',
'_?sparse_csr_tensor.*',
    '_arange.*', '_range.*', 'linspace.*', 'logspace.*',
'_sparse_add_out', '_sparse_div.*', '_sparse_mul.*', '_sparse_sub.*', '_sparse_dense_add_out',
'index', 'unique_dim_consecutive',
'_cumsum.*', '_cumprod.*', '_sum.*', '_prod.*',
"
699,"namespace torch {
namespace jit {
// Constants relating to maintaining the topological index of nodes.
//
// Lower and upper bounds of the index. Inclusive range.
static constexpr topo_position_t kLowerBound = INT64_MIN;
static constexpr topo_position_t kUpperBound = INT64_MAX;
static constexpr topo_position_t kMidPoint = 0;
// How far away to space nodes that are appended to the graph.
// should be 2^n, where:
//   - n is the maximum number of repeated insertions without a re-index
//   - 2^(64-n) is the maximum number of appends to the end without reindex
static constexpr topo_position_t kAppendInterval = 1099511627776ULL /* 2^40 */;
static void printValueRef(std::ostream& out, const Value* n) {
out << ""%"" << n->debugName();
}
// NB: This overload will become ambiguous with the one Caffe2 provides in its
// logging, if they ever intersect.
template <typename T>
","namespace torch {
namespace jit {
namespace {

// Constants relating to maintaining the topological index of nodes.
//
// Lower and upper bounds of the index. Inclusive range.
constexpr topo_position_t kLowerBound = INT64_MIN;
constexpr topo_position_t kUpperBound = INT64_MAX;
constexpr topo_position_t kMidPoint = 0;
// How far away to space nodes that are appended to the graph.
// should be 2^n, where:
//   - n is the maximum number of repeated insertions without a re-index
//   - 2^(64-n) is the maximum number of appends to the end without reindex
constexpr topo_position_t kAppendInterval = 1099511627776ULL /* 2^40 */;
void printValueRef(std::ostream& out, const Value* n) {
out << ""%"" << n->debugName();
}
bool isNumber(c10::string_view str) {
  return str.find_first_not_of(""0123456789"") == std::string::npos;
}

std::string normalizeAttrName(c10::string_view field) {
  if (isNumber(field)) {
    return ""_"" + std::string{field};
  }
  return std::string{field};
}

} // namespace

// NB: This overload will become ambiguous with the one Caffe2 provides in its
// logging, if they ever intersect.
template <typename T>
"
700,"Args:
input (Tensor): the tensor whose elements to repeat.
    reps (tuple): the number of repetitions per dimension.
Example::
","Args:
input (Tensor): the tensor whose elements to repeat.
    dims (tuple): the number of repetitions per dimension.
Example::
"
701,"default: {
GRAPH_DEBUG(""Can't infer sizes for the node: "", *v->node());
GRAPH_DEBUG(""Full fusion group graph:\n"", *v->node()->owningGraph());
      std::string msg =
          std::string(""Unhandled node kind (in inferSizesForValue): "") +
          v->node()->kind().toQualString();
      throw malformed_input(msg);
}
}
}
","default: {
GRAPH_DEBUG(""Can't infer sizes for the node: "", *v->node());
GRAPH_DEBUG(""Full fusion group graph:\n"", *v->node()->owningGraph());
      throw std::runtime_error(""Unhandled node kind"");
}
}
}
"
702,"):
super(DistributedDataParallel, self).__init__()

        assert any((p.requires_grad for p in module.parameters())), (
            ""DistributedDataParallel is not needed when a module ""
            ""doesn't have any parameter that requires a gradient.""
        )
if device_ids is not None and len(device_ids) > 1:
            raise ValueError(""device_ids can only be None or contain a single element."")
self.is_multi_device_module = len({p.device for p in module.parameters()}) > 1
distinct_device_types = {p.device.type for p in module.parameters()}
if len(distinct_device_types) != 1:
            raise ValueError(
""DistributedDataParallel's input module must be on ""
""the same type of devices, but input module parameters locate in {}."".format(
distinct_device_types
)
)
self.device_type = list(distinct_device_types)[0]
if (
","):
super(DistributedDataParallel, self).__init__()
        self.logger = None
        if not any((p.requires_grad for p in module.parameters())):
            self._log_and_throw(
                RuntimeError,
                ""DistributedDataParallel is not needed when a module ""
                ""doesn't have any parameter that requires a gradient.""
            )
if device_ids is not None and len(device_ids) > 1:
            self._log_and_throw(
                ValueError,
                ""device_ids can only be None or contain a single element.""
            )
self.is_multi_device_module = len({p.device for p in module.parameters()}) > 1
distinct_device_types = {p.device.type for p in module.parameters()}
if len(distinct_device_types) != 1:
            self._log_and_throw(
                ValueError,
""DistributedDataParallel's input module must be on ""
""the same type of devices, but input module parameters locate in {}."".format(
distinct_device_types
)
)

self.device_type = list(distinct_device_types)[0]
if (
"
703,"# Check that a module does not have Uninitialized parameters
for param in module.parameters():
if isinstance(param, torch.nn.parameter.UninitializedParameter):
                raise RuntimeError(
""Modules with uninitialized parameters can't be used with `DistributedDataParallel`. ""
""Run a dummy forward pass to correctly initialize the modules""
)
","# Check that a module does not have Uninitialized parameters
for param in module.parameters():
if isinstance(param, torch.nn.parameter.UninitializedParameter):
                self._log_and_throw(
                    RuntimeError,
""Modules with uninitialized parameters can't be used with `DistributedDataParallel`. ""
""Run a dummy forward pass to correctly initialize the modules""
)
"
704,"for (size_t i = 0; i < tensors.size(); i++) {
const auto& tensor = tensors[i];
    TORCH_CHECK(!tensor.is_sparse(), ""No support for sparse tensors."");
// when tensor_indices is empty, the index of tensors[i] assigned to
// bucket is i, otherwise the tensor index is tensor_indices[i].
","for (size_t i = 0; i < tensors.size(); i++) {
const auto& tensor = tensors[i];
    // TODO: This is not a reducer method so it does not have access to logger,
    // pass in logger directly here.
    TORCH_CHECK(
      !tensor.is_sparse(),
      ""No support for sparse tensors.""
    );
// when tensor_indices is empty, the index of tensors[i] assigned to
// bucket is i, otherwise the tensor index is tensor_indices[i].
"
705,"custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", None)
assert custom_module_class_mapping is not None
observed_custom_module = modules[str(node.target)]
        # TODO: remove quantizer here
if activation_is_statically_quantized(qconfig):
            assert node.name in quantizer.activation_post_process_map  # type: ignore[name-defined]
            cur_idx = quantizer.activation_post_process_indexes[node.name]  # type: ignore[name-defined]
            observed_custom_module.activation_post_process = \
                modules[quantizer.activation_post_process_map[node.name][cur_idx]]  # type: ignore[name-defined]
            quantizer.activation_post_process_indexes[node.name] += 1  # type: ignore[name-defined]
quantized_custom_module_class = get_swapped_custom_module_class(
observed_custom_module, custom_module_class_mapping, qconfig)
quantized_custom_module = \
","custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", None)
assert custom_module_class_mapping is not None
observed_custom_module = modules[str(node.target)]
if activation_is_statically_quantized(qconfig):
            activation_post_process = \
                self._maybe_get_last_node_only_observer(modules)
            assert activation_post_process is not None
            observed_custom_module.activation_post_process = activation_post_process
quantized_custom_module_class = get_swapped_custom_module_class(
observed_custom_module, custom_module_class_mapping, qconfig)
quantized_custom_module = \
"
706,"CAFFE_ENFORCE(nnapi_.Execution_getOutputOperandRank);
int ret = nnapi_.Execution_getOutputOperandRank(execution,index,rank);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Execution_getOutputOperandDimensions(ANeuralNetworksExecution* execution, int32_t index, uint32_t* dimensions) {
CAFFE_ENFORCE(nnapi_.Execution_getOutputOperandDimensions);
int ret = nnapi_.Execution_getOutputOperandDimensions(execution,index,dimensions);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
void nnapi_wrapper_load(struct nnapi_wrapper** nnapi, struct nnapi_wrapper** check_nnapi) {
","CAFFE_ENFORCE(nnapi_.Execution_getOutputOperandRank);
int ret = nnapi_.Execution_getOutputOperandRank(execution,index,rank);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Execution_getOutputOperandRank"", ""failed with error "", ret
  );
return ret;
}
int check_Execution_getOutputOperandDimensions(ANeuralNetworksExecution* execution, int32_t index, uint32_t* dimensions) {
CAFFE_ENFORCE(nnapi_.Execution_getOutputOperandDimensions);
int ret = nnapi_.Execution_getOutputOperandDimensions(execution,index,dimensions);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Execution_getOutputOperandDimensions"", ""failed with error "", ret
  );
return ret;
}
void nnapi_wrapper_load(struct nnapi_wrapper** nnapi, struct nnapi_wrapper** check_nnapi) {
"
707,"return out;
}
#ifdef USE_ROCM
static const char* device_resource_string = R""(
#define POS_INFINITY INFINITY
#define NEG_INFINITY -INFINITY

)"";
#else
static const char* device_resource_string = R""(
#define NAN __int_as_float(0x7fffffff)
#define POS_INFINITY __int_as_float(0x7f800000)
#define NEG_INFINITY __int_as_float(0xff800000)
)"";
#endif
static const char* shared_resource_string = R""(
template<typename T>
","return out;
}
static const char* device_resource_string = R""(
#define NAN __int_as_float(0x7fffffff)
#define POS_INFINITY __int_as_float(0x7f800000)
#define NEG_INFINITY __int_as_float(0xff800000)
)"";
static const char* shared_resource_string = R""(
template<typename T>
"
708,"def __init__(
self,
remote_device: str,
        module_cls: nn.Module,
args: Tuple = None,
kwargs: Dict[str, Any] = None,
):
","def __init__(
self,
remote_device: str,
        module_cls: Type[nn.Module],
args: Tuple = None,
kwargs: Dict[str, Any] = None,
):
"
709,"# Reentering the quantized zone
attn_output = self.quant_attn_output(attn_output)
        attn_output = self.out_proj(attn_output)
attn_output_weights = self.quant_attn_output_weights(attn_output_weights)
if need_weights:
","# Reentering the quantized zone
attn_output = self.quant_attn_output(attn_output)
        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969
        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]
attn_output_weights = self.quant_attn_output_weights(attn_output_weights)
if need_weights:
"
710,"source_range = source_range_it->second;
}
auto callee = deserialize(tup_elems[2], source_range_map, cu);
InlinedCallStackPtr cs_ptr;
if (callee) {
cs_ptr = c10::make_intrusive<InlinedCallStack>(
","source_range = source_range_it->second;
}
auto callee = deserialize(tup_elems[2], source_range_map, cu);
  auto function_name = tup_elems[3].toStringRef();
InlinedCallStackPtr cs_ptr;
if (callee) {
cs_ptr = c10::make_intrusive<InlinedCallStack>(
"
711,"}
}
// check output types
  // Static Runtime supports output types include None, Tensor,  List/Tuple
  // of Tensor, or Dict
for (Value* output : graph->outputs()) {
VLOG(1) << ""output: %"" << output->debugName()
<< "" has type: "" << output->type()->repr_str();
","}
}
// check output types
  // Static Runtime doesn't support complex outputs such as List of Lists
for (Value* output : graph->outputs()) {
VLOG(1) << ""output: %"" << output->debugName()
<< "" has type: "" << output->type()->repr_str();
"
712,"// NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
outputs_.emplace_back(const_cast<IValue*>(&sm.constants()[out_idx]));
} else {
      auto& n = nodes_.at(node_idx);
      auto* out = &n.Output(out_idx);
outputs_.emplace_back(out);
}
}
","// NOLINTNEXTLINE(cppcoreguidelines-pro-type-const-cast)
outputs_.emplace_back(const_cast<IValue*>(&sm.constants()[out_idx]));
} else {
      auto* out = &nodes_[node_idx].Output(out_idx);
outputs_.emplace_back(out);
}
}
"
713,"MKL_INT nrows,
MKL_INT ncols,
MKL_INT dense_ncols) {
    int stat = mkl_sparse_d_mm(
SPARSE_OPERATION_NON_TRANSPOSE,
alpha,
A,
desc,
SPARSE_LAYOUT_ROW_MAJOR,
dense,
        dense_ncols,
        dense_ncols,
beta,
res,
dense_ncols);
    TORCH_CHECK(stat == 0, ""mkl_sparse_d_mm failed with error code: "", stat);
}
~SparseCsrMKLInterface() {
","MKL_INT nrows,
MKL_INT ncols,
MKL_INT dense_ncols) {
    int stat;
    if (dense_ncols == 1) {
      stat = mkl_sparse_d_mv(
        SPARSE_OPERATION_NON_TRANSPOSE,
        alpha,
        A,
        desc,
        dense,
        beta,
        res);
      TORCH_CHECK(stat == 0, ""mkl_sparse_d_mv failed with error code: "", stat);
    }
    else {
      stat = mkl_sparse_d_mm(
SPARSE_OPERATION_NON_TRANSPOSE,
alpha,
A,
desc,
SPARSE_LAYOUT_ROW_MAJOR,
dense,
        nrows,
        ncols,
beta,
res,
dense_ncols);
      TORCH_CHECK(stat == 0, ""mkl_sparse_d_mm failed with error code: "", stat);
    }
}
~SparseCsrMKLInterface() {
"
714,"return debug_handle;
}
std::unordered_map<int64_t, DebugInfoPair> BackendDebugHandleManager::
    getCallStackPtrMap() {
// Note that this is return by copy and since
// InlinedCallStackPtrs are intrusive ptr it will result in
// bump of refcount. Not performant, but this is not intented
","return debug_handle;
}
BackendDebugInfoMapType BackendDebugInfoRecorder::stopRecording() {
// Note that this is return by copy and since
// InlinedCallStackPtrs are intrusive ptr it will result in
// bump of refcount. Not performant, but this is not intented
"
715,"std::pair<IValue, IValue> getFunctionTuple(
const Module& module,
const Function& func,
    BackendDebugHandleManager& debug_handle_manager) {
auto graph = func.graph()->copy();
Inline(*graph);
","std::pair<IValue, IValue> getFunctionTuple(
const Module& module,
const Function& func,
    BackendDebugInfoRecorder& debug_info_recorder) {
auto graph = func.graph()->copy();
Inline(*graph);
"
716,"return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple =
          getFunctionTuple(module, setstate, debug_handle_manager);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
debug_info_elements.push_back(func_tuple.second);
","return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple = getFunctionTuple(module, setstate, debug_info_recorder);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
debug_info_elements.push_back(func_tuple.second);
"
717,"code_parts = []
for di, di_next in zip(debug_info, debug_info[1:]):
                # accounting for source range serialization format change
                start, source_range, _ = di
end = di_next[0]
assert end > start
source, s_start, s_end = source_range
","code_parts = []
for di, di_next in zip(debug_info, debug_info[1:]):
                start, source_range, *_ = di
end = di_next[0]
assert end > start
source, s_start, s_end = source_range
"
718,"is_master_only=False,
is_pr_only=True),
AndroidGradleJob(
        ""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single_lite_interpreter"",
""pytorch_android_gradle_custom_build_single"",
[DOCKER_REQUIREMENT_NDK],
is_master_only=False,
is_pr_only=True,
extra_props=tuple({
            ""lite_interpreter"": miniutils.quote(str(int(True)))
}.items())),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build"",
","is_master_only=False,
is_pr_only=True),
AndroidGradleJob(
        ""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit"",
""pytorch_android_gradle_custom_build_single"",
[DOCKER_REQUIREMENT_NDK],
is_master_only=False,
is_pr_only=True,
extra_props=tuple({
            ""lite_interpreter"": miniutils.quote(str(int(False)))
}.items())),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build"",
"
719,"we don't know the value of the proxy, but a custom tracer can attach more
information to the graph node using create_node and can choose to return an iterator.
""""""
        raise TraceError('Proxy object cannot be iterated. '
                         'This can be attempted when used in a for loop or as a *args or **kwargs function argument.')
def keys(self, obj: 'Proxy') -> Any:
""""""Called when a proxy object is has the keys() method called.
","we don't know the value of the proxy, but a custom tracer can attach more
information to the graph node using create_node and can choose to return an iterator.
""""""
        raise TraceError('Proxy object cannot be iterated. This can be '
                         'attempted when the Proxy is used in a loop or'
                         ' as a *args or **kwargs function argument. '
                         'See the torch.fx docs on pytorch.org for a '
                         'more detailed explanation of what types of '
                         'control flow can be traced, and check out the'
                         ' Proxy docstring for help troubleshooting '
                         'Proxy iteration errors')
def keys(self, obj: 'Proxy') -> Any:
""""""Called when a proxy object is has the keys() method called.
"
720,"from tools.codegen.utils import Target, concatMap
import tools.codegen.dest as dest
def parse_backend_yaml(
backend_yaml_path: str,
grouped_native_functions: Sequence[Union[NativeFunction, NativeFunctionsGroup]]
) -> Tuple[str, List[Union[ExternalBackendFunction, ExternalBackendFunctionsGroup]]]:
with open(backend_yaml_path, 'r') as f:
        yaml_values = yaml.load(f, Loader=LineLoader)
assert isinstance(yaml_values, dict)
    cpp_namespace = yaml_values.pop('cpp_namespace')
    backend = yaml_values.pop('backend')
supported = yaml_values.pop('supported', [])
    assert isinstance(supported, list), f'expected ""supported"" to be a list, but got: {supported}'
supported_autograd = yaml_values.pop('autograd', [])
assert isinstance(supported, list), f'expected ""autograd"" to be a list, but got: {supported_autograd}'
    assert len(yaml_values.keys()) > 0, \
        f'{backend_yaml_path} contains unexpected keys: {"", "".join(yaml_values.keys())}'
metadata: Dict[OperatorName, ExternalBackendMetadata] = {}
for op in supported:
","from tools.codegen.utils import Target, concatMap
import tools.codegen.dest as dest
try:
    # use faster C loader if available
    from yaml import CSafeLoader as Loader
except ImportError:
    from yaml import SafeLoader as Loader  # type: ignore[misc]


def parse_backend_yaml(
backend_yaml_path: str,
grouped_native_functions: Sequence[Union[NativeFunction, NativeFunctionsGroup]]
) -> Tuple[str, List[Union[ExternalBackendFunction, ExternalBackendFunctionsGroup]]]:
with open(backend_yaml_path, 'r') as f:
        yaml_values = yaml.load(f, Loader=Loader)
assert isinstance(yaml_values, dict)
    valid_keys = ['backend', 'cpp_namespace', 'supported', 'autograd']

    backend = yaml_values.pop('backend', None)
    assert backend is not None, 'You must provide a value for ""backend""'
    cpp_namespace = yaml_values.pop('cpp_namespace', None)
    assert cpp_namespace is not None, 'You must provide a value for ""cpp_namespace""'
supported = yaml_values.pop('supported', [])
    if supported is None:
        supported = []  # Allow an empty list of supported ops
    assert isinstance(supported, list), f'expected ""supported"" to be a list, but got: {supported} (of type {type(supported)})'
supported_autograd = yaml_values.pop('autograd', [])
assert isinstance(supported, list), f'expected ""autograd"" to be a list, but got: {supported_autograd}'
    assert len(yaml_values.keys()) == 0, \
        f'{backend_yaml_path} contains unexpected keys: {"", "".join(yaml_values.keys())}. \
Only the following keys are supported: {"", "".join(valid_keys)}'
metadata: Dict[OperatorName, ExternalBackendMetadata] = {}
for op in supported:
"
721,"else:
assert_never(g)
for op_name in metadata.keys():
        if op_name not in native_functions_map:
            raise AssertionError(f""Found an invalid operator name: {op_name}"")
return cpp_namespace, [native_to_external(g) for g in grouped_native_functions]
def main() -> None:
","else:
assert_never(g)
for op_name in metadata.keys():
        assert op_name in native_functions_map, f""Found an invalid operator name: {op_name}""
return cpp_namespace, [native_to_external(g) for g in grouped_native_functions]
def main() -> None:
"
722,"'--dry_run', type=bool, default=False, help='output directory')
options = parser.parse_args()
# Assumes that this file lives at PYTORCH_ROOT/tools/codegen/gen_backend_stubs.py
pytorch_root = pathlib.Path(__file__).parent.parent.parent.absolute()
template_dir = os.path.join(pytorch_root, ""aten/src/ATen/templates"")
def make_file_manager(install_dir: str) -> FileManager:
        return FileManager(install_dir=install_dir, template_dir=template_dir, dry_run=options.dry_run)
    fm = make_file_manager(options.output_dir)
native_yaml_path = os.path.join(pytorch_root, 'aten/src/ATen/native/native_functions.yaml')
grouped_native_functions = get_grouped_native_functions(native_yaml_path)
    cpp_namespace, external_backend_functions = parse_backend_yaml(options.source_yaml, grouped_native_functions)
native_functions = parse_native_yaml(native_yaml_path)
","'--dry_run', type=bool, default=False, help='output directory')
options = parser.parse_args()
    run(options.source_yaml, options.output_dir, options.dry_run)

def run(source_yaml: str, output_dir: str, dry_run: bool) -> None:

# Assumes that this file lives at PYTORCH_ROOT/tools/codegen/gen_backend_stubs.py
pytorch_root = pathlib.Path(__file__).parent.parent.parent.absolute()
template_dir = os.path.join(pytorch_root, ""aten/src/ATen/templates"")
def make_file_manager(install_dir: str) -> FileManager:
        return FileManager(install_dir=install_dir, template_dir=template_dir, dry_run=dry_run)
    fm = make_file_manager(output_dir)
native_yaml_path = os.path.join(pytorch_root, 'aten/src/ATen/native/native_functions.yaml')
grouped_native_functions = get_grouped_native_functions(native_yaml_path)
    cpp_namespace, external_backend_functions = parse_backend_yaml(source_yaml, grouped_native_functions)
native_functions = parse_native_yaml(native_yaml_path)
"
723,"optimization is applied to the model during export. Constant-folding
optimization will replace some of the ops that have all constant
inputs, with pre-computed constant nodes.
        example_outputs (tuple of Tensors, default None): Model's example outputs being exported.
example_outputs must be provided when exporting a ScriptModule or TorchScript Function.
strip_doc_string (bool, default True): if True, strips the field
""doc_string"" from the exported model, which information about the stack
","optimization is applied to the model during export. Constant-folding
optimization will replace some of the ops that have all constant
inputs, with pre-computed constant nodes.
        example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None):
            Model's example outputs being exported. 'example_outputs' must be provided when exporting
            a ScriptModule or TorchScript Function. If there is more than one item, it should be passed
            in tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should
            be passed as the example output, e.g. example_outputs=x.
example_outputs must be provided when exporting a ScriptModule or TorchScript Function.
strip_doc_string (bool, default True): if True, strips the field
""doc_string"" from the exported model, which information about the stack
"
724,"Optional,
Sequence,
Set,
    Tuple,
Union,
)
from urllib.parse import quote
","Optional,
Sequence,
Set,
Union,
)
from urllib.parse import quote
"
725,"detach_ = _C._add_docstr(_C._TensorBase.detach_, r""""""
Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.
"""""")
def retain_grad(self):
","detach_ = _C._add_docstr(_C._TensorBase.detach_, r""""""
Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.

    This method also affects forward mode AD gradients and the result will never
    have forward mode AD gradients.
"""""")
def retain_grad(self):
"
726,"m.impl(""_sparse_log_softmax.int"", CppFunction::makeFallthrough());
m.impl(""_sparse_softmax.Dimname"", CppFunction::makeFallthrough());
m.impl(""_sparse_softmax.int"", CppFunction::makeFallthrough());
  m.impl(""_std"", CppFunction::makeFallthrough());
  m.impl(""_var"", CppFunction::makeFallthrough());
m.impl(""abs"", CppFunction::makeFallthrough());
m.impl(""abs.out"", CppFunction::makeFallthrough());
m.impl(""abs_"", CppFunction::makeFallthrough());
","m.impl(""_sparse_log_softmax.int"", CppFunction::makeFallthrough());
m.impl(""_sparse_softmax.Dimname"", CppFunction::makeFallthrough());
m.impl(""_sparse_softmax.int"", CppFunction::makeFallthrough());
m.impl(""abs"", CppFunction::makeFallthrough());
m.impl(""abs.out"", CppFunction::makeFallthrough());
m.impl(""abs_"", CppFunction::makeFallthrough());
"
727,"#include <ATen/ATen.h>
#include <ATen/AccumulateType.h>
#include <ATen/ExpandUtils.h>
#include <ATen/NativeFunctions.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
","#include <ATen/ATen.h>
#include <ATen/AccumulateType.h>
#include <ATen/ExpandUtils.h>
#include <ATen/LegacyTHFunctionsCPU.h>
#include <ATen/NativeFunctions.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
"
728,"return at::std_mean(self, dimnames_to_positions(self, dim), unbiased, keepdim);
}
Tensor& norm_out(const Tensor& self, const optional<Scalar>& p, DimnameList dim, bool keepdim, ScalarType dtype, Tensor& result) {
return at::norm_out(result, self, p, dimnames_to_positions(self, dim), keepdim, dtype);
}
","return at::std_mean(self, dimnames_to_positions(self, dim), unbiased, keepdim);
}
Tensor std(const Tensor& self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  return at::std(self, dimnames_to_positions(self, dim), correction, keepdim);
}

Tensor& std_out(const Tensor& self, DimnameList dim, c10::optional<int64_t> correction,
                bool keepdim, Tensor& result) {
  return at::std_out(result, self, dimnames_to_positions(self, dim), correction, keepdim);
}

Tensor var(const Tensor& self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
  return at::var(self, dimnames_to_positions(self, dim), correction, keepdim);
}

Tensor& var_out(const Tensor& self, DimnameList dim, c10::optional<int64_t> correction,
                bool keepdim, Tensor& result) {
  return at::var_out(
      result, self, dimnames_to_positions(self, dim), correction, keepdim);
}

std::tuple<Tensor,Tensor> var_mean(const Tensor& self, DimnameList dim,
                                   c10::optional<int64_t> correction, bool keepdim) {
  return at::var_mean(self, dimnames_to_positions(self, dim), correction, keepdim);
}

std::tuple<Tensor,Tensor> std_mean(const Tensor& self, DimnameList dim,
                                   c10::optional<int64_t> correction, bool keepdim) {
  return at::std_mean(self, dimnames_to_positions(self, dim), correction, keepdim);
}

Tensor& norm_out(const Tensor& self, const optional<Scalar>& p, DimnameList dim, bool keepdim, ScalarType dtype, Tensor& result) {
return at::norm_out(result, self, p, dimnames_to_positions(self, dim), keepdim, dtype);
}
"
729,"keepdim: bool):
std_out = torch.std(self, dim, unbiased, keepdim)
def backward(grad_output):
                grad_self = AD_var_backward_1(grad_output / (std_out * 2), self, dim, unbiased, keepdim)
return grad_self, None, None, None
return std_out, backward
","keepdim: bool):
std_out = torch.std(self, dim, unbiased, keepdim)
def backward(grad_output):
                correction = AD_bool_to_int(unbiased)
                grad_self = AD_var_backward_1(grad_output / (std_out * 2), self, dim, correction, keepdim)
                return grad_self, None, None, None

            return std_out, backward

        def std_2(self,
                  dim: Optional[List[int]],
                  *,
                  correction: Optional[int],
                  keepdim: bool):
            std_out = torch.std(self, dim, correction=correction, keepdim=keepdim)
            def backward(grad_output):
                grad_self = AD_var_backward_2(grad_output / (std_out * 2), self, dim, correction, keepdim)
return grad_self, None, None, None
return std_out, backward
"
730,"def var_0(self,
unbiased: bool=True):
def backward(grad_output):
                grad_self = AD_var_backward_0(grad_output, self, unbiased)
return grad_self, None
return torch.var(self, unbiased), backward
","def var_0(self,
unbiased: bool=True):
def backward(grad_output):
                correction = AD_bool_to_int(unbiased)
                grad_self = AD_var_backward_0(grad_output, self, correction)
return grad_self, None
return torch.var(self, unbiased), backward
"
731,"grad_mat2 = AD_bmm_backward_mat2(grad_output, self)
return grad_self, grad_mat2
return torch.bmm(self, mat2), backward

def AD_mat_transpose(mat):
dim = mat.dim()
if dim == 1:
","grad_mat2 = AD_bmm_backward_mat2(grad_output, self)
return grad_self, grad_mat2
return torch.bmm(self, mat2), backward
    )"",
    R""(
def AD_mat_transpose(mat):
dim = mat.dim()
if dim == 1:
"
732,"template_name,
dependencies,
is_master_only=True,
                 is_pr_only=False):
self.job_name = job_name
self.template_name = template_name
self.dependencies = dependencies
self.is_master_only = is_master_only
self.is_pr_only = is_pr_only
def gen_tree(self):
","template_name,
dependencies,
is_master_only=True,
                 is_pr_only=False,
                 extra_props=tuple()):
self.job_name = job_name
self.template_name = template_name
self.dependencies = dependencies
self.is_master_only = is_master_only
self.is_pr_only = is_pr_only
        self.extra_props = dict(extra_props)
def gen_tree(self):
"
733,"OpCode op_code = parseOpCode(ins_item[0].toString()->string().c_str());
int X = ins_item[1].toInt();
int N = ins_item[2].toInt();
      // TODO: Save debug handles for all instructions, not just for OP
      if (op_code == OP) {
        if (has_debug_handles) {
          // Why X is used to index into debug_handles?
          // X is the offset into opnames table and since debug handles
          // were ""appended"" in the debug handles vector, during serialization,
          // only for OP/OPN X is safe to index into it.
          // This is not super reliable and once we move to saving debug
          // handles for all instructions we can remove this strange behavior.
          int64_t debug_handle = debug_handles_list[X].toInt();
          function->append_instruction(op_code, X, N, debug_handle);
        } else {
          function->append_instruction(op_code, X, N);
        }
} else {
function->append_instruction(op_code, X, N);
}
","OpCode op_code = parseOpCode(ins_item[0].toString()->string().c_str());
int X = ins_item[1].toInt();
int N = ins_item[2].toInt();
      if (has_debug_handles) {
        int64_t debug_handle = debug_handles_list[i].toInt();
        function->append_instruction(op_code, X, N, debug_handle);
} else {
function->append_instruction(op_code, X, N);
}
"
734,"int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
deserializer.deserialize(tup_elems[kSourceRangeIndex]);
          source_range_map_.emplace(debug_handle, std::move(source_range));
}
}
}
}
}
std::string MobileDebugTable::getSourceDebugString(
    const int64_t debug_handle) const {
  const auto it = source_range_map_.find(debug_handle);
  if (it == source_range_map_.end()) {
    return """";
}
  return source_range_map_.at(debug_handle).str();
}
} // namespace jit
","int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
deserializer.deserialize(tup_elems[kSourceRangeIndex]);
          source_range_map.emplace(debug_handle, std::move(source_range));
}
}
}
}
  const std::string callstack_debug_file(""callstack_debug_map.pkl"");
  if (reader->hasRecord(""callstack_debug_map.pkl"")) {
    at::DataPtr callstack_data;
    size_t callstack_data_size{0};
    std::tie(callstack_data, callstack_data_size) =
        reader->getRecord(callstack_debug_file);
    CallStackDebugInfoUnpickler unpickler;
    callstack_ptr_map_ = unpickler.unpickle(
        std::move(callstack_data), callstack_data_size, source_range_map, cu);
  }
}

std::string MobileDebugTable::getModuleHierarchyInfo(
    const int64_t debug_handle,
    const std::string& top_module_type_name) const {
  const auto it = callstack_ptr_map_.find(debug_handle);
  if (it == callstack_ptr_map_.end()) {
    return ""debug_handle:"" + std::to_string(debug_handle);
  }
  return (getStackTraceWithModuleHierarchy(
              it->second, ""top"", top_module_type_name))
      .second;
}
std::string MobileDebugTable::getSourceDebugString(
    const int64_t debug_handle,
    const std::string& top_module_type_name) const {
  const auto it = callstack_ptr_map_.find(debug_handle);
  if (it == callstack_ptr_map_.end()) {
    return ""debug_handle:"" + std::to_string(debug_handle);
}
  return (getStackTraceWithModuleHierarchy(
              it->second, ""top"", top_module_type_name))
      .first;
}
} // namespace jit
"
735,"return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple = getFunctionTuple(
          module, setstate, save_mobile_debug_info, source_range_map);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
      if (save_mobile_debug_info) {
        debug_info_elements->push_back(func_tuple.second.value());
      }
}
} else {
for (size_t i = 0, n = type->numAttributes(); i < n; ++i) {
","return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple =
          getFunctionTuple(module, setstate, debug_handle_manager);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
      debug_info_elements.push_back(func_tuple.second);
}
} else {
for (size_t i = 0, n = type->numAttributes(); i < n; ++i) {
"
736,">>> torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])
.. function:: mul(input, other, *, out=None)
Each element of the tensor :attr:`input` is multiplied by the corresponding
element of the Tensor :attr:`other`. The resulting tensor is returned.
",">>> torch.mul(a, 100)
tensor([  20.1494,  -42.5491,  260.8663])
.. function:: mul(input, other, *, out=None) -> Tensor
Each element of the tensor :attr:`input` is multiplied by the corresponding
element of the Tensor :attr:`other`. The resulting tensor is returned.
"
737,"return std::make_tuple(hx, cx);
}
struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
","return std::make_tuple(hx, cx);
}
/**
 * Note [DropoutState and CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * (1) Telling a capturing stream to wait on an event recorded in a non-capturing stream is an error.
 * (2) Telling a non-capturing stream to wait on an event recorded during capture is also an error.
 *
 * So DropoutState's usage syncs could error if an RNN with dropout is called in an uncaptured region
 * then called in a captured region (triggering 1), or called in a captured region then called
 # in an uncaptured region (triggering 2).
 *
 * To prevent 1 and 2, lock() only syncs on the last usage event if it was recorded in the same
 * capture state as the current state (which also means the same graph, if capture is in progress).
 *
 * The solution should be safe as long as capture obeys the following restrictions:
 *  - Only one capture may be underway at a time in a given process.
 *  - While a capture is underway, no calls to eager ops on noncapturing streams (on any thread)
 *    may interleave with the captured ops.
 *
 * TODO: As people experiment with capture, keep an eye out for use cases that might need to
 * relax those restrictions.
 *
 * See https://github.com/pytorch/pytorch/pull/56433 for more discussion.
 */

struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
"
738,"return result;
}
Tensor linalg_cholesky(const Tensor &self) {
  if (self.numel() == 0) {
    return at::empty_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
}
  squareCheckInputs(self);
  return at::_cholesky_helper(self, /*upper=*/false).tril_();
}
Tensor& linalg_cholesky_out(const Tensor &self, Tensor &result) {
  checkSameDevice(""linalg_cholesky"", result, self);
  checkLinalgCompatibleDtype(""linalg_cholesky"", result, self);
  Tensor result_tmp = at::linalg_cholesky(self);
  at::native::resize_output(result, result_tmp.sizes());
  result.copy_(result_tmp);
return result;
}
","return result;
}
void linalg_cholesky_out_info(const Tensor& input, const Tensor& result, const Tensor& info) {
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(input.dim() >= 2);
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(input.size(-1) == input.size(-2));

  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(result.scalar_type() == input.scalar_type());
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(result.device() == input.device());

  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(info.scalar_type() == at::kInt);
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(info.device() == input.device());

  // if result has no elements we can modify it
  if (result.numel() == 0) {
    at::native::resize_as_(result, input.transpose(-2, -1), MemoryFormat::Contiguous);
    result.transpose_(-2, -1);
  }

  // result tensor must be in batched column major order (Fortran contiguous)
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(result.transpose(-2, -1).is_contiguous());
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(result.sizes().equals(input.sizes()));

  // cholesky_stub (apply_cholesky) performs calculations in-place and result must be a copy of input
  result.copy_(input);

  // if info has no elements we can modify it
  auto expected_info_shape = IntArrayRef(input.sizes().cbegin(), input.sizes().cend() - 2); // input.shape[:-2]
  if (info.numel() == 0) {
    info.resize_(expected_info_shape);
  }

  // info must be contiguous
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(info.is_contiguous());
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(info.sizes().equals(expected_info_shape));
  info.fill_(0);

  cholesky_stub(result.device().type(), result, info, /*upper=*/false);

  result.tril_();
}

std::tuple<Tensor&, Tensor&> linalg_cholesky_ex_out(const Tensor& input, bool check_errors, Tensor& L, Tensor& info) {
  squareCheckInputs(input);
  checkSameDevice(""torch.linalg.cholesky_ex"", L, input, ""L"");
  checkLinalgCompatibleDtype(""torch.linalg.cholesky_ex"", L, input, ""L"");
  checkSameDevice(""torch.linalg.cholesky_ex"", info, input, ""info"");

  // Do not allow type promotion for the `info` tensor, it must be of Int dtype
  // Int is used because current interface to LAPACK and its CUDA implementation use ""int"" type.
  // https://github.com/pytorch/pytorch/pull/56724#discussion_r618916774
  ScalarType info_output_type = ScalarType::Int;
  TORCH_CHECK(
      info.scalar_type() == info_output_type,
      ""torch.linalg.cholesky_ex: "",
      ""Expected info to have "", info_output_type, "" dtype, but got info with dtype "", info.scalar_type());

  bool L_input_same_type = (L.scalar_type() == input.scalar_type());
  bool L_equal_expected_shape = L.sizes().equals(input.sizes());
  bool is_L_batched_column_major = false;
  if (L.dim() >= 2) {
    is_L_batched_column_major = L.transpose(-2, -1).is_contiguous();
  }

  // if L is not empty and not in batched column major format
  bool copy_needed = (L.numel() != 0 && !is_L_batched_column_major);
  copy_needed |= (L.numel() != 0 && !L_equal_expected_shape); // or L does not have the expected shape
  copy_needed |= !L_input_same_type;  // or L does not have the same dtype as input
  // we have to allocate a temporary tensor

  // similar conditions for info tensor
  auto expected_info_shape = IntArrayRef(input.sizes().cbegin(), input.sizes().cend() - 2); // input.shape[:-2]
  copy_needed |= (info.numel() != 0 && !info.is_contiguous());
  copy_needed |= (info.numel() != 0 && !(info.sizes().equals(expected_info_shape))); // or L does not have the expected shape

  if (copy_needed) {
    Tensor L_tmp = at::empty({0}, input.options());
    Tensor info_tmp = at::empty({0}, input.options().dtype(kInt));
    linalg_cholesky_out_info(input, L_tmp, info_tmp);
    at::native::resize_output(L, L_tmp.sizes());
    L.copy_(L_tmp);
    at::native::resize_output(info, info_tmp.sizes());
    info.copy_(info_tmp);
  } else {
    // use ""out"" tensors' memory directly
    linalg_cholesky_out_info(input, L, info);
  }

  if (check_errors) {
    if (input.dim() > 2) {
      batchCheckErrors(info, ""torch.linalg.cholesky_ex"");
    } else {
      singleCheckErrors(info.item<int64_t>(), ""torch.linalg.cholesky_ex"");
    }
  }

  return std::tuple<Tensor&, Tensor&>(L, info);
}

std::tuple<Tensor, Tensor> linalg_cholesky_ex(const Tensor& input, bool check_errors) {
  Tensor L = at::empty({0}, input.options());
  Tensor info = at::empty({0}, input.options().dtype(kInt));
  std::tie(L, info) = at::native::linalg_cholesky_ex_out(input, check_errors, L, info);
  return std::make_tuple(L, info);
}

Tensor linalg_cholesky(const Tensor &self) {
  Tensor result, info;
  std::tie(result, info) = at::linalg_cholesky_ex(self, /*check_errors=*/false);

  // we pass check_errors=false above and do the check here
  // so that the name of the function is correct in the error message
  if (self.dim() > 2) {
    batchCheckErrors(info, ""torch.linalg.cholesky"");
  } else {
    singleCheckErrors(info.item<int64_t>(), ""torch.linalg.cholesky"");
}

  return result;
}
Tensor& linalg_cholesky_out(const Tensor &self, Tensor &result) {
  // linalg_cholesky_ex_outf includes these checks, but we do it here
  // so that the name of the function is correct in the error message
  checkSameDevice(""torch.linalg.cholesky"", result, self);
  checkLinalgCompatibleDtype(""torch.linalg.cholesky"", result, self);

  Tensor info = at::empty({0}, self.options().dtype(kInt));
  std::tie(result, info) = at::linalg_cholesky_ex_outf(self, /*check_errors=*/false, result, info);

  // we pass check_errors=false above and do the check here
  // so that the name of the function is correct in the error message
  if (self.dim() > 2) {
    batchCheckErrors(info, ""torch.linalg.cholesky"");
  } else {
    singleCheckErrors(info.item<int64_t>(), ""torch.linalg.cholesky"");
  }

return result;
}
"
739,"}} // namespace autograd::VariableType
namespace InplaceOrView {
Tensor & copy_(c10::DispatchKeySet ks, Tensor & self, const Tensor & src, bool non_blocking) {
{
at::AutoDispatchBelowInplaceOrView guard;
","}} // namespace autograd::VariableType
namespace InplaceOrView {
  #define CREATION_META_DEFINITION InferenceMode::is_enabled() ? CreationMeta::INFERENCE_MODE : (at::GradMode::is_enabled() ? CreationMeta::DEFAULT : CreationMeta::NO_GRAD_MODE)

Tensor & copy_(c10::DispatchKeySet ks, Tensor & self, const Tensor & src, bool non_blocking) {
{
at::AutoDispatchBelowInplaceOrView guard;
"
740,"END_HANDLE_TH_ERRORS
}
//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, modernize-avoid-c-arrays)
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  THPModule_initExtension,   METH_O,       nullptr},
{""_autograd_init"",  THPAutograd_initExtension, METH_NOARGS,  nullptr},
","END_HANDLE_TH_ERRORS
}
//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, cppcoreguidelines-avoid-non-const-global-variables, modernize-avoid-c-arrays)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  THPModule_initExtension,   METH_O,       nullptr},
{""_autograd_init"",  THPAutograd_initExtension, METH_NOARGS,  nullptr},
"
741,"0,                                     /* tp_itemsize */
(destructor)THCPEvent_dealloc,         /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
0,                                     /* tp_getattr */
0,                                     /* tp_setattr */
0,                                     /* tp_reserved */
0,                                     /* tp_repr */
0,                                     /* tp_as_number */
0,                                     /* tp_as_sequence */
0,                                     /* tp_as_mapping */
0,                                     /* tp_hash  */
0,                                     /* tp_call */
0,                                     /* tp_str */
0,                                     /* tp_getattro */
0,                                     /* tp_setattro */
0,                                     /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
0,                                     /* tp_traverse */
0,                                     /* tp_clear */
0,                                     /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
0,                                     /* tp_iter */
0,                                     /* tp_iternext */
THCPEvent_methods,                     /* tp_methods */
0,                                     /* tp_members */
THCPEvent_properties,                  /* tp_getset */
0,                                     /* tp_base */
0,                                     /* tp_dict */
0,                                     /* tp_descr_get */
0,                                     /* tp_descr_set */
0,                                     /* tp_dictoffset */
0,                                     /* tp_init */
0,                                     /* tp_alloc */
THCPEvent_pynew,                       /* tp_new */
};
","0,                                     /* tp_itemsize */
(destructor)THCPEvent_dealloc,         /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_getattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_setattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_reserved */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_repr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_number */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_sequence */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_mapping */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_hash  */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_call */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_str */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_getattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_setattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_traverse */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_clear */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_iter */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_iternext */
THCPEvent_methods,                     /* tp_methods */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_members */
THCPEvent_properties,                  /* tp_getset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_base */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_dict */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_descr_get */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_descr_set */
0,                                     /* tp_dictoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_init */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_alloc */
THCPEvent_pynew,                       /* tp_new */
};
"
742,"PyTypeObject *type, PyObject *args, PyObject *kwargs) {
HANDLE_TH_ERRORS
int current_device;
THCudaCheck(cudaGetDevice(&current_device));
int priority = 0;
uint64_t cdata = 0;
static char *kwlist[] = {""priority"", ""_cdata"", nullptr};
if (!PyArg_ParseTupleAndKeywords(
args, kwargs, ""|iK"", kwlist, &priority, &cdata)) {
","PyTypeObject *type, PyObject *args, PyObject *kwargs) {
HANDLE_TH_ERRORS
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
int current_device;
THCudaCheck(cudaGetDevice(&current_device));
int priority = 0;
uint64_t cdata = 0;
  // NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)
static char *kwlist[] = {""priority"", ""_cdata"", nullptr};
if (!PyArg_ParseTupleAndKeywords(
args, kwargs, ""|iK"", kwlist, &priority, &cdata)) {
"
743,"static PyObject * THCPStream_priority_range(PyObject *_unused, PyObject* noargs) {
HANDLE_TH_ERRORS
int least_priority, greatest_priority;
std::tie(least_priority, greatest_priority) =
at::cuda::CUDAStream::priority_range();
","static PyObject * THCPStream_priority_range(PyObject *_unused, PyObject* noargs) {
HANDLE_TH_ERRORS
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
int least_priority, greatest_priority;
std::tie(least_priority, greatest_priority) =
at::cuda::CUDAStream::priority_range();
"
744,"#include <thread>
#include <vector>
#include <assert.h>
#include <torch/deploy.h>
","#include <thread>
#include <vector>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <torch/deploy.h>
"
745,"50.,
95.}; //{1., 5., 25., 50., 75., 90., 95., 99., 99.25, 99.5, 99.75, 99.9};
struct Report {
std::string benchmark;
std::string strategy;
","50.,
95.}; //{1., 5., 25., 50., 75., 90., 95., 99., 99.25, 99.5, 99.75, 99.9};
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)
struct Report {
std::string benchmark;
std::string strategy;
"
746,"void FutureFactoryRegistry::registerFutureFactory(
c10::DeviceType type,
future_factory_t factory) {
factories_[static_cast<size_t>(type) & 0xFF] = std::move(factory);
}
","void FutureFactoryRegistry::registerFutureFactory(
c10::DeviceType type,
future_factory_t factory) {
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
factories_[static_cast<size_t>(type) & 0xFF] = std::move(factory);
}
"
747,"options.data(),
option_vals.data()));
size_t cubinSize;
void* cubin;
AT_CUDA_DRIVER_CHECK(at::globalContext().getNVRTC().cuLinkComplete(
linkState, &cubin, &cubinSize));
","options.data(),
option_vals.data()));
    // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
size_t cubinSize;
    // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
void* cubin;
AT_CUDA_DRIVER_CHECK(at::globalContext().getNVRTC().cuLinkComplete(
linkState, &cubin, &cubinSize));
"
748,"return true;
} else if (val->type()->isSubtypeOf(
static_cast<c10::TypePtr>(BoolType::get()))) {
CgValue cg_val;
if (auto ival = constant_as<bool>(val)) {
cg_val = new Bool(ival.value());
","return true;
} else if (val->type()->isSubtypeOf(
static_cast<c10::TypePtr>(BoolType::get()))) {
      // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
CgValue cg_val;
if (auto ival = constant_as<bool>(val)) {
cg_val = new Bool(ival.value());
"
749,"}
bool registerTensor(const JitValue* val) {
CgValue cg_val;
if (auto tensor_type = val->type()->cast<TensorType>()) {
// TODO: make this a static function in Tensor class;
","}
bool registerTensor(const JitValue* val) {
    // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
CgValue cg_val;
if (auto tensor_type = val->type()->cast<TensorType>()) {
// TODO: make this a static function in Tensor class;
"
750,"CudaVersion dev_version = CudaVersion(prop->major, prop->minor);
CudaVersion max_dev_version(dev_version);
if (nvrtc_version.first <= 7) { // 7 supports 2-5.x
max_dev_version = CudaVersion(5, 0);
} else if (nvrtc_version.first <= 8) { // 8 supports 2-6.x
max_dev_version = CudaVersion(6, 0);
} else if (nvrtc_version.first <= 9) { // 9 supports 3-7.2
max_dev_version = CudaVersion(7, 2);
} else if (nvrtc_version.first <= 10) { // 10 supports 3-7.5
max_dev_version = CudaVersion(7, 5);
} else if (nvrtc_version.first == 11 && nvrtc_version.second == 0) {
// 11.0 supports 3-8.0
max_dev_version = CudaVersion(8, 0);
}
if (dev_version > max_dev_version) {
","CudaVersion dev_version = CudaVersion(prop->major, prop->minor);
CudaVersion max_dev_version(dev_version);
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
if (nvrtc_version.first <= 7) { // 7 supports 2-5.x
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
max_dev_version = CudaVersion(5, 0);
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_version.first <= 8) { // 8 supports 2-6.x
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
max_dev_version = CudaVersion(6, 0);
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_version.first <= 9) { // 9 supports 3-7.2
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
max_dev_version = CudaVersion(7, 2);
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_version.first <= 10) { // 10 supports 3-7.5
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
max_dev_version = CudaVersion(7, 5);
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_version.first == 11 && nvrtc_version.second == 0) {
// 11.0 supports 3-8.0
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
max_dev_version = CudaVersion(8, 0);
}
if (dev_version > max_dev_version) {
"
751,"std::to_string(v->start()));
}
if (prev == nullptr) {
gpu_block_extents_[gpu_block_index] = v->stop();
} else if (prev->isConstant() && immediateEquals(prev, 1)) {
","std::to_string(v->start()));
}
    // NOLINTNEXTLINE(bugprone-branch-clone)
if (prev == nullptr) {
gpu_block_extents_[gpu_block_index] = v->stop();
} else if (prev->isConstant() && immediateEquals(prev, 1)) {
"
752,"v->indices().size() == nested_store_->indices().size()) {
// also check indices
bool same = true;
for (int i = 0; i < v->indices().size(); ++i) {
if (!exprEquals(v->indices()[i], nested_store_->indices()[i])) {
same = false;
","v->indices().size() == nested_store_->indices().size()) {
// also check indices
bool same = true;
        // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
for (int i = 0; i < v->indices().size(); ++i) {
if (!exprEquals(v->indices()[i], nested_store_->indices()[i])) {
same = false;
"
753,"ResourceGuard holdProgram(
[&] { AT_CUDA_NVRTC_CHECK(nvrtc().nvrtcDestroyProgram(&program)); });
AT_CUDA_NVRTC_CHECK(result);
size_t ptx_size;
std::vector<char> ptx;
#if CUDA_VERSION >= 11010
","ResourceGuard holdProgram(
[&] { AT_CUDA_NVRTC_CHECK(nvrtc().nvrtcDestroyProgram(&program)); });
AT_CUDA_NVRTC_CHECK(result);
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
size_t ptx_size;
std::vector<char> ptx;
#if CUDA_VERSION >= 11010
"
754,"log.info(
f""User process failed with error data: {json.dumps(self.error_file_data, indent=2)}""
)
                    self.message = self.error_file_data[""message""][""message""]
                    self.timestamp = int(
                        self.error_file_data[""message""][""extraInfo""][""timestamp""]
)
except Exception:
log.exception(f""Failed to parse reply file: {self.error_file}"")
","log.info(
f""User process failed with error data: {json.dumps(self.error_file_data, indent=2)}""
)
                    self.message, self.timestamp = self._get_error_data(
                        self.error_file_data
)
except Exception:
log.exception(f""Failed to parse reply file: {self.error_file}"")
"
755,"# int can be promoted to List[int]
return True
    def is_homogeneous_int_tuple(t):
        if not getattr(t, '__origin__', None) in {tuple, Tuple}:
            return False

        contained = t.__args__
        if t.__args__ == ((),):  # Tuple[()].__args__ == ((),) for some reason
            return True
        return all(c is int or (c is Ellipsis) for c in contained)

    if signature_type is List[int] and is_homogeneous_int_tuple(argument_type):
        # Tuple[int] is accepted for List[int] parameters
        return True
# Dtype is an int in schemas
if signature_type is int and argument_type is torch.dtype:
","# int can be promoted to List[int]
return True
    if getattr(signature_type, '__origin__', None) in {list, List}:
        sig_el_type = signature_type.__args__[0]
        if not inspect.isclass(sig_el_type):
            raise RuntimeError(
                f""Does not support nested parametric types, got {sig_el_type}. Please file a bug."")
        if getattr(argument_type, '__origin__', None) in {list, List}:
            return issubclass(argument_type.__args__[0], sig_el_type)

        def is_homogeneous_tuple(t):
            if not getattr(t, '__origin__', None) in {tuple, Tuple}:
                return False
            contained = t.__args__
            if t.__args__ == ((),):  # Tuple[()].__args__ == ((),) for some reason
                return True
            return all((c is Ellipsis) or issubclass(c, sig_el_type) for c in contained)

        # Tuple[T] is accepted for List[T] parameters
        return is_homogeneous_tuple(argument_type)
# Dtype is an int in schemas
if signature_type is int and argument_type is torch.dtype:
"
756,"for node_a_arg in node_a.args[num_non_param_args:]:
if isinstance(node_a_arg, Node):
arg_a = return_first_non_observer_node(node_a_arg, gm_a)
            if arg_a.op == 'get_attr':
                arg_a_copy_name = \
                    get_new_attr_name_with_prefix(arg_a.name + '_shadow_copy_')(gm_b)
                arg_a_obj = getattr_from_fqn(gm_a, arg_a.target)  # type: ignore[arg-type]
                setattr(gm_b, arg_a_copy_name, arg_a_obj.detach())
                node_a_arg_copy = graph_c.create_node(
                    'get_attr', arg_a_copy_name, (), {}, arg_a_copy_name)
                new_args.append(node_a_arg_copy)
            else:
                raise AssertionError(
                    f""handling of node with op {arg_a.op} is not implemented"")
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
","for node_a_arg in node_a.args[num_non_param_args:]:
if isinstance(node_a_arg, Node):
arg_a = return_first_non_observer_node(node_a_arg, gm_a)
            node_a_arg_copy = _copy_node_from_a_to_c(arg_a, gm_a, gm_b, graph_c)
            new_args.append(node_a_arg_copy)
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
"
757,"# s = fn(z) where z = x for real valued input
# and z = x + yj for complex valued input
jacobians_cols: List[torch.Tensor] = []
    ds_dx_tup = jvp_fn(delta)
if input_is_complex:  # C -> R
        ds_dy_tup = jvp_fn(delta * 1j)
for ds_dx, ds_dy in zip(ds_dx_tup, ds_dy_tup):
assert(not ds_dx.is_complex())
# conjugate wirtinger derivative
","# s = fn(z) where z = x for real valued input
# and z = x + yj for complex valued input
jacobians_cols: List[torch.Tensor] = []
    ds_dx_tup = jvp_fn(delta[0] if isinstance(delta, tuple) else delta)
if input_is_complex:  # C -> R
        ds_dy_tup = jvp_fn(delta[1] * 1j) if isinstance(delta, tuple) else jvp_fn(delta * 1j)
for ds_dx, ds_dy in zip(ds_dx_tup, ds_dy_tup):
assert(not ds_dx.is_complex())
# conjugate wirtinger derivative
"
758,"wrapped_fn = with_prepped_inputs(fn, inputs, input_idx, input_to_perturb, True)
nbhd_checks_fn = functools.partial(check_outputs_same_dtype_and_shape, eps=eps)
jvp_fn = get_jvp_fn(wrapped_fn, input_to_perturb, eps, nbhd_checks_fn)
    if u.layout != torch.sparse_coo:
        u = u.reshape(input_to_perturb.shape)
    return compute_numerical_jacobian_cols(jvp_fn, u * eps, input.is_complex())
def check_jacobians_equal(j1, j2, atol):
","wrapped_fn = with_prepped_inputs(fn, inputs, input_idx, input_to_perturb, True)
nbhd_checks_fn = functools.partial(check_outputs_same_dtype_and_shape, eps=eps)
jvp_fn = get_jvp_fn(wrapped_fn, input_to_perturb, eps, nbhd_checks_fn)
    u = reshape_tensor_or_tuple(u, input_to_perturb.shape)
    u = mul_tensor_or_tuple(u, eps)
    return compute_numerical_jacobian_cols(jvp_fn, u, input.is_complex())
def check_jacobians_equal(j1, j2, atol):
"
759,"(node_input_type_a == NodeInputOrOutputType.FP32 and
node_input_type_c == NodeInputOrOutputType.INT8) or
(node_input_type_a == NodeInputOrOutputType.FP32 and
         node_input_type_c == NodeInputOrOutputType.FP16) or
        # TODO(future PR): determine the actual dtype of node_c,
        # the current code only works because dequantize works with
        # multiple input dtypes.
        (node_input_type_a == NodeInputOrOutputType.FP32 and
         node_input_type_c == NodeInputOrOutputType.FP32_OR_INT8)
):
dtype_cast_op = torch.dequantize
elif (
","(node_input_type_a == NodeInputOrOutputType.FP32 and
node_input_type_c == NodeInputOrOutputType.INT8) or
(node_input_type_a == NodeInputOrOutputType.FP32 and
         node_input_type_c == NodeInputOrOutputType.FP16)
):
dtype_cast_op = torch.dequantize
elif (
"
760,"else:
raise AssertionError(f""type f{type(prev_node_c)} is not handled"")
def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
input_node_c_2: Optional[Union[Node, List[Node]]],
","else:
raise AssertionError(f""type f{type(prev_node_c)} is not handled"")
# TODO(future PR): look into using copy_node API instead
def _copy_node_from_a_to_c(
    node_a: Node,
    gm_a: GraphModule,
    gm_b: GraphModule,
    graph_c: Graph,
) -> Node:
    """"""
    Simple copy of node_a to graph_c.
    """"""
    if node_a.op == 'get_attr':
        node_a_copy_name = \
            get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)  # type: ignore
        node_a_obj = getattr_from_fqn(gm_a, node_a.target)  # type: ignore
        setattr(gm_b, node_a_copy_name, node_a_obj.detach())
        node_a_copy = graph_c.create_node(
            node_a.op, node_a_copy_name, (), {}, node_a_copy_name)
        return node_a_copy
    elif node_a.op == 'call_method':
        assert node_a.target in ('dequantize', 'to'), \
            f""target {node_a.target} is not implemented""
        if node_a.target == 'dequantize':
            arg_copy = _copy_node_from_a_to_c(node_a.args[0], gm_a, gm_b, graph_c)  # type: ignore
            node_a_copy_name = \
                get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)  # type: ignore
            node_a_copy = graph_c.create_node(
                node_a.op, node_a.target, (arg_copy,), {}, node_a_copy_name)
            return node_a_copy
        else:  # to
            arg_copy = _copy_node_from_a_to_c(node_a.args[0], gm_a, gm_b, graph_c)  # type: ignore
            node_a_copy_name = \
                get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)  # type: ignore
            node_a_copy = graph_c.create_node(
                node_a.op, node_a.target, (arg_copy, node_a.args[1]), {},
                node_a_copy_name)
            return node_a_copy

    else:
        raise AssertionError(
            f""handling of node with op {node_a.op} is not implemented"")

def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
input_node_c_2: Optional[Union[Node, List[Node]]],
"
761,"from torch.distributed.elastic.metrics.api import prof
from torch.distributed.elastic.multiprocessing import start_processes, PContext
from torch.distributed.elastic.utils import macros

log = logging.getLogger(__name__)
class LocalElasticAgent(SimpleElasticAgent):
","from torch.distributed.elastic.metrics.api import prof
from torch.distributed.elastic.multiprocessing import start_processes, PContext
from torch.distributed.elastic.utils import macros
from torch.distributed.elastic.utils.logging import get_logger
log = get_logger()
class LocalElasticAgent(SimpleElasticAgent):
"
762,"timestamp: int = field(init=False)
def __post_init__(self):
if os.path.isfile(self.error_file):
            with open(self.error_file, ""r"") as fp:
                self.error_file_data = json.load(fp)
                self.message = self.error_file_data[""message""][""message""]
                self.timestamp = int(
                    self.error_file_data[""message""][""extraInfo""][""timestamp""]
                )
else:
            self.error_file = _NOT_AVAILABLE
            self.error_file_data = _EMPTY_ERROR_DATA
            self.message = """"
            self.timestamp = int(time.time())
# make up an informative message if not already present
if not self.message:
","timestamp: int = field(init=False)
def __post_init__(self):
        self.error_file_data = _EMPTY_ERROR_DATA
if os.path.isfile(self.error_file):
            try:
                with open(self.error_file, ""r"") as fp:
                    self.error_file_data = json.load(fp)
                    log.info(
                        f""User process failed with error data: {json.dumps(self.error_file_data, indent=2)}""
                    )
                    self.message = self.error_file_data[""message""][""message""]
                    self.timestamp = int(
                        self.error_file_data[""message""][""extraInfo""][""timestamp""]
                    )
            except Exception:
                log.exception(f""Failed to parse reply file: {self.error_file}"")
                raise
else:
            self._set_no_reply_file()
# make up an informative message if not already present
if not self.message:
"
763,"Shape:
 Input: :math:`(N, *)` where :math:`*` means any number of additional
dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Var: :math:`(N, 1)` or :math:`(N, *)`, same shape as the input
 Output: scalar if :attr:`reduction` is ``'mean'`` (default) or
          ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N)`
Examples::

>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
","Shape:
dimensions
        - Target: :math:`(N, *)`, same shape as the input, or same shape as the input
          but with one dimension equal to 1 (to allow for broadcasting)
        - Var: :math:`(N, *)`, same shape as the input, or same shape as the input but
          with one dimension equal to 1, or same shape as the input but with one fewer
          dimension (to allow for broadcasting)
          ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
          shape as the input
Examples::
>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
"
764,"{c10::Symbol::fromQualString(""aten::reshape""),
c10::Symbol::fromQualString(""static_runtime::reshape_copy"")},
{c10::Symbol::fromQualString(""aten::flatten""),
       c10::Symbol::fromQualString(""static_runtime::flatten_copy"")},
      {c10::Symbol::fromQualString(""aten::to""),
c10::Symbol::fromQualString(""static_runtime::to_copy"")}};
bool has_inplace_ops = HasInplaceOp(graph, db);
std::vector<std::pair<Node*, Node*>> replacement;
for (auto* n : graph->nodes()) {
    if (!supported.count(n->kind()) ||
        !opIsRegistered(supported.at(n->kind()))) {
continue;
}
DCHECK(n->outputs().size() == 1);
","{c10::Symbol::fromQualString(""aten::reshape""),
c10::Symbol::fromQualString(""static_runtime::reshape_copy"")},
{c10::Symbol::fromQualString(""aten::flatten""),
       c10::Symbol::fromQualString(""static_runtime::flatten_copy"")}};

  // for ops that have overloads, match the schema
  const std::vector<std::pair<c10::FunctionSchema, c10::Symbol>> supported_schema = {
      {torch::schema(
           ""aten::to.prim_dtype(Tensor(a) self, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor(a|b)""),
       c10::Symbol::fromQualString(""static_runtime::to_copy"")},
      {torch::schema(
           ""to.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor""),
c10::Symbol::fromQualString(""static_runtime::to_copy"")}};
  auto match_schema = [&supported_schema](
                          const Node* node, c10::Symbol& out_matched_symbol) {
    for (auto& schema : supported_schema) {
      if (node->matches(schema.first)) {
        out_matched_symbol = schema.second;
        return true;
      }
    }
    return false;
  };

bool has_inplace_ops = HasInplaceOp(graph, db);
std::vector<std::pair<Node*, Node*>> replacement;
for (auto* n : graph->nodes()) {
    c10::Symbol new_symbol;
    if (supported.count(n->kind()) && opIsRegistered(supported.at(n->kind()))) {
      new_symbol = supported.at(n->kind());
    } else if (!match_schema(n, new_symbol)) {
continue;
}
DCHECK(n->outputs().size() == 1);
"
765,"//
//  - Otherwise, output tensor will have contiguous memory layout.
//
Tensor& resize_as_(
    Tensor& self,
const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
if (self.is_sparse() && the_template.is_sparse()) {
","//
//  - Otherwise, output tensor will have contiguous memory layout.
//
const Tensor& resize_as_(
    const Tensor& self,
const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
if (self.is_sparse() && the_template.is_sparse()) {
"
766,"return self;
}
Tensor& resize_as_(
    Tensor& self,
const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
if (torch::jit::tracer::isTracing()) {
","return self;
}
const Tensor& resize_as_(
    const Tensor& self,
const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
if (torch::jit::tracer::isTracing()) {
"
767,"parser.add_argument(
""--rdzv_id"",
action=env,
        default=""<NONE>"",
type=str,
help=""user defined group id"",
)
","parser.add_argument(
""--rdzv_id"",
action=env,
        default=""none"",
type=str,
help=""user defined group id"",
)
"
768,"py_ast = ast.parse(dedent_src)
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, False)
    return build_class_def(ctx, py_ast.body[0], methods, properties, self_name)
def normalize_source_lines(sourcelines: List[str]) -> List[str]:
","py_ast = ast.parse(dedent_src)
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, False)
    class_ast = py_ast.body[0]
    assert isinstance(class_ast, ast.ClassDef)
    assigns = get_class_assigns(ctx, class_ast)

    return build_class_def(ctx, class_ast, methods, properties, self_name, assigns)
def normalize_source_lines(sourcelines: List[str]) -> List[str]:
"
769,"const ReduceScatterOptions& opts) {
check_gpu_tensors(outputTensors);
auto inputFlattened =
flatten_for_scatter_gather(inputTensors, outputTensors, size_);
check_gpu_tensors(inputFlattened);
","const ReduceScatterOptions& opts) {
check_gpu_tensors(outputTensors);
  // @lint-ignore CLANGTIDY
  auto tensor = outputTensors.back();
  RECORD_PARAM_COMMS(
      rank_,                // rank
      ""reduce_scatter"",     // colName
      tensor.numel() *      // inSize
        this->getSize(),    // outSize
      tensor.numel(),       // dType
      tensor.scalar_type(), // inSplitSizes
      {},                   // outSplitSizes
      {});

auto inputFlattened =
flatten_for_scatter_gather(inputTensors, outputTensors, size_);
check_gpu_tensors(inputFlattened);
"
770,"}
}
// TLDR: Don't call with `use_default_dtype` true -- this is only necessary to support the partial
// type-promotion that torch.where supports.  Once torch.where fully supports type promotion, we
// won't need this function.
//
// Longer explanation:
// `use_default_dtype` is a bit of a hack because torch.where doesn't support type promotion, but
// does support `torch.where(tensor, scalar1, scalar2)` with default scalar types.  The trickiness is we
// usually convert double scalars to doubles, and `set_wrapped_number` defines type promotion priority
// as being below tensor types rather than as the default dtype (perhaps we should?).  This wouldn't matter
// if we just supported type normal type promotion on torch.where, however.
Tensor wrapped_scalar_tensor(
const Scalar& scalar,
    Device device,
    bool use_default_dtype = false) {
at::Tensor tensor;
  if (use_default_dtype) {
    tensor = scalar_to_tensor_default_dtype(scalar, device);
  } else {
    tensor = scalar_to_tensor(scalar, device);
  }
tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
return tensor;
}
","}
}
// TLDR: Don't call `wrapped_scalar_tensor_default_dtype` -- this function is only necessary to support the partial
// type-promotion that torch.where supports.  Once torch.where fully supports type promotion, we
// won't need this function.
//
// Longer explanation:
// `wrapped_scalar_tensor_default_dtype` is a bit of a hack because torch.where doesn't support type promotion, but
// does support `torch.where(tensor, scalar1, scalar2)` with default scalar types.  The trickiness is we
// usually convert double scalars to doubles, and `set_wrapped_number` defines type promotion priority
// as being below tensor types rather than as the default dtype (perhaps we should?).  This wouldn't matter
// if we just supported type normal type promotion on torch.where, however.
Tensor wrapped_scalar_tensor_default_dtype(
const Scalar& scalar,
    Device device) {
at::Tensor tensor;
  tensor = scalar_to_tensor_default_dtype(scalar, device);
tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
return tensor;
}
"
771,"Tensor where(const Tensor& condition, const Scalar& self, const Scalar& other) {
const auto device = condition.device();
  const Tensor& other_t = wrapped_scalar_tensor(other, device, /*use_default_dtype=*/true);
  const Tensor& self_t = wrapped_scalar_tensor(self, device, /*use_default_dtype=*/true);
return at::where(condition, self_t, other_t);
}
","Tensor where(const Tensor& condition, const Scalar& self, const Scalar& other) {
const auto device = condition.device();
  const Tensor& other_t = wrapped_scalar_tensor_default_dtype(other, device);
  const Tensor& self_t = wrapped_scalar_tensor_default_dtype(self, device);
return at::where(condition, self_t, other_t);
}
"
772,"return size;
}
static Tensor wrapped_scalar_tensor(const Scalar& scalar) {
  auto tensor = scalar_to_tensor(scalar);
  tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
  return tensor;
}

Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result) {
if (!at::isComplexType(self_st) && gradient_result.is_complex()) {
// R -> C
","return size;
}
Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result) {
if (!at::isComplexType(self_st) && gradient_result.is_complex()) {
// R -> C
"
773,"# TODO(future PR): make more generic, handle everything
if isinstance(mod, nn.Linear):
return mod.weight.detach()
else:
return mod._weight_bias()[0]  # type: ignore
","# TODO(future PR): make more generic, handle everything
if isinstance(mod, nn.Linear):
return mod.weight.detach()
    elif isinstance(mod, nni.LinearReLU):
        return mod[0].weight.detach()
else:
return mod._weight_bias()[0]  # type: ignore
"
774,"WEIGHT_PREPACK_OPS = {
torch._ops.ops.quantized.linear_prepack,
torch._ops.ops.quantized.linear_prepack_fp16,
torch._ops.ops.quantized.conv2d_prepack,
torch._ops.ops.quantized.conv3d_prepack,
}
","WEIGHT_PREPACK_OPS = {
torch._ops.ops.quantized.linear_prepack,
torch._ops.ops.quantized.linear_prepack_fp16,
    torch._ops.ops.quantized.conv1d_prepack,
torch._ops.ops.quantized.conv2d_prepack,
torch._ops.ops.quantized.conv3d_prepack,
}
"
775,"return quantizer.quantized_graph.create_node(
""call_function"", torch.nn.functional.relu, tuple(relu_args), relu_kwargs)
else:
                        return quantizer.quantized_graph.node_copy(node, load_arg(quantized=None))
@register_quant_pattern(torch.nn.BatchNorm2d)
@register_quant_pattern(torch.nn.BatchNorm3d)
","return quantizer.quantized_graph.create_node(
""call_function"", torch.nn.functional.relu, tuple(relu_args), relu_kwargs)
else:
                        return quantizer.quantized_graph.node_copy(node, load_arg(quantized=False))
@register_quant_pattern(torch.nn.BatchNorm2d)
@register_quant_pattern(torch.nn.BatchNorm3d)
"
776,"def id(self):
""""""
Returns internal identifier that torch.package uses to distinguish PackageImporter instances.
        Looks like:
<torch_package_0>
""""""
return self._mangler.parent_name()
","def id(self):
""""""
Returns internal identifier that torch.package uses to distinguish PackageImporter instances.
        Looks like::

<torch_package_0>
""""""
return self._mangler.parent_name()
"
777,"return self.profiler.export_stacks(path, metric)
def key_averages(self, group_by_input_shape: bool = False, group_by_stack_n: int = 0):
        """"""
        Averages events, grouping them by operator name and (optionally) input shapes and
stack.
        Note: to use shape/stack functionality make sure to set record_shapes/with_stack
        when creating profiler context manager.
""""""
assert self.profiler
return self.profiler.key_averages(group_by_input_shape, group_by_stack_n)
","return self.profiler.export_stacks(path, metric)
def key_averages(self, group_by_input_shape: bool = False, group_by_stack_n: int = 0):
        """"""Averages events, grouping them by operator name and (optionally) input shapes and
stack.

        .. note::
            To use shape/stack functionality make sure to set record_shapes/with_stack
            when creating profiler context manager.
""""""
assert self.profiler
return self.profiler.key_averages(group_by_input_shape, group_by_stack_n)
"
778,"observed_nodes: Set[Node] = set()
copy_nodes: Set[Node] = set()
non_tensor_input_binary_op_nodes: Set[Node] = set()
app_to_remove: Set[Node] = set()
env: Dict[Any, Any] = {}
","observed_nodes: Set[Node] = set()
copy_nodes: Set[Node] = set()
non_tensor_input_binary_op_nodes: Set[Node] = set()
    unmatched_nodes: Set[Node] = set()
app_to_remove: Set[Node] = set()
env: Dict[Any, Any] = {}
"
779,"# and all it's inputs.
if len(quant_uses) == 1:
quantized.graph.erase_node(node)
                            for arg in quant_args[1 :]:
                                quantized.graph.erase_node(arg)
return quantized
def convert(self, model: GraphModule, is_reference: bool = False,
","# and all it's inputs.
if len(quant_uses) == 1:
quantized.graph.erase_node(node)
                            for arg in quant_args[1:]:
                                if isinstance(arg, Node):
                                    quantized.graph.erase_node(arg)
return quantized
def convert(self, model: GraphModule, is_reference: bool = False,
"
780,"Node* unpack =
graph->insertNode(graph->createListUnpack(value_, *size_hint));
return fmap(unpack->outputs(), make_simple_value);
}
throw ErrorReport(loc) << value_->type()->repr_str()
<< "" cannot be used as a tuple"";
","Node* unpack =
graph->insertNode(graph->createListUnpack(value_, *size_hint));
return fmap(unpack->outputs(), make_simple_value);
  } else if (value_->type()->kind() == TypeKind::AnyTupleType) {
    throw ErrorReport(loc)
        << ""Provided tuple is not fully defined/refined including its element types, please provide a value of type like Tuple[int, int]"";
}
throw ErrorReport(loc) << value_->type()->repr_str()
<< "" cannot be used as a tuple"";
"
781,"""""""
_write_error(e, self._get_error_file_path())
    def copy_error_file(self, rootcause_error_file: str):
with open(rootcause_error_file, ""r"") as fp:
log.info(
f""child error file ({rootcause_error_file}) contents:\n""
                f""{json.dumps(json.load(fp), indent=2)}""
)
my_error_file = self._get_error_file_path()
","""""""
_write_error(e, self._get_error_file_path())

    def dump_error_file(self, rootcause_error_file: str, error_code: int = 0):
        """"""
        Dumps parent error file from child process's root cause error and error code.
        """"""
with open(rootcause_error_file, ""r"") as fp:
            rootcause_error = json.load(fp)
            # Override error code since the child process cannot capture the error code if it
            # is terminated by singals like SIGSEGV.
            if error_code:
                if ""message"" not in rootcause_error:
                    log.warning(
                        f""child error file ({rootcause_error_file}) does not have field `message`. \n""
                        f""cannot override error code: {error_code}""
                    )
                else:
                    rootcause_error[""message""][""errorCode""] = error_code

log.info(
f""child error file ({rootcause_error_file}) contents:\n""
                f""{json.dumps(rootcause_error, indent=2)}""
)
my_error_file = self._get_error_file_path()
"
782,":attr:`offsets`, if those are not None.
include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
        The last element is the size of the input, or the ending index position of the last bag (sequence).
Shape:

 :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)
          - If :attr:`input` is 2D of shape `(B, N)`,

            it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and
            this will return ``B`` values aggregated in a way depending on the :attr:`mode`.
            :attr:`offsets` is ignored and required to be ``None`` in this case.

          - If :attr:`input` is 1D of shape `(N)`,
            it will be treated as a concatenation of multiple bags (sequences).
            :attr:`offsets` is required to be a 1D tensor containing the
            starting index positions of each bag in :attr:`input`. Therefore,
            for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as
            having ``B`` bags. Empty bags (i.e., having 0-length) will have
            returned vectors filled by zeros.
        - :attr:`weight` (Tensor): the learnable weights of the module of
          shape `(num_embeddings, embedding_dim)`
        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as
          :attr:`input`.
 :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`
",":attr:`offsets`, if those are not None.
include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
            The last element is the size of the input, or the ending index position of the last bag (sequence).
Shape:
          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)
            each of fixed length ``N``, and this will return ``B`` values aggregated in a way
            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.
          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of
            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing
            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`
            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.
            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.
"
783,"assert node.inputsSize() == 2
assert node.outputsSize() == 1
        _, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
_, value = self.constants[node.inputsAt(1)]
res = in_oper.shape[value]
output = node.outputsAt(0)
","assert node.inputsSize() == 2
assert node.outputsSize() == 1
        _, in_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
_, value = self.constants[node.inputsAt(1)]
res = in_oper.shape[value]
output = node.outputsAt(0)
"
784,"size_t getDefaultNumThreads();
PThreadPool* pthreadpool() {
  static std::unique_ptr<PThreadPool> threadpool =
      std::make_unique<PThreadPool>(getDefaultNumThreads());
return threadpool.get();
}
","size_t getDefaultNumThreads();
PThreadPool* pthreadpool() {
  static auto threadpool =
    std::make_unique<PThreadPool>(getDefaultNumThreads());
#ifndef WIN32
  static std::once_flag flag;
  std::call_once(flag, []() {
    pthread_atfork(nullptr, nullptr, child_atfork);
  });
#endif
  auto true_bool = true;
  if (leak_corrupted_threadpool.compare_exchange_strong(true_bool, false)) {
    if (auto leaked = threadpool.release()) {
      auto num_threads = leaked->get_thread_count();
      threadpool.reset(new PThreadPool(num_threads));
      TORCH_WARN(""Leaking Caffe2 thread-pool after fork."");
    }
  }
return threadpool.get();
}
"
785,"def clip_grad_norm(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.,
        error_if_nonfinite: bool = True) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
.. warning::
","def clip_grad_norm(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.,
        error_if_nonfinite: bool = False) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
.. warning::
"
786,"const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
auto& the_template_ = unpack(the_template, ""the_template"", 1);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
","const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
  assert_no_inference_tensor(self);
auto& the_template_ = unpack(the_template, ""the_template"", 1);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
"
787,"return params;
}
const std::map<std::string, at::Tensor> Module::named_parameters() const {
std::map<std::string, at::Tensor> params;
const std::string name = """";
","return params;
}
// Returns a mapping for all attributes that requires_grad=True in a module.
// This behavior differs from full torch script modules. This is a bug,
// but currently there is no way to correctly label parameters in the
// loading of a mobile module. TODO
const std::map<std::string, at::Tensor> Module::named_parameters() const {
std::map<std::string, at::Tensor> params;
const std::string name = """";
"
788,"_python_cu = torch._C.CompilationUnit()
# qualified_name => ScriptClass mapping
_script_classes = {}
def _add_script_class(cls, name):
    global _script_classes
    _script_classes[name] = cls
def _get_script_class(name):
    global _script_classes
    if name not in _script_classes:
        return None
    return _script_classes[name]
# Caching: we currently cache compilation of free functions and overloaded functions.
","_python_cu = torch._C.CompilationUnit()
# python class => ScriptClass mapping
_script_classes = {}
_name_to_pyclass = {}
def _add_script_class(python_class, script_class):
    _script_classes[python_class] = script_class
    _name_to_pyclass[script_class.qualified_name()] = python_class

def _get_script_class(python_class):
    override = getattr(python_class, ""_jit_override_qualname"", None)
    if override is not None:
        python_class = _get_python_class(override)
    return _script_classes.get(python_class, None)


def _get_python_class(qualified_name):
    return _name_to_pyclass.get(qualified_name, None)


def _clear_class_state():
    _script_classes.clear()
    _name_to_pyclass.clear()
# Caching: we currently cache compilation of free functions and overloaded functions.
"
789,"if ann is type(None):
return NoneType.get()
if inspect.isclass(ann) and hasattr(ann, ""__torch_script_interface__""):
        return InterfaceType(_qualified_name(ann))
if ann is torch.device:
return DeviceObjType.get()
if ann is torch.Stream:
","if ann is type(None):
return NoneType.get()
if inspect.isclass(ann) and hasattr(ann, ""__torch_script_interface__""):
        return InterfaceType(ann.__torch_script_interface__)
if ann is torch.device:
return DeviceObjType.get()
if ann is torch.Stream:
"
790,"check_scalar_type_device_layout_equal(indices, at::empty({0}, self.options().dtype(at::kLong)));
{
NoNamesGuard guard;
    values.resize_(self.sizes());
    indices.resize_(self.sizes());
if(self.dim() == 0) {
values.fill_(self);
indices.fill_(0);
","check_scalar_type_device_layout_equal(indices, at::empty({0}, self.options().dtype(at::kLong)));
{
NoNamesGuard guard;
    resize_output(values, self.sizes());
    resize_output(indices, self.sizes());
if(self.dim() == 0) {
values.fill_(self);
indices.fill_(0);
"
791,"(void)_any_requires_grad;
"""""")
SETUP_ASSERT_NO_INFERENCE_TENSOR = CodeTemplate(""""""\
assert_no_inference_tensor( ${tensor_args} );
"""""")

SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
","(void)_any_requires_grad;
"""""")
SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
"
792,"if (grad_fn) {
msg = c10::str(""Output "", diff_view_meta->output_nr_, "" of "", grad_fn->name(), "" is a view and "",
modified_obj, "" modified inplace."");
    } else if (creation_meta == CreationMeta::INFERENCE_MODE) {
      msg = c10::str(""A view was created in inference mode and "", modified_obj, "" modified inplace in normal mode."");
} else {
msg = c10::str(""A view was created in no_grad mode and "", modified_obj, "" modified inplace with grad mode enabled."");
}
","if (grad_fn) {
msg = c10::str(""Output "", diff_view_meta->output_nr_, "" of "", grad_fn->name(), "" is a view and "",
modified_obj, "" modified inplace."");
} else {
msg = c10::str(""A view was created in no_grad mode and "", modified_obj, "" modified inplace with grad mode enabled."");
}
"
793,"return torch.tensor(0.)
device = parameters[0].grad.device
if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)
else:
total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
","return torch.tensor(0.)
device = parameters[0].grad.device
if norm_type == inf:
        norms = [p.grad.detach().abs().max().to(device) for p in parameters]
        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
else:
total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
    if error_if_nonfinite and (total_norm.isnan() or total_norm.isinf()):
        raise RuntimeError(
            f'The total norm of order {norm_type} for gradients from '
            '`parameters` is non-finite, so it cannot be clipped. To disable '
            'this error and scale the gradients by the non-finite norm anyway, '
            'set `error_if_nonfinite=False`')
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
"
794,"""""""
warnings.warn(""torch.nn.utils.clip_grad_norm is now deprecated in favor ""
""of torch.nn.utils.clip_grad_norm_."", stacklevel=2)
    return clip_grad_norm_(parameters, max_norm, norm_type)
def clip_grad_value_(parameters: _tensor_or_tensors, clip_value: float) -> None:
","""""""
warnings.warn(""torch.nn.utils.clip_grad_norm is now deprecated in favor ""
""of torch.nn.utils.clip_grad_norm_."", stacklevel=2)
    return clip_grad_norm_(parameters, max_norm, norm_type, error_if_nonfinite)
def clip_grad_value_(parameters: _tensor_or_tensors, clip_value: float) -> None:
"
795,"else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('at::GradMode::is_enabled() ? '
                             'CreationMeta::DEFAULT : '
                             'CreationMeta::NO_GRAD_MODE')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
","else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('InferenceMode::is_enabled() ? '
                             'CreationMeta::INFERENCE_MODE : '
                             '(at::GradMode::is_enabled() ? CreationMeta::DEFAULT : CreationMeta::NO_GRAD_MODE)')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
"
796,"return [SETUP_ANY_REQUIRES_GRAD.substitute(
args_with_derivatives=[arg.name for arg in args_with_derivatives]), ]
def emit_check_inplace() -> List[str]:
if not inplace:
return []
","return [SETUP_ANY_REQUIRES_GRAD.substitute(
args_with_derivatives=[arg.name for arg in args_with_derivatives]), ]
    def emit_assert_no_inference_tensor() -> List[str]:
        tensor_arg_names = []
        for arg in f.func.arguments.tensor_args:
            a = arg.argument if isinstance(arg, SelfArgument) else arg
            tensor_arg_names.append(a.name)
        return [SETUP_ASSERT_NO_INFERENCE_TENSOR.substitute(
            tensor_args=tensor_arg_names
        )]

def emit_check_inplace() -> List[str]:
if not inplace:
return []
"
797,"IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
}
","IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
  assert_no_inference_tensor(self);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
}
"
798,"const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
auto& the_template_ = unpack(the_template, ""the_template"", 1);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
","const Tensor& the_template,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
  assert_no_inference_tensor(self);
auto& the_template_ = unpack(the_template, ""the_template"", 1);
if (self.requires_grad()) {
AT_ERROR(""cannot resize variables that require grad"");
"
799,"# With the expanded context, do the impl call (if not a meta
# function)
            if self.dispatch_key == DispatchKey.DefaultBackend:
# TODO: https://github.com/pytorch/pytorch/issues/53023
out_sig_group = CppSignatureGroup.from_native_function(
self.g.out, method=False, fallback_binding=f.manual_cpp_binding)
","# With the expanded context, do the impl call (if not a meta
# function)
            if self.dispatch_key == DispatchKey.CompositeExplicitAutograd:
# TODO: https://github.com/pytorch/pytorch/issues/53023
out_sig_group = CppSignatureGroup.from_native_function(
self.g.out, method=False, fallback_binding=f.manual_cpp_binding)
"
800,"env[node.name] = result
continue
elif root_node is not None:
continue
# handle activation post process calls
","env[node.name] = result
continue
elif root_node is not None:
                if qconfig is None:
                    # This branch is hit if all of these conditions are met:
                    # 1. we are in a fusion pattern of multiple nodes (i.e. add-relu)
                    # 2. the current node is not the ""root_node"" of the pattern
                    # 3. quantization for this pattern is disabled
                    #
                    # In this case, we need to make sure to populate the env with
                    # intermediate nodes manually, because the QuantizeHandler.convert
                    # function will not be called.
                    result = self.quantized_graph.node_copy(
                        node, load_non_quantized)
                    env[node.name] = result
continue
# handle activation post process calls
"
801,"// ensures that profiling callbacks have ran. To ensure that this is
// transparent, we must make this future propagate the value of the RPC
// future.
        return fut->constValue();
};
// Define a future that completes after the profiling callbacks are run.
auto profiledFut = fut->then(at::wrapPropagateTLSState<c10::IValue>(
","// ensures that profiling callbacks have ran. To ensure that this is
// transparent, we must make this future propagate the value of the RPC
// future.
        // Use value() here instead of constValue() to ensure we propagate errors.
        return fut->value();
};
// Define a future that completes after the profiling callbacks are run.
auto profiledFut = fut->then(at::wrapPropagateTLSState<c10::IValue>(
"
802,"// all dimensions with 0 stride won't move. This is the same behavior as TensorIterator.
// eg. Given tensor with size/stride (6, 5, 4, 3, 2)/(6, 0, 120, 0, 1), the initial `perm`
//     is (4, 3, 2, 1, 0) and the sorted `perm` will be (4, 3, 0, 1, 2)
  for (int i = 1; i < ndim; ++i) {
    int dim1 = i;
    for (int dim0 = i - 1; dim0 >= 0; --dim0) {
int comparison = should_swap(perm[dim0], perm[dim1]);
if (comparison > 0) {
std::swap(perm[dim0], perm[dim1]);
","// all dimensions with 0 stride won't move. This is the same behavior as TensorIterator.
// eg. Given tensor with size/stride (6, 5, 4, 3, 2)/(6, 0, 120, 0, 1), the initial `perm`
//     is (4, 3, 2, 1, 0) and the sorted `perm` will be (4, 3, 0, 1, 2)
  for (const auto i : c10::irange(1, ndim)) {
    auto dim1 = i;
    for (const auto j : c10::irange(1, i + 1)) {
      auto dim0 = i - j;
int comparison = should_swap(perm[dim0], perm[dim1]);
if (comparison > 0) {
std::swap(perm[dim0], perm[dim1]);
"
803,"Tensor grad_input_n;
Tensor grad_output_n;
        int elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per sample:
","Tensor grad_input_n;
Tensor grad_output_n;
        int64_t elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per sample:
"
804,"scalar_t scale = static_cast<scalar_t>(scale_);
        int elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per output:
","scalar_t scale = static_cast<scalar_t>(scale_);
        int64_t elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per output:
"
805,"// decoding
uint64_t bit_start = 0;
const uint64_t segment_size = input_size - 10;
  for (int start = 0; start < output_size; start += segment_size) {
uint64_t stride = start + segment_size <= output_size ? segment_size
: output_size - start;
uint8_t mask = (1 << bitwidth) - 1;
    int i = 0;
// Can process 8 elements at a time because we need to expand uint8_t
// to int32_t to use epi32 vector instructions.
constexpr int VLEN = 8;
","// decoding
uint64_t bit_start = 0;
const uint64_t segment_size = input_size - 10;
  for (uint64_t start = 0; start < output_size; start += segment_size) {
uint64_t stride = start + segment_size <= output_size ? segment_size
: output_size - start;
uint8_t mask = (1 << bitwidth) - 1;
    uint64_t i = 0;
// Can process 8 elements at a time because we need to expand uint8_t
// to int32_t to use epi32 vector instructions.
constexpr int VLEN = 8;
"
806,"bool all_pads_non_positive = true;
auto c_input = self;
    for (int i = l_diff; i < l_inp; i++) {
auto pad_idx = 2 * (l_inp - i - 1);
if (pad[pad_idx] < 0) {
c_input = c_input.narrow(i, -pad[pad_idx], c_input.size(i) + pad[pad_idx]);
","bool all_pads_non_positive = true;
auto c_input = self;
    for (const auto i : c10::irange(l_diff, l_inp)) {
auto pad_idx = 2 * (l_inp - i - 1);
if (pad[pad_idx] < 0) {
c_input = c_input.narrow(i, -pad[pad_idx], c_input.size(i) + pad[pad_idx]);
"
807,"std::unique_ptr<index_t[]> counts;
if (scale_grad_by_freq) {
counts.reset(new index_t[num_weights]);
      for (int i = 0; i < numel; i++) {
counts[indices_data[i]] = 0;
}
      for (int i = 0; i < numel; i++) {
counts[indices_data[i]]++;
}
}
","std::unique_ptr<index_t[]> counts;
if (scale_grad_by_freq) {
counts.reset(new index_t[num_weights]);
      for (const auto i : c10::irange(numel)) {
counts[indices_data[i]] = 0;
}
      for (const auto i : c10::irange(numel)) {
counts[indices_data[i]]++;
}
}
"
808,"#include <ATen/native/CPUBlas.h>
#ifdef USE_FBGEMM
#include <fbgemm/Fbgemm.h>
#else
","#include <ATen/native/CPUBlas.h>
#include <c10/util/irange.h>

#ifdef USE_FBGEMM
#include <fbgemm/Fbgemm.h>
#else
"
809,"}
// Fill in the ellipsis dimensions
  for (auto tensor_idx = 0U; tensor_idx < tensor_dim; ++tensor_idx) {
if (order_has_tensor_name.test(tensor_idx)) {
continue;
}
","}
// Fill in the ellipsis dimensions
  for (const auto tensor_idx : c10::irange(tensor_dim)) {
if (order_has_tensor_name.test(tensor_idx)) {
continue;
}
"
810,"// This is the end of the pipeline, pass the resulting matrix through
fbgemm::DoNothing<float, float> kDoNothingObj{};
    for (int task_id = begin; task_id < end; ++task_id) {
// After the uint8 * int8 matrix multiplication is performed, this
// operation does:
//  1) Add in row and column offsets to the rows and columns, respectively
","// This is the end of the pipeline, pass the resulting matrix through
fbgemm::DoNothing<float, float> kDoNothingObj{};
    for (const auto task_id : c10::irange(begin, end)) {
// After the uint8 * int8 matrix multiplication is performed, this
// operation does:
//  1) Add in row and column offsets to the rows and columns, respectively
"
811,"const IndexType* indices,
int64_t n,
IndexType indexing_axis_dim) {
  for (auto i = 0; i < n; ++i) {
auto idx = indices[i];
TORCH_CHECK(
0 <= idx && idx < indexing_axis_dim,
","const IndexType* indices,
int64_t n,
IndexType indexing_axis_dim) {
  for (const auto i : c10::irange(n)) {
auto idx = indices[i];
TORCH_CHECK(
0 <= idx && idx < indexing_axis_dim,
"
812,"#include <torch/custom_class.h>
#include <algorithm>
#include <string>
","#include <torch/custom_class.h>
#include <c10/util/irange.h>

#include <algorithm>
#include <string>
"
813,"#include <ATen/native/cpu/Loops.h>
#include <ATen/native/quantized/cpu/quantized_ops.h>
#include <ATen/quantized/Quantizer.h>
#include <cstring>
","#include <ATen/native/cpu/Loops.h>
#include <ATen/native/quantized/cpu/quantized_ops.h>
#include <ATen/quantized/Quantizer.h>

#include <c10/util/irange.h>

#include <cstring>
"
814,"formulate_greedy_allocation_plan(
allocation_plan_->allocation_sizes, allocation_plan_->allocation_lifetimes);
allocation_plan_->total_size = 0;
  for (auto i = 0; i < allocation_plan_->allocation_sizes.size(); ++i) {
if (allocation_plan_->allocation_lifetimes[i] ==
std::numeric_limits<uint64_t>::max()) {
continue;
","formulate_greedy_allocation_plan(
allocation_plan_->allocation_sizes, allocation_plan_->allocation_lifetimes);
allocation_plan_->total_size = 0;
  for (const auto i : c10::irange(allocation_plan_->allocation_sizes.size())) {
if (allocation_plan_->allocation_lifetimes[i] ==
std::numeric_limits<uint64_t>::max()) {
continue;
"
815,"proto.mutable_string_data()->Reserve(chunkSize);
if (chunkSize > 0) {
const char* raw_data = static_cast<const char*>(input.raw_data());
        for (int i = chunkBegin; i < chunkBegin + chunkSize; ++i) {
proto.add_string_data(SerializeBlob(
raw_data + i * input.itemsize(), input.dtype(), """"));
}
","proto.mutable_string_data()->Reserve(chunkSize);
if (chunkSize > 0) {
const char* raw_data = static_cast<const char*>(input.raw_data());
        for (const auto i : c10::irange(chunkBegin, chunkBegin + chunkSize)) {
proto.add_string_data(SerializeBlob(
raw_data + i * input.itemsize(), input.dtype(), """"));
}
"
816,"#include <cmath>
#include <cstdint>
using std::uint64_t;
using std::uint8_t;
","#include <cmath>
#include <cstdint>
#include <c10/util/irange.h>

using std::uint64_t;
using std::uint8_t;
"
817,"DCHECK(canRead());
auto& result = queue_[reader_ % queue_.size()];
CAFFE_ENFORCE(inputs.size() >= result.size());
  for (auto i = 0; i < result.size(); ++i) {
auto bytes = BlobStat::sizeBytes(*result[i]);
CAFFE_EVENT(stats_, queue_dequeued_bytes, bytes, i);
using std::swap;
","DCHECK(canRead());
auto& result = queue_[reader_ % queue_.size()];
CAFFE_ENFORCE(inputs.size() >= result.size());
  for (const auto i : c10::irange(result.size())) {
auto bytes = BlobStat::sizeBytes(*result[i]);
CAFFE_EVENT(stats_, queue_dequeued_bytes, bytes, i);
using std::swap;
"
818,"return native::add_sparse_(self, other, -alpha);
}
Tensor& sub_out_sparse(Tensor& r, const Tensor& self, const Tensor& other, const Scalar& alpha) {
sub_check(self, other);
return at::add_out(r, self, other, -alpha);  // redispatch!
}
","return native::add_sparse_(self, other, -alpha);
}
Tensor& sub_out_sparse(const Tensor& self, const Tensor& other, const Scalar& alpha, Tensor& r) {
sub_check(self, other);
return at::add_out(r, self, other, -alpha);  // redispatch!
}
"
819,"return backendToDispatchKey(backendToBackendOfDeviceType(dispatchKeyToBackend(dispatch_key), device_type));
}
// NB: device_idx here is NOT a DeviceIndex, but index into PythonArgs
c10::DispatchKey denseTypeIdWithDefault(PythonArgs& r, int64_t device_idx, c10::DispatchKey dispatch_key) {
  auto device_type = r.isNone(device_idx) ? computeDeviceType(dispatch_key) : r.device(device_idx).type();
  return backendToDispatchKey(toDense(backendToBackendOfDeviceType(dispatchKeyToBackend(dispatch_key), device_type)));
}
} // namespace
Tensor legacy_tensor_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
","return backendToDispatchKey(backendToBackendOfDeviceType(dispatchKeyToBackend(dispatch_key), device_type));
}
} // namespace
Tensor legacy_tensor_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
"
820,"static inline void check_cat_shape_except_dim(const Tensor & first, const Tensor & second, int64_t dimension, int64_t index) {
int64_t first_dims = first.dim();
int64_t second_dims = second.dim();
  TORCH_CHECK(first_dims == second_dims, ""Tensors must have same number of dimensions: got "",
first_dims, "" and "", second_dims);
for (int64_t dim = 0; dim < first_dims; dim++) {
if (dim == dimension) {
","static inline void check_cat_shape_except_dim(const Tensor & first, const Tensor & second, int64_t dimension, int64_t index) {
int64_t first_dims = first.dim();
int64_t second_dims = second.dim();
  TORCH_CHECK(first_dims == second_dims, ""torch.cat(): Tensors must have same number of dimensions: got "",
first_dims, "" and "", second_dims);
for (int64_t dim = 0; dim < first_dims; dim++) {
if (dim == dimension) {
"
821,"# this is an indicator of whether all the inputs are Node or not
# since some op might be quantized differently depending on whether
# all inputs are tensors or not, e.g. add/mul
        self.num_node_args = len(node.args)
        self.all_node_args = True
@abstractmethod
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
","# this is an indicator of whether all the inputs are Node or not
# since some op might be quantized differently depending on whether
# all inputs are tensors or not, e.g. add/mul
        self.num_tensor_args = len(node.args)
        self.all_node_args_are_tensors = True
@abstractmethod
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
"
822,"def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
        if not self.all_node_args:
return NotImplemented
assert node.op in ['call_module', 'call_function'], 'Only call_module and ' + \
'call_function are handled in DefaultNode'
","def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
        if not self.all_node_args_are_tensors:
return NotImplemented
assert node.op in ['call_module', 'call_function'], 'Only call_module and ' + \
'call_function are handled in DefaultNode'
"
823,"activation_is_statically_quantized(qconfig)) or
(is_weight and weight_is_quantized(qconfig)) or
(is_bias and activation_dtype(qconfig) == torch.float16)
                    and weight_dtype(qconfig) == torch.float16)
if should_add_handler:
act_post_process_ctr = qconfig.weight if is_weight else \
","activation_is_statically_quantized(qconfig)) or
(is_weight and weight_is_quantized(qconfig)) or
(is_bias and activation_dtype(qconfig) == torch.float16)
                    and weight_dtype(qconfig) == torch.float16) and \
                    (not no_tensors)
if should_add_handler:
act_post_process_ctr = qconfig.weight if is_weight else \
"
824,"else:
padding_idx = -1
if max_norm is not None:
# `embedding_renorm_` will call .contiguous() on input anyways, so we
# call it here and take advantage of the improved locality in the
# `embedding` call below too.
input = input.contiguous()
# XXX: equivalent to
# with torch.no_grad():
        #   torch.nembedding_renorm_
# remove once script supports set_grad_enabled
_no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
","else:
padding_idx = -1
if max_norm is not None:
        # Note [embedding_renorm contiguous]
# `embedding_renorm_` will call .contiguous() on input anyways, so we
# call it here and take advantage of the improved locality in the
# `embedding` call below too.
input = input.contiguous()
        # Note [embedding_renorm set_grad_enabled]
# XXX: equivalent to
# with torch.no_grad():
        #   torch.embedding_renorm_
# remove once script supports set_grad_enabled
_no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
"
825,"});
m.def(
""static_runtime::permute_copy(Tensor self, int[] dims) -> Tensor"",
      [](at::Tensor self, ArrayRef<int64_t> dims) -> at::Tensor {
at::Tensor out = at::empty_like(self);
at::native::copy_(out, self);
return out.permute(dims);
});
m.def(
      ""static_runtime::to_copy(Tensor self, ScalarType dtype, bool non_blocking, bool copy) -> Tensor"",
      [](at::Tensor self, at::ScalarType dtype, bool non_blocking, bool copy)
          -> at::Tensor {
at::Tensor out = at::empty_like(self);
at::native::copy_(out, self);
        return out.to(dtype, non_blocking, copy);
});
}
","});
m.def(
""static_runtime::permute_copy(Tensor self, int[] dims) -> Tensor"",
      [](const at::Tensor& self, ArrayRef<int64_t> dims) -> at::Tensor {
at::Tensor out = at::empty_like(self);
at::native::copy_(out, self);
return out.permute(dims);
});
m.def(
      ""static_runtime::to_copy(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor"",
      [](const at::Tensor& self,
         at::ScalarType dtype,
         bool non_blocking,
         bool copy,
         c10::optional<c10::MemoryFormat> format) -> at::Tensor {
at::Tensor out = at::empty_like(self);
at::native::copy_(out, self);
        return out.to(dtype, non_blocking, copy, format);
});
}
"
826,"// Parameters may have been unused in forward pass, or not all outputs
// were used in producing loss.
kBaseErrorMsg += ""You can enable unused parameter detection by passing the ""
      ""keyword argument `find_unused_parameters=True` to ""
      ""`torch.nn.parallel.DistributedDataParallel`, and by \n"";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
} else {
","// Parameters may have been unused in forward pass, or not all outputs
// were used in producing loss.
kBaseErrorMsg += ""You can enable unused parameter detection by passing the ""
          ""keyword argument `find_unused_parameters=True` to ""
          ""`torch.nn.parallel.DistributedDataParallel`, and by \n"";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
} else {
"
827,"// since user may have enabled detection but this particular iteration
// could have used or not used all parameters.
kBaseErrorMsg += ""Since `find_unused_parameters=True` is enabled, this likely ""
      "" means that not all `forward` outputs participate in computing loss. You can fix this by "";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
}
     TORCH_CHECK(false, kBaseErrorMsg);
}
}
","// since user may have enabled detection but this particular iteration
// could have used or not used all parameters.
kBaseErrorMsg += ""Since `find_unused_parameters=True` is enabled, this likely ""
          "" means that not all `forward` outputs participate in computing loss. You can fix this by "";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
}
    TORCH_CHECK(false, kBaseErrorMsg);
}
}
"
828,".copy_(copy_param))
copy_param.requires_grad = param.requires_grad
else:
            self._module_copies = [self.module]
        self.modules_params = [list(parameters(m)) for m in self._module_copies]
        # Collect buffers for modules, filtering out buffers that should be ignored.
        named_module_buffers = [
            [(buffer, buffer_name) for buffer_name, buffer in m.named_buffers()]
            for m in self._module_copies
        ]
        self.modules_buffers = [
            [
                buffer
                for (buffer, buffer_name) in module_buffers
                if buffer_name not in self.parameters_to_ignore
            ]
            for module_buffers in named_module_buffers
        ]
# Build tuple of (module, parameter) for all parameters that require grads.
if self.device_ids and len(self.device_ids) > 1:
# Single-process multi-device mode,does not support self.parameters_to_ignore.
",".copy_(copy_param))
copy_param.requires_grad = param.requires_grad
            return module_copies

else:
            return [self.module]
    def _build_params_for_reducer(self):
# Build tuple of (module, parameter) for all parameters that require grads.
if self.device_ids and len(self.device_ids) > 1:
# Single-process multi-device mode,does not support self.parameters_to_ignore.
"
829,"} break;
case WARN: {
drop(stack, 1);
        TORCH_WARN(pop(stack).toStringRef());
} break;
default:
","} break;
case WARN: {
drop(stack, 1);
        // Note: Please don't move the pop(stack) code below into the TORCH_WARN
        // macro since TORCH_WARN fails to evaluate its arguments when
        // STRIP_ERROR_MESSAGES is defined (which happens for production
        // mobile builds). This will cause the stack to be in an inconsistent
        // state. It has previously resulted in a SEV (S22350).
        auto sref = pop(stack).toStringRef();
        TORCH_WARN(sref);
+pc;
} break;
default:
"
830,"common_device = op.tensor.device();
}
    // Determines if there are varying input dtypes
    // NOTE: the common dtype is set to the first defined input dtype observed
    if (!op.is_output && op.target_dtype != common_dtype_) {
      if (common_dtype_ == ScalarType::Undefined) {
        common_dtype_ = op.target_dtype;
      } else {
        has_different_input_dtypes = true;
}
    } else if (op.is_output && op.target_dtype != common_dtype_) {
      if (output_dtype == ScalarType::Undefined) {
        output_dtype = op.target_dtype;
      } else {
        has_different_output_dtypes = true;
}
}
}
","common_device = op.tensor.device();
}
    if (!op.is_output) {
      // Determines if there are varying input dtypes
      // NOTE: the common dtype is set to the first defined input dtype observed
      if (op.target_dtype != common_dtype_) {
        if (common_dtype_ == ScalarType::Undefined) {
          common_dtype_ = op.target_dtype;
        } else {
          has_different_input_dtypes = true;
        }
}
    } else {  // op.is_output
      // Determines if there are varying output dtypes
      // NOTE: the output dtype is set to the first defined output dtype observed
      if (op.target_dtype != output_dtype) {
        if (output_dtype == ScalarType::Undefined) {
          output_dtype = op.target_dtype;
        } else {
          has_different_output_dtypes = true;
        }
}
}
}
"
831,"to.image = bundle.image;
}
#ifdef DEBUG
  // Forward declaration
  std::ostream& operator<<(
      std::ostream&,
      const View::State::Bundle&);

std::cout << ""From:"" << std::endl << from << std::endl;
std::cout << ""To:"" << std::endl << to << std::endl;
#endif /* DEBUG */
return Transition{
from,
","to.image = bundle.image;
}
#ifdef VULKAN_TENSOR_DEBUG
std::cout << ""From:"" << std::endl << from << std::endl;
std::cout << ""To:"" << std::endl << to << std::endl;
#endif /* VULKAN_TENSOR_DEBUG */
return Transition{
from,
"
832,"// Debug
//
namespace {
// Considering that VkAccessFlags is a weak typedef of a built-in data type, we
","// Debug
//
#ifdef VULKAN_TENSOR_DEBUG

namespace {
// Considering that VkAccessFlags is a weak typedef of a built-in data type, we
"
833,"world_size (int, optional): The total number of store users (number of clients + 1 for the server). Default is -1 (a negative value indicates an non-fixed number of store users).
is_master (bool, optional): True when initializing the server store and False for client stores. Default is False.
timeout (timedelta, optional): Timeout used by the store during initialization and for methods such as :meth:`~torch.distributed.store.get` and :meth:`~torch.distributed.store.wait`. Default is timedelta(seconds=300)
Example::
>>> import torch.distributed as dist
","world_size (int, optional): The total number of store users (number of clients + 1 for the server). Default is -1 (a negative value indicates an non-fixed number of store users).
is_master (bool, optional): True when initializing the server store and False for client stores. Default is False.
timeout (timedelta, optional): Timeout used by the store during initialization and for methods such as :meth:`~torch.distributed.store.get` and :meth:`~torch.distributed.store.wait`. Default is timedelta(seconds=300)
    wait_for_worker (bool, optional): Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.
Example::
>>> import torch.distributed as dist
"
834,"T_co = TypeVar('T_co', covariant=True)
class FilterIterDataPipe(MapIterDataPipe[T_co]):
r"""""" :class:`FilterIterDataPipe`.
","T_co = TypeVar('T_co', covariant=True)
@functional_datapipe('filter')
class FilterIterDataPipe(MapIterDataPipe[T_co]):
r"""""" :class:`FilterIterDataPipe`.
"
835,"except ImportError:
_GLOO_AVAILABLE = False

logger = logging.getLogger(__name__)


# Some reduce ops are not supported by complex numbers and will result in an error.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
","except ImportError:
_GLOO_AVAILABLE = False
# Some reduce ops are not supported by complex numbers and will result in an error.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
"
836,"""""""
store_key = ""{}:{}"".format(STORE_BASED_BARRIER_PREFIX, _group_count)
store.add(store_key, 1)
    logger.info('Added key: {} to store for rank: {}'.format(store_key, rank))
# Now wait for all workers to check in with the store.
world_size = get_world_size()
","""""""
store_key = ""{}:{}"".format(STORE_BASED_BARRIER_PREFIX, _group_count)
store.add(store_key, 1)
    logging.info('Added key: {} to store for rank: {}'.format(store_key, rank))
# Now wait for all workers to check in with the store.
world_size = get_world_size()
"
837,"with open(filename, 'rb') as f:
b = f.read()
return b.decode('utf-8')

GlobPattern = Union[str, Iterable[str]]


class _GlobGroup:
    def __init__(self, include: 'GlobPattern', exclude: 'GlobPattern'):
        self._dbg = f'_GlobGroup(include={include}, exclude={exclude})'
        self.include = _GlobGroup._glob_list(include)
        self.exclude = _GlobGroup._glob_list(exclude)

    def __str__(self):
        return self._dbg

    def matches(self, candidate: str) -> bool:
        candidate = '.' + candidate
        return any(p.fullmatch(candidate) for p in self.include) and all(not p.fullmatch(candidate) for p in self.exclude)

    @staticmethod
    def _glob_list(elems: 'GlobPattern'):
        if isinstance(elems, str):
            return [_GlobGroup._glob_to_re(elems)]
        else:
            return [_GlobGroup._glob_to_re(e) for e in elems]

    @staticmethod
    def _glob_to_re(pattern: str):
        # to avoid corner cases for the first component, we prefix the candidate string
        # with '.' so `import torch` will regex against `.torch`
        def component_to_re(component):
            if '**' in component:
                if component == '**':
                    return '(\\.[^.]+)*'
                else:
                    raise ValueError('** can only appear as an entire path segment')
            else:
                return '\\.' + '[^.]*'.join(re.escape(x) for x in component.split('*'))

        result = ''.join(component_to_re(c) for c in pattern.split('.'))
        return re.compile(result)
","with open(filename, 'rb') as f:
b = f.read()
return b.decode('utf-8')
"
838,"int64_t quant_min,
int64_t quant_max) {
// This assumes the per channel zero point vector is single-dimensioned.
  for (int i = 0; i < zero_point.sizes()[0]; ++i) {
    zero_point[i] = static_cast<int64_t>(zero_point[i].item<float>() + 0.5);
  }
  return zero_point.clamp(quant_min, quant_max).to(at::kFloat);
}
Tensor _fake_quantize_learnable_per_channel_affine(
","int64_t quant_min,
int64_t quant_max) {
// This assumes the per channel zero point vector is single-dimensioned.
  return zero_point.round().clamp_(quant_min, quant_max);
}
Tensor _fake_quantize_learnable_per_channel_affine(
"
839,"0 & \text{ else }
\end{cases}
*/
TORCH_CHECK(dY.scalar_type() == ScalarType::Float);
TORCH_CHECK(X.scalar_type() == ScalarType::Float);
TORCH_CHECK(scale.scalar_type() == ScalarType::Float);
","0 & \text{ else }
\end{cases}
*/
  auto zero_point_rounded = _get_rounded_zero_point(zero_point, quant_min, quant_max);

TORCH_CHECK(dY.scalar_type() == ScalarType::Float);
TORCH_CHECK(X.scalar_type() == ScalarType::Float);
TORCH_CHECK(scale.scalar_type() == ScalarType::Float);
"
840,"""dimensions of scale and zero-point are not consistent with input tensor"")
TORCH_CHECK(
      at::min(zero_point).item().toLong() >= quant_min &&
          at::max(zero_point).item().toLong() <= quant_max,
""`zero_point` must be between `quant_min` and `quant_max`."");
TORCH_CHECK(
","""dimensions of scale and zero-point are not consistent with input tensor"")
TORCH_CHECK(
      at::min(zero_point_rounded).item().toLong() >= quant_min &&
          at::max(zero_point_rounded).item().toLong() <= quant_max,
""`zero_point` must be between `quant_min` and `quant_max`."");
TORCH_CHECK(
"
841,"}
auto X_shape = X.sizes();
auto scale_vectorized = scale.reshape(at::IntArrayRef(axis_mask, numDimensions)).expand(X_shape);
  auto zero_point_vectorized = zero_point.reshape(at::IntArrayRef(axis_mask, numDimensions)).expand(X_shape);
auto iter = TensorIteratorConfig()
.add_output(dX)
","}
auto X_shape = X.sizes();
auto scale_vectorized = scale.reshape(at::IntArrayRef(axis_mask, numDimensions)).expand(X_shape);
  auto zero_point_vectorized = zero_point_rounded.reshape(at::IntArrayRef(axis_mask, numDimensions)).expand(X_shape);
auto iter = TensorIteratorConfig()
.add_output(dX)
"
842,"name_base = name.substr(0, last_dot_pos);
}
}
std::string replacement_name;
do {
std::stringstream ss;
ss << name_base << ""."" << suffix++;
replacement_name = ss.str();
} while (names.count(replacement_name) > 0);
old_owner_of_name->second->setDebugName(replacement_name);
}
","name_base = name.substr(0, last_dot_pos);
}
}

    auto& names_suffixes = node()->owningGraph()->name_base_suffix_;
    auto it = names_suffixes.find(name_base);
    if (it != names_suffixes.end()) {
      suffix = std::max(suffix, it->second + 1);
    }

    // Verify that new name is not used and find next usable name in case
    // suffix is used.
std::string replacement_name;
do {
std::stringstream ss;
ss << name_base << ""."" << suffix++;
replacement_name = ss.str();
} while (names.count(replacement_name) > 0);

    names_suffixes[name_base] = suffix;

old_owner_of_name->second->setDebugName(replacement_name);
}
"
843,"@abstractmethod
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                debug: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
"""""" Convert the given node to a quantized node and insert
it to the quantized graph
","@abstractmethod
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
"""""" Convert the given node to a quantized node and insert
it to the quantized graph
"
844,"self.linear = quantizer.modules[self.linear_node.target]
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                debug: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
# Supported combinations are:
# quant_type | activation (compute_type) | weight
","self.linear = quantizer.modules[self.linear_node.target]
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
# Supported combinations are:
# quant_type | activation (compute_type) | weight
"
845,"(load_arg(quantized=activation_statically_quantized)(self.linear_node.args[0]),), {})
else:  # call_function
assert self.linear_node.op == 'call_function'
            if debug:
quantized_input_idxs = []
if activation_statically_quantized:
quantized_input_idxs.append(0)
","(load_arg(quantized=activation_statically_quantized)(self.linear_node.args[0]),), {})
else:  # call_function
assert self.linear_node.op == 'call_function'
            if is_reference:
quantized_input_idxs = []
if activation_statically_quantized:
quantized_input_idxs.append(0)
"
846,"@register_quant_pattern(torch.nn.functional.elu)
class ELU(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                debug: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
","@register_quant_pattern(torch.nn.functional.elu)
class ELU(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
"
847,"}
static void conj_kernel(TensorIterator& iter) {
  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""conj_cpu"", [&]() {
    cpu_kernel_vec(
        iter,
        [=](scalar_t a) -> scalar_t { return conj_impl(a); },
        [=](Vec256<scalar_t> a) { return a.conj(); });
  });
}
static void bitwise_not_kernel(TensorIterator& iter) {
","}
static void conj_kernel(TensorIterator& iter) {
  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
      kBool, kBFloat16, kHalf, iter.common_dtype(), ""conj_cpu"", [&]() {
        cpu_kernel_vec(
            iter,
            [=](scalar_t a) -> scalar_t { return conj_impl(a); },
            [=](Vec256<scalar_t> a) { return a.conj(); });
      });
}
static void bitwise_not_kernel(TensorIterator& iter) {
"
848,"return self.is_quantized();
}
#define DEFINE_CAST(T, name)                     \
  template <>                                    \
  TORCH_API T* Tensor::data_ptr() const {           \
    TORCH_CHECK(                                 \
        scalar_type() == ScalarType::name,       \
        ""expected scalar type "",                 \
        #name,                                   \
        "" but found "",                           \
        c10::toString(scalar_type()));           \
return static_cast<T*>(this->unsafeGetTensorImpl()->data());    \
}
","return self.is_quantized();
}
#define DEFINE_CAST(T, name)                                        \
  template <>                                                       \
  TORCH_API T* Tensor::data_ptr() const {                           \
    TORCH_CHECK(                                                    \
        scalar_type() == ScalarType::name,                          \
        ""expected scalar type ""                                     \
        #name                                                       \
        "" but found "",                                              \
        scalar_type());                                             \
return static_cast<T*>(this->unsafeGetTensorImpl()->data());    \
}
"
849,"import collections
try:
import numpy as np
except ModuleNotFoundError:
    np = None
from torch._six import string_classes
from .common import amp_definitely_not_available
","import collections
try:
import numpy as np
    HAS_NUMPY = True
except ModuleNotFoundError:
    HAS_NUMPY = False
from torch._six import string_classes
from .common import amp_definitely_not_available
"
850,"clean_ctx: GeneratedFileCleaner,
show_progress: bool) -> HipifyResult:
"""""" Executes the CUDA -> HIP conversion on the specified file. """"""
    fin_path = os.path.join(output_directory, filepath)
with open(fin_path, 'r', encoding='utf-8') as fin:
if fin.readline() == HIPIFY_C_BREADCRUMB:
","clean_ctx: GeneratedFileCleaner,
show_progress: bool) -> HipifyResult:
"""""" Executes the CUDA -> HIP conversion on the specified file. """"""
    fin_path = os.path.abspath(os.path.join(output_directory, filepath))
with open(fin_path, 'r', encoding='utf-8') as fin:
if fin.readline() == HIPIFY_C_BREADCRUMB:
"
851,"prev_lower = c.islower()
return ''.join(chars)
def get_qualified_name(func: Callable[..., Any]) -> str:
    # things like getattr just appear in builtins
    if getattr(builtins, func.__name__, None) is func:
        return func.__name__
    name = func.__name__
    module = _find_module_of_method(func)
    module = module.replace('torch._ops', 'torch.ops')  # WAR for bug in how torch.ops assigns module
    return f'{module}.{name}'

# this is fixed on master, WAR for 1.5
def _find_module_of_method(orig_method: Callable[..., Any]) -> str:
    name = orig_method.__name__
    module = orig_method.__module__
    if module is not None:
        return module
    for guess in [torch, torch.nn.functional]:
        if getattr(guess, name, None) is orig_method:
            return guess.__name__
    raise RuntimeError(f'cannot find module for {orig_method}')

def _format_args(args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) -> str:
args_s = ', '.join(repr(a) for a in args)
kwargs_s = ', '.join(f'{k} = {repr(v)}' for k, v in kwargs.items())
","prev_lower = c.islower()
return ''.join(chars)
def _format_args(args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) -> str:
args_s = ', '.join(repr(a) for a in args)
kwargs_s = ', '.join(f'{k} = {repr(v)}' for k, v in kwargs.items())
"
852,"import torch.autograd.profiler as prof
from torch.autograd import ProfilerActivity
","import torch
import torch.autograd.profiler as prof
from torch.autograd import ProfilerActivity
"
853,"profile_memory: bool = False,
with_stack: bool = False,
# deprecated:
            use_gpu: Optional[bool] = None):
if activities:
            self.activities = activities
else:
            if use_gpu is not None:
                warn(""use_gpu is deprecated, use activities argument instead"")
                self.activities = set([ProfilerActivity.CPU])
                if use_gpu:
                    self.activities.add(ProfilerActivity.CUDA)
            else:
                raise RuntimeError(""Profiler activities are not specified"")
if schedule:
self.schedule = schedule
","profile_memory: bool = False,
with_stack: bool = False,
# deprecated:
            use_cuda: Optional[bool] = None):
if activities:
            self.activities = set(activities)
else:
            self.activities = set([ProfilerActivity.CPU])
            if torch.cuda.is_available():
                self.activities.add(ProfilerActivity.CUDA)

        if use_cuda is not None:
            warn(""use_cuda is deprecated, use activities argument instead"")
            if use_cuda:
                self.activities.add(ProfilerActivity.CUDA)
            elif ProfilerActivity.CUDA in self.activities:
                self.activities.remove(ProfilerActivity.CUDA)

        assert len(self.activities) > 0, ""No profiler activities specified""
        assert (ProfilerActivity.CUDA not in self.activities) or torch.cuda.is_available(), \
            ""CUDA activity specified, but CUDA is not available""
if schedule:
self.schedule = schedule
"
854,"'suffix': '_args_sizes',
'type': 'std::vector<std::vector<int64_t>>',
}),
# replace TensorGeometry(self) with self_geometry
(r'TensorGeometry\({}\)', {
'suffix': '_geometry',
","'suffix': '_args_sizes',
'type': 'std::vector<std::vector<int64_t>>',
}),
        # replace to_args_scalartypes(self) with self_args_scalartypes
        (r'to_args_scalartypes\({}\)', {
            'suffix': '_args_scalartypes',
            'type': 'std::vector<ScalarType>',
        }),
# replace TensorGeometry(self) with self_geometry
(r'TensorGeometry\({}\)', {
'suffix': '_geometry',
"
855,"return Module(owner_);
}
void Method::run(Stack& stack) {
  stack.insert(stack.begin(), owner()._ivalue());
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
function_->run(stack);
}
IValue Method::operator()(std::vector<IValue> stack, const Kwargs& kwargs) {
  stack.insert(stack.begin(), owner()._ivalue());
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
return (*function_)(std::move(stack), kwargs);
}
","return Module(owner_);
}
void Method::run(Stack& stack) {
  stack.insert(stack.begin(), owner()._ivalue()); // self
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
function_->run(stack);
}
IValue Method::operator()(std::vector<IValue> stack, const Kwargs& kwargs) {
  stack.insert(stack.begin(), owner()._ivalue()); // self
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
return (*function_)(std::move(stack), kwargs);
}
"
856,"function->set_register_size(register_size);
mcu.register_function(std::move(function));
}
}
// The deserializer class which loads the bytecode package from bc files.
class BytecodeDeserializer final {
 public:
  explicit BytecodeDeserializer(std::unique_ptr<PyTorchStreamReader> reader);
  mobile::Module deserialize(c10::optional<at::Device> device);
  mobile::Module deserialize(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);
  std::unordered_map<std::string, std::string> deserializeMetadata(
      c10::optional<at::Device> device);
  void deserialize_only_extra(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);

 private:
  c10::IValue readArchive(
      const std::string& archive_name,
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::unordered_map<std::string, std::string> readMobileMetadata(
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::shared_ptr<CompilationUnit> compilation_unit_;
  std::unordered_set<std::string> imported_libs_;
  std::unique_ptr<PyTorchStreamReader> reader_;
  c10::optional<at::Device> device_;
};

BytecodeDeserializer::BytecodeDeserializer(
    std::unique_ptr<PyTorchStreamReader> reader)
    : compilation_unit_(std::make_shared<CompilationUnit>()),
      reader_(std::move(reader)) {}

std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
","function->set_register_size(register_size);
    // function schema
    if (schemaTable) { // (schema is optional for back compat)
      auto parseArgList = [this](const std::vector<IValue>& argTables) {
        std::vector<c10::Argument> args;
        for (auto&& argTable : argTables) {
          auto name =
              expect_field(argTable, ""name"", BYTECODE_INDEX_ARGUMENT_NAME)
                  .toStringRef();
          const auto& type = resolveTypeName(
              (expect_field(argTable, ""type"", BYTECODE_INDEX_ARGUMENT_TYPE))
                  .toStringRef());
          auto default_value = expect_field(
                                   argTable,
                                   ""default_value"",
                                   BYTECODE_INDEX_ARGUMENT_DEFAULT_VALUE)
                                   .toIValue();
          auto arg =
              c10::Argument(name, type, c10::nullopt /*N*/, default_value);
          args.emplace_back(std::move(arg));
        }
        return args;
      };
      const auto& arg_list =
          expect_field(
              *schemaTable, ""arguments"", BYTECODE_INDEX_SCHEMA_ARGUMENTS)
              .toTuple()
              ->elements();
      const auto& ret_list =
          expect_field(*schemaTable, ""returns"", BYTECODE_INDEX_SCHEMA_RETURNS)
              .toTuple()
              ->elements();
      c10::FunctionSchema schema(
          function_name,
          """" /*overload_name*/,
          parseArgList(arg_list),
          parseArgList(ret_list),
          false /*is_varargs*/,
          false /*is_varret*/);
      function->setSchema(std::move(schema));
    }

mcu.register_function(std::move(function));
}
}
std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
"
857,"at::DebugInfoGuard guard(at::DebugInfoKind::MOBILE_RUNTIME_INFO, debug_info);
try {
    stack.insert(stack.begin(), owner_->_ivalue());
function_->run(stack);
if (observer) {
observer->onExitRunMethod(instance_key);
","at::DebugInfoGuard guard(at::DebugInfoKind::MOBILE_RUNTIME_INFO, debug_info);
try {
    stack.insert(stack.begin(), owner_->_ivalue()); // self
function_->run(stack);
if (observer) {
observer->onExitRunMethod(instance_key);
"
858,"import torch.distributed as dist
def allreduce_hook(
process_group: dist.ProcessGroup, bucket: dist._GradBucket
) -> torch.futures.Future:
","import torch.distributed as dist
def _allreduce_fut(
    process_group: dist.ProcessGroup, tensor: torch.Tensor
) -> torch.futures.Future:
    group_to_use = process_group if process_group is not None else dist.group.WORLD

    ""Averages the input gradient tensor by allreduce and returns a future.""
    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()

    def div_by_group_size(fut):
        return [fut.value()[0].div_(group_to_use.size())]

    return fut.then(div_by_group_size)


def allreduce_hook(
process_group: dist.ProcessGroup, bucket: dist._GradBucket
) -> torch.futures.Future:
"
859,"r""""""Context-manager that enable anomaly detection for the autograd engine.
This does two things:
 Running the forward pass with detection enabled will allow the backward
    pass to print the traceback of the forward operation that created the failing
    backward function.
 Any backward computation that generate ""nan"" value will raise an error.
.. warning::
","r""""""Context-manager that enable anomaly detection for the autograd engine.
This does two things:

      pass to print the traceback of the forward operation that created the failing
      backward function.
.. warning::
"
860,"# Run vanilla allreduce in the first `start_powerSGD_iter` iterations.
if state.iter < state.start_powerSGD_iter:
state.maybe_increase_iter(bucket)
        return default.allreduce_fut(group_to_use, input_tensor)
# Apply PowerSGD after `start_powerSGD_iter` iterations.
device = input_tensor.device
","# Run vanilla allreduce in the first `start_powerSGD_iter` iterations.
if state.iter < state.start_powerSGD_iter:
        fut = dist.all_reduce(
            input_tensor, group=group_to_use, async_op=True
        ).get_future()

        def div_callback(fut):
            return [fut.value()[0].div_(world_size)]

state.maybe_increase_iter(bucket)
        return fut.then(div_callback)
# Apply PowerSGD after `start_powerSGD_iter` iterations.
device = input_tensor.device
"
861,"new_tensor = self.new()
new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())
new_tensor.requires_grad = self.requires_grad
memo[id(self)] = new_tensor
return new_tensor
","new_tensor = self.new()
new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())
new_tensor.requires_grad = self.requires_grad
            if self.grad is not None:
                new_tensor.grad = self.grad.__deepcopy__(memo)
memo[id(self)] = new_tensor
return new_tensor
"
862,"int64_t num_features = input_t.size(1);
save_mean = at::empty({ num_features }, weight_t.options());
save_var = at::empty({ num_features }, weight_t.options());

#if CUDNN_VERSION >= 7400
auto op = CUDNN_BATCHNORM_OPS_BN;
size_t workspace_size;
","int64_t num_features = input_t.size(1);
save_mean = at::empty({ num_features }, weight_t.options());
save_var = at::empty({ num_features }, weight_t.options());

#if CUDNN_VERSION >= 7400
auto op = CUDNN_BATCHNORM_OPS_BN;
size_t workspace_size;
"
863,"for (const at::Tensor tensor : value.toTensorList()) {
addOutputForTensor(tensor);
}
    } else {
addOutputForTensor(value.toTensor());
}
}
","for (const at::Tensor tensor : value.toTensorList()) {
addOutputForTensor(tensor);
}
    } else if (value.isTensor()) {
addOutputForTensor(value.toTensor());
    } else {
      // We could have None passed here via `Optional[Tensor]`
      add_next_edge(autograd::Edge{});
}
}
"
864,"from torch.jit._fuser import optimized_execution, fuser, last_executed_optimized_graph
from torch.jit.cuda import stream
from torch.jit._freeze import freeze
# For backwards compatibility
_fork = fork
","from torch.jit._fuser import optimized_execution, fuser, last_executed_optimized_graph
from torch.jit.cuda import stream
from torch.jit._freeze import freeze, optimize_frozen_module
# For backwards compatibility
_fork = fork
"
865,""""""".format(**factory_like_common_args))
add_docstr(torch.randperm,
           r""""""
randperm(n, \*, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False,
    pin_memory=False) -> LongTensor

Returns a random permutation of integers from ``0`` to ``n - 1``.
Args:
",""""""".format(**factory_like_common_args))
add_docstr(torch.randperm,
           """"""
randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, \
device=None, requires_grad=False, pin_memory=False) -> Tensor
"""""" + r""""""
Returns a random permutation of integers from ``0`` to ``n - 1``.
Args:
"
866,"#include <torch/csrc/jit/api/module.h>
#include <ATen/record_function.h>
#include <c10/util/Exception.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
","#include <torch/csrc/jit/api/module.h>

#include <ATen/record_function.h>
#include <c10/util/Exception.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
"
867,"#include <torch/csrc/jit/codegen/cuda/index_compute.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
","#include <torch/csrc/jit/codegen/cuda/index_compute.h>

#include <c10/util/Exception.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
"
868,"#include <torch/csrc/jit/codegen/cuda/interface.h>
#include <ATen/core/dispatch/OperatorOptions.h>
#include <torch/csrc/jit/runtime/custom_operator.h>
#include <torch/csrc/jit/runtime/register_ops_utils.h>
","#include <torch/csrc/jit/codegen/cuda/interface.h>

#include <ATen/core/dispatch/OperatorOptions.h>
#include <torch/csrc/jit/runtime/custom_operator.h>
#include <torch/csrc/jit/runtime/register_ops_utils.h>
"
869,"#include <torch/csrc/jit/codegen/cuda/ir_cloner.h>
#include <torch/csrc/jit/codegen/cuda/fusion.h>
#include <torch/csrc/jit/codegen/cuda/ir_all_nodes.h>
","#include <torch/csrc/jit/codegen/cuda/ir_cloner.h>

#include <torch/csrc/jit/codegen/cuda/fusion.h>
#include <torch/csrc/jit/codegen/cuda/ir_all_nodes.h>
"
870,"#include <torch/csrc/jit/codegen/cuda/kernel.h>
#include <torch/csrc/jit/codegen/cuda/dispatch.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/kernel_ir_printer.h>
","#include <torch/csrc/jit/codegen/cuda/kernel.h>

#include <torch/csrc/jit/codegen/cuda/dispatch.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/kernel_ir_printer.h>
"
871,"#include <torch/csrc/jit/codegen/cuda/shape_inference.h>
#include <c10/core/ScalarType.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/ir/constants.h>
","#include <torch/csrc/jit/codegen/cuda/shape_inference.h>

#include <c10/core/ScalarType.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/ir/constants.h>
"
872,"#include <torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h>
#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <torch/csrc/jit/codegen/fuser/compiler.h>
","#include <torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h>

#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <torch/csrc/jit/codegen/fuser/compiler.h>
"
873,"#include <torch/csrc/jit/frontend/error_report.h>
#include <c10/util/Optional.h>
#include <torch/csrc/jit/frontend/tree.h>
#include <torch/csrc/utils/memory.h>
","#include <torch/csrc/jit/frontend/error_report.h>

#include <c10/util/Optional.h>
#include <torch/csrc/jit/frontend/tree.h>
#include <torch/csrc/utils/memory.h>
"
874,"#include <torch/csrc/jit/frontend/function_schema_parser.h>
#include <ATen/core/Reduction.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/frontend/lexer.h>
","#include <torch/csrc/jit/frontend/function_schema_parser.h>

#include <ATen/core/Reduction.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/frontend/lexer.h>
"
875,"#include <torch/csrc/jit/frontend/schema_type_parser.h>
#include <ATen/core/alias_info.h>
#include <ATen/core/interned_strings.h>
#include <ATen/core/jit_type.h>
","#include <torch/csrc/jit/frontend/schema_type_parser.h>

#include <ATen/core/alias_info.h>
#include <ATen/core/interned_strings.h>
#include <ATen/core/jit_type.h>
"
876,"#include <torch/csrc/jit/mobile/function.h>
#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/runtime/instruction.h>
","#include <torch/csrc/jit/mobile/function.h>

#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/runtime/instruction.h>
"
877,"#include <torch/csrc/jit/passes/canonicalize.h>
#include <torch/csrc/jit/ir/ir_views.h>
namespace torch {
","#include <torch/csrc/jit/passes/canonicalize.h>

#include <torch/csrc/jit/ir/ir_views.h>
namespace torch {
"
878,"#include <torch/csrc/jit/passes/erase_number_types.h>
#include <torch/csrc/jit/ir/constants.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
","#include <torch/csrc/jit/passes/erase_number_types.h>

#include <torch/csrc/jit/ir/constants.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
"
879,"#include <torch/csrc/jit/passes/guard_elimination.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
","#include <torch/csrc/jit/passes/guard_elimination.h>

#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
"
880,"#include <torch/csrc/jit/passes/onnx/helper.h>
#include <onnx/onnx_pb.h>
namespace torch {
","#include <torch/csrc/jit/passes/onnx/helper.h>

#include <onnx/onnx_pb.h>
namespace torch {
"
881,"#include <torch/csrc/jit/passes/onnx/preprocess_for_onnx.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/onnx/helper.h>
","#include <torch/csrc/jit/passes/onnx/preprocess_for_onnx.h>

#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/onnx/helper.h>
"
882,"#include <torch/csrc/jit/passes/quantization/dedup_module_uses.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/quantization/helper.h>
","#include <torch/csrc/jit/passes/quantization/dedup_module_uses.h>

#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/quantization/helper.h>
"
883,"#include <torch/csrc/jit/passes/quantization/helper.h>
#include <torch/csrc/jit/passes/graph_rewrite_helper.h>
namespace torch {
","#include <torch/csrc/jit/passes/quantization/helper.h>

#include <torch/csrc/jit/passes/graph_rewrite_helper.h>
namespace torch {
"
884,".AS(removeAttribute)
.AS(attributeNames)
#undef AS
#define CREATE_ACCESSOR(Kind, method)                          \
  def(#method ""_"",                                             \
      [](Node& n, const char* name, Kind##Attr::ValueType v) { \
        return n.method##_(Symbol::attr(name), std::move(v));  \
      })                                                       \
      .def(#method, [](Node& n, const char* name) {            \
        return n.method(Symbol::attr(name));                   \
      })
.CREATE_ACCESSOR(Float, f)
.CREATE_ACCESSOR(Floats, fs)
.CREATE_ACCESSOR(String, s)
",".AS(removeAttribute)
.AS(attributeNames)
#undef AS
#define CREATE_ACCESSOR(Kind, method)                                       \
  def(#method ""_"", [](Node& n, const char* name, Kind##Attr::ValueType v) { \
    return n.method##_(Symbol::attr(name), std::move(v));                   \
  }).def(#method, [](Node& n, const char* name) {                           \
    return n.method(Symbol::attr(name));                                    \
  })
.CREATE_ACCESSOR(Float, f)
.CREATE_ACCESSOR(Floats, fs)
.CREATE_ACCESSOR(String, s)
"
885,"graph->insertNode(graph->createTupleUnpack(backward_value));
auto tuple_outputs = tuple_unpack_node->outputs();
AT_ASSERT(tuple_outputs.size() == size_t(3));
      return {tuple_outputs[0],
              tuple_outputs[1],
              tuple_outputs[2],
              nullptr,
              nullptr,
              nullptr,
              nullptr,
              nullptr};
}
throw std::runtime_error(
","graph->insertNode(graph->createTupleUnpack(backward_value));
auto tuple_outputs = tuple_unpack_node->outputs();
AT_ASSERT(tuple_outputs.size() == size_t(3));
      return {
          tuple_outputs[0],
          tuple_outputs[1],
          tuple_outputs[2],
          nullptr,
          nullptr,
          nullptr,
          nullptr,
          nullptr};
}
throw std::runtime_error(
"
886,"#include <torch/csrc/jit/runtime/vararg_functions.h>
#include <ATen/ATen.h>
namespace torch {
","#include <torch/csrc/jit/runtime/vararg_functions.h>

#include <ATen/ATen.h>
namespace torch {
"
887,"#ifdef TORCH_ENABLE_LLVM
#include <torch/csrc/jit/tensorexpr/llvm_codegen.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/tensorexpr/llvm_jit.h>
","#ifdef TORCH_ENABLE_LLVM
#include <torch/csrc/jit/tensorexpr/llvm_codegen.h>

#include <c10/util/Exception.h>
#include <torch/csrc/jit/tensorexpr/llvm_jit.h>
"
888,"module_qconfig = qconfig_dict.get(prefix, module_qconfig)
module_qconfig = getattr(module, 'qconfig', module_qconfig)
module.qconfig = module_qconfig
for name, child in module.named_children():
module_prefix = prefix + '.' + name if prefix else name
","module_qconfig = qconfig_dict.get(prefix, module_qconfig)
module_qconfig = getattr(module, 'qconfig', module_qconfig)
    torch.quantization.qconfig.assert_valid_qconfig(module_qconfig, module)

module.qconfig = module_qconfig
for name, child in module.named_children():
module_prefix = prefix + '.' + name if prefix else name
"
889,"free_vars: List[str] = []
modules_used : Set[str] = set()
body: List[str] = []
        maybe_return_annotation : str = ''
def register_modules_used(qualified_name : str):
if '.' in qualified_name:
","free_vars: List[str] = []
modules_used : Set[str] = set()
body: List[str] = []

        # Wrap string in list to pass by reference
        maybe_return_annotation : List[str] = ['']
def register_modules_used(qualified_name : str):
if '.' in qualified_name:
"
890,"func = f.func
faithful_signature: Optional[CppSignature]
if func.arguments.tensor_options is not None or len(func.arguments.out) > 0:
            faithful_signature = CppSignature(func=func, faithful=True, method=method, fallback_binding=fallback_binding)
else:
faithful_signature = None
        signature = CppSignature(func=func, faithful=False, method=method, fallback_binding=fallback_binding)
return CppSignatureGroup(
func=func,
signature=signature,
","func = f.func
faithful_signature: Optional[CppSignature]
if func.arguments.tensor_options is not None or len(func.arguments.out) > 0:
            faithful_signature = CppSignature(
                func=func,
                faithful=True,
                method=method,
                fallback_binding=fallback_binding,
                cpp_no_default_args=f.cpp_no_default_args
            )
else:
faithful_signature = None
        signature = CppSignature(
            func=func,
            faithful=False,
            method=method,
            fallback_binding=fallback_binding,
            cpp_no_default_args=f.cpp_no_default_args
        )
return CppSignatureGroup(
func=func,
signature=signature,
"
891,"assert isinstance(funcs, str), f'not a str: {funcs}'
func = FunctionSchema.parse(funcs)
use_c10_dispatcher_s = e.pop('use_c10_dispatcher', None)
if use_c10_dispatcher_s is None:
use_c10_dispatcher = UseC10Dispatcher.full
","assert isinstance(funcs, str), f'not a str: {funcs}'
func = FunctionSchema.parse(funcs)
        cpp_no_default_args_list = e.pop('cpp_no_default_args', [])
        assert isinstance(cpp_no_default_args_list, list)
        cpp_no_default_args = set(cpp_no_default_args_list)

use_c10_dispatcher_s = e.pop('use_c10_dispatcher', None)
if use_c10_dispatcher_s is None:
use_c10_dispatcher = UseC10Dispatcher.full
"
892,"self.profiler_kind,
self.record_shapes,
self.profile_memory,
            self.with_stack)
def __enter__(self):
if not self.enabled:
","self.profiler_kind,
self.record_shapes,
self.profile_memory,
            self.with_stack,
            self.with_flops)
def __enter__(self):
if not self.enabled:
"
893,"Batched version for complex inputs is only supported on the CPU.
Arguments:
    input (Tensor): The input tensor of size :math:`(*, m, n)` where :math:`*` is zero or more batch dimensions
    rcond (float): A floating point value to determine the cutoff for small singular values.
                   Default: 1e-15
Returns:
The pseudo-inverse of :attr:`input` of dimensions :math:`(*, n, m)`
","Batched version for complex inputs is only supported on the CPU.
Arguments:
    input (Tensor): The input tensor of size :math:`(*, m, n)` where :math:`*` is
        zero or more batch dimensions.
    rcond (float, optional): A floating point value to determine the cutoff for
        small singular values. Default: ``1e-15``.
Returns:
The pseudo-inverse of :attr:`input` of dimensions :math:`(*, n, m)`
"
894,"\text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}
.. note::
        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_
        where the SiLU (Sigmoid Linear Unit) was originally coined, and see
        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation
        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:
        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_
where the SiLU was experimented with later.
Shape:
","\text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}
.. note::
        See `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_
        where the SiLU (Sigmoid Linear Unit) was originally coined, and see
        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation
        in Reinforcement Learning <https://arxiv.org/abs/1702.03118>`_ and `Swish:
        a Self-Gated Activation Function <https://arxiv.org/abs/1710.05941v1>`_
where the SiLU was experimented with later.
Shape:
"
895,"attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
the batches while a 3D mask allows to specify a different mask for the entries of each batch.
    Shape:
        - Inputs:
 query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
the embedding dimension.
 key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
","attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
the batches while a 3D mask allows to specify a different mask for the entries of each batch.
    Shapes for inputs:
the embedding dimension.
"
896,");
} else if (exp == -0.5) {
cpu_kernel_vec(iter,
          [](scalar_t base) -> scalar_t {
return 1.0 / std::sqrt(base);
},
[](Vec base) -> Vec { return base.rsqrt(); }
",");
} else if (exp == -0.5) {
cpu_kernel_vec(iter,
          [](scalar_t base) __ubsan_ignore_float_divide_by_zero__ -> scalar_t {
return 1.0 / std::sqrt(base);
},
[](Vec base) -> Vec { return base.rsqrt(); }
"
897,"// The import process to serialize the bytecode package.
// An example for bytecode.pkl of a small mobile_module looks like:
// (5,  # model version number (caffe2::serialize::kProducedBytecodeVersion)
//  # first method
//  (
//   # function name
//   '__torch__.m.forward',
//   # code
//   (('instructions',
//     (('STOREN', 1, 2),
//      ('DROPR', 1, 0),
//      ('MOVE', 2, 0),
//      ('OP', 0, 0),
//      ('RET', 0, 0))),
//    ('operators', (('aten::Int', 'Tensor'),)),
//    ('constants', ()),
//    ('types', ()),
//    ('register_size', 2)),
//   # schema
//   (('arguments',
//     ((('name', 'x'), ('type', 'Tensor'), ('default_value', 13)),
//      ...)),  # more args follow here
//    ('returns',
//     ((('name', ''), ('type', 'Tensor'), ('default_value', None)),
//      ...)),  # more return values follow here
//   )),
//  # more methods follow here
//  ...)
// In addition, the module debugging information can be saved
// in mobile_debug.pkl. An example for it looks like:
// (5,
//  ('__torch__.m.forward',
//   (('module_debug_info', (top(A).foo(B).forward)))))
// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisited and redesigned if we want to
// support backward compatibility in future.
// Note that the following function-schema fields are not supported:
//  - Argument::{known_length_,kwarg_only_}
//  - FunctionSchema::{overload_name_, is_vararg_, is_varret_}

namespace c10 {
// std::string serializeType(const Type &t);
TypePtr parseType(const std::string& pythonStr);
","// The import process to serialize the bytecode package.
// An example for bytecode.pkl of a small mobile_module looks like:
// (3,
//   ('__torch__.m.forward',
//     (('instructions',
//       (('STOREN', 1, 2),
//        ('DROPR', 1, 0),
//        ('MOVE', 2, 0),
//        ('OP', 0, 0),
//        ('RET', 0, 0))),
//      ('operators', (('aten::Int', 'Tensor'),)),
//      ('constants', ()),
//      ('types', ()),
//      ('register_size', 2))))
// In addition, the module debugging information can be saved
// in mobile_debug.pkl. An example for it looks like:
// (3,
//   ('__torch__.m.forward',
//     (('module_debug_info', (top(A).foo(B).forward)))))
// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisted and redesigned if we want to
// support backward compatibility in future.
namespace c10 {
// std::string serializeType(const Type &t);
TypePtr parseType(const std::string& pythonStr);
"
898,"auto win_length = win_lengthOpt.value_or(n_fft);
const bool return_complex = return_complexOpt.value_or(
self.is_complex() || (window.defined() && window.is_complex()));
  if (!return_complex) {
    TORCH_CHECK(return_complexOpt.has_value(),
        ""stft requires the return_complex parameter be given for real inputs.""
        ""You should pass return_complex=True to opt-in to complex dtype returns ""
        ""(which will be required in a future pytorch release). ""
      );

    TORCH_WARN_ONCE(
        ""stft with return_complex=False is deprecated. In a future pytorch ""
        ""release, stft will return complex tensors for all inputs, and ""
        ""return_complex=False will raise an error.\n""
        ""Note: you can still call torch.view_as_real on the complex output to ""
        ""recover the old return format."");
}
if (!at::isFloatingType(self.scalar_type()) && !at::isComplexType(self.scalar_type())) {
","auto win_length = win_lengthOpt.value_or(n_fft);
const bool return_complex = return_complexOpt.value_or(
self.is_complex() || (window.defined() && window.is_complex()));
  if (!return_complexOpt && !return_complex) {
    TORCH_WARN_ONCE(""stft will require the return_complex parameter be explicitly ""
                    "" specified in a future PyTorch release. Use return_complex=False ""
                    "" to preserve the current behavior or return_complex=True to return ""
                    "" a complex output."");
}
if (!at::isFloatingType(self.scalar_type()) && !at::isComplexType(self.scalar_type())) {
"
899,"}
}
c10::IValue Method::operator()(std::vector<IValue> stack) {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
","}
}
c10::IValue Method::operator()(std::vector<IValue> stack) const {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
"
900,"{
TensorArg grad_output{ grad_output_t, ""grad_output"", 1 },
input{ input_t, ""input"", 2 };
  setMIOpenStreamToCurrent();
return miopen_convolution_backward_weight(
""miopen_convolution_backward_weight"",
weight_size, grad_output, input,
","{
TensorArg grad_output{ grad_output_t, ""grad_output"", 1 },
input{ input_t, ""input"", 2 };
return miopen_convolution_backward_weight(
""miopen_convolution_backward_weight"",
weight_size, grad_output, input,
"
901,"));
auto workspace = at::empty(workspace_size, input.options().dtype(kByte));
    setMIOpenStreamToCurrent();
MIOPEN_CHECK(miopenRNNBackwardData(
handle,
descs.rnn_desc.desc(),
","));
auto workspace = at::empty(workspace_size, input.options().dtype(kByte));
MIOPEN_CHECK(miopenRNNBackwardData(
handle,
descs.rnn_desc.desc(),
"
902,"# For each node in the current partition, find its users
users = node.users
for n in users:
                # Find which the partition the user belongs to.
# Note that if the node itself is also belongs to that partition,
# that partition is not the child of the current partition
for p in partitions:
","# For each node in the current partition, find its users
users = node.users
for n in users:
                # Find which the partition the user node belongs to.
# Note that if the node itself is also belongs to that partition,
# that partition is not the child of the current partition
for p in partitions:
"
903,"return ret
def find_single_partition(self, total_size_of_graph) -> None:
        """"""Only one partition (one graph on one device).""""""
partition_0 = self.create_partition()
for node in self.graph_module.graph.nodes:
if node.op == 'output':
","return ret
def find_single_partition(self, total_size_of_graph) -> None:
        """"""Fit the whole fx module into one device
        """"""
partition_0 = self.create_partition()
for node in self.graph_module.graph.nodes:
if node.op == 'output':
"
904,"return
def do_partition(self) -> GraphModule:
        """"""Return a module with submodules (partitions).""""""
module_with_submodules = split_module(
self.graph_module,
self.torch_module,
","return
def do_partition(self) -> GraphModule:
        """"""Return a new fx module with submodule nodes (partitions).""""""
module_with_submodules = split_module(
self.graph_module,
self.torch_module,
"
905,"node_to_latency_mapping: Dict[Node, NodeLatency]
) -> None:
""""""This method is to partition the fx module based on the cost.
           The cost is the total latency of running the whole graph.
In partitioner_utils.py, the cost model is built.
           The algorithm is:
#1. At every begining, each node is a partition.
Then we map all the partitions to the devices
and calculate the cost
","node_to_latency_mapping: Dict[Node, NodeLatency]
) -> None:
""""""This method is to partition the fx module based on the cost.
           The cost is the total latency of running the whole fx module.
In partitioner_utils.py, the cost model is built.
           The cost aware partition algorithm is:
#1. At every begining, each node is a partition.
Then we map all the partitions to the devices
and calculate the cost
"
906,"transfer_rate_bytes_per_sec,
node_to_latency_mapping
)
        # Make sure all partitions are set up correctly.
reorganize_partitions(self.partitions)
# Set up node to partition mapping
self.node_to_partition = get_node_to_partition_mapping(self.partitions)
","transfer_rate_bytes_per_sec,
node_to_latency_mapping
)
        # Make sure all partitions are set up correctly
reorganize_partitions(self.partitions)
# Set up node to partition mapping
self.node_to_partition = get_node_to_partition_mapping(self.partitions)
"
907,"Using size_based_partition, n0 and n1 are in Partition p0.
n2, n3 and n4 in Partition p1. The current cost is esimated.
We first tried using n0 to swap with n2 from the other partiton.
           Then we found swapping n0 and n2 shows a lower cost
than the current cost and it is the minimum among other pairs like
(n0, None)(This means moving n0 to Partition without swapping other nodes),
(n0, n3) and (n0, n4). We swap n0 and n2 and set the new cost
","Using size_based_partition, n0 and n1 are in Partition p0.
n2, n3 and n4 in Partition p1. The current cost is esimated.
We first tried using n0 to swap with n2 from the other partiton.
           Then we see that swapping n0 and n2 shows a lower cost
than the current cost and it is the minimum among other pairs like
(n0, None)(This means moving n0 to Partition without swapping other nodes),
(n0, n3) and (n0, n4). We swap n0 and n2 and set the new cost
"
908,"TORCH_CUSOLVER_CHECK(cusolverDnZgetrf_bufferSize(
handle, m, n, reinterpret_cast<cuDoubleComplex*>(dA), ldda, &lwork));
auto& allocator = *::c10::cuda::CUDACachingAllocator::get();
  void* buffer = allocator.allocate(sizeof(cuDoubleComplex) * lwork).get();
TORCH_CUSOLVER_CHECK(cusolverDnZgetrf(
handle,
m,
n,
reinterpret_cast<cuDoubleComplex*>(dA),
ldda,
      static_cast<cuDoubleComplex*>(buffer),
ipiv,
info));
}
","TORCH_CUSOLVER_CHECK(cusolverDnZgetrf_bufferSize(
handle, m, n, reinterpret_cast<cuDoubleComplex*>(dA), ldda, &lwork));
auto& allocator = *::c10::cuda::CUDACachingAllocator::get();
  auto dataPtr = allocator.allocate(sizeof(cuDoubleComplex) * lwork);
TORCH_CUSOLVER_CHECK(cusolverDnZgetrf(
handle,
m,
n,
reinterpret_cast<cuDoubleComplex*>(dA),
ldda,
      static_cast<cuDoubleComplex*>(dataPtr.get()),
ipiv,
info));
}
"
909,"TORCH_CUSOLVER_CHECK(cusolverDnCgetrf_bufferSize(
handle, m, n, reinterpret_cast<cuComplex*>(dA), ldda, &lwork));
auto& allocator = *::c10::cuda::CUDACachingAllocator::get();
  void* buffer = allocator.allocate(sizeof(cuComplex) * lwork).get();
TORCH_CUSOLVER_CHECK(cusolverDnCgetrf(
handle,
m,
n,
reinterpret_cast<cuComplex*>(dA),
ldda,
      static_cast<cuComplex*>(buffer),
ipiv,
info));
}
","TORCH_CUSOLVER_CHECK(cusolverDnCgetrf_bufferSize(
handle, m, n, reinterpret_cast<cuComplex*>(dA), ldda, &lwork));
auto& allocator = *::c10::cuda::CUDACachingAllocator::get();
  auto dataPtr = allocator.allocate(sizeof(cuComplex) * lwork);
TORCH_CUSOLVER_CHECK(cusolverDnCgetrf(
handle,
m,
n,
reinterpret_cast<cuComplex*>(dA),
ldda,
      static_cast<cuComplex*>(dataPtr.get()),
ipiv,
info));
}
"
910,"if self._worker_pids_set:
_utils.signal_handling._remove_worker_pids(id(self))
self._worker_pids_set = False
def __del__(self):
self._shutdown_workers()
","if self._worker_pids_set:
_utils.signal_handling._remove_worker_pids(id(self))
self._worker_pids_set = False
                for w in self._workers:
                    if w.is_alive():
                        # Existing mechanisms try to make the workers exit
                        # peacefully, but in case that we unfortunately reach
                        # here, which we shouldn't, (e.g., pytorch/pytorch#39570),
                        # we kill the worker.
                        w.terminate()
def __del__(self):
self._shutdown_workers()
"
911,"minor = 0;
} else if (nvrtc_major <= 9 && prop->major >= 7) { // 9 supports 3-7.2
major = 7;
    if (prop->major == 7 && prop->minor <= 2)
      minor = prop->minor;
    else
      minor = 0;
} else if (nvrtc_major <= 10 && prop->major >= 7) { // 10 supports 3-7.5
major = 7;
    if (prop->major == 7 && prop->minor <= 5)
      minor = prop->minor;
    else
      minor = 0;
}
}
","minor = 0;
} else if (nvrtc_major <= 9 && prop->major >= 7) { // 9 supports 3-7.2
major = 7;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
    minor = (prop->major == 7 && prop->minor <= 2) ? prop->minor : 0;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_major <= 10 && prop->major >= 7) { // 10 supports 3-7.5
major = 7;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
    minor = (prop->major == 7 && prop->minor <= 5) ? prop->minor : 0;
  } else if (
      // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
      nvrtc_major == 11 && nvrtc_minor == 0 &&
      // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
      prop->major >= 8) { // 11.0 supports 3.5-8.0
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
    major = 8;
    minor = 0;
}
}
"
912,"// enable only for the RecordFunction
enableRecordFunction(true);
}
        RECORD_FUNCTION(code_->op_names_[inst.X].name, stack);
if (!prev_value) {
enableRecordFunction(false);
}
","// enable only for the RecordFunction
enableRecordFunction(true);
}
        RECORD_USER_SCOPE_WITH_INPUTS(code_->op_names_[inst.X].name, stack);
if (!prev_value) {
enableRecordFunction(false);
}
"
913,"padded_total_length = square_side_length ** 2
input_tensor.resize_(padded_total_length)
input_tensor[total_length:padded_total_length].fill_(0)
matrix = input_tensor.view(square_side_length, square_side_length)
def create_low_rank_tensor(fill_random_values, rng):
","padded_total_length = square_side_length ** 2
input_tensor.resize_(padded_total_length)
input_tensor[total_length:padded_total_length].fill_(0)

    # Incorporate the error from the previous state into the gradients.
    if state.use_error_feedback:
        if input_tensor in state.error_dict:
            input_tensor.add_(state.error_dict[input_tensor])
        else:
            state.error_dict[input_tensor] = torch.zeros(padded_total_length, device=device)
        # Keep a copy of the input tensor,
        # so that we can compute the local error caused by compression later,
        # by comparing this copy and the input tensor updated after decompression.
        input_tensor_cp = torch.clone(input_tensor).detach()
matrix = input_tensor.view(square_side_length, square_side_length)
def create_low_rank_tensor(fill_random_values, rng):
"
914,"in_channels, out_channels, kernel_size, stride, padding, dilation,
False, _pair(0), groups, bias, padding_mode)
    def _conv_forward(self, input, weight):
if self.padding_mode != 'zeros':
return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
                            weight, self.bias, self.stride,
_pair(0), self.dilation, self.groups)
        return F.conv2d(input, weight, self.bias, self.stride,
self.padding, self.dilation, self.groups)
def forward(self, input: Tensor) -> Tensor:
        return self._conv_forward(input, self.weight)
class Conv3d(_ConvNd):
__doc__ = r""""""Applies a 3D convolution over an input signal composed of several input
","in_channels, out_channels, kernel_size, stride, padding, dilation,
False, _pair(0), groups, bias, padding_mode)
    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
if self.padding_mode != 'zeros':
return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
                            weight, bias, self.stride,
_pair(0), self.dilation, self.groups)
        return F.conv2d(input, weight, bias, self.stride,
self.padding, self.dilation, self.groups)
def forward(self, input: Tensor) -> Tensor:
        return self._conv_forward(input, self.weight, self.bias)
class Conv3d(_ConvNd):
__doc__ = r""""""Applies a 3D convolution over an input signal composed of several input
"
915,"self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self._conv_forward(input, self.weight_fake_quant(self.weight))
@classmethod
def from_float(cls, mod):
","self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self._conv_forward(input, self.weight_fake_quant(self.weight), self.bias)
@classmethod
def from_float(cls, mod):
"
916,"args = parser.parse_args()
if len(args.file) != 2:
        raise ""Must specify 2 files to diff""
ja = load(args.file[0])
jb = load(args.file[1])
","args = parser.parse_args()
if len(args.file) != 2:
        raise RuntimeError(""Must specify 2 files to diff"")
ja = load(args.file[0])
jb = load(args.file[1])
"
917,"}
/* static */
std::shared_ptr<DebugInfoBase> ThreadLocalDebugInfo::get(
    DebugInfoKind kind) {
ThreadLocalDebugInfo* cur = debug_info.get();
while (cur) {
if (cur->kind_ == kind) {
      return cur->info_;
}
cur = cur->parent_info_.get();
}
","}
/* static */
DebugInfoBase* ThreadLocalDebugInfo::get(DebugInfoKind kind) {
ThreadLocalDebugInfo* cur = debug_info.get();
while (cur) {
if (cur->kind_ == kind) {
      return cur->info_.get();
}
cur = cur->parent_info_.get();
}
"
918,"}
elif dtype == torch.quint8:
qconfig_spec = {
                nn.EmbeddingBag : float_qparams_dynamic_qconfig,
}
else:
raise ValueError(
","}
elif dtype == torch.quint8:
qconfig_spec = {
                nn.EmbeddingBag : float_qparams_weight_only_qconfig,
}
else:
raise ValueError(
"
919,"set_parents_and_children(self.partitions)
# Get bfs level for each partition
get_bfs_level_partition(self.partitions)

find_combination = True
while find_combination:
# Search for a pair partition to generate the minimum new cost,
","set_parents_and_children(self.partitions)
# Get bfs level for each partition
get_bfs_level_partition(self.partitions)
find_combination = True
while find_combination:
# Search for a pair partition to generate the minimum new cost,
"
920,"def quantization_pertensor_hook(
    process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_tensor`` logic to DDP using ``allgather``
","def quantization_pertensor_hook(
    process_group: dist.ProcessGroup, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_tensor`` logic to DDP using ``allgather``
"
921,"from torch.distributed.rpc.utils import _parse_remote_device
from torch.nn.parameter import Parameter
from torch.utils.hooks import RemovableHandle
_grad_t = Union[Tuple[Tensor, ...], Tensor]
","from torch.distributed.rpc.utils import _parse_remote_device
from torch.nn.parameter import Parameter
from torch.utils.hooks import RemovableHandle
from torch.nn import Module
_grad_t = Union[Tuple[Tensor, ...], Tensor]
"
922,"import tempfile
import torch
from torch.distributed.nn.jit.templates.remote_module_template import (
REMOTE_MODULE_TEMPLATE,
)
","import tempfile
import torch
from typing import Optional
from torch.distributed.nn.jit.templates.remote_module_template import (
REMOTE_MODULE_TEMPLATE,
)
"
923,"arg_str_list = []
arg_type_str_list = []
for argument in method_schema.arguments:
arg_str_list.append(argument.name)
if argument.has_default_value():
            default_value_str = "" = {}"".format(argument.default)
else:
default_value_str = """"
arg_type_str = ""{name}: {type}{default_value}"".format(
","arg_str_list = []
arg_type_str_list = []
    assert method_schema is not None
for argument in method_schema.arguments:
arg_str_list.append(argument.name)
if argument.has_default_value():
            default_value_str = "" = {}"".format(argument.default_value)
else:
default_value_str = """"
arg_type_str = ""{name}: {type}{default_value}"".format(
"
924,"ReduceScatterOptions,
ScatterOptions,
ReduceOp,
PrefixStore,
)
","ReduceScatterOptions,
ScatterOptions,
ReduceOp,
    Store,
PrefixStore,
    ProcessGroup,
)
"
925,"# If this is a subgroup (which means group_ranks is specified),
# we check if the current process is a member of the new group.
if not is_default_group:
            global_rank = _default_pg.rank()
if global_rank not in group_ranks:
return GroupMember.NON_GROUP_MEMBER
","# If this is a subgroup (which means group_ranks is specified),
# we check if the current process is a member of the new group.
if not is_default_group:
            global_rank = _check_default_pg().rank()
if global_rank not in group_ranks:
return GroupMember.NON_GROUP_MEMBER
"
926,"return
if group == GroupMember.WORLD:
        _check_default_pg()
        return _default_pg.send([tensor], dst, tag)
else:
group_dst_rank = _get_group_rank(group, dst)
return group.send([tensor], group_dst_rank, tag)
","return
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        return default_pg.send([tensor], dst, tag)
else:
group_dst_rank = _get_group_rank(group, dst)
return group.send([tensor], group_dst_rank, tag)
"
927,"opts = AllreduceOptions()
opts.reduceOp = op
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.allreduce(tensor_list, opts)
else:
work = group.allreduce(tensor_list, opts)
","opts = AllreduceOptions()
opts.reduceOp = op
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.allreduce(tensor_list, opts)
else:
work = group.allreduce(tensor_list, opts)
"
928,"tensor = tensor if not tensor.is_complex() else torch.view_as_real(tensor)
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.allgather([tensor_list], [tensor])
else:
work = group.allgather([tensor_list], [tensor])
","tensor = tensor if not tensor.is_complex() else torch.view_as_real(tensor)
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.allgather([tensor_list], [tensor])
else:
work = group.allgather([tensor_list], [tensor])
"
929,"m.def(TORCH_SELECTIVE_SCHEMA(""quantized::embedding_bag_4bit(__torch__.torch.classes.quantized.EmbeddingPackedParamsBase weight, Tensor indices, Tensor? offsets=None, bool scale_grad_by_freq=False, int mode=0, bool pruned_weights=False, Tensor? per_sample_weights=None, Tensor? compressed_indices_mapping=None, bool include_last_offset=False) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::embedding_byte(__torch__.torch.classes.quantized.EmbeddingPackedParamsBase weight, Tensor indices, bool pruned_weights=False) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::celu(Tensor self, float output_scale, int output_zero_point, Scalar alpha=1) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::group_norm(Tensor input, int num_groups, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::hardswish(Tensor input, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::instance_norm(Tensor input, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor""));
","m.def(TORCH_SELECTIVE_SCHEMA(""quantized::embedding_bag_4bit(__torch__.torch.classes.quantized.EmbeddingPackedParamsBase weight, Tensor indices, Tensor? offsets=None, bool scale_grad_by_freq=False, int mode=0, bool pruned_weights=False, Tensor? per_sample_weights=None, Tensor? compressed_indices_mapping=None, bool include_last_offset=False) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::embedding_byte(__torch__.torch.classes.quantized.EmbeddingPackedParamsBase weight, Tensor indices, bool pruned_weights=False) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::celu(Tensor self, float output_scale, int output_zero_point, Scalar alpha=1) -> Tensor""));
  m.def(TORCH_SELECTIVE_SCHEMA(""quantized::hardswish(Tensor input, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::group_norm(Tensor input, int num_groups, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::hardswish(Tensor input, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::instance_norm(Tensor input, Tensor? weight, Tensor? bias, float eps, float output_scale, int output_zero_point) -> Tensor""));
"
930,"}
}
Scalar Scalar::log() const {
  if (isComplex()) {
    return std::log(v.z);
  } else if (isFloatingPoint()) {
    return std::log(v.d);
  } else {
    return std::log(v.i);
  }
}

}  // namespace c10
","}
}
}  // namespace c10
"
931,"return norm_backward(grad, self, p_, norm);
}
Tensor pow_backward(Tensor grad, const Tensor & self, const Scalar & exponent) {
  if (exponent.equal(0.0)) {
return at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
} else {
    auto grad_lambda = [&](auto exp) { return grad * (exp * self.pow(exp - 1)).conj(); };
    Tensor out = (exponent.isComplex()) ? grad_lambda(exponent.toComplexDouble()) : grad_lambda(exponent.toDouble());
return handle_r_to_c(self, out);
}
}
","return norm_backward(grad, self, p_, norm);
}
Tensor pow_backward(Tensor grad, const Tensor & self, const Scalar & exponent_) {
  auto exponent = (exponent_.isComplex()) ? exponent_.toComplexDouble() : exponent_.toDouble();
  if (exponent == 0.0) {
return at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
} else {
    auto out = grad * (exponent * self.pow(exponent - 1)).conj();
return handle_r_to_c(self, out);
}
}
"
932,"""result type "", common_dtype, ""can't be cast to the desired output type "",
result.scalar_type());
  auto exponent = (exp.isComplex()) ? exp.toComplexDouble() : exp.toDouble();

  if (exponent == 0.0) {
result.resize_as_(base).fill_(1);
  } else if (exponent == 1.0) {
result.resize_as_(base).copy_(base);
} else {
auto iter = TensorIterator::unary_op(result, base.to(common_dtype));
","""result type "", common_dtype, ""can't be cast to the desired output type "",
result.scalar_type());
  if (exp.equal(0.0)) {
result.resize_as_(base).fill_(1);
  } else if (exp.equal(1.0)) {
result.resize_as_(base).copy_(base);
} else {
auto iter = TensorIterator::unary_op(result, base.to(common_dtype));
"
933,"if not sym_help._is_fp(self):
other = g.op(""Cast"", other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
two_pow = g.op('Pow', two, other)

rshift = g.op('Div', self, two_pow)
return rshift
","if not sym_help._is_fp(self):
other = g.op(""Cast"", other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
two_pow = g.op('Pow', two, other)
    two_pow = g.op('Cast', two_pow, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
rshift = g.op('Div', self, two_pow)
return rshift
"
934,"export_type=ExportTypes.PROTOBUF_FILE, example_outputs=None,
google_printer=False, opset_version=None, _retain_param_name=False,
do_constant_folding=True, keep_initializers_as_inputs=None,
                             fixed_batch_size=False, custom_opsets=None, add_node_names=True):
from torch.onnx.symbolic_helper import _default_onnx_opset_version, _set_opset_version
from torch.onnx.symbolic_helper import _set_operator_export_type
if opset_version is None:
","export_type=ExportTypes.PROTOBUF_FILE, example_outputs=None,
google_printer=False, opset_version=None, _retain_param_name=False,
do_constant_folding=True, keep_initializers_as_inputs=None,
                             fixed_batch_size=False, custom_opsets=None, add_node_names=True,
                             onnx_shape_inference=True):
from torch.onnx.symbolic_helper import _default_onnx_opset_version, _set_opset_version
from torch.onnx.symbolic_helper import _set_operator_export_type
if opset_version is None:
"
935,"/* static */
std::shared_ptr<DebugInfoBase> ThreadLocalDebugInfo::get(
DebugInfoKind kind) {
  auto cur = debug_info;
while (cur) {
if (cur->kind_ == kind) {
return cur->info_;
}
    cur = cur->parent_info_;
}
return nullptr;
}
","/* static */
std::shared_ptr<DebugInfoBase> ThreadLocalDebugInfo::get(
DebugInfoKind kind) {
  ThreadLocalDebugInfo* cur = debug_info.get();
while (cur) {
if (cur->kind_ == kind) {
return cur->info_;
}
    cur = cur->parent_info_.get();
}
return nullptr;
}
"
936,"reduceOp = ro;
}
  auto consumer_bounds_info = inferBounds(consumer);
auto bounds_it = consumer_bounds_info.find(producer);
if (bounds_it == consumer_bounds_info.end()) {
throw std::runtime_error(""consumer does not use the Tensor produced"");
return {nullptr, nullptr};
}
  std::vector<const Expr*> starts;
  std::vector<const Expr*> stops;

  bool hasReads = false;
  bool hasWrites = false;
  // Find the safe size of the temprorary buffer by determining the outer
  // extents of a union of all bounds.
  for (const TensorAccessBoundsInfo& p : bounds_it->second) {
    hasReads |= p.kind == kLoad;
    hasWrites |= p.kind == kStore;

    for (size_t i = 0; i < p.start.size(); i++) {
      if (starts.size() <= i) {
        starts.push_back(p.start[i]);
      } else {
        starts[i] =
            IRSimplifier::simplify(new Min(starts[i], p.start[i], true));
      }

      if (stops.size() <= i) {
        stops.push_back(p.stop[i]);
      } else {
        stops[i] = IRSimplifier::simplify(new Max(stops[i], p.stop[i], true));
      }
    }
  }
std::vector<std::string> var_names = {""i"", ""j"", ""k"", ""l"", ""m"", ""n"", ""o"", ""p""};
std::vector<const Expr*> tmp_dims;
","reduceOp = ro;
}
  // Check bounds but don't care about AccessKind.
  auto consumer_bounds_info = inferBounds(consumer, false);
auto bounds_it = consumer_bounds_info.find(producer);
if (bounds_it == consumer_bounds_info.end()) {
throw std::runtime_error(""consumer does not use the Tensor produced"");
return {nullptr, nullptr};
}
  TORCH_INTERNAL_ASSERT(bounds_it->second.size() == 1);
  TensorAccessBoundsInfo& info = bounds_it->second[0];
  bool hasReads = info.kind == kLoad || info.kind == kMutate;
  bool hasWrites = info.kind == kStore || info.kind == kMutate;
std::vector<std::string> var_names = {""i"", ""j"", ""k"", ""l"", ""m"", ""n"", ""o"", ""p""};
std::vector<const Expr*> tmp_dims;
"
937,"from functools import partial
import torch.distributed as dist
import torch.distributed.algorithms.ddp_comm_hooks.default_hooks as default
import torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks as quantization
from torch.nn.parallel import DistributedDataParallel
def _ddp_comm_hook_wrapper(comm_hook, model, state):
    model._register_comm_hook(state, comm_hook)
class DDPCommHookType(Enum):
","from functools import partial
import torch.distributed as dist
from . import default_hooks as default
from . import quantization_hooks as quantization
from torch.nn.parallel import DistributedDataParallel
def _ddp_comm_hook_wrapper(comm_hook, model, state):
    model.register_comm_hook(state, comm_hook)
class DDPCommHookType(Enum):
"
938,"process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
        This DDP communication hook just calls ``allreduce`` using ``GradBucket``
        tensors. Once gradient tensors are aggregated across all workers, its ``then``
        callback takes the mean and returns the result. If user registers this hook,
        DDP results is expected to be same as the case where no hook was registered.
        Hence, this won't change behavior of DDP and user can use this as a reference
        or modify this hook to log useful information or any other purposes while
        unaffecting DDP behavior.

        Example::
            >>> ddp_model._register_comm_hook(process_group, allreduce_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
world_size = (
","process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
    This DDP communication hook just calls ``allreduce`` using ``GradBucket``
    tensors. Once gradient tensors are aggregated across all workers, its ``then``
    callback takes the mean and returns the result. If user registers this hook,
    DDP results is expected to be same as the case where no hook was registered.
    Hence, this won't change behavior of DDP and user can use this as a reference
    or modify this hook to log useful information or any other purposes while
    unaffecting DDP behavior.

    Example::
        >>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
world_size = (
"
939,"op.original_tensor = op.tensor;
op.tensor = op.tensor.to(common_dtype_);
op.current_dtype = common_dtype_;
}
}
}
","op.original_tensor = op.tensor;
op.tensor = op.tensor.to(common_dtype_);
op.current_dtype = common_dtype_;
        op.target_dtype = common_dtype_;
}
}
}
"
940,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.ge(b);
});
      });
}
}
void eq_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""eq_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a == b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""eq_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.ge(b);
});
    });
}
}
void eq_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""eq_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a == b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), ""eq_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
941,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.eq(b);
});
      });
}
}
void ne_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""ne_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a != b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""ne_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.eq(b);
});
    });
}
}
void ne_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""ne_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a != b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), ""ne_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
942,"tensors: Tuple[Tensor, ...]
def __init__(self, *tensors: Tensor) -> None:
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)
self.tensors = tensors
def __getitem__(self, index):
","tensors: Tuple[Tensor, ...]
def __init__(self, *tensors: Tensor) -> None:
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), ""Size mismatch between tensors""
self.tensors = tensors
def __getitem__(self, index):
"
943,"""""""
partition = self.create_partition()
total_size_needed = get_extra_size_of(node, set())
            partition.nodes.add(node)
partition.used_mem_bytes = total_size_needed
single_node_partitions.append(partition)
","""""""
partition = self.create_partition()
total_size_needed = get_extra_size_of(node, set())
            partition.add_node(node)
partition.used_mem_bytes = total_size_needed
single_node_partitions.append(partition)
"
944,"void mul(VulkanTensor& output, const VulkanTensor& input, const float s) {
const auto sizes = input.sizes();
  const auto C = std::accumulate(
      sizes.cbegin(), sizes.cend() - 2, 1, std::multiplies<int64_t>());
const auto C_4 = UP_DIV(C, 4);
const auto H = sizes[2];
const auto W = sizes[3];
","void mul(VulkanTensor& output, const VulkanTensor& input, const float s) {
const auto sizes = input.sizes();
  const auto C = prod_intlist(sizes.cbegin(), sizes.cend() - 2);
const auto C_4 = UP_DIV(C, 4);
const auto H = sizes[2];
const auto W = sizes[3];
"
945,"#include <ATen/Functions.h>
#include <ATen/core/dispatch/Dispatcher.h>
#include <ATen/CPUType.h>
#include <ATen/QuantizedCPUType.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
namespace at {
","#include <ATen/Functions.h>
#include <ATen/core/dispatch/Dispatcher.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
namespace at {
"
946,"fm = cuda_fm if 'CUDA' in dispatch else cpu_fm
        fm.write_with_template(f'{dispatch}Type.h', h_template, lambda: {
            'Type': f'{dispatch}Type',
            'type_derived_method_declarations': list(mapMaybe(
                compute_type_method(dispatch, target=Target.DECLARATION, selector=selector),
                native_functions
            )),
        })
fm.write_with_template(f'{dispatch}Type.cpp', cpp_template, lambda: {
'Type': f'{dispatch}Type',
'extra_cuda_headers': extra_cuda_headers if 'CUDA' in dispatch else '',
","fm = cuda_fm if 'CUDA' in dispatch else cpu_fm
fm.write_with_template(f'{dispatch}Type.cpp', cpp_template, lambda: {
'Type': f'{dispatch}Type',
'extra_cuda_headers': extra_cuda_headers if 'CUDA' in dispatch else '',
"
947,"#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include ""torch/csrc/autograd/FunctionsManual.h""
#include <ATen/TypeDefault.h>
#include <torch/library.h>
// ${generated_comment}
","#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include ""torch/csrc/autograd/FunctionsManual.h""
#include <torch/library.h>
// ${generated_comment}
"
948,"returns_type = native.returns_type(f.func.returns)
args = native.arguments(f.func)
args_str = ', '.join(map(str, args))
        # TODO: don't need dispatch is None shortly
        dispatch_to_all_backends = dispatch is None or dispatch in KEYWORD_ALL_BACKENDS
if target is Target.DECLARATION:
return f""{returns_type} {name}({args_str});""
elif target is Target.DEFINITION:
assert dispatch is not None
","returns_type = native.returns_type(f.func.returns)
args = native.arguments(f.func)
args_str = ', '.join(map(str, args))
        dispatch_to_all_backends = dispatch is not None and dispatch in KEYWORD_ALL_BACKENDS
if target is Target.DECLARATION:
            assert dispatch is not None
return f""{returns_type} {name}({args_str});""
elif target is Target.DEFINITION:
assert dispatch is not None
"
949,"if abs(p1.bfs_level - p2.bfs_level) <= 1:
# Combining p1 and p2 into p0
p0 = self.combine_two_partitions(p1, p2)
                            self.set_parents_and_children()
p0.logical_device_ids = p2.logical_device_ids
# Remove p2 from non_single_node_partitions
non_single_node_partitions.remove(p2)
","if abs(p1.bfs_level - p2.bfs_level) <= 1:
# Combining p1 and p2 into p0
p0 = self.combine_two_partitions(p1, p2)
p0.logical_device_ids = p2.logical_device_ids
# Remove p2 from non_single_node_partitions
non_single_node_partitions.remove(p2)
"
950,"""""""
partition = self.create_partition()
partition.nodes = partition_0.nodes.union(partition_1.nodes)
self.partitions.remove(partition_0)
self.partitions.remove(partition_1)
# reset parents and children for all partitions
","""""""
partition = self.create_partition()
partition.nodes = partition_0.nodes.union(partition_1.nodes)
        partition.recalculate_mem_size()
self.partitions.remove(partition_0)
self.partitions.remove(partition_1)
# reset parents and children for all partitions
"
951,"WithCPUCachingAllocatorGuard::WithCPUCachingAllocatorGuard(
CPUCachingAllocator* allocator) {
  caching_allocator_ptr = allocator;
prev_caching_allocator_ptr_ = GetThreadLocalCachingAllocator();
}
WithCPUCachingAllocatorGuard::~WithCPUCachingAllocatorGuard() {
","WithCPUCachingAllocatorGuard::WithCPUCachingAllocatorGuard(
CPUCachingAllocator* allocator) {
prev_caching_allocator_ptr_ = GetThreadLocalCachingAllocator();
  caching_allocator_ptr = allocator;
}
WithCPUCachingAllocatorGuard::~WithCPUCachingAllocatorGuard() {
"
952,"continue
prefix = node.name + '_activation_post_process_'
            root_node, _, pattern, obj, qconfig = matches.get(node.name, (None, None, None, None, None))
if root_node is None:
env[node.name] = observed_graph.node_copy(node, load_arg)
elif root_node is node:
","continue
prefix = node.name + '_activation_post_process_'
            root_node, matched_nodes, pattern, obj, qconfig = matches.get(node.name, (None, None, None, None, None))
if root_node is None:
env[node.name] = observed_graph.node_copy(node, load_arg)
elif root_node is node:
"
953,"batch_sizes.end(),
1,
std::multiplies<int64_t>());
// Strategy: For each batch, we are going to push slices (where applicable)
// of the arguments onto `stack`, call `op`, and store the result in
","batch_sizes.end(),
1,
std::multiplies<int64_t>());
  // Without a shape-checking API, we're unable to compute the correct shape of
  // the output so we just error out.
  TORCH_CHECK(num_batches > 0,
      ""Batching rule not implemented for "", schema.operator_name(), "". "",
      ""The fallback path does not support vmap over dims of size 0."");
// Strategy: For each batch, we are going to push slices (where applicable)
// of the arguments onto `stack`, call `op`, and store the result in
"
954,"val_map[node] = self.node_copy(node, lambda n : val_map[n])
return None
def create_node(self, op: str, target: Target,
args: Optional[Tuple[Argument, ...]] = None,
kwargs: Optional[Dict[str, Argument]] = None,
","val_map[node] = self.node_copy(node, lambda n : val_map[n])
return None
    def __deepcopy__(self, memo=None) -> 'Graph':
        """"""
        Explicitly implement __deepcopy__ to prevent excessive recursion depth
        from the default implementation. This uses graph_copy to copy the nodes
        in an iterative way, rather than recursive. It also populates the
        memoization table to prevent unnecessary copies (e.g. references to
        nodes or other parts of the Graph from a custom GraphModule implementation
        """"""
        memo = memo if memo else {}
        g = Graph()
        output_val = g.graph_copy(self, val_map=memo)
        g.output(output_val)
        return g

def create_node(self, op: str, target: Target,
args: Optional[Tuple[Argument, ...]] = None,
kwargs: Optional[Dict[str, Argument]] = None,
"
955,"except ImportError:
_GLOO_AVAILABLE = False
# Some reduce ops are not supported by complex numbers.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
# these unsupported ops will return garbage values rather than error out.
","except ImportError:
_GLOO_AVAILABLE = False
# Some reduce ops are not supported by complex numbers and will result in an error.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
# these unsupported ops will return garbage values rather than error out.
"
956,"# for the entry or not (as this affects whether or not the operation is
# overrideable or not.)  Once this all gets cleaned up, this
# property will be obsolete.
        ('abstract', f.dispatch is not None),
('device_guard', f.device_guard),
('with_gil', False),
('deprecated', False),
","# for the entry or not (as this affects whether or not the operation is
# overrideable or not.)  Once this all gets cleaned up, this
# property will be obsolete.
        ('abstract', is_abstract),
('device_guard', f.device_guard),
('with_gil', False),
('deprecated', False),
"
957,"input_graph = model.graph
self.modules = dict(input_root.named_modules())
        fusion_patterns = get_fusion_patterns()
# find fusion
fusion_pairs = self._find_matches(input_root, input_graph, fusion_patterns)
self.fused_graph = Graph()
","input_graph = model.graph
self.modules = dict(input_root.named_modules())
        fusion_patterns = get_default_fusion_patterns()
# find fusion
fusion_pairs = self._find_matches(input_root, input_graph, fusion_patterns)
self.fused_graph = Graph()
"
958,"from collections import OrderedDict
# pattern for conv bn fusion
FUSION_PATTERNS = OrderedDict()
def register_fusion_pattern(pattern):
def insert(fn):
        FUSION_PATTERNS[pattern] = fn
return fn
return insert
def get_fusion_patterns():
    return FUSION_PATTERNS
QUANTIZATION_PATTERNS = OrderedDict()
# Register pattern for both static quantization and qat
def register_quant_pattern(pattern):
def insert(fn):
        QUANTIZATION_PATTERNS[pattern] = fn
return fn
return insert
# Get patterns for both static quantization and qat
def get_quant_patterns():
    return QUANTIZATION_PATTERNS
# Example use of register pattern function:
# @register_fusion_pattern(torch.nn.ReLU, (torch.nn.BatchNorm2d, torch.nn.Conv2d)))
","from collections import OrderedDict
# pattern for conv bn fusion
DEFAULT_FUSION_PATTERNS = OrderedDict()
def register_fusion_pattern(pattern):
def insert(fn):
        DEFAULT_FUSION_PATTERNS[pattern] = fn
return fn
return insert
def get_default_fusion_patterns():
    return DEFAULT_FUSION_PATTERNS
DEFAULT_QUANTIZATION_PATTERNS = OrderedDict()
# Register pattern for both static quantization and qat
def register_quant_pattern(pattern):
def insert(fn):
        DEFAULT_QUANTIZATION_PATTERNS[pattern] = fn
return fn
return insert
# Get patterns for both static quantization and qat
def get_default_quant_patterns():
    return DEFAULT_QUANTIZATION_PATTERNS
# Example use of register pattern function:
# @register_fusion_pattern(torch.nn.ReLU, (torch.nn.BatchNorm2d, torch.nn.Conv2d)))
"
959,"""""""
if mapping is None:
        mapping = get_static_quant_module_mappings()
if convert_custom_config_dict is None:
convert_custom_config_dict = {}
custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", {})
","""""""
if mapping is None:
        mapping = get_default_static_quant_module_mappings()
if convert_custom_config_dict is None:
convert_custom_config_dict = {}
custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", {})
"
960,"import torch.nn.quantized as nnq
import torch.nn.quantized.dynamic as nnqd
from torch.quantization import prepare
from .quantization_mappings import (
get_compare_output_module_list,
","import torch.nn.quantized as nnq
import torch.nn.quantized.dynamic as nnqd
from torch.quantization import prepare
from typing import Dict
from .quantization_mappings import (
get_compare_output_module_list,
"
961,"if (diff_view_meta->creation_meta != CreationMeta::MULTI_OUTPUT_SAFE) {
// Do not use handle_view_on_rebase here as check_inplace should have been called before this
// and either throw an error or clear the warning
        TORCH_INTERNAL_ASSERT(diff_view_meta->creation_meta == CreationMeta::DEFAULT);
TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
","if (diff_view_meta->creation_meta != CreationMeta::MULTI_OUTPUT_SAFE) {
// Do not use handle_view_on_rebase here as check_inplace should have been called before this
// and either throw an error or clear the warning
        // Temporary error message as a full fix is too risky for now
        // Should be an internal assert again
        if (diff_view_meta->creation_meta != CreationMeta::DEFAULT) {
          auto grad_fn = diff_view_meta->grad_fn_.get();
          auto grad_fn_name = grad_fn? grad_fn->name() : ""a function created in no_grad mode"";
          TORCH_CHECK(false, ""Output "", diff_view_meta->output_nr_, "" of "", grad_fn_name, "" is a view and ""
                      ""is being modified inplace but this inplace operation is not allowed. You should ""
                      ""either replace it by an out of place operation or do a .clone() of the Tensor ""
                      ""before modifying it inplace. Note that this can happen when using DataParallel or ""
                      ""DistributedDataParallel, which can send views of the original input to the forward ""
                      ""method of the model."");
        }
TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
"
962,"torch.isclose: lambda input, other, rtol=1e-05, atol=1e-08, equal_nan=False: -1,
torch.isnan: lambda input: -1,
torch.istft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
                      normalized=False, onesided=True, length=None, return_complex=False: -1),
torch.kl_div: lambda input, target, size_average=None, reduce=None, reduction='mean', log_target=False: -1,
torch.kthvalue: lambda input, k, dim=None, keepdim=False, out=None: -1,
torch.layer_norm: lambda input, normalized_shape, weight=None, bias=None, esp=1e-05, cudnn_enabled=True: -1,
","torch.isclose: lambda input, other, rtol=1e-05, atol=1e-08, equal_nan=False: -1,
torch.isnan: lambda input: -1,
torch.istft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
                      normalized=False, onesided=None, length=None, return_complex=False: -1),
torch.kl_div: lambda input, target, size_average=None, reduce=None, reduction='mean', log_target=False: -1,
torch.kthvalue: lambda input, k, dim=None, keepdim=False, out=None: -1,
torch.layer_norm: lambda input, normalized_shape, weight=None, bias=None, esp=1e-05, cudnn_enabled=True: -1,
"
963,"torch.nn.functional.margin_ranking_loss: (lambda input1, input2, target, margin=0, size_average=None,
reduce=None, reduction='mean': -1),
torch.nn.functional.max_pool1d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
                                         return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool1d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool2d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
                                         return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool2d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool3d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
","torch.nn.functional.margin_ranking_loss: (lambda input1, input2, target, margin=0, size_average=None,
reduce=None, reduction='mean': -1),
torch.nn.functional.max_pool1d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
                                         ceil_mode=False, return_indices=False: -1),
torch.nn.functional.max_pool1d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool2d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
                                         ceil_mode=False, return_indices=False: -1),
torch.nn.functional.max_pool2d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
return_indices=False, ceil_mode=False: -1),
torch.nn.functional.max_pool3d: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
"
964,"freed_offset + freed_size, freed_block_it);
}
}
  TORCH_CHECK(validate_allocation_plan(allocation_sizes, allocation_offsets),
      ""Allocation plan invaild."");
return allocation_offsets;
}
","freed_offset + freed_size, freed_block_it);
}
}
  TORCH_CHECK(validate_allocation_plan(mem_events, allocation_offsets),
      ""ProfilingAllocator: Allocation plan invaild."");
return allocation_offsets;
}
"
965,"if _rank_not_in_group(group):
return
if group == GroupMember.WORLD:
_check_default_pg()
work = _default_pg.allgather(output_tensor_lists, input_tensor_list)
","if _rank_not_in_group(group):
return
    output_tensor_lists = [[t if not t.is_complex() else torch.view_as_real(t) for t in l] for l in output_tensor_lists]
    input_tensor_list = [t if not t.is_complex() else torch.view_as_real(t) for t in input_tensor_list]

if group == GroupMember.WORLD:
_check_default_pg()
work = _default_pg.allgather(output_tensor_lists, input_tensor_list)
"
966,"component_distribution: `torch.distributions.Distribution`-like
instance. Right-most batch dimension indexes component.
""""""
    arg_constraints = {}
has_rsample = False
def __init__(self,
","component_distribution: `torch.distributions.Distribution`-like
instance. Right-most batch dimension indexes component.
""""""
    arg_constraints: Dict[str, constraints.Constraint] = {}
has_rsample = False
def __init__(self,
"
967,"auto val = std::stoi(errorHandle);
if (val == 1) {
asyncErrorHandling_ = true;
      LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL Async Error Handling enabled."";
} else if (val != 0) {
throw std::runtime_error(
""Invalid value for environment variable: "" +
","auto val = std::stoi(errorHandle);
if (val == 1) {
asyncErrorHandling_ = true;
} else if (val != 0) {
throw std::runtime_error(
""Invalid value for environment variable: "" +
"
968,"return ncclSuccess;
},
dstRank,
      NCCLCommType::SEND);
return ret;
}
","return ncclSuccess;
},
dstRank,
      OpType::SEND);
return ret;
}
"
969,"}
bool Reducer::rebuild_buckets() {
std::lock_guard<std::mutex> lock(mutex_);
if (!should_rebuild_buckets() || rebuilt_params_.empty()) {
return false;
","}
bool Reducer::rebuild_buckets() {
  // Ensure reduction for previous backwards pass is finished. If user's model
  // has unused parameters for example, this will raise an error recommending to
  // run with find_unused_parameters=True, instead of the size mismatch
  // exception below.
  ensure_prior_reduction_finished();
std::lock_guard<std::mutex> lock(mutex_);
if (!should_rebuild_buckets() || rebuilt_params_.empty()) {
return false;
"
970,"dedent_src = dedent(source)
py_ast = ast.parse(dedent_src)
if len(py_ast.body) != 1 or not isinstance(py_ast.body[0], ast.FunctionDef):
        raise RuntimeError(""Expected a single top-level function"")
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
type_line = torch.jit.annotations.get_type_line(source)
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, True)
","dedent_src = dedent(source)
py_ast = ast.parse(dedent_src)
if len(py_ast.body) != 1 or not isinstance(py_ast.body[0], ast.FunctionDef):
        raise RuntimeError(f""Expected a single top-level function: {filename}:{file_lineno}"")
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
type_line = torch.jit.annotations.get_type_line(source)
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, True)
"
971,"i += 2
return f'""{s}""'
return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
","i += 2
return f'""{s}""'

    if isinstance(t, OptionalType):
        if d == 'None':
            return 'c10::nullopt'

        return default_expr(d, t.elem)

    if isinstance(t, ListType):
        if (d.startswith('[') and d.endswith(']')):
            return '{' + d[1:-1] + '}'
        elif t.size is None:
            # NOTE: Sized lists can have scalar defaults
            raise ValueError(f""Expected a list default '[...]' but found: '{d}'"")

return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
"
972,"const auto& e_a1 = *it_a1;
const auto& e_a2 = *it_a2;
      if (!ivaluesEqual(e_a1.key(), e_a2.key()) &&
!ivaluesEqual(e_a1.value(), e_a2.value())) {
return false;
}
","const auto& e_a1 = *it_a1;
const auto& e_a2 = *it_a2;
      if (!ivaluesEqual(e_a1.key(), e_a2.key()) ||
!ivaluesEqual(e_a1.value(), e_a2.value())) {
return false;
}
"
973,"std::vector<at::Tensor>& tensors,
Fn fn,
int peer,
    OpType opType,
PreProcess pre,
PostProcess post) {
const auto devices = getDeviceList(tensors);
const auto key = getKeySendRecv(rank_, peer);
int p2pRank = rank_ < peer ? 0 : 1;
  auto& ncclComms = getNCCLComm(key, devices, opType, p2pRank);
// First let NCCL streams wait for input tensors allocation streams
syncStreams(devices, ncclEvents_[key], ncclStreams_[key]);
// Work itself will create the CUDA events on all GPUs of tensors
  auto work = initWork(devices, rank_, opType);
  if (opType == OpType::RECV) {
// Store references to outputs and futureNCCLCallbackStream to be used by
// WorkNCCL::getFuture.
work->outputs_ = std::make_shared<std::vector<at::Tensor>>(tensors);
","std::vector<at::Tensor>& tensors,
Fn fn,
int peer,
    NCCLCommType commType,
PreProcess pre,
PostProcess post) {
const auto devices = getDeviceList(tensors);
const auto key = getKeySendRecv(rank_, peer);
int p2pRank = rank_ < peer ? 0 : 1;
  auto& ncclComms = getNCCLComm(key, devices, commType, p2pRank);
// First let NCCL streams wait for input tensors allocation streams
syncStreams(devices, ncclEvents_[key], ncclStreams_[key]);
// Work itself will create the CUDA events on all GPUs of tensors
  auto work = initWork(devices);
  if (commType == NCCLCommType::RECV) {
// Store references to outputs and futureNCCLCallbackStream to be used by
// WorkNCCL::getFuture.
work->outputs_ = std::make_shared<std::vector<at::Tensor>>(tensors);
"
974,"for (const auto& abortedCommId : abortedCommIds) {
abortedComms_.emplace(abortedCommId);
const auto& storeKey = getNcclAbortedCommStoreKey(abortedCommId);
        store_->set(storeKey, {});
        LOG(INFO) << ""Watchdog wrote aborted communicator id to store: ""
<< storeKey;
}
","for (const auto& abortedCommId : abortedCommIds) {
abortedComms_.emplace(abortedCommId);
const auto& storeKey = getNcclAbortedCommStoreKey(abortedCommId);
        auto rankStr = std::to_string(rank_);
        store_->set(
            storeKey,
            std::vector<uint8_t>(
                reinterpret_cast<const uint8_t*>(rankStr.data()),
                reinterpret_cast<const uint8_t*>(rankStr.data()) +
                    rankStr.size()));
        LOG(INFO) << ""[Rank "" << rank_
                  << ""] Watchdog wrote aborted communicator id to store: ""
<< storeKey;
}
"
975,"root,
comm,
stream.stream());
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
","root,
comm,
stream.stream());
      },
      OpType::BROADCAST);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
"
976,"int srcRank,
int /* unused */) {
check_gpu_tensors(tensors);
  auto ret= pointToPoint(
tensors,
[&](at::Tensor& output,
ncclComm_t comm,
","int srcRank,
int /* unused */) {
check_gpu_tensors(tensors);
  auto ret = pointToPoint(
tensors,
[&](at::Tensor& output,
ncclComm_t comm,
"
977,"# so we try using max_poolxd_with_indices, and if it is not possible
# (input is not a complete tensor or output size not factor of input size)
# then we call GlobalAveragePool and return None for the indices
if output_size == [1] * len(output_size) and type == ""AveragePool"":
return g.op(""GlobalAveragePool"", input)
if not input.isCompleteTensor():
","# so we try using max_poolxd_with_indices, and if it is not possible
# (input is not a complete tensor or output size not factor of input size)
# then we call GlobalAveragePool and return None for the indices
        try:
            output_size = _parse_arg(output_size, 'is')
        except Exception:
            return sym_help._onnx_unsupported('adaptive pooling, since output_size is not constant.')
if output_size == [1] * len(output_size) and type == ""AveragePool"":
return g.op(""GlobalAveragePool"", input)
if not input.isCompleteTensor():
"
978,"def default_expr(d: str, t: Type) -> str:
if d == 'None' and str(t) == 'Tensor?':
return '{}'
return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
","def default_expr(d: str, t: Type) -> str:
if d == 'None' and str(t) == 'Tensor?':
return '{}'
    if isinstance(t, BaseType) and t.name is BaseTy.str:
        # Schema allows single quotes but C++ needs double
        if len(d) >= 2 and d[0] == ""'"" and d[-1] == ""'"":
            s = ''
            i = 1
            while i + 1 < len(d):
                if d[i] != '\\':
                    if d[i] == '""':
                        s += '\\""'
                    else:
                        s += d[i]
                    i += 1
                else:
                    if d[i + 1] == ""'"":
                        s += ""'""
                    else:
                        s += d[i:i + 2]
                    i += 2

            return f'""{s}""'
return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
"
979,"# def registration only happens in TypeDefault
def_registration = """"
if dispatch is None:
                def_registration = f'm.def(""{f.func}"");\n'
impl_registration = """"
if not def_only and not f.manual_kernel_registration and (dispatch is not None or f.dispatch is None):
","# def registration only happens in TypeDefault
def_registration = """"
if dispatch is None:
                def_registration = f'm.def({cpp_string(str(f.func))});\n'
impl_registration = """"
if not def_only and not f.manual_kernel_registration and (dispatch is not None or f.dispatch is None):
"
980,"returns_type = dispatcher.returns_type(f.func.returns)
args = dispatcher.arguments(f.func)
args_str = ', '.join(map(str, args))
    dispatch = f.dispatch is not None
    math = dispatch and 'Math' in f.dispatch  # type: ignore
    return f""""""{returns_type} {name}({args_str}); // {{""schema"": ""aten::{f.func}"", ""dispatch"": ""{dispatch}"", ""math"": ""{math}""}}
""""""
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
","returns_type = dispatcher.returns_type(f.func.returns)
args = dispatcher.arguments(f.func)
args_str = ', '.join(map(str, args))
    comment_data : Dict[str, str] = {
        'schema': f'aten::{f.func}',
        'dispatch': str(f.dispatch is not None),
        'math': str(f.dispatch is not None and 'Math' in f.dispatch)
    }
    return f""""""{returns_type} {name}({args_str}); // {json.dumps(comment_data)}
""""""
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
"
981,"for (int i = 0; i < data.dims_size(); ++i) {
if (i >= starts.size()) {
continue;
}
        if (data.dims_size() > 0) {
auto start = starts[i];
auto end = ends[i];
if (start < 0) {
","for (int i = 0; i < data.dims_size(); ++i) {
if (i >= starts.size()) {
          dst_sizes[i] = data.dims(i);
continue;
}
        if (data.dims(i) > 0) {
auto start = starts[i];
auto end = ends[i];
if (start < 0) {
"
982,"def _adaptive_pool(name, type, tuple_fn, fn=None):
    @parse_args('v', 'is')
def symbolic_fn(g, input, output_size):
# _adaptive_pool is supported for cases where output_size is 1 for all dimensions,
# by executing a GlobalPool.
","def _adaptive_pool(name, type, tuple_fn, fn=None):
def symbolic_fn(g, input, output_size):
# _adaptive_pool is supported for cases where output_size is 1 for all dimensions,
# by executing a GlobalPool.
"
983,"if mod != [0] * len(mod):
if output_size == [1] * len(output_size):
return g.op(""GlobalMaxPool"", input), None
            return _unimplemented(name, 'output size that are not factor of input size')
k = [int(dim[i] / output_size[i]) for i in range(0, len(dim))]
# call max_poolxd_with_indices to get indices in the output
if type == ""MaxPool"":
","if mod != [0] * len(mod):
if output_size == [1] * len(output_size):
return g.op(""GlobalMaxPool"", input), None
            if sym_help._operator_export_type == torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK:
                return _unimplemented(name, 'output size that are not factor of input size')
            else:
                return sym_help._onnx_unsupported(name, ', since output size is not factor of input size')
k = [int(dim[i] / output_size[i]) for i in range(0, len(dim))]
# call max_poolxd_with_indices to get indices in the output
if type == ""MaxPool"":
"
984,"int device;
C10_CUDA_CHECK(cudaGetDevice(&device));
void* r = nullptr;
if (size != 0) {
caching_allocator.malloc(&r, device, size, cuda::getCurrentCUDAStream(device));
}
","int device;
C10_CUDA_CHECK(cudaGetDevice(&device));
void* r = nullptr;
    if (forceUncachedAllocator()) {
      C10_CUDA_CHECK(cudaMalloc(&r, size));
      return {r, r, &uncached_delete, Device(DeviceType::CUDA, device)};
    }
if (size != 0) {
caching_allocator.malloc(&r, device, size, cuda::getCurrentCUDAStream(device));
}
"
985,"name = f.func.out_arguments[i].name
# If the return argument is explicitly named...
elif r.name:
            # See Note [Byte-for-byte compatibility]
            #
            # Check if it would conflict with an existing argument.
            # Downstream codegen assumes that return names and argument
            # names don't conflict with each other, so we disambiguate
            # (by adding a trailing _return) this case.  Notice that
            # historically, the collision check was buggy: it just did a
            # straight string contains test on the entirety of the
            # inputs part of the format string, meaning that it also
            # picked up occurrences of the argument name in the NAME of
            # the function, as well as substring occurrences of the name
            # in arguments.  We have simulated the old logic here...
            buggy_name_conflict = r.name in str(f.func.name) or \
                any(r.name in a.name for a in f.func.schema_order_arguments())
            # ... but a more correct version is simply
            # name_conflict = any(r.name == a.name for a in f.func.schema_order_arguments())
            if buggy_name_conflict and not f.func.is_out_fn():
name = f'{r.name}_return'
else:
name = r.name
","name = f.func.out_arguments[i].name
# If the return argument is explicitly named...
elif r.name:
            name_conflict = any(r.name == a.name for a in f.func.schema_order_arguments())
            if name_conflict and not f.func.is_out_fn():
name = f'{r.name}_return'
else:
name = r.name
"
986,"arg['default'] = pythonify_default(cpp.default_expr(a.default, a.type))
if a.name in kwarg_only_set:
arg['kwarg_only'] = True
    # See Note [Byte-for-byte compatibility]
    # The default value of kwarg_only is False; this case exists for
    # byte-for-byte compatibility
    elif a.name in out_arg_set:
        arg['kwarg_only'] = False
if a.name in out_arg_set:
arg['output'] = True
        # See Note [Byte-for-byte compatibility]
        # This is probably a bug in the original implementation, where
        # the specification of allocate was not properly propagated to
        # the schema-order arguments.  In any case, this field
        # is redundant with the output field
        if not schema_order:
            arg['allocate'] = True
# See Note [name and field_name]
if a.name in name_to_field_name:
arg['field_name'] = name_to_field_name[a.name]
","arg['default'] = pythonify_default(cpp.default_expr(a.default, a.type))
if a.name in kwarg_only_set:
arg['kwarg_only'] = True
if a.name in out_arg_set:
arg['output'] = True
        arg['allocate'] = True
# See Note [name and field_name]
if a.name in name_to_field_name:
arg['field_name'] = name_to_field_name[a.name]
"
987,"PyRRef,
RemoteProfilerManager,
WorkerInfo,
_cleanup_python_rpc_handler,
_delete_all_user_and_unforked_owner_rrefs,
_destroy_rref_context,
","PyRRef,
RemoteProfilerManager,
WorkerInfo,
    get_rpc_timeout,
_cleanup_python_rpc_handler,
_delete_all_user_and_unforked_owner_rrefs,
_destroy_rref_context,
"
988,"_build_rpc_profiling_key,
)
from .constants import UNSET_RPC_TIMEOUT
logger = logging.getLogger(__name__)
","_build_rpc_profiling_key,
)
from .constants import DEFAULT_SHUTDOWN_TIMEOUT, UNSET_RPC_TIMEOUT
logger = logging.getLogger(__name__)
"
989,"framework will work after this method returns.
""""""
try:
        _all_gather(None)
except RuntimeError as ex:
logger.error(
f""Failed to respond to 'Shutdown Proceed' in time, got error {ex}""
","framework will work after this method returns.
""""""
try:
        _all_gather(None, timeout=DEFAULT_SHUTDOWN_TIMEOUT)
except RuntimeError as ex:
logger.error(
f""Failed to respond to 'Shutdown Proceed' in time, got error {ex}""
"
990,"if (FLAGS_backend == """" || FLAGS_backend == ""cpu"") {
optimized_module = torch::jit::optimizeForMobile(module);
} else if (FLAGS_backend == ""vulkan"") {
    optimized_module = torch::jit::vulkanOptimizeForMobile(module);
} else {
CAFFE_ENFORCE(false, ""Unknown backend: "" + FLAGS_backend);
}
","if (FLAGS_backend == """" || FLAGS_backend == ""cpu"") {
optimized_module = torch::jit::optimizeForMobile(module);
} else if (FLAGS_backend == ""vulkan"") {
    std::vector<std::string> empty_preserved_methods;
    optimized_module = torch::jit::vulkanOptimizeForMobile(module, empty_preserved_methods);
} else {
CAFFE_ENFORCE(false, ""Unknown backend: "" + FLAGS_backend);
}
"
991,"return new Block(stmts);
}
Stmt* TermExpander::mutate(const Block* v) {
Stmt* new_stmt = IRSimplifierBase::mutate(v);
Block* new_block = dynamic_cast<Block*>(new_stmt);
","return new Block(stmts);
}
Stmt* TermExpander::fuseSyncThreads(Block* block) {
  // only really first if highest level Block.
  bool first = block->get_parent() == nullptr;
  SyncThreads* last = nullptr;
  std::vector<Stmt*> stmts;
  bool did_anything = false;

  for (auto* s : *block) {
    SyncThreads* sync = dynamic_cast<SyncThreads*>(s);
    if (!sync) {
      first = false;
      last = nullptr;
      stmts.push_back(s);
      continue;
    }

    if (first || last) {
      did_anything = true;
      continue;
    }

    last = sync;
    first = false;
    stmts.push_back(s);
  }

  if (last) {
    stmts.pop_back();
    did_anything = true;
  }

  if (!did_anything) {
    return block;
  }

  // clean up parents.
  for (auto* s : stmts) {
    if (s->get_parent() == block) {
      block->remove_stmt(s);
    }
  }

  return new Block({stmts});
}

Stmt* TermExpander::mutate(const Block* v) {
Stmt* new_stmt = IRSimplifierBase::mutate(v);
Block* new_block = dynamic_cast<Block*>(new_stmt);
"
992,"device = next(iter(devices)) if len(devices) > 0 else None
return device
# A dictionary for querying the weight index for a given op
WEIGHT_INDEX_DICT = {
","device = next(iter(devices)) if len(devices) > 0 else None
return device
def is_activation_post_process(module):
    return (isinstance(module, torch.quantization.ObserverBase) or
            isinstance(module, torch.quantization.FakeQuantize))
# A dictionary for querying the weight index for a given op
WEIGHT_INDEX_DICT = {
"
993,"auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
    auto* src_data = src.contiguous().data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
","auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
    auto src_contig = src.contiguous();
    auto* src_data = src_contig.data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
"
994,"to_be_removed.append(name)
for n in to_be_removed:
delattr(model, n)
model = GraphModule(model, self.quantized_graph)
return model
","to_be_removed.append(name)
for n in to_be_removed:
delattr(model, n)
        _remove_qconfig(model)
model = GraphModule(model, self.quantized_graph)
return model
"
995,"def _pad_circular(input, padding):
# type: (Tensor, List[int]) -> Tensor
    """"""
    Arguments
        :param input: tensor of shape :math:`(N, C_{\text{in}}, H, [W, D]))`
        :param padding: (tuple): m-elem tuple where m is the degree of convolution
    Returns
        :return: tensor of shape :math:`(N, C_{\text{in}}, [D + 2 * padding[0],
                 H + 2 * padding[1]], W + 2 * padding[2]))`
    """"""
    input = torch.cat([input, input[:, :, 0:padding[-1]]], dim=2)
    input = torch.cat([input[:, :, -(padding[-1] + padding[-2]):-padding[-1]], input], dim=2)
    if len(padding) > 2:
        input = torch.cat([input, input[:, :, :, 0:padding[-3]]], dim=3)
        input = torch.cat([input[:, :, :, -(padding[-3] + padding[-4]):-padding[-3]], input], dim=3)
    if len(padding) > 4:
        input = torch.cat([input, input[:, :, :, :, 0:padding[-5]]], dim=4)
        input = torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4)
    return input
def multi_head_attention_forward(query: Tensor,
","def _pad_circular(input, padding):
# type: (Tensor, List[int]) -> Tensor
    """"""Circularly pads tensor.
    Tensor values at the beginning are used to pad the end, and values at the
    end are used to pad the beginning. For example, consider a single dimension
    with values [0, 1, 2, 3]. With circular padding of (1, 1) it would be
    padded to [3, 0, 1, 2, 3, 0], and with padding (1, 2) it would be padded to
    [3, 0, 1, 2, 3, 0, 1]. If negative padding is applied then the ends of the
    tensor get removed. With circular padding of (-1, -1) the previous example
    would become [1, 2]. Circular padding of (-1, 1) would produce
    [1, 2, 3, 1].
    The first and second dimensions of the tensor are not padded.
    Args:
        input: Tensor with shape :math:`(N, C, D[, H, W])`.
        padding: Tuple containing the number of elements to pad each side of
            the tensor. The length of padding must be twice the number of
            paddable dimensions. For example, the length of padding should be 4
            for a tensor of shape :math:`(N, C, H, W)`, and the length should
            be 6 for a tensor of shape :math:`(N, C, D, H, W)`.
    Examples::

        >>> x = torch.tensor([[[[0, 1, 2], [3, 4, 5]]]])  # Create tensor
        >>> # Example 1
        >>> padding = (1, 1, 1, 1)
        >>> y = F.pad(x, padding, mode='circular')
        >>> print(y)
        tensor([[[[5, 3, 4, 5, 3],
                  [2, 0, 1, 2, 0],
                  [5, 3, 4, 5, 3],
                  [2, 0, 1, 2, 0]]]])
        >>> print(y.shape)
        torch.Size([1, 1, 4, 5])
        >>> # Example 2
        >>> padding = (1, 1, 2, 2)
        >>> z = F.pad(x, padding, mode='circular')
        >>> print(z)
        tensor([[[[2, 0, 1, 2, 0],
                  [5, 3, 4, 5, 3],
                  [2, 0, 1, 2, 0],
                  [5, 3, 4, 5, 3],
                  [2, 0, 1, 2, 0],
                  [5, 3, 4, 5, 3]]]])
        >>> print(z.shape)
        torch.Size([1, 1, 6, 5])
    """"""
    in_shape = input.shape
    paddable_shape = in_shape[2:]
    ndim = len(paddable_shape)

    for idx, size in enumerate(paddable_shape):
        # Only supports wrapping around once
        assert padding[-(idx * 2 + 1)] <= size, \
            ""Padding value causes wrapping around more than once.""
        assert padding[-(idx * 2 + 2)] <= size, \
            ""Padding value causes wrapping around more than once.""
        # Negative padding should not result in negative sizes
        assert padding[-(idx * 2 + 1)] + padding[-(idx * 2 + 2)] + size >= 0, \
            ""Negative padding value is resulting in an empty dimension.""

    # Get shape of padded tensor
    out_shape = in_shape[:2]
    for idx, size in enumerate(paddable_shape):
        out_shape += (size + padding[-(idx * 2 + 1)] + padding[-(idx * 2 + 2)],)

    out = torch.empty(out_shape, dtype=input.dtype, layout=input.layout,
                      device=input.device)

    # Put original array in padded array
    if ndim == 1:
        out_d0 = max(padding[-2], 0)
        out_d1 = out_shape[2] - max(padding[-1], 0)

        in_d0 = max(-padding[-2], 0)
        in_d1 = in_shape[2] - max(-padding[-1], 0)

        out[..., out_d0:out_d1] = input[..., in_d0:in_d1]
    elif ndim == 2:
        out_d0 = max(padding[-2], 0)
        out_d1 = out_shape[2] - max(padding[-1], 0)

        out_h0 = max(padding[-4], 0)
        out_h1 = out_shape[3] - max(padding[-3], 0)

        in_d0 = max(-padding[-2], 0)
        in_d1 = in_shape[2] - max(-padding[-1], 0)

        in_h0 = max(-padding[-4], 0)
        in_h1 = in_shape[3] - max(-padding[-3], 0)

        out[..., out_d0:out_d1, out_h0:out_h1] = \
            input[..., in_d0:in_d1, in_h0:in_h1]
    elif ndim == 3:
        out_d0 = max(padding[-2], 0)
        out_d1 = out_shape[2] - max(padding[-1], 0)

        out_h0 = max(padding[-4], 0)
        out_h1 = out_shape[3] - max(padding[-3], 0)

        out_w0 = max(padding[-6], 0)
        out_w1 = out_shape[4] - max(padding[-5], 0)

        in_d0 = max(-padding[-2], 0)
        in_d1 = in_shape[2] - max(-padding[-1], 0)

        in_h0 = max(-padding[-4], 0)
        in_h1 = in_shape[3] - max(-padding[-3], 0)

        in_w0 = max(-padding[-6], 0)
        in_w1 = in_shape[4] - max(-padding[-5], 0)

        out[..., out_d0:out_d1, out_h0:out_h1, out_w0:out_w1] = \
            input[..., in_d0:in_d1, in_h0:in_h1, in_w0:in_w1]

    # The following steps first pad the beginning of the tensor (left side),
    # and then pad the end of the tensor (right side).
    # Note: Corners will be written more than once when ndim > 1.

    # Only in cases where padding values are > 0 are when additional copying
    # is required.

    # Pad first dimension (depth)
    if padding[-2] > 0:
        i0 = out_shape[2] - padding[-2] - max(padding[-1], 0)
        i1 = out_shape[2] - max(padding[-1], 0)
        o0 = 0
        o1 = padding[-2]
        out[:, :, o0:o1] = out[:, :, i0:i1]
    if padding[-1] > 0:
        i0 = max(padding[-2], 0)
        i1 = max(padding[-2], 0) + padding[-1]
        o0 = out_shape[2] - padding[-1]
        o1 = out_shape[2]
        out[:, :, o0:o1] = out[:, :, i0:i1]

    # Pad second dimension (height)
    if len(padding) > 2:
        if padding[-4] > 0:
            i0 = out_shape[3] - padding[-4] - max(padding[-3], 0)
            i1 = out_shape[3] - max(padding[-3], 0)
            o0 = 0
            o1 = padding[-4]
            out[:, :, :, o0:o1] = \
                out[:, :, :, i0:i1]
        if padding[-3] > 0:
            i0 = max(padding[-4], 0)
            i1 = max(padding[-4], 0) + padding[-3]
            o0 = out_shape[3] - padding[-3]
            o1 = out_shape[3]
            out[:, :, :, o0:o1] = \
                out[:, :, :, i0:i1]

    # Pad third dimension (width)
    if len(padding) > 4:
        if padding[-6] > 0:
            i0 = out_shape[4] - padding[-6] - max(padding[-5], 0)
            i1 = out_shape[4] - max(padding[-5], 0)
            o0 = 0
            o1 = padding[-6]
            out[:, :, :, :, o0:o1] = \
                out[:, :, :, :, i0:i1]
        if padding[-5] > 0:
            i0 = max(padding[-6], 0)
            i1 = max(padding[-6], 0) + padding[-5]
            o0 = out_shape[4] - padding[-5]
            o1 = out_shape[4]
            out[:, :, :, :, o0:o1] = \
                out[:, :, :, :, i0:i1]

    return out
def multi_head_attention_forward(query: Tensor,
"
996,"""cudaHostRegisterIoMemory"",
(""hipHostRegisterIoMemory"", CONV_MEM, API_RUNTIME),
),
        # (""warpSize"", (""hipWarpSize"", CONV_SPECIAL_FUNC, API_RUNTIME), (HIP actually uses warpSize...),
(""cudaEventCreate"", (""hipEventCreate"", CONV_EVENT, API_RUNTIME)),
(
""cudaEventCreateWithFlags"",
","""cudaHostRegisterIoMemory"",
(""hipHostRegisterIoMemory"", CONV_MEM, API_RUNTIME),
),
        # (""warpSize"", (""hipWarpSize"", CONV_SPECIAL_FUNC, API_RUNTIME), (HIP actually uses warpSize...)),
(""cudaEventCreate"", (""hipEventCreate"", CONV_EVENT, API_RUNTIME)),
(
""cudaEventCreateWithFlags"",
"
997,"from typing import List, Optional
from ..util.setting import TOOLS_FOLDER, CompilerType, TestType
from ..util.utils import check_compiler_type, print_error, remove_file
def get_oss_binary_folder(test_type: TestType) -> str:
","from typing import List, Optional
from ..util.setting import TOOLS_FOLDER, CompilerType, TestType
from ..util.utils import print_error, remove_file
def get_oss_binary_folder(test_type: TestType) -> str:
"
998,"}
void CudaPrinter::visit(const Cast* v) {
os() << cudaDtypeCppString(v->dtype());
os() << ""("";
v->src_value()->accept(this);
","}
void CudaPrinter::visit(const Cast* v) {
  if (v->dtype().scalar_type() == ScalarType::Half) {
    os() << ""__float2half("";
    v->src_value()->accept(this);
    os() << "")"";
    return;
  } else if (v->src_value()->dtype().scalar_type() == ScalarType::Half) {
    os() << ""__half2float("";
    v->src_value()->accept(this);
    os() << "")"";
    return;
  }

os() << cudaDtypeCppString(v->dtype());
os() << ""("";
v->src_value()->accept(this);
"
999,"# <project folder>
HOME_DIR = os.environ[""HOME""]
setting_file_path = os.path.realpath(__file__)
SCRIPT_FOLDER = os.path.join(
    os.path.dirname(setting_file_path), os.path.pardir, os.path.pardir
)
# <profile folder>
PROFILE_DIR = os.path.join(SCRIPT_FOLDER, ""profile"")
JSON_FOLDER_BASE_DIR = os.path.join(PROFILE_DIR, ""json"")
MERGED_FOLDER_BASE_DIR = os.path.join(PROFILE_DIR, ""merged"")
SUMMARY_FOLDER_DIR = os.path.join(PROFILE_DIR, ""summary"")
","# <project folder>
HOME_DIR = os.environ[""HOME""]
TOOLS_FOLDER = os.path.join(
    os.path.dirname(os.path.realpath(__file__)), os.path.pardir, os.path.pardir
)
# <profile folder>
PROFILE_DIR = os.path.join(TOOLS_FOLDER, ""profile"")
JSON_FOLDER_BASE_DIR = os.path.join(PROFILE_DIR, ""json"")
MERGED_FOLDER_BASE_DIR = os.path.join(PROFILE_DIR, ""merged"")
SUMMARY_FOLDER_DIR = os.path.join(PROFILE_DIR, ""summary"")
"
1000,"Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_fill_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_fill_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1001,"std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_mode_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_mode_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1002,"Tensor & _th_fmod_(Tensor & self, const Tensor & other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_fmod_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_fmod_(Tensor & self, const Tensor & other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_fmod_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1003,"std::tuple<Tensor &,Tensor &> _thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_log_sigmoid_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_log_sigmoid_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1004,"Tensor & _thnn_rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<at::Generator> generator) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_rrelu_with_noise_forward_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _thnn_rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<at::Generator> generator) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_rrelu_with_noise_forward_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1005,"return at::native::std_out(result, self, dim, unbiased, keepdim);
}
Tensor &std_out(Tensor &result, const Tensor &self, IntArrayRef dim, bool unbiased, bool keepdim) {
return std_var_out(result, self, dim, unbiased, keepdim, true);
}
","return at::native::std_out(result, self, dim, unbiased, keepdim);
}
Tensor& std_out(Tensor& result, const Tensor& self, IntArrayRef dim, bool unbiased, bool keepdim) {
return std_var_out(result, self, dim, unbiased, keepdim, true);
}
"
1006,"torch.Size([2, 2, 1, 2])
"""""".format(**common_args))
add_docstr(torch.std,
           r""""""
std(input, unbiased=True) -> Tensor
Returns the standard-deviation of all elements in the :attr:`input` tensor.
","torch.Size([2, 2, 1, 2])
"""""".format(**common_args))
add_docstr(torch.std, r""""""
std(input, unbiased=True) -> Tensor
Returns the standard-deviation of all elements in the :attr:`input` tensor.
"
1007,"[ 4]])
"""""".format(**common_args))
add_docstr(torch.var,
           r""""""
var(input, unbiased=True) -> Tensor
Returns the variance of all elements in the :attr:`input` tensor.
","[ 4]])
"""""".format(**common_args))
add_docstr(torch.var, r""""""
var(input, unbiased=True) -> Tensor
Returns the variance of all elements in the :attr:`input` tensor.
"
1008,"nn.EmbeddingBag: nnqd.EmbeddingBag,
}
# Whitelist for propagating the qconfig
_EXCLUDE_QCONFIG_PROPAGATE_LIST = {
DeQuantStub,
}
","nn.EmbeddingBag: nnqd.EmbeddingBag,
}
# Allowed list for propagating the qconfig
_EXCLUDE_QCONFIG_PROPAGATE_LIST = {
DeQuantStub,
}
"
1009,"module._modules[name] = add_quant_dequant(child)
return module
def prepare(model, inplace=False, white_list=None,
observer_non_leaf_module_list=None, prehook=None):
r""""""Prepares a copy of the model for quantization calibration or quantization-aware training.
","module._modules[name] = add_quant_dequant(child)
return module
def prepare(model, inplace=False, allow_list=None,
observer_non_leaf_module_list=None, prehook=None):
r""""""Prepares a copy of the model for quantization calibration or quantization-aware training.
"
1010,"const Tensor& input,
const Tensor& buffer) {
auto iter = TensorIteratorConfig()
    .set_check_mem_overlap(true)
.add_output(grad_input)
.add_input(input)
.add_input(buffer)
","const Tensor& input,
const Tensor& buffer) {
auto iter = TensorIteratorConfig()
.add_output(grad_input)
.add_input(input)
.add_input(buffer)
"
1011,"auto result_stride_bytes = result.stride(dim) * elementSize(result.scalar_type());
auto iter = TensorIteratorConfig()
.resize_outputs(false)
.add_output(result_slice)
.add_input(source_slice)
","auto result_stride_bytes = result.stride(dim) * elementSize(result.scalar_type());
auto iter = TensorIteratorConfig()
      .set_check_mem_overlap(false)  // Already checked above
.resize_outputs(false)
.add_output(result_slice)
.add_input(source_slice)
"
1012,"// Phase 0. Inline functions, then clean up any artifacts that the inliner
//          left in that may inhibit optimization
Inline(*opt_graph);
    GRAPH_DUMP(""After Inline, before LowerGradOf"", opt_graph);
LowerGradOf(*opt_graph);
    GRAPH_DUMP(""After LowerGradOf, before specializeAutogradZero"", opt_graph);
specializeAutogradZero(opt_graph);
    GRAPH_DUMP(
        ""After specializeAutogradZero, before LowerSimpleTuples"", opt_graph);
LowerSimpleTuples(opt_graph);
    GRAPH_DUMP(""After LowerSimpleTuples, before ConstantPooling"", opt_graph);
ConstantPooling(opt_graph);
    GRAPH_DUMP(""After ConstantPooling, before runRequiredPasses"", opt_graph);
// Phase 1. Specialize to input definedness (this is very important for
//          gradient graphs), and run required passes to bring the graph
//          to an executable form.
runRequiredPasses(opt_graph);
    GRAPH_DUMP(
        ""After runRequiredPasses, before ConstantPropagation"", opt_graph);
// Phase 2. Propagate detailed information about the spec through the
//          graph (enabled more specializations in later passes).
","// Phase 0. Inline functions, then clean up any artifacts that the inliner
//          left in that may inhibit optimization
Inline(*opt_graph);
    GRAPH_DEBUG(""After Inline, before LowerGradOf\n"", *opt_graph);
LowerGradOf(*opt_graph);
    GRAPH_DEBUG(
        ""After LowerGradOf, before specializeAutogradZero\n"", *opt_graph);
specializeAutogradZero(opt_graph);
    GRAPH_DEBUG(
        ""After specializeAutogradZero, before LowerSimpleTuples\n"", *opt_graph);
LowerSimpleTuples(opt_graph);
    GRAPH_DEBUG(
        ""After LowerSimpleTuples, before ConstantPooling\n"", *opt_graph);
ConstantPooling(opt_graph);
    GRAPH_DEBUG(
        ""After ConstantPooling, before runRequiredPasses\n"", *opt_graph);
// Phase 1. Specialize to input definedness (this is very important for
//          gradient graphs), and run required passes to bring the graph
//          to an executable form.
runRequiredPasses(opt_graph);
    GRAPH_DEBUG(
        ""After runRequiredPasses, before ConstantPropagation\n"", *opt_graph);
// Phase 2. Propagate detailed information about the spec through the
//          graph (enabled more specializations in later passes).
"
1013,"} else {
FuseGraph(graph, strict_fuser_check);
}
  GRAPH_DUMP(""After Fusion"", graph);
// Run custom post-fusion passes
for (const auto& passPair : getCustomPostPasses()) {
passPair.first(graph);
}
  GRAPH_DUMP(""After customPostPassses (end of runNondiffOptimization)"", graph);
}
void runOptimization(std::shared_ptr<Graph>& graph, bool unroll) {
// Basic graph preprocessing to eliminate noise.
  GRAPH_DUMP(""Before EliminateDeadCode (beginning of runOptimization)"", graph);
EliminateDeadCode(graph);
  GRAPH_DUMP(
      ""After EliminateDeadCode, before EliminateCommonSubexpression"", graph);
EliminateCommonSubexpression(graph);
  GRAPH_DUMP(
      ""After EliminateCommonSubexpression, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
  GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
  GRAPH_DUMP(""After ConstantPropagation, before ConstantPooling"", graph);
ConstantPooling(graph);
  GRAPH_DUMP(""After ConstantPooling"", graph);
// Unroll small loops, and eliminate expressions that are the same at every
// iteration.
if (unroll) {
UnrollLoops(graph);
    GRAPH_DUMP(""After UnrollLoops, before RemoveListMutation"", graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DUMP(""After RemoveListMutation, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
    GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
    GRAPH_DUMP(""After ConstantPropagation"", graph);
}
EliminateCommonSubexpression(graph);
  GRAPH_DUMP(""After EliminateCommonSubexpression, before CheckInplace"", graph);
CheckInplace(graph);
  GRAPH_DUMP(""After CheckInplace (end of runOptimization)"", graph);
}
} // namespace jit
","} else {
FuseGraph(graph, strict_fuser_check);
}
  GRAPH_DEBUG(""After Fusion\n"", *graph);
// Run custom post-fusion passes
for (const auto& passPair : getCustomPostPasses()) {
passPair.first(graph);
}
  GRAPH_DEBUG(
      ""After customPostPassses (end of runNondiffOptimization)\n"", *graph);
}
void runOptimization(std::shared_ptr<Graph>& graph, bool unroll) {
// Basic graph preprocessing to eliminate noise.
  GRAPH_DEBUG(
      ""Before EliminateDeadCode (beginning of runOptimization)\n"", *graph);
EliminateDeadCode(graph);
  GRAPH_DEBUG(
      ""After EliminateDeadCode, before EliminateCommonSubexpression\n"", *graph);
EliminateCommonSubexpression(graph);
  GRAPH_DEBUG(
      ""After EliminateCommonSubexpression, before PeepholeOptimize\n"", *graph);
PeepholeOptimize(graph);
  GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
  GRAPH_DEBUG(""After ConstantPropagation, before ConstantPooling\n"", *graph);
ConstantPooling(graph);
  GRAPH_DEBUG(""After ConstantPooling\n"", *graph);
// Unroll small loops, and eliminate expressions that are the same at every
// iteration.
if (unroll) {
UnrollLoops(graph);
    GRAPH_DEBUG(""After UnrollLoops, before RemoveListMutation\n"", *graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DEBUG(""After RemoveListMutation, before PeepholeOptimize\n"", *graph);
PeepholeOptimize(graph);
    GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
    GRAPH_DEBUG(""After ConstantPropagation\n"", *graph);
}
EliminateCommonSubexpression(graph);
  GRAPH_DEBUG(
      ""After EliminateCommonSubexpression, before CheckInplace\n"", *graph);
CheckInplace(graph);
  GRAPH_DEBUG(""After CheckInplace (end of runOptimization)"", *graph);
}
} // namespace jit
"
1014,"ClearUndefinedness(graph);
// runRequiredPasses
{
    GRAPH_DUMP(""After ClearUndefinedness, before RemoveExpands"", graph);
RemoveExpands(graph);
    GRAPH_DUMP(""After RemoveExpands, before CanonicalizeOps"", graph);
CanonicalizeOps(graph);
    GRAPH_DUMP(""After CanonicalizeOps, before EliminateDeadCode"", graph);
EliminateDeadCode(graph);
}
if (!getGraphExecutorOptimize()) {
    GRAPH_DUMP(
        ""After EliminateDeadCode (end of runProfilingInsensitiveOptimizations)"",
        graph);
return;
}
  GRAPH_DUMP(""After EliminateDeadCode, before DecomposeOps"", graph);
DecomposeOps(graph);
  GRAPH_DUMP(""After DecomposeOps, before ConstantPropagation"", graph);
ConstantPropagation(graph);
  GRAPH_DUMP(""After ConstantPropagation, before EliminateDeadCode"", graph);
EliminateDeadCode(graph);
  GRAPH_DUMP(
      ""After EliminateDeadCode, before EliminateCommonSubexpression"", graph);
EliminateCommonSubexpression(graph);
  GRAPH_DUMP(
      ""After EliminateCommonSubexpression, before ConstantPooling"", graph);
ConstantPooling(graph);
  GRAPH_DUMP(""After ConstantPooling, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
  GRAPH_DUMP(""After PeepholeOptimize, before EliminateDeadCode"", graph);
EliminateDeadCode(graph);
  GRAPH_DUMP(""After EliminateDeadCode, before LowerSimpleTuples"", graph);
LowerSimpleTuples(graph);
  GRAPH_DUMP(""After LowerSimpleTuples, before CheckInplace"", graph);
CheckInplace(graph);
  GRAPH_DUMP(
      ""After CheckInplace (end of runProfilingInsensitiveOptimizations)"",
      graph);
}
ProfilingGraphExecutorImpl::ProfilingGraphExecutorImpl(
","ClearUndefinedness(graph);
// runRequiredPasses
{
    GRAPH_DEBUG(""After ClearUndefinedness, before RemoveExpands\n"", *graph);
RemoveExpands(graph);
    GRAPH_DEBUG(""After RemoveExpands, before CanonicalizeOps\n"", *graph);
CanonicalizeOps(graph);
    GRAPH_DEBUG(""After CanonicalizeOps, before EliminateDeadCode\n"", *graph);
EliminateDeadCode(graph);
}
if (!getGraphExecutorOptimize()) {
    GRAPH_DEBUG(
        ""After EliminateDeadCode (end of runProfilingInsensitiveOptimizations)\n"",
        *graph);
return;
}
  GRAPH_DEBUG(""After EliminateDeadCode, before DecomposeOps\n"", *graph);
DecomposeOps(graph);
  GRAPH_DEBUG(""After DecomposeOps, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
  GRAPH_DEBUG(""After ConstantPropagation, before EliminateDeadCode\n"", *graph);
EliminateDeadCode(graph);
  GRAPH_DEBUG(
      ""After EliminateDeadCode, before EliminateCommonSubexpression\n"", *graph);
EliminateCommonSubexpression(graph);
  GRAPH_DEBUG(
      ""After EliminateCommonSubexpression, before ConstantPooling\n"", *graph);
ConstantPooling(graph);
  GRAPH_DEBUG(""After ConstantPooling, before PeepholeOptimize\n"", *graph);
PeepholeOptimize(graph);
  GRAPH_DEBUG(""After PeepholeOptimize, before EliminateDeadCode\n"", *graph);
EliminateDeadCode(graph);
  GRAPH_DEBUG(""After EliminateDeadCode, before LowerSimpleTuples\n"", *graph);
LowerSimpleTuples(graph);
  GRAPH_DEBUG(""After LowerSimpleTuples, before CheckInplace\n"", *graph);
CheckInplace(graph);
  GRAPH_DEBUG(
      ""After CheckInplace (end of runProfilingInsensitiveOptimizations)\n"",
      *graph);
}
ProfilingGraphExecutorImpl::ProfilingGraphExecutorImpl(
"
1015,"futureMessage->addCallback([this](const rpc::FutureMessage& futureMessage) {
if (futureMessage.hasError()) {
// If we have an error, let the local autograd engine know about it.
      std::runtime_error err((*futureMessage.error()).what());
std::unique_lock<std::mutex> lock(lock_);
if (graphTask_) {
graphTask_->set_exception_without_signal(nullptr);
lock.unlock();
if (!graphTask_->future_completed_.exchange(true)) {
          graphTask_->future_result_->setErrorIfNeeded(err.what());
}
} else {
LOG(WARNING) << ""Ignoring error since GraphTask is no longer valid: ""
                     << err.what();
}
}
});
","futureMessage->addCallback([this](const rpc::FutureMessage& futureMessage) {
if (futureMessage.hasError()) {
// If we have an error, let the local autograd engine know about it.
std::unique_lock<std::mutex> lock(lock_);
if (graphTask_) {
graphTask_->set_exception_without_signal(nullptr);
lock.unlock();
if (!graphTask_->future_completed_.exchange(true)) {
          graphTask_->future_result_->setErrorIfNeeded(
              std::make_exception_ptr(*futureMessage.error()));
}
} else {
LOG(WARNING) << ""Ignoring error since GraphTask is no longer valid: ""
                     << (*futureMessage.error()).what();
}
}
});
"
1016,"ownerRRef->setValue(std::move(py_ivalue));
} catch (py::error_already_set& e) {
// py::error_already_set requires GIL to destruct, take special care.
      ownerRRef->setError(e.what());
py::gil_scoped_acquire acquire;
e.restore();
PyErr_Clear();
} catch (std::exception& e) {
      ownerRRef->setError(e.what());
}
markComplete(RemoteRet(rrefId, forkId).toMessage());
}
","ownerRRef->setValue(std::move(py_ivalue));
} catch (py::error_already_set& e) {
// py::error_already_set requires GIL to destruct, take special care.
      ownerRRef->setError(std::current_exception());
py::gil_scoped_acquire acquire;
e.restore();
PyErr_Clear();
} catch (std::exception& e) {
      ownerRRef->setError(std::current_exception());
}
markComplete(RemoteRet(rrefId, forkId).toMessage());
}
"
1017,"return result;
} else {
    AT_ASSERTM(self.sizes() == other.sizes(),
""mkldnn_mul_out: currently mkldnn not support broadcasting"");
ideep::tensor y = itensor_from_mkldnn(other);
ideep::binary::compute(x, y, z, dnnl::algorithm::binary_mul);
","return result;
} else {
    TORCH_CHECK(self.sizes() == other.sizes(),
""mkldnn_mul_out: currently mkldnn not support broadcasting"");
ideep::tensor y = itensor_from_mkldnn(other);
ideep::binary::compute(x, y, z, dnnl::algorithm::binary_mul);
"
1018,"bool count_include_pad,
c10::optional<int64_t> divisor_override) {
TORCH_CHECK(!divisor_override.has_value(),
           ""mkldnn_avg_pool2d operator does not support divisor"");
return _mkldnn_pooling(
input,
kernel_size,
","bool count_include_pad,
c10::optional<int64_t> divisor_override) {
TORCH_CHECK(!divisor_override.has_value(),
      ""mkldnn_avg_pool2d operator does not support divisor"");
return _mkldnn_pooling(
input,
kernel_size,
"
1019,"return_counts=return_counts, dim=dim)
if dim is not None:
        output, inverse_indices, counts = _VF.unique_dim(
input,
dim,
sorted=sorted,
","return_counts=return_counts, dim=dim)
if dim is not None:
        output, inverse_indices, counts = _VF.unique_dim(  # type: ignore
input,
dim,
sorted=sorted,
"
1020,"output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
return output
def _return_inverse(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
","output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
return output

def _return_inverse(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
"
1021,"ndim = input.dim()

# catch default case
if dim is None and out is None and dtype is None and p is not None:
if isinstance(p, str):
if p == ""fro"":
                return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)
if not isinstance(p, str):
_dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))
            return _VF.norm(input, p, dim=_dim, keepdim=keepdim)
# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed
# remove the overloads where dim is an int and replace with BraodcastingList1
","ndim = input.dim()
# catch default case
if dim is None and out is None and dtype is None and p is not None:
if isinstance(p, str):
if p == ""fro"":
                return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)  # type: ignore
if not isinstance(p, str):
_dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))
            return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore
# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed
# remove the overloads where dim is an int and replace with BraodcastingList1
"
1022,"else:
_dim = dim
else:
        _dim = None
if isinstance(p, str):
if p == ""fro"":
","else:
_dim = dim
else:
        _dim = None  # type: ignore
if isinstance(p, str):
if p == ""fro"":
"
1023,"def __getattr__(self, attr):
return getattr(self.vf, attr)
sys.modules[__name__] = VFModule(__name__)
","def __getattr__(self, attr):
return getattr(self.vf, attr)

sys.modules[__name__] = VFModule(__name__)
"
1024,"{aten::linalg_det, aten::det},
{aten::outer, aten::ger},
{aten::arccosh, aten::acosh},
    {aten::arccosh_, aten::acosh_}};
void replaceNodeWithNewSymbol(Node* node, Symbol new_symbol) {
WithInsertPoint insert_guard{node};
","{aten::linalg_det, aten::det},
{aten::outer, aten::ger},
{aten::arccosh, aten::acosh},
    {aten::arccosh_, aten::acosh_},
    {aten::fix, aten::trunc},
    {aten::fix_, aten::trunc_},
};
void replaceNodeWithNewSymbol(Node* node, Symbol new_symbol) {
WithInsertPoint insert_guard{node};
"
1025,"torch.fbgemm_pack_quantized_matrix: lambda input, a, b: -1,
torch.feature_alpha_dropout: lambda input, p, train: -1,
torch.feature_dropout: lambda input, p, train: -1,
torch.fft: lambda input, signal_ndim, normalized=False: -1,
torch.flatten: lambda input, start_dim=0, end_dim=-1: -1,
torch.flip: lambda input, dims: -1,
","torch.fbgemm_pack_quantized_matrix: lambda input, a, b: -1,
torch.feature_alpha_dropout: lambda input, p, train: -1,
torch.feature_dropout: lambda input, p, train: -1,
        torch.fix: lambda input, out=None: -1,
torch.fft: lambda input, signal_ndim, normalized=False: -1,
torch.flatten: lambda input, start_dim=0, end_dim=-1: -1,
torch.flip: lambda input, dims: -1,
"
1026,"Example::
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
    >>>      pred = model.forward()
    >>>      loss = loss_func(pred, loss)
    >>>      dist_autograd.backward(context_id, loss)
)"",
py::arg(""contextId""),
py::arg(""roots""),
","Example::
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
    >>>     pred = model.forward()
    >>>     loss = loss_func(pred, loss)
    >>>     dist_autograd.backward(context_id, loss)
)"",
py::arg(""contextId""),
py::arg(""roots""),
"
1027,"""XNNPACK Convolution not usable! ""
""Reason: The provided input tensor is either invalid or unsupported by XNNPACK."");
  Tensor output = empty_with_tail_padding(
conv_output_size(
padded_input_nhwc.sizes(),
context.weight_size_,
","""XNNPACK Convolution not usable! ""
""Reason: The provided input tensor is either invalid or unsupported by XNNPACK."");
  Tensor output = mobile::empty_with_tail_padding(
conv_output_size(
padded_input_nhwc.sizes(),
context.weight_size_,
"
1028,"float a = alpha.to<float>();
VulkanTensor output{self.sizes().vec()};
  output.allocate_storage();
vulkan::detail::add(output, x, y, a);
x = std::move(output);
return self;
","float a = alpha.to<float>();
VulkanTensor output{self.sizes().vec()};
vulkan::detail::add(output, x, y, a);
x = std::move(output);
return self;
"
1029,"#include ""caffe2/core/context.h""
#include ""caffe2/utils/math.h""
namespace caffe2 {
// dimA(before transpose) = M x N, dimA (after transpose) = N x M.
","#include ""caffe2/core/context.h""
#include ""caffe2/utils/math.h""
C10_DECLARE_bool(caffe2_fbgemm_fake_fp16_clamp);

namespace caffe2 {
// dimA(before transpose) = M x N, dimA (after transpose) = N x M.
"
1030,"return py::make_iterator(b.outputs().begin(), b.outputs().end());
})
.def(""returnNode"", [](Block& b) { return b.return_node(); })
      .def(""paramNode"", [](Block& b) { return b.param_node(); });
#define NS(name) def(#name, &Node ::name)
py::class_<Node, std::unique_ptr<Node, py::nodelete>>(m, ""Node"")
","return py::make_iterator(b.outputs().begin(), b.outputs().end());
})
.def(""returnNode"", [](Block& b) { return b.return_node(); })
      .def(""paramNode"", [](Block& b) { return b.param_node(); })
      .def(""addNode"", [](Block& b, Value& input, const char* str) {
        return addNodeToBlock(&b, &input, Symbol::fromQualString(str));
      });
#define NS(name) def(#name, &Node ::name)
py::class_<Node, std::unique_ptr<Node, py::nodelete>>(m, ""Node"")
"
1031,"def _check_args_can_be_mapped_with_in_dims(
in_dims_as_tuple: Tuple[Optional[int], ...],
args: Tuple,
        fn_name: str,
in_dims: in_dims_t) -> None:
for idx, (in_dim, arg) in enumerate(zip(in_dims_as_tuple, args)):
if in_dim is None:
continue
if not isinstance(in_dim, int):
raise ValueError(
                f'vmap({fn_name}, in_dims={in_dims}, ...)(<inputs>): in_dims '
f'must be a flat tuple containing ints and/or Nones. If you were '
f'trying to vmap over a Tensor inside a Python collection in '
f'`inputs`, we do not yet support that.')
if not isinstance(arg, Tensor):
raise ValueError(
                f'vmap({fn_name}, in_dims={in_dims}, ...)(<inputs>): Got '
f'in_dim={in_dim} for input {idx}, but input {idx} is not a '
f'Tensor (got {type(arg)}) so it cannot be vmap\'ed over. '
f'If you were trying to vmap over a Tensor inside a Python '
","def _check_args_can_be_mapped_with_in_dims(
in_dims_as_tuple: Tuple[Optional[int], ...],
args: Tuple,
        func: Callable,
in_dims: in_dims_t) -> None:
for idx, (in_dim, arg) in enumerate(zip(in_dims_as_tuple, args)):
if in_dim is None:
continue
if not isinstance(in_dim, int):
raise ValueError(
                f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): in_dims '
f'must be a flat tuple containing ints and/or Nones. If you were '
f'trying to vmap over a Tensor inside a Python collection in '
f'`inputs`, we do not yet support that.')
if not isinstance(arg, Tensor):
raise ValueError(
                f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): Got '
f'in_dim={in_dim} for input {idx}, but input {idx} is not a '
f'Tensor (got {type(arg)}) so it cannot be vmap\'ed over. '
f'If you were trying to vmap over a Tensor inside a Python '
"
1032,"if in_dim >= 0 and in_dim < arg.dim():
continue
raise ValueError(
            f'vmap({fn_name}, in_dims={in_dims}, ...)(<inputs>): Got in_dim={in_dim} '
f'for input {idx}, but input {idx} is a Tensor of dimensionality '
f'{arg.dim()} so expected in_dim to satisfy 0 <= in_dim < {arg.dim()}.')
","if in_dim >= 0 and in_dim < arg.dim():
continue
raise ValueError(
            f'vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>): Got in_dim={in_dim} '
f'for input {idx}, but input {idx} is a Tensor of dimensionality '
f'{arg.dim()} so expected in_dim to satisfy 0 <= in_dim < {arg.dim()}.')
"
1033,"qconfig=qconfig)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
        self.activation_post_process = self.qconfig.activation()
self.weight_fake_quant = self.qconfig.weight()
def forward(self, input):
        return self.activation_post_process(F.relu(
            self._conv_forward(input, self.weight_fake_quant(self.weight))))
@classmethod
def from_float(cls, mod, qconfig=None):
","qconfig=qconfig)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
self.weight_fake_quant = self.qconfig.weight()
def forward(self, input):
        return F.relu(
            self._conv_forward(input, self.weight_fake_quant(self.weight)))
@classmethod
def from_float(cls, mod, qconfig=None):
"
1034,"class Conv2d(nn.Conv2d):
r""""""
    A Conv2d module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.
We adopt the same interface as `torch.nn.Conv2d`, please see
https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d
","class Conv2d(nn.Conv2d):
r""""""
    A Conv2d module attached with FakeQuantize modules for weight,
    used for quantization aware training.
We adopt the same interface as `torch.nn.Conv2d`, please see
https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d
"
1035,"default.
Attributes:
        activation_post_process: fake quant module for output activation
weight_fake_quant: fake quant module for weight
""""""
_FLOAT_MODULE = nn.Conv2d
","default.
Attributes:
weight_fake_quant: fake quant module for weight
""""""
_FLOAT_MODULE = nn.Conv2d
"
1036,"class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
","class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for weight,
    used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
"
1037,"import torch.nn as nn
import torch.nn.intrinsic as nni
import torch.nn.quantized as nnq
from .default_mappings import (DEFAULT_DYNAMIC_MODULE_MAPPING,
DEFAULT_MODULE_MAPPING,
","import torch.nn as nn
import torch.nn.intrinsic as nni
import torch.nn.quantized as nnq
import torch.nn.intrinsic.qat as nniqat
from .default_mappings import (DEFAULT_DYNAMIC_MODULE_MAPPING,
DEFAULT_MODULE_MAPPING,
"
1038,"#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/python_anomaly_mode.h>
#include <torch/csrc/autograd/python_function.h>
#include <pybind11/pybind11.h>
#ifndef _WIN32
","#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/python_anomaly_mode.h>
#include <torch/csrc/autograd/python_function.h>
#include <ATen/BatchedTensorImpl.h>
#include <ATen/VmapMode.h>
#include <pybind11/pybind11.h>
#ifndef _WIN32
"
1039,"#include ""caffe2/core/tensor_int8.h""
#include ""caffe2_dnnlowp_utils.h""
C10_DECLARE_int32(caffe2_dnnlowp_nbits_in_non_outlier);
C10_DECLARE_double(caffe2_dnnlowp_acc16_density_threshold);
","#include ""caffe2/core/tensor_int8.h""
#include ""caffe2_dnnlowp_utils.h""
#include <fbgemm/FbgemmConvert.h>
C10_DECLARE_int32(caffe2_dnnlowp_nbits_in_non_outlier);
C10_DECLARE_double(caffe2_dnnlowp_acc16_density_threshold);
"
1040,"b_quantized[i] = std::max(std::min(b_quantized[i], INT32_MAX), INT32_MIN);
}
}

}
}
}
","b_quantized[i] = std::max(std::min(b_quantized[i], INT32_MAX), INT32_MIN);
}
}
}
}
}
"
1041,"return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
Tensor det(const Tensor& self) {
squareCheckInputs(self);
TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
","return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
// torch.linalg.det, alias for torch.det
Tensor linalg_det(const Tensor& self) {
  return self.det();
}

Tensor det(const Tensor& self) {
squareCheckInputs(self);
TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
"
1042,"See :func:`torch.ger`
"""""")
add_docstr_all('indices',
r""""""
indices() -> Tensor
","See :func:`torch.ger`
"""""")
add_docstr_all('outer', r""""""
outer(vec2) -> Tensor

See :func:`torch.outer`.
"""""")

add_docstr_all('indices',
r""""""
indices() -> Tensor
"
1043,""""""")
add_docstr(torch.ger,
r""""""
ger(input, vec2, out=None) -> Tensor
",""""""")
add_docstr(torch.outer, r""""""
outer(input, vec2, *, out=None) -> Tensor

Alias of :func:`torch.ger`.
"""""")

add_docstr(torch.ger,
r""""""
ger(input, vec2, out=None) -> Tensor
"
1044,"raise ValueError(""You must define either total_steps OR (epochs AND steps_per_epoch)"")
elif total_steps is not None:
if total_steps <= 0 or not isinstance(total_steps, int):
                raise ValueError(""Expected non-negative integer total_steps, but got {}"".format(total_steps))
self.total_steps = total_steps
else:
if epochs <= 0 or not isinstance(epochs, int):
                raise ValueError(""Expected non-negative integer epochs, but got {}"".format(epochs))
if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):
                raise ValueError(""Expected non-negative integer steps_per_epoch, but got {}"".format(steps_per_epoch))
self.total_steps = epochs * steps_per_epoch
self.step_size_up = float(pct_start * self.total_steps) - 1
self.step_size_down = float(self.total_steps - self.step_size_up) - 1
","raise ValueError(""You must define either total_steps OR (epochs AND steps_per_epoch)"")
elif total_steps is not None:
if total_steps <= 0 or not isinstance(total_steps, int):
                raise ValueError(""Expected positive integer total_steps, but got {}"".format(total_steps))
self.total_steps = total_steps
else:
if epochs <= 0 or not isinstance(epochs, int):
                raise ValueError(""Expected positive integer epochs, but got {}"".format(epochs))
if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):
                raise ValueError(""Expected positive integer steps_per_epoch, but got {}"".format(steps_per_epoch))
self.total_steps = epochs * steps_per_epoch
self.step_size_up = float(pct_start * self.total_steps) - 1
self.step_size_down = float(self.total_steps - self.step_size_up) - 1
"
1045,"#include <c10/cuda/CUDAGuard.h>
#include <c10d/Utils.hpp>

namespace c10d {
constexpr const char* const kNCCLAbortedCommStoreKey = ""NCCLABORTEDCOMM"";
","#include <c10/cuda/CUDAGuard.h>
#include <c10d/Utils.hpp>
namespace c10d {
constexpr const char* const kNCCLAbortedCommStoreKey = ""NCCLABORTEDCOMM"";
"
1046,"};
// NCCL op mapping
std::map<ReduceOp, ncclRedOp_t> ncclOp = {
{ReduceOp::MIN, ncclMin},
{ReduceOp::MAX, ncclMax},
{ReduceOp::SUM, ncclSum},
","};
// NCCL op mapping
const std::map<ReduceOp, ncclRedOp_t> ncclOp = {
{ReduceOp::MIN, ncclMin},
{ReduceOp::MAX, ncclMax},
{ReduceOp::SUM, ncclSum},
"
1047,"void runNondiffOptimization(
std::shared_ptr<Graph>& graph,
bool strict_fuser_check) {
// Run custom passes that different backends can register.
for (const auto& passPair : getCustomPrePasses()) {
passPair.first(graph);
}
// decomposition pass, decompose certain ops that will be used in the
// following passes (like batchmm and jit fusion)
if (!getProfilingMode()) {
DecomposeOps(graph);
}
// TupleConstruct / TupleUnpack pairs can still be present at this point
// and must be removed for fusion.
LowerSimpleTuples(graph);
// Rewrite subgraphs with many MMs into expressions that batch them.
BatchMM(graph);
if (getProfilingMode()) {
if (tensorExprFuserEnabled()) {
FuseTensorExprs(graph);
","void runNondiffOptimization(
std::shared_ptr<Graph>& graph,
bool strict_fuser_check) {
  GRAPH_DUMP(
      ""Before customPrePassses (beginning of runNondiffOptimization)"", graph);
// Run custom passes that different backends can register.
for (const auto& passPair : getCustomPrePasses()) {
passPair.first(graph);
}
  GRAPH_DUMP(""After customPrePassses"", graph);
// decomposition pass, decompose certain ops that will be used in the
// following passes (like batchmm and jit fusion)
if (!getProfilingMode()) {
DecomposeOps(graph);
    GRAPH_DUMP(""After DecomposeOps"", graph);
}
// TupleConstruct / TupleUnpack pairs can still be present at this point
// and must be removed for fusion.
LowerSimpleTuples(graph);
  GRAPH_DUMP(""After LowerSimpleTuples, before BatchMM"", graph);
// Rewrite subgraphs with many MMs into expressions that batch them.
BatchMM(graph);
  GRAPH_DUMP(""After BatchMM, before Fusion"", graph);
if (getProfilingMode()) {
if (tensorExprFuserEnabled()) {
FuseTensorExprs(graph);
"
1048,"} else {
w_data_type = w_shape.data_type();
}
// Note: for FbFCPacked, weight is fp16 but activations are in fp32
CheckAndSetTensorBoundShape(
        op.input(0), dimTypes, dims, w_data_type, int8_fc ? true : false);
} else {
ShapeInfo& x_shape_info = x_it->second;
if (x_shape_info.getDimType(0) == TensorBoundShape_DimType_UNKNOWN) {
","} else {
w_data_type = w_shape.data_type();
}

    if (int8_fc) {
      scale = helper.GetSingleArgument<float>(""Y_scale"", 1);
      offset = helper.GetSingleArgument<int>(""Y_zero_point"", 0);
    }
// Note: for FbFCPacked, weight is fp16 but activations are in fp32
CheckAndSetTensorBoundShape(
        op.input(0),
        dimTypes,
        dims,
        w_data_type,
        int8_fc ? true : false,
        false,
        scale,
        offset);
} else {
ShapeInfo& x_shape_info = x_it->second;
if (x_shape_info.getDimType(0) == TensorBoundShape_DimType_UNKNOWN) {
"
1049,"(op.type() != ""Int8GenQuantParams"");
float scale = 1;
int offset = 0;
TensorProto::DataType infered_data_type = TensorProto::UNDEFINED;
if (is_quantized) {
const static std::map<std::string, int> type_info_from_input = {
","(op.type() != ""Int8GenQuantParams"");
float scale = 1;
int offset = 0;

TensorProto::DataType infered_data_type = TensorProto::UNDEFINED;
if (is_quantized) {
const static std::map<std::string, int> type_info_from_input = {
"
1050,"#ifndef __HIP_PLATFORM_HCC__
template <>
void gemv<c10::complex<double>>(CUDABLAS_GEMV_ARGTYPES(c10::complex<double>)) {
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t op = _cublasOpFromChar(trans);
_cublasAdjustLdLevel2(m, n, &lda);
","#ifndef __HIP_PLATFORM_HCC__
template <>
void gemv<c10::complex<double>>(CUDABLAS_GEMV_ARGTYPES(c10::complex<double>)) {
    globalContext().alertCuBLASConfigNotDeterministic();
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t op = _cublasOpFromChar(trans);
_cublasAdjustLdLevel2(m, n, &lda);
"
1051,"Split a tensor into a list of tensors, given a lengths input, along the specified
'axis'. If `K` outputs are provided, the op assumes `len(lengths) % K == 0`.
The `input` will be split into `K` parts. Each part of length
`sum(lengths[i*k:i*k+k))`)DOC"");
OpSchema::Cost CostInferenceForConcat(
const OperatorDef& def,
","Split a tensor into a list of tensors, given a lengths input, along the specified
'axis'. If `K` outputs are provided, the op assumes `len(lengths) % K == 0`.
The `input` will be split into `K` parts. Each part of length
`sum(lengths[i*k:i*k+k))`

<details>

<summary> <b>Example 1</b> </summary>

**Code**

```

workspace.ResetWorkspace()

op = core.CreateOperator(
    ""SplitByLengths"",
    [""input"", ""lengths""],
    [""output_0"",""output_1"",""output_2""],
    axis=0
)

workspace.FeedBlob(""input"", np.random.randint(10, size=(9)))
workspace.FeedBlob(""lengths"", np.array([3,2,4], dtype=np.int32))
print(""input:"", workspace.FetchBlob(""input""))
print(""lengths:"", workspace.FetchBlob(""lengths""))
workspace.RunOperatorOnce(op)
print(""output_0:"", workspace.FetchBlob(""output_0""))
print(""output_1:"", workspace.FetchBlob(""output_1""))
print(""output_2:"", workspace.FetchBlob(""output_2""))

```

**Result**

```

input: [2 2 6 6 6 0 5 7 4]
lengths: [3 2 4]
output_0: [2 2 6]
output_1: [6 6]
output_2: [0 5 7 4]

```

<summary> <b>Example 2</b> </summary>

**Code**

```

workspace.ResetWorkspace()

op = core.CreateOperator(
    ""SplitByLengths"",
    [""input"", ""lengths""],
    [""output_0"",""output_1"",""output_2""],
    axis=0,
    scaling=true,
)

workspace.FeedBlob(""input"", np.random.randint(10, size=(9)))
workspace.FeedBlob(""lengths"", np.array([1,1,1], dtype=np.int32))
print(""input:"", workspace.FetchBlob(""input""))
print(""lengths:"", workspace.FetchBlob(""lengths""))
print(""output_0:"", workspace.FetchBlob(""output_0""))
print(""output_1:"", workspace.FetchBlob(""output_1""))
print(""output_2:"", workspace.FetchBlob(""output_2""))

```

**Result**

```

input: [2 2 6 6 6 0 5 7 4]
lengths: [1 1 1]
output_0: [2 2 6]
output_1: [6 6 6]
output_2: [5 7 4]

```

</details>

)DOC"");
OpSchema::Cost CostInferenceForConcat(
const OperatorDef& def,
"
1052,"return result;
}
Tensor _gather_sparse_backward(const Tensor& self, int64_t dim, const Tensor& index, const Tensor& grad){
// special case scalar input and/or index
if (self.ndimension() == 0) return at::_sparse_coo_tensor_unsafe(at::empty({0,grad.numel()}, index.options()), grad, self.sizes());
","return result;
}
static Tensor & masked_select_out_impl_cpu(Tensor & result, const Tensor & self, const Tensor & mask) {
  NoNamesGuard guard;

  TORCH_CHECK(mask.scalar_type() == ScalarType::Byte || mask.scalar_type() == ScalarType::Bool,
              ""masked_select: expected BoolTensor or ByteTensor for mask"");
  TORCH_CHECK(self.scalar_type() == result.scalar_type(),
              ""masked_select(): self and result must have the same scalar type"");

  if (mask.dtype() == at::ScalarType::Byte) {
    TORCH_WARN(""masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,"" \
            ""please use a mask with dtype torch.bool instead."");
  }

  Tensor _mask, _self;
  std::tie(_mask, _self) = expand_outplace(mask, self);

  auto shape = _self.sizes();
  int64_t numel = _mask.sum().item().toLong();
  result.resize_({numel});
  if (numel == 0) {
    return result;
  }

  // Create strided view of result before feeding into TensorIterator
  auto strides = DimVector(shape.size(), 0);
  auto orig_stride = result.strides()[0];
  auto result_strided = result.as_strided(shape, strides);

  // serial kernel
  // serial kernel requires that src is traversed in its logical order. However, TensorIterator might
  // have reordered dimensions so that src would be traversed in its physical order, producing wrong
  // answers. A sufficient condition that no reorder happened is that both _self and _mask is contiguous.
  // If it is not satisfied, use parallel kernel that handles permutations correctly
  bool use_serial_kernel = (self.numel() < at::internal::GRAIN_SIZE || at::get_num_threads() == 1 ) &&
  _self.is_contiguous() && _mask.is_contiguous();
  if (use_serial_kernel) {
    auto iter = TensorIteratorConfig()
      .check_all_same_dtype(false)
      .resize_outputs(false)
      .add_output(result_strided)
      .add_input(_self)
      .add_input(_mask)
      .build();

    masked_select_serial_stub(iter.device_type(), iter, orig_stride);
    return result;
  }

  // Use a prefix sum to record the output locations of the masked elements,
  // so as to parallel with TensorIterator.
  auto mask_long = at::empty(shape, self.options().dtype(at::kLong)).copy_(_mask);
  auto mask_prefix_sum = at::empty(shape, self.options().dtype(at::kLong));
  auto mask_long_data = mask_long.data_ptr<int64_t>();
  auto mask_prefix_sum_data = mask_prefix_sum.data_ptr<int64_t>();
  // TODO: Here can only use std::partial_sum for C++14,
  // use std::exclusive_scan when PyTorch upgrades to C++17, which have better peformance.
  // std::exclusive_scan(mask_long_data, mask_long_data + mask_long.numel(), mask_prefix_sum_data, 0);
  std::partial_sum(mask_long_data, mask_long_data + mask_long.numel(), mask_prefix_sum_data);

  auto iter = TensorIteratorConfig()
    .check_all_same_dtype(false)
    .resize_outputs(false)
    .add_output(result_strided)
    .add_input(_self)
    .add_input(_mask)
    .add_input(mask_prefix_sum)
    .build();

  masked_select_stub(iter.device_type(), iter, orig_stride);
  return result;
}

Tensor & masked_select_out_cpu(Tensor & result, const Tensor & self, const Tensor & mask) {
  namedinference::compute_broadcast_outnames(self, mask);
  return masked_select_out_impl_cpu(result, self, mask);
}

Tensor masked_select_cpu(const Tensor & self, const Tensor & mask) {
  Tensor result = at::empty({0}, self.options());
  return masked_select_out_cpu(result, self, mask);
}

Tensor _gather_sparse_backward(const Tensor& self, int64_t dim, const Tensor& index, const Tensor& grad){
// special case scalar input and/or index
if (self.ndimension() == 0) return at::_sparse_coo_tensor_unsafe(at::empty({0,grad.numel()}, index.options()), grad, self.sizes());
"
1053,"VContext::~VContext() {
if (enableValidationLayers_) {
    auto func = (PFN_vkDestroyDebugReportCallbackEXT)vkGetInstanceProcAddr(
instance_, ""vkDestroyDebugReportCallbackEXT"");
if (func) {
func(instance_, debugReportCallback_, nullptr);
","VContext::~VContext() {
if (enableValidationLayers_) {
    const auto func = (PFN_vkDestroyDebugReportCallbackEXT)vkGetInstanceProcAddr(
instance_, ""vkDestroyDebugReportCallbackEXT"");
if (func) {
func(instance_, debugReportCallback_, nullptr);
"
1054,"}
void createDescriptorSetLayoutSinglePool(
    VkDevice device,
    std::vector<VkDescriptorType> descrTypes,
    VkDescriptorSetLayout* descrSetLayout,
    VkDescriptorPool* descrPool,
    VkDescriptorSet* descrSet) {
  auto size = descrTypes.size();
std::vector<VkDescriptorSetLayoutBinding> bindings;
std::vector<VkDescriptorPoolSize> poolSizes;
uint32_t i = 0;
","}
void createDescriptorSetLayoutSinglePool(
    const VkDevice device,
    const std::vector<VkDescriptorType>& descrTypes,
    VkDescriptorSetLayout* const descrSetLayout,
    VkDescriptorPool* const descrPool,
    VkDescriptorSet* const descrSet) {
  const auto size = descrTypes.size();
std::vector<VkDescriptorSetLayoutBinding> bindings;
std::vector<VkDescriptorPoolSize> poolSizes;
uint32_t i = 0;
"
1055,"// VBuffer <-> VImage
void copy_buffer_to_image(const VBuffer& buffer, VImage& image) {
  auto device = context().device();
  auto physicalDevice = context().physicalDevice();
struct ConstBlock {
int32_t w;
int32_t h;
};
  ConstBlock constBlock{image.w(), image.h()};
VBuffer constBuffer =
      makeUniformConstBuffer((void*)&constBlock, sizeof(constBlock));
VkDescriptorSetLayout descrSetLayout{};
VkDescriptorSetLayoutBinding bindings[] = {
","// VBuffer <-> VImage
void copy_buffer_to_image(const VBuffer& buffer, VImage& image) {
  const auto device = context().device();
  const auto physicalDevice = context().physicalDevice();
struct ConstBlock {
int32_t w;
int32_t h;
};
  const ConstBlock constBlock{image.w(), image.h()};
VBuffer constBuffer =
      makeUniformConstBuffer(&constBlock, sizeof(constBlock));
VkDescriptorSetLayout descrSetLayout{};
VkDescriptorSetLayoutBinding bindings[] = {
"
1056,"const Tensor& self,
const Tensor& mat1,
const Tensor& mat2,
    Scalar beta,
    Scalar alpha) {
const VulkanTensor t =
vtensor_from_vulkan(self.is_vulkan() ? self : self.vulkan());
const VulkanTensor m1 =
vtensor_from_vulkan(mat1.is_vulkan() ? mat1 : mat1.vulkan());
const VulkanTensor m2 =
vtensor_from_vulkan(mat2.is_vulkan() ? mat2 : mat2.vulkan());
  float b = beta.to<float>();
  float a = alpha.to<float>();
VulkanTensor output = VulkanTensor{self.sizes().vec()};
output.allocate_storage();
","const Tensor& self,
const Tensor& mat1,
const Tensor& mat2,
    const Scalar beta,
    const Scalar alpha) {
const VulkanTensor t =
vtensor_from_vulkan(self.is_vulkan() ? self : self.vulkan());
const VulkanTensor m1 =
vtensor_from_vulkan(mat1.is_vulkan() ? mat1 : mat1.vulkan());
const VulkanTensor m2 =
vtensor_from_vulkan(mat2.is_vulkan() ? mat2 : mat2.vulkan());
  const float b = beta.to<float>();
  const float a = alpha.to<float>();
VulkanTensor output = VulkanTensor{self.sizes().vec()};
output.allocate_storage();
"
1057,"Tensor vulkan_clamp(
const Tensor& self,
    c10::optional<Scalar> min,
    c10::optional<Scalar> max) {
VulkanTensor& x = vtensor_from_vulkan(self);
VulkanTensor output = VulkanTensor{self.sizes().vec()};
output.allocate_storage();
","Tensor vulkan_clamp(
const Tensor& self,
    const c10::optional<Scalar> min,
    const c10::optional<Scalar> max) {
VulkanTensor& x = vtensor_from_vulkan(self);
VulkanTensor output = VulkanTensor{self.sizes().vec()};
output.allocate_storage();
"
1058,"output_min,
output_max,
std::move(op_context));
  return conv2d_op_context;
}
Tensor VulkanConv2dOpContext::run(const Tensor& input) {
","output_min,
output_max,
std::move(op_context));
}
Tensor VulkanConv2dOpContext::run(const Tensor& input) {
"
1059,"TORCH_CHECK(infos.ge(0).all().item<uint8_t>(), ""Invalid argument passed to lu"");
auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
  auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1);
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
","TORCH_CHECK(infos.ge(0).all().item<uint8_t>(), ""Invalid argument passed to lu"");
auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
  // NB: the `.contiguous()` call is added due to the bug in `.prod()` as reported in
  // issue #https://github.com/pytorch/pytorch/issues/34061
  auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1).contiguous();
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
"
1060,"}
bool useQuantizable(const Use& use, QuantType quant_type) {
  for (const auto& func_input : _observe_inputs_aten_func) {
    if (matchAtenFuncToUse(use, func_input.func_name, c10::nullopt)) {
      return use.offset == func_input.arg_index;
}
  }
  for (const auto& func_input : _observe_inputs_call_func) {
    if (matchCallFuncToUse(use, func_input.func_name, c10::nullopt)) {
      return use.offset == func_input.arg_index;
}
}
","}
bool useQuantizable(const Use& use, QuantType quant_type) {
  if (quant_type == QuantType::STATIC) {
    for (const auto& func_input : _observe_inputs_aten_func) {
      if (matchAtenFuncToUse(use, func_input.func_name, c10::nullopt)) {
        return use.offset == func_input.arg_index;
      }
}
    for (const auto& func_input : _observe_inputs_call_func) {
      if (matchCallFuncToUse(use, func_input.func_name, c10::nullopt)) {
        return use.offset == func_input.arg_index;
      }
}
}
"
1061,"const auto& match_vmap = match.values_map;
auto dequant_node = match_vmap.at(vmap.at(""a_dequant""))->node();
Value* dequant_out = dequant_node->output();
    TORCH_CHECK(
        dequant_out->uses().size() == 1,
        ""Expect dequant output to have single use"");
Node* user = dequant_out->uses()[0].user;
return !nodeQuantizable(user, QuantType::DYNAMIC);
};
","const auto& match_vmap = match.values_map;
auto dequant_node = match_vmap.at(vmap.at(""a_dequant""))->node();
Value* dequant_out = dequant_node->output();
    // Values can be used multiple times in a single node
    if (dequant_out->uses().size() != 1) {
      return false;
    }
Node* user = dequant_out->uses()[0].user;
return !nodeQuantizable(user, QuantType::DYNAMIC);
};
"
1062,"continue;
}
if (n->kind() == prim::CallMethod) {
        invoked_methods.push_back(std::make_pair(
            getInvokedModule(module, n, graph->inputs()[0]), n->s(attr::name)));
}
for (Block* subblock : n->blocks()) {
","continue;
}
if (n->kind() == prim::CallMethod) {
        auto m_opt = getInvokedModuleOpt(module, n, graph->inputs()[0]);
        if (m_opt.has_value()) {
          invoked_methods.push_back(std::make_pair(*m_opt, n->s(attr::name)));
        }
}
for (Block* subblock : n->blocks()) {
"
1063,"module = module_cls(*args, **kwargs)
if not isinstance(module, nn.Module):
raise ValueError(
            ""Expect `module_cls(*args, **kwargs)` returns an instancee of <class nn.Module>, ""
f""but it returns an instance of {type(module)}.""
)
if module_interface_cls is not None:
","module = module_cls(*args, **kwargs)
if not isinstance(module, nn.Module):
raise ValueError(
            ""Expect `module_cls(*args, **kwargs)` returns an instance of <class nn.Module>, ""
f""but it returns an instance of {type(module)}.""
)
if module_interface_cls is not None:
"
1064,"containing data.
model (torch.nn.Module): model for which we seek to update BatchNorm
statistics.
        device (torch.device, optional): If set, data will be trasferred to
:attr:`device` before being passed into :attr:`model`.
Example:
","containing data.
model (torch.nn.Module): model for which we seek to update BatchNorm
statistics.
        device (torch.device, optional): If set, data will be transferred to
:attr:`device` before being passed into :attr:`model`.
Example:
"
1065,"def set_default_dtype(d):
    r""""""Sets the default floating point dtype to :attr:`d`. This type will be
    used as default floating point type for type inference in
    :func:`torch.tensor`.
The default floating point dtype is initially ``torch.float32``.
","def set_default_dtype(d):
    r""""""Sets the default floating point dtype to :attr:`d`.
    This dtype is:
    1. The inferred dtype for python floats in :func:`torch.tensor`.
    2. Used to infer dtype for python complex numbers. The default complex dtype is set to
       ``torch.complex128`` if default floating point dtype is ``torch.float64``,
       otherwise it's set to ``torch.complex64``
The default floating point dtype is initially ``torch.float32``.
"
1066,"auto index_dim_stride = ensure_nonempty_stride(index, dim);
auto index_dim_size = ensure_nonempty_size(index, dim);

auto index_upper_bound = self_dim_size;
AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
","auto index_dim_stride = ensure_nonempty_stride(index, dim);
auto index_dim_size = ensure_nonempty_size(index, dim);

auto index_upper_bound = self_dim_size;
AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
"
1067,"}
);
}

  template <typename func_t>
void operator()(Tensor& self, int64_t dim,
const Tensor& index, const Tensor& src,
const std::string& method_name, func_t& kernel_func) {
","}
);
}

  template <typename func_t>
void operator()(Tensor& self, int64_t dim,
const Tensor& index, const Tensor& src,
const std::string& method_name, func_t& kernel_func) {
"
1068,"const TensorOptions& options,
QuantizerPtr quantizer) {
auto memory_format = options.memory_format_opt().value_or(MemoryFormat::Contiguous);
  at::Allocator* allocator = GetAllocator(options.device().type());
#ifdef USE_PYTORCH_QNNPACK
if (at::globalContext().qEngine() == at::QEngine::QNNPACK) {
","const TensorOptions& options,
QuantizerPtr quantizer) {
auto memory_format = options.memory_format_opt().value_or(MemoryFormat::Contiguous);
  at::Allocator* allocator = options.device().type() == DeviceType::CUDA
    ? at::detail::getCUDAHooks().getCUDADeviceAllocator()
    : at::getCPUAllocator();
#ifdef USE_PYTORCH_QNNPACK
if (at::globalContext().qEngine() == at::QEngine::QNNPACK) {
"
1069,"def _create_batched_inputs(
in_dims: in_dims_t, args: Tuple, vmap_level: int, fn_name: str) -> Tuple[Tuple, int]:
if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
        raise ValueError(EXPECTED_IN_DIMS_TO_BE_INT_OR_TUPLE.format(
            fn=fn_name, in_dims=in_dims, actual_type=str(type(in_dims))))
# NB: Checks that len(in_dims) == len(args) (if in_dims is a tuple).
in_dims_as_tuple = _as_tuple(
in_dims, len(args),
        lambda: IN_DIMS_AND_NUM_INPUTS_MISMATCH.format(
            fn=fn_name, in_dims=in_dims, num_inputs=len(args)))
if len(args) == 0:
        raise ValueError(NO_INPUTS.format(fn=fn_name))
_check_args_can_be_mapped_with_in_dims(in_dims_as_tuple, args, fn_name, in_dims)
batch_size = _validate_and_get_batch_size(in_dims_as_tuple, args)
","def _create_batched_inputs(
in_dims: in_dims_t, args: Tuple, vmap_level: int, fn_name: str) -> Tuple[Tuple, int]:
if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
        raise ValueError(
            f'vmap({fn_name}, in_dims={in_dims}, ...): expected `in_dims` to '
            f'be int or tuple, got: {type(in_dims)}.')
# NB: Checks that len(in_dims) == len(args) (if in_dims is a tuple).
in_dims_as_tuple = _as_tuple(
in_dims, len(args),
        lambda: f'vmap({fn_name}, in_dims={in_dims}, ...)(<inputs>): expected '
                f'one `in_dim` per input (got {len(args)} inputs) of {fn_name}')
if len(args) == 0:
        raise ValueError(
            f'vmap({fn_name})(<inputs>): got no inputs. Maybe you forgot to add '
            f'inputs, or you are trying to vmap over a function with no inputs. '
            f'The latter is unsupported.')
_check_args_can_be_mapped_with_in_dims(in_dims_as_tuple, args, fn_name, in_dims)
batch_size = _validate_and_get_batch_size(in_dims_as_tuple, args)
"
1070,"// Make an exception for the case in which the annotated type is
// float and the Tensor data type is also float; the elements will
// be casted to double later.
             auto scalarTypeForJitType = tryScalarTypeFromJitType(out_ty);
             if (scalarTypeForJitType != at::ScalarType::Double ||
                 t.scalar_type() != at::ScalarType::Float) {
               TORCH_CHECK(
                   tryScalarTypeFromJitType(out_ty) == t.scalar_type(),
                   ""Output annotation element type and runtime tensor element type must match for tolist()"")
             }
// Check that the dimension of the Tensor matches that of the
// annotation.
TORCH_CHECK(
dim_val == t.dim(),
                 ""Output annotation list dimension and runtime tensor dimension must match for tolist()"")
// Wrap out_ty in a ListType dim times.
for (int i = 0; i < dim_val; ++i) {
","// Make an exception for the case in which the annotated type is
// float and the Tensor data type is also float; the elements will
// be casted to double later.
             TORCH_CHECK(
                 (out_ty == FloatType::get() && t.is_floating_point()) ||
                     tryScalarTypeFromJitType(out_ty) == t.scalar_type(),
                 ""Output annotation element type and runtime tensor element type must match for tolist()"");
// Check that the dimension of the Tensor matches that of the
// annotation.
TORCH_CHECK(
dim_val == t.dim(),
                 ""Output annotation list dimension and runtime tensor dimension must match for tolist()"");
// Wrap out_ty in a ListType dim times.
for (int i = 0; i < dim_val; ++i) {
"
1071,"dtype, device, dim, /*requires_grad=*/c10::nullopt)};
};
// Requirements:
//   dims           : preserved
//   scalar type    : equal to value of dtype
","dtype, device, dim, /*requires_grad=*/c10::nullopt)};
};
    static const auto factory_like_with_ndim = [](Node* node,
                                                  int dim) -> type_vec_t {
      auto tt = node->input(0)->type()->expect<TensorType>();
      auto in_type = tt->scalarType();
      auto in_dev = tt->device();

      at::optional<IValue> maybe_layout_option = node->get(attr::layout);
      if (!maybe_layout_option)
        return {};

      at::optional<IValue> maybe_device_option = node->get(attr::device);
      if (!maybe_device_option)
        return {};

      if (!maybe_device_option->isNone()) {
        in_dev = maybe_device_option->toDevice();
      }

      at::optional<IValue> maybe_dtype_option = node->get(attr::dtype);
      if (!maybe_dtype_option)
        return {};

      if (!maybe_dtype_option->isNone()) {
        in_type = maybe_dtype_option->toScalarType();
      }

      return {TensorType::create(
          in_type, in_dev, dim, /*requires_grad=*/c10::nullopt)};
    };

// Requirements:
//   dims           : preserved
//   scalar type    : equal to value of dtype
"
1072,".. warning ::
This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes.
The hook will be called every time before :func:`forward` is invoked.
It should have the following signature::
",".. warning ::
This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes.
The hook will be called every time before :func:`forward` is invoked.
It should have the following signature::
"
1073,"else:
self._non_persistent_buffers_set.add(name)
    def register_parameter(self, name: str, param: Parameter) -> None:
r""""""Adds a parameter to the module.
The parameter can be accessed as an attribute using given name.
","else:
self._non_persistent_buffers_set.add(name)
    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:
r""""""Adds a parameter to the module.
The parameter can be accessed as an attribute using given name.
"
1074,"return 0;
},
aliasAnalysisFromSchema()),
      // only used internally in range() translation
Operator(
""aten::__range_length(int lo, int hi, int step) -> int"",
[](Stack& stack) {
","return 0;
},
aliasAnalysisFromSchema()),
     // only used internally in range() translation
Operator(
""aten::__range_length(int lo, int hi, int step) -> int"",
[](Stack& stack) {
"
1075,"throw e;
}

void ThrowEnforceFiniteNotMet(
const char* file,
const int line,
const char* condition,
const std::string& msg,
const void* caller) {
    throw c10::EnforceFiniteError(
      file, line, condition, msg, (*GetFetchStackTrace())(), caller
    );
}
// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, std::string msg)
    : Error(std::move(msg), str(""Exception raised from "", source_location, "" (most recent call first):\n"", (*GetFetchStackTrace())())) {
}
using APIUsageLoggerType = std::function<void(const std::string&)>;
","throw e;
}
void ThrowEnforceFiniteNotMet(
const char* file,
const int line,
const char* condition,
const std::string& msg,
const void* caller) {
  throw c10::EnforceFiniteError(
      file, line, condition, msg, (*GetFetchStackTrace())(), caller);
}
// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, std::string msg)
    : Error(
          std::move(msg),
          str(""Exception raised from "",
              source_location,
              "" (most recent call first):\n"",
              (*GetFetchStackTrace())())) {}
using APIUsageLoggerType = std::function<void(const std::string&)>;
"
1076,"[-0.2098, -0.6699],
[ 0.3470, -0.9451],
[-0.5174, -1.3136]])
"""""")
add_docstr(torch.view_as_complex,
r""""""
","[-0.2098, -0.6699],
[ 0.3470, -0.9451],
[-0.5174, -1.3136]])
"""""".format(**common_args))
add_docstr(torch.view_as_complex,
r""""""
"
1077,"return isAtenFunc(n, _propagate_quant_binary_ops);
}
bool isPropagateQuantNode(Node* n) {
return isPropagateQuantSingleInputOp(n) || isPropagateQuantBinaryOp(n);
}
","return isAtenFunc(n, _propagate_quant_binary_ops);
}
bool isPropagateQuantOp(Node* n) {
return isPropagateQuantSingleInputOp(n) || isPropagateQuantBinaryOp(n);
}
"
1078,"std::move(wrappedRpc),
wrappedMsgType,
std::move(wrappedMessage.tensors()),
      std::move(cfg));
}
} // namespace autograd
} // namespace distributed
","std::move(wrappedRpc),
wrappedMsgType,
std::move(wrappedMessage.tensors()),
      std::move(cfg),
      profilerId);
}
} // namespace autograd
} // namespace distributed
"
1079,"""RRef creation via rpc.remote() timed out, and it ""
""is possible that the RRef on the owner node does not exist."");
// see Note [Best-Effort Check on Deleted UserRRefs]
  RECORD_USER_SCOPE(""to_here"");
TORCH_CHECK(
!deletedOnOwner_,
*this,
"" has been deleted. Cannot call to_here() on it after deletion."");
TORCH_CHECK(
!type_->is_module(),
*this,
","""RRef creation via rpc.remote() timed out, and it ""
""is possible that the RRef on the owner node does not exist."");
// see Note [Best-Effort Check on Deleted UserRRefs]
TORCH_CHECK(
!deletedOnOwner_,
*this,
"" has been deleted. Cannot call to_here() on it after deletion."");
  auto toHereKey = std::string("""");
  if (torch::autograd::profiler::profilerEnabled()) {
    toHereKey = fmt::format(
        ""to_here#({})->({})"",
        RpcAgent::getCurrentRpcAgent()->getWorkerInfo().name_,
        RpcAgent::getCurrentRpcAgent()->getWorkerInfo(ownerId_).name_);
    auto& remoteProfilerManager =
        torch::distributed::rpc::RemoteProfilerManager::getInstance();
    remoteProfilerManager.setCurrentKey(toHereKey);
  }
  RECORD_USER_SCOPE(toHereKey);
TORCH_CHECK(
!type_->is_module(),
*this,
"
1080,"get_worker_info().name,
dst_worker_info.name,
)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
","get_worker_info().name,
dst_worker_info.name,
)
        RemoteProfilerManager.set_current_profiling_key(rpc_profiling_key)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
"
1081,"get_worker_info().name,
dst_worker_info.name,
)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
","get_worker_info().name,
dst_worker_info.name,
)
        RemoteProfilerManager.set_current_profiling_key(rpc_profiling_key)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
"
1082,"DLContext getDLContext(const Tensor& tensor, const int64_t& device_id) {
DLContext ctx;
ctx.device_id = device_id;
  if (tensor.is_cuda()) {
    ctx.device_type = DLDeviceType::kDLGPU;
  } else {
    ctx.device_type = DLDeviceType::kDLCPU;
}
return ctx;
}
","DLContext getDLContext(const Tensor& tensor, const int64_t& device_id) {
DLContext ctx;
ctx.device_id = device_id;
  switch (tensor.device().type()) {
    case DeviceType::CPU:
      ctx.device_type = DLDeviceType::kDLCPU;
      break;
    case DeviceType::CUDA:
#ifdef USE_ROCM
      // ROCM, if enabled will look like cuda to PyTorch
      // while everyone else should see HIP
      ctx.device_type = DLDeviceType::kDLROCM;
#else
      ctx.device_type = DLDeviceType::kDLGPU;
#endif
      break;
    case DeviceType::OPENCL:
      ctx.device_type = DLDeviceType::kDLOpenCL;
      break;
    case DeviceType::HIP:
      ctx.device_type = DLDeviceType::kDLROCM;
      break;
    default:
      throw std::logic_error(""Cannot pack tensors on "" + tensor.device().str());
}
return ctx;
}
"
1083,"namespace {
void checkRoundingMode(const std::string& fn_name) {
  TORCH_WARN_ONCE(
std::fegetround() != FE_TONEAREST,
fn_name,
"" current rounding mode is not set to round-to-nearest-ties-to-even (FE_TONEAREST). This will cause accuracy issues in quantized models."");
}
void checkCPUTensor(const std::string& fn_name, Tensor t) {
","namespace {
void checkRoundingMode(const std::string& fn_name) {
// Disabling this warning message for now as it is printed incorrectly. Need to fix

/*  TORCH_WARN_ONCE(
std::fegetround() != FE_TONEAREST,
fn_name,
"" current rounding mode is not set to round-to-nearest-ties-to-even (FE_TONEAREST). This will cause accuracy issues in quantized models."");
*/
  return;
}
void checkCPUTensor(const std::string& fn_name, Tensor t) {
"
1084,"asList.push_back(f->fut);
}
return std::make_shared<jit::PythonFutureWrapper>(
            c10::collectAll(asList));
});
m.def(""_jit_assert_is_instance"", [](py::object obj, TypePtr type) {
","asList.push_back(f->fut);
}
return std::make_shared<jit::PythonFutureWrapper>(
            c10::collectAll(asList),
            /* unwrap_func */ [futures](const py::object& /*unused*/) {
              // Throw errors when calling wait() on the returned Future if
              // any of the original futures would throw.
              // NB: PythonFutureWrapper takes an unwrap_func which serves as a
              // callback to evalute the value in the Future. RPC uses this
              // unwrap_func to check whether the returned py::object is a
              // RemoteException object, and re-throw the exception if it is.
              // By extracting the c10::ivalue::Future from PythonFutureWrapper
              // the unwrap_func on the original PythonFutureWrapper objects are
              // discarded, and hence it will return the RemoteException as an
              // object instead of re-throwing it.
              for (auto& fut : futures) {
                fut->wait();
              }
            });
});
m.def(""_jit_assert_is_instance"", [](py::object obj, TypePtr type) {
"
1085,"(getattr(ann, '__origin__', None) is Tuple or
getattr(ann, '__origin__', None) is tuple)

def is_named_tuple(ann):
    return inspect.isclass(ann) and issubclass(ann, tuple) and hasattr(ann, ""_fields"")


def try_make_named_tuple_type(ann, instance=None, loc=None):
    if not is_named_tuple(ann):
        return None

    if hasattr(ann, ""_field_defaults"") and len(ann._field_defaults) > 0:
        msg = """"""Default values are currently not supported on NamedTuple fields in TorchScript.
                Fields with default values: [""""""
        first = True

        # Print out all fields with default values
        msg += "", "".join(ann._field_defaults.keys())

        if loc:
            error_report = torch._C.ErrorReport(loc)
            msg += error_report.what().lstrip()

        raise RuntimeError(msg)

    # Bail if an instance is provided and its members are not JITable themselves.
    if instance is not None:
        for member in instance:
            if not torch._C._jit_try_infer_type(member):
                return None

    qualified_name = torch.jit._qualified_name(ann)
    props = torch.jit._get_named_tuple_properties(ann)
    new_type = torch._C.TupleType.createNamed(qualified_name, props[1], props[2])
    existing_type = torch.jit._python_cu.get_type(qualified_name)

    if existing_type:
        if existing_type.isSubtypeOf(new_type):
            return existing_type
        else:
            msg = ""Cannot redefine NamedTuple: "" + str(existing_type)
            if loc:
                error_report = torch._C.ErrorReport(loc)
                msg += error_report.what().lstrip()

            raise RuntimeError(msg)

    torch.jit._python_cu.register_type(new_type)
    return new_type


def is_list(ann):
if not hasattr(ann, '__module__'):
return False
","(getattr(ann, '__origin__', None) is Tuple or
getattr(ann, '__origin__', None) is tuple)
def is_list(ann):
if not hasattr(ann, '__module__'):
return False
"
1086,"x = x_orig.detach()
min_val = self.min_val
max_val = self.max_val
        if min_val.numel() == 0 or max_val.numel() == 0:
min_val = torch.min(x)
max_val = torch.max(x)
self.min_val.resize_(min_val.shape)
","x = x_orig.detach()
min_val = self.min_val
max_val = self.max_val
        prev_zeros = False
        if min_val.numel() > 0 and max_val.numel() > 0:
            prev_zeros = (min_val.item() == 0) and (max_val.item() == 0)
        if min_val.numel() == 0 or max_val.numel() == 0 or prev_zeros:
min_val = torch.min(x)
max_val = torch.max(x)
self.min_val.resize_(min_val.shape)
"
1087,"void setQuantType(QuantType quant_type) {
quant_type_ = quant_type;
}

  void setDebug(bool debug) {
    debug_ = debug;
  }

// Cleanup observer nodes from graph and observer modules
// from module object and ClassType
void cleanup(Module& module);
","void setQuantType(QuantType quant_type) {
quant_type_ = quant_type;
}
// Cleanup observer nodes from graph and observer modules
// from module object and ClassType
void cleanup(Module& module);
"
1088,"}
std::ostream& operator<<(std::ostream& os, GloballyUniqueId const& globalId) {
  return os << ""GloballyUniqueId("" << globalId.createdOn_ << "", ""
            << globalId.localId_ << "")"";
}
///////////////////////////  SerializedPyObj   ///////////////////////////
","}
std::ostream& operator<<(std::ostream& os, GloballyUniqueId const& globalId) {
  return os << ""GloballyUniqueId(created_on="" << globalId.createdOn_
            << "", local_id="" << globalId.localId_ << "")"";
}
///////////////////////////  SerializedPyObj   ///////////////////////////
"
1089,"Tensor& div_out(Tensor& result, const Tensor& self, const Tensor& other) {
if (isIntegralType(result.scalar_type(), /*includeBool=*/ true)) {
    TORCH_WARN_ONCE(
      ""Integer division of tensors using div or / is deprecated, "",
""and in a future release div will perform true division as in Python 3. "",
""Use true_divide or floor_divide (// in Python) instead."");
}
","Tensor& div_out(Tensor& result, const Tensor& self, const Tensor& other) {
if (isIntegralType(result.scalar_type(), /*includeBool=*/ true)) {
    TORCH_CHECK(false,
      ""Integer division of tensors using div or / is no longer supported, "",
""and in a future release div will perform true division as in Python 3. "",
""Use true_divide or floor_divide (// in Python) instead."");
}
"
1090,"}
if (uprc.isAsyncExecution()) {
        processAsyncExecution(
            uprc.pythonUdf(),
            messageId,
            responseFuture,
            [ownerRRef, rrefId, forkId](
                const py::object& result,
                const int64_t messageId,
                PythonRpcHandler& pythonRpcHandler,
                const std::shared_ptr<FutureMessage>& responseFuture) {
              IValue py_ivalue = jit::toIValue(result, PyObjectType::get());

              py::gil_scoped_release release;
              ownerRRef->setValue(std::move(py_ivalue));
              auto m = RemoteRet(rrefId, forkId).toMessage();
              m.setId(messageId);
              responseFuture->markCompleted(std::move(m));
            });
} else {
IValue py_ivalue;
try {
","}
if (uprc.isAsyncExecution()) {
        try {
          processAsyncExecution(
              uprc.pythonUdf(),
              messageId,
              responseFuture,
              [ownerRRef, rrefId, forkId](
                  const py::object& result,
                  const int64_t messageId,
                  PythonRpcHandler& /* unused */,
                  const std::shared_ptr<FutureMessage>& responseFuture) {
                IValue py_ivalue = jit::toIValue(result, PyObjectType::get());

                py::gil_scoped_release release;
                ownerRRef->setValue(std::move(py_ivalue));
                auto m = RemoteRet(rrefId, forkId).toMessage();
                m.setId(messageId);
                responseFuture->markCompleted(std::move(m));
              });
        } catch (std::exception& e) {
          ownerRRef->setError(e.what());
          auto m = RemoteRet(rrefId, forkId).toMessage();
          m.setId(messageId);
          responseFuture->markCompleted(std::move(m));
        }
} else {
IValue py_ivalue;
try {
"
1091,"}
}
// This is the pass to handle ops that does not require observation
// for example: flatten, average_pool, upsample
// This is called after inline and before graph execution
void PropagateQuantizationOps(std::shared_ptr<Graph>& graph) {
  propagateQuantizationOps(graph->block());
}

Module InsertQuantDeQuant(
Module& input_module,
const std::string& method_name,
","}
}
Module InsertQuantDeQuant(
Module& input_module,
const std::string& method_name,
"
1092,"# This is the sys.modules replacement trick, see
# https://stackoverflow.com/questions/2447353/getattr-on-a-module/7668273#7668273
sys.modules[__name__] = CudnnModule(sys.modules[__name__], __name__)
","# This is the sys.modules replacement trick, see
# https://stackoverflow.com/questions/2447353/getattr-on-a-module/7668273#7668273
sys.modules[__name__] = CudnnModule(sys.modules[__name__], __name__)

# Add type annotation for the replaced module
enabled: bool
deterministic: bool
benchmark: bool
"
1093,"except ImportError:
# Uses of all the functions below should be guarded by torch.backends.cudnn.is_available(),
# so it's safe to not emit any checks here.
    _cudnn = None
def get_cudnn_mode(mode):
","except ImportError:
# Uses of all the functions below should be guarded by torch.backends.cudnn.is_available(),
# so it's safe to not emit any checks here.
    _cudnn = None  # type: ignore
def get_cudnn_mode(mode):
"
1094,"is set to 1.
group_name (str, optional, deprecated): Group name.
    To enable ``backend == Backend.MPI``, PyTorch needs to built from source
on a system that supports MPI.
""""""
","is set to 1.
group_name (str, optional, deprecated): Group name.
    To enable ``backend == Backend.MPI``, PyTorch needs to be built from source
on a system that supports MPI.
""""""
"
1095,"pipe,
[this, pipe](
const tensorpipe::Error& error, Message&& requestMessage) mutable {
        // TODO: Handle server pipe read error
if (error) {
          LOG(WARNING) << ""Server read message: "" << error.what();
return;
}
","pipe,
[this, pipe](
const tensorpipe::Error& error, Message&& requestMessage) mutable {
        // FIXME Find a way for the client to tell the server they are done with
        // the pipe and are intentionally shutting it down. Perhaps sending an
        // empty message?
if (error) {
          LOG(WARNING) << ""RPC agent for "" << workerInfo_.name_
                       << "" encountered error when reading incoming request: ""
                       << error.what();
return;
}
"
1096,"""Future marked complete from outside the thread pool"");
});
  ++clientActiveCalls_;
// Use the default RPC timeout if no timeout is specified for this send call
auto timeout = rpcTimeoutSeconds == kUnsetRpcTimeout
? getRpcTimeout()
","""Future marked complete from outside the thread pool"");
});
  increaseCallCount(clientActiveCalls_);
// Use the default RPC timeout if no timeout is specified for this send call
auto timeout = rpcTimeoutSeconds == kUnsetRpcTimeout
? getRpcTimeout()
"
1097,"std::unordered_map<std::string, std::string> metrics;
metrics[kThreadPoolSize] = c10::to_string(threadPool_.size());
metrics[kNumIdleThreads] = c10::to_string(threadPool_.numAvailable());
  metrics[kClientActiveCalls] = c10::to_string(clientActiveCalls_.load());
  metrics[kServerActiveCalls] = c10::to_string(serverActiveCalls_.load());
  metrics[kServerActiveAsyncCalls] =
      c10::to_string(serverActiveAsyncCalls_.load());
if (isGILProfilingEnabled()) {
{
std::unique_lock<std::mutex> lock(metricsMutex_);
","std::unordered_map<std::string, std::string> metrics;
metrics[kThreadPoolSize] = c10::to_string(threadPool_.size());
metrics[kNumIdleThreads] = c10::to_string(threadPool_.numAvailable());
  {
    std::unique_lock<std::mutex> lock(callCountMutex_);
    metrics[kClientActiveCalls] = c10::to_string(clientActiveCalls_);
    metrics[kServerActiveCalls] = c10::to_string(serverActiveCalls_);
    metrics[kServerActiveAsyncCalls] = c10::to_string(serverActiveAsyncCalls_);
  }
if (isGILProfilingEnabled()) {
{
std::unique_lock<std::mutex> lock(metricsMutex_);
"
1098,"// Completing the future will run its callbacks, which could execute
// arbitrary user code. To prevent blocking or stalling the TensorPipe event
// loops, we defer this to a worker thread.
    threadPool_.run([futureMessage{std::move(futureMessage)},
errorMsg{std::move(errorMsg)}]() mutable {
futureMessage->futMsg.setError(std::move(errorMsg));
});
}
}
","// Completing the future will run its callbacks, which could execute
// arbitrary user code. To prevent blocking or stalling the TensorPipe event
// loops, we defer this to a worker thread.
    threadPool_.run([this,
                     futureMessage{std::move(futureMessage)},
errorMsg{std::move(errorMsg)}]() mutable {
futureMessage->futMsg.setError(std::move(errorMsg));
      // The future's callbacks may schedule further RPCs, increasing the count.
      // Thus we must decrease it after completing the future, otherwise it may
      // briefly dip to zero and trick join into thinking all work is done.
      decreaseCallCount(clientActiveCalls_);
});
}
}
"
1099,"r""""""
searchsorted(sorted_sequence, values, out_int32=False, right=False, out=None) -> Tensor
Find the indices from the *innermost* dimension of :attr:`sorted_sequence` such that, if the
corresponding values in :attr:`values` were inserted before the indices, the order of the
corresponding *innermost* dimension within :attr:`sorted_sequence` would be preserved.
Return a new tensor with the same size as :attr:`values`. If :attr:`right` is False (default),
then the left boundary of :attr:`sorted_sequence` is closed. More formally, the returned index
satisfies the following rules:
.. list-table::
   :widths: 12 10 78
:header-rows: 1
* - :attr:`sorted_sequence`
","r""""""
searchsorted(sorted_sequence, values, out_int32=False, right=False, out=None) -> Tensor
Find the indices from the *innermost* dimension of :attr:`sorted_sequence` such that, if the
corresponding values in :attr:`values` were inserted before the indices, the order of the
corresponding *innermost* dimension within :attr:`sorted_sequence` would be preserved.
Return a new tensor with the same size as :attr:`values`. If :attr:`right` is False (default),
then the left boundary of :attr:`sorted_sequence` is closed. More formally, the returned index
satisfies the following rules:
.. list-table::
   :widths: 12 10 78
:header-rows: 1
* - :attr:`sorted_sequence`
"
1100,"pass
# Iterate over modules that we know contain a lot of builtins
    for mod in torch.jit._modules_containing_builtins:
name = mod.__name__
for elem in dir(mod):
builtin = torch.jit._find_builtin(getattr(mod, elem))
","pass
# Iterate over modules that we know contain a lot of builtins
    for mod in torch.jit._builtins._modules_containing_builtins:
name = mod.__name__
for elem in dir(mod):
builtin = torch.jit._find_builtin(getattr(mod, elem))
"
1101,"#include <c10/cuda/CUDAStream.h>
#endif
#include <gloo/config.h>
#include <gloo/rendezvous/context.h>
#include <gloo/rendezvous/prefix_store.h>
","#include <c10/cuda/CUDAStream.h>
#endif
#include <c10/util/StringUtil.h>
#include <gloo/config.h>
#include <gloo/rendezvous/context.h>
#include <gloo/rendezvous/prefix_store.h>
"
1102,"case c10::kCPU:
break;
default:
      invalidArgument(""unsupported device type"");
}
switch (layout) {
","case c10::kCPU:
break;
default:
      invalidArgument(c10::str(""unsupported device type "", device.type()));
}
switch (layout) {
"
1103,"process_group)
if module.affine:
with torch.no_grad():
                    module_output.weight.copy_(module.weight)
                    module_output.bias.copy_(module.bias)
                # keep requires_grad unchanged
                module_output.weight.requires_grad = module.weight.requires_grad
                module_output.bias.requires_grad = module.bias.requires_grad
module_output.running_mean = module.running_mean
module_output.running_var = module.running_var
module_output.num_batches_tracked = module.num_batches_tracked
","process_group)
if module.affine:
with torch.no_grad():
                    module_output.weight = module.weight
                    module_output.bias = module.bias
module_output.running_mean = module.running_mean
module_output.running_var = module.running_var
module_output.num_batches_tracked = module.num_batches_tracked
"
1104,"return kHandle;
case ScalarType::Uninitialized:
return kUninitialized;
default:
throw unsupported_dtype();
}
","return kHandle;
case ScalarType::Uninitialized:
return kUninitialized;
    case ScalarType::None:
      return kVoid;
default:
throw unsupported_dtype();
}
"
1105,"using graph_rewrite_helper::getValue;
using graph_rewrite_helper::PatternInfo;
// Map of quantization parameter name and value
// for example _scale, _zero_point,
// _scalar_type and _axis(for per channel quantization)
using QParamVector = std::vector<std::pair<std::string, IValue>>;
// dynamic quantization ops for activation: choose_qparams, quant, dequant
using DynamicQuantOps = std::tuple<Node*, Node*, Node*>;
","using graph_rewrite_helper::getValue;
using graph_rewrite_helper::PatternInfo;
// dynamic quantization ops for activation: choose_qparams, quant, dequant
using DynamicQuantOps = std::tuple<Node*, Node*, Node*>;
"
1106,"rec._end();
}
void _call_end_callbacks_on_fut(
const at::Tensor& handle,
const c10::intrusive_ptr<c10::ivalue::Future>& fut) {
// Save and pass thread local state into the callback
at::ThreadLocalState tls_state;
  // Add a callback onto the future to mark run RecordFunction's end callbacks
  // when the future is completed.
  fut->addCallback(
      // Copy handle and tls_state by value to persist after the python
      // context manager is exited.
      [handle, tls_state = std::move(tls_state)]() {
TORCH_INTERNAL_ASSERT(
handle.defined(),
""Undefined RecordFunction handle. This can happen if the handle is ""
","rec._end();
}
c10::intrusive_ptr<c10::ivalue::Future> _call_end_callbacks_on_fut(
const at::Tensor& handle,
const c10::intrusive_ptr<c10::ivalue::Future>& fut) {
// Save and pass thread local state into the callback
at::ThreadLocalState tls_state;
  // Profiling callback that ends the associated record_function
  // and returns the value of the passed in future.
  std::function<c10::IValue(void)> futureProfilingFunc =
      [fut, handle, tls_state = std::move(tls_state)]() {
TORCH_INTERNAL_ASSERT(
handle.defined(),
""Undefined RecordFunction handle. This can happen if the handle is ""
"
1107,"reduce_range=False),
weight=default_weight_fake_quant)
else:
        raise ValueError(""Unknown backend, please specify qconfig manually"")

return qconfig
","reduce_range=False),
weight=default_weight_fake_quant)
else:
        qconfig = default_qat_qconfig
return qconfig
"
1108,"}
if (div->rhs()->isConstant() && other->isConstant()) {
// If they are both scalar we may be able to find a common factor.
if (immediateEquals(evaluateOp(new Mod(other, div->rhs())), 0)) {
Expr* scalar = evaluateOp(new Div(other, div->rhs()));
","}
if (div->rhs()->isConstant() && other->isConstant()) {
    if (immediateEquals(div->rhs(), 0) || immediateEquals(other, 0)) {
      return nullptr;
    }
// If they are both scalar we may be able to find a common factor.
if (immediateEquals(evaluateOp(new Mod(other, div->rhs())), 0)) {
Expr* scalar = evaluateOp(new Div(other, div->rhs()));
"
1109,")
if self.engine in FP16_ENGINES:
                assert self.weight_decay == 0, f'weight decay is not tested for engine: {self.engine}'
shapes, types = workspace.InferShapesAndTypes([param_init_net])
assert str(param) in shapes, shapes
",")
if self.engine in FP16_ENGINES:
                assert self.weight_decay == 0, 'weight decay is not tested for engine: {}'.format(self.engine)
shapes, types = workspace.InferShapesAndTypes([param_init_net])
assert str(param) in shapes, shapes
"
1110,"ConstantPooling(graph);
ConstantPropagation(graph);
// must do constant propagation first before replacement
  replaceConvolutionWithConv2d(graph);
// fuse decomposed linear into aten::linear
FuseLinear(graph);
","ConstantPooling(graph);
ConstantPropagation(graph);
// must do constant propagation first before replacement
  replaceConvolutionWithAtenConv(graph);
// fuse decomposed linear into aten::linear
FuseLinear(graph);
"
1111,".NumOutputs(1)
.IdenticalTypeAndShape()
.InputsCanCrossDevices()
.SetDoc(R""DOC(
Copy input tensor into output, potentially across devices.
",".NumOutputs(1)
.IdenticalTypeAndShape()
.InputsCanCrossDevices()
    .InheritOnnxSchema(""Identity"")
.SetDoc(R""DOC(
Copy input tensor into output, potentially across devices.
"
1112,"auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1);

  // We have to manually set the diagonal to 0 due to an issue with MAGMA's getrf_batched routine
  if (self.dim() > 2 && self.is_cuda()) {
    u_diagonal.index_put_(infos.nonzero_numpy(), at::zeros({}, self.options()));
  }
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
","auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1);
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
"
1113,"if (observed_values_.count(v)) {
return;
}
  Module observer = observer_module.clone_instance();
std::string observer_name = ""_observer_"" + c10::to_string(uid_++);
while (module.hasattr(observer_name)) {
observer_name = ""_observer_"" + c10::to_string(uid_++);
","if (observed_values_.count(v)) {
return;
}
  Module observer = observer_module.deepcopy();
std::string observer_name = ""_observer_"" + c10::to_string(uid_++);
while (module.hasattr(observer_name)) {
observer_name = ""_observer_"" + c10::to_string(uid_++);
"
1114,"which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
# Use os.path.realpath to resolve any symlinks, in particular from 'c++' to e.g. 'g++'.
compiler_path = os.path.realpath(which.decode().strip())
    return any(name in compiler_path for name in _accepted_compilers_for_platform())
def check_compiler_abi_compatibility(compiler):
","which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
# Use os.path.realpath to resolve any symlinks, in particular from 'c++' to e.g. 'g++'.
compiler_path = os.path.realpath(which.decode().strip())
    # Check the compiler name
    if any(name in compiler_path for name in _accepted_compilers_for_platform()):
        return True
    # If ccache is used the compiler path is /usr/bin/ccache. Check by -v flag.
    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT).decode()
    if sys.platform.startswith('linux'):
        # Check for 'gcc' or 'g++'
        pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)
        results = re.findall(pattern, version_string)
        if len(results) != 1:
            return False
        compiler_path = os.path.realpath(results[0].strip())
        return any(name in compiler_path for name in _accepted_compilers_for_platform())
    if sys.platform.startswith('darwin'):
        # Check for 'clang' or 'clang++'
        return version_string.startswith(""Apple clang"")
    return False
def check_compiler_abi_compatibility(compiler):
"
1115,"}
if (opts_.debug) {
    dumpNet(*pred_net, shape_hints, ""debug_ssa_net.pb_txt"");
}
extractPartitionInfo(*pred_net);
","}
if (opts_.debug) {
    caffe2::NetDef ssa_net;
    ssa_net.CopyFrom(*pred_net);
    auto* w_arg = ssa_net.add_arg();
    w_arg->set_name(kInitializers);
    for (const auto& w : weights) {
      w_arg->add_strings(w);
    }
    dumpNet(ssa_net, shape_hints, ""debug_ssa_net.pb_txt"");
}
extractPartitionInfo(*pred_net);
"
1116,"// this node is safe to inline, so assign the output value
// to that expression directly
assignValue(node->output(), ss);
}
}
}
","// this node is safe to inline, so assign the output value
// to that expression directly
assignValue(node->output(), ss);
          if (isLongLine(ss->str())) {
            splitLongInlines(node->output());
          }
}
}
}
"
1117,"class Bilinear(Module):
r""""""Applies a bilinear transformation to the incoming data:
    :math:`y = x_1 A x_2 + b`
Args:
in1_features: size of each first input sample
","class Bilinear(Module):
r""""""Applies a bilinear transformation to the incoming data:
    :math:`y = x_1^T A x_2 + b`
Args:
in1_features: size of each first input sample
"
1118,"//   _0 = x.add_(b)
//   _1 = some_long + expression
//   r = foo(_0, _1)
  void splitLongInlines(at::ArrayRef<Value*> inputs) {
    size_t long_inline_slice = 0;
    // find the last input that is too long
    for (size_t i = 0; i < inputs.size(); ++i) {
      if (isLongInline(inputs[i]->node())) {
        long_inline_slice = i + 1;
      }
}
    // un-inline everything through the last long line
    // constants are ignored since long constants are never inlined in the
    // first place
    for (size_t i = 0; i < long_inline_slice; ++i) {
      if (isNonConstantInline(inputs[i])) {
        printOutputDefinition(inputs[i]->node(), *useOf(inputs[i]));
}
}
}
template <typename T>
","//   _0 = x.add_(b)
//   _1 = some_long + expression
//   r = foo(_0, _1)

  void splitLongInlines(Value* v) {
    std::vector<Value*> to_split_reversed;
    Use u = v->uses().at(0);
    scanLongInlines(u.user, u.offset, to_split_reversed);
    for (auto it = to_split_reversed.rbegin(), end = to_split_reversed.rend();
         it != end;
         ++it) {
      printOutputDefinition((*it)->node(), *useOf(*it));
}
  }

  void scanLongInlines(
      Node* user,
      int64_t offset,
      std::vector<Value*>& to_split_reversed) {
    auto it = visited_split_inline_uses_.find(user);
    bool present = it != visited_split_inline_uses_.end();
    for (int64_t i = offset; i >= (present ? it->second + 1 : 0); --i) {
      Value* prev_arg = user->input(i);
      if (isNonConstantInline(prev_arg)) {
        to_split_reversed.push_back(prev_arg);
}
}
    visited_split_inline_uses_[user] = offset;
    if (!present && output_inline_.count(user)) {
      Use u = user->output()->uses().at(0);
      scanLongInlines(u.user, int64_t(u.offset) - 1, to_split_reversed);
      // -1 because the actual use is still being
      // emitted so it cannot be split
    }
}
template <typename T>
"
1119,"processBlock(graph_->block(), SparseBitVector{});
} while (changed_);
std::unordered_map<Node*, std::vector<Value*>> result;
for (const auto& e : liveness_sets_) {
","processBlock(graph_->block(), SparseBitVector{});
} while (changed_);
    removeCounterNodes(counters);
std::unordered_map<Node*, std::vector<Value*>> result;
for (const auto& e : liveness_sets_) {
"
1120,"return qy;
}
// Keep the registry in the anonymous namespace.
namespace {

template <bool ReLUFused = false>
class QBatchNorm2d final : public torch::OperatorKernel {
 public:
  Tensor operator()(
      Tensor qx,
      Tensor weight,
      Tensor bias,
      Tensor mean,
      Tensor var,
      double eps,
      double output_scale,
      int64_t output_zero_point) {
    return q_batch_norm2d_impl<ReLUFused>(
        qx, weight, bias, mean, var, eps, output_scale, output_zero_point);
  }
};

template <bool ReLUFused = false>
class QBatchNorm3d final : public torch::OperatorKernel {
 public:
  Tensor operator()(
      Tensor qx,
      Tensor weight,
      Tensor bias,
      Tensor mean,
      Tensor var,
      double eps,
      double output_scale,
      int64_t output_zero_point) {
    return q_batch_norm3d_impl<ReLUFused>(
        qx, weight, bias, mean, var, eps, output_scale, output_zero_point);
  }
};

static auto registry = torch::RegisterOperators().op(
    ""quantized::batch_norm2d(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm2d<false>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm2d_relu(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm2d<true>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm3d(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm3d<false>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm3d_relu(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm3d<true>>(
        DispatchKey::QuantizedCPU));
} // namespace
} // namespace native
} // namespace at
","return qy;
}
TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
  m.impl(""batch_norm2d"",      q_batch_norm2d_impl<false>);
  m.impl(""batch_norm2d_relu"", q_batch_norm2d_impl<true>);
  m.impl(""batch_norm3d"",      q_batch_norm3d_impl<false>);
  m.impl(""batch_norm3d_relu"", q_batch_norm3d_impl<true>);
}
} // namespace native
} // namespace at
"
1121,"""operator FloatToFused2BitRowwiseQuantized"")
.Input(
1,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        2,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
    .Input(
        3,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
.Output(0, ""output"", ""output"");
NO_GRADIENT(SparseLengthsWeightedSumFused2BitRowwise);
","""operator FloatToFused2BitRowwiseQuantized"")
.Input(
1,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
    .Input(
        2,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        3,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
.Output(0, ""output"", ""output"");
NO_GRADIENT(SparseLengthsWeightedSumFused2BitRowwise);
"
1122,"AT_CUDA_CHECK(cudaGetDevice(&device));
if (!myPoolWindow)
    myPoolWindow.reset(pool.newPoolWindow());
auto handle = myPoolWindow->reserve(device);
auto stream = c10::cuda::getCurrentCUDAStream();
TORCH_CUDABLAS_CHECK(cublasSetStream(handle, stream));
","AT_CUDA_CHECK(cudaGetDevice(&device));
if (!myPoolWindow)
    myPoolWindow.reset(pool->newPoolWindow());
auto handle = myPoolWindow->reserve(device);
auto stream = c10::cuda::getCurrentCUDAStream();
TORCH_CUDABLAS_CHECK(cublasSetStream(handle, stream));
"
1123,"LOG(INFO) << ""Plan "" << plan.name() << "" executed successfully."";
return true;
}
}
","LOG(INFO) << ""Plan "" << plan.name() << "" executed successfully."";
return true;
}
} // namespace caffe2
"
1124,"// Something is wrong if all variables contained in this bucket replica have
// already been marked as ready.
if (replica.pending == 0) {
    // Receiving a call to `mark_variable_ready` twice for the same variable
    // is only possible if the variable was initially deemed unused, and was
    // marked ready from the `prepare_for_backward` function, only to become
    // part of the autograd graph at a later point in time.
    TORCH_INTERNAL_ASSERT(has_marked_unused_parameters_);
    TORCH_CHECK(
        false,
""Expected to mark a variable ready only once. "",
"""",
        ""This error is caused by use of a module parameter outside the "",
        ""`forward` function. The return value of the `forward` function "",
        ""is inspected by the distributed data parallel wrapper to figure "",
        ""out if any of the module's parameters went unused. If this is the "",
        ""case, it knows they won't receive gradients in a backward pass. "",
        ""If any of those parameters are then used outside `forward`, this "",
        ""error condition is triggered. "",
        """",
        ""You can disable unused parameter detection by passing the keyword ""
        ""argument `find_unused_parameters=False` to "",
""`torch.nn.parallel.DistributedDataParallel`."");
}
if (bucket.expect_sparse_gradient) {
","// Something is wrong if all variables contained in this bucket replica have
// already been marked as ready.
if (replica.pending == 0) {
    const auto common_error = c10::str(
""Expected to mark a variable ready only once. "",
"""",
        ""This error is caused by one of the following reasons: "",
        ""1) Use of a module parameter outside the `forward` function. "",
        ""Please make sure model parameters are not shared across multiple "",
        ""concurrent forward-backward passes"",
        ""2) Reused parameters in multiple reentrant backward passes. For "",
        ""example, if you use multiple `checkpoint` functions to wrap the "",
        ""same part of your model, it would result in the same set of "",
        ""parameters been used by different reentrant backward passes "",
        ""multiple times, and hence marking a variable ready multiple times. "",
        ""DDP does not support such use cases yet."");
    TORCH_CHECK(
        has_marked_unused_parameters_,
        common_error,
        ""3) Incorrect unused parameter detection. The return value of the "",
        ""`forward` function is inspected by the distributed data parallel "",
        ""wrapper to figure out if any of the module's parameters went "",
        ""unused. For unused parameters, DDP would not expect gradients from "",
        ""then. However, if an unused parameter becomes part of the autograd "",
        ""graph at a later point in time (e.g., in a reentrant backward when "",
        ""using `checkpoint`), the gradient will show up unexpectedly. If all "",
        ""parameters in the model participate in the backward pass, you can "",
        ""disable unused parameter detection by passing the keyword argument "",
        ""`find_unused_parameters=False` to "",
""`torch.nn.parallel.DistributedDataParallel`."");
    TORCH_CHECK(!has_marked_unused_parameters_, common_error);
}
if (bucket.expect_sparse_gradient) {
"
1125,"is_float_scale_factor = False
for scale in scale_factors:
            is_float_scale_factor = math.floor(scale) == scale
if is_float_scale_factor:
break
","is_float_scale_factor = False
for scale in scale_factors:
            is_float_scale_factor = math.floor(scale) != scale
if is_float_scale_factor:
break
"
1126,"llvm::FunctionType* fntype = llvm::FunctionType::get(retTy, params, false);
fn_ = llvm::Function::Create(
fntype, llvm::Function::PrivateLinkage, ""pytorch"", module_.get());
  for (int i = 0; i < args.size(); i++) {
if (!args[i].isVar()) {
fn_->addParamAttr(i, llvm::Attribute::NoAlias);
}
","llvm::FunctionType* fntype = llvm::FunctionType::get(retTy, params, false);
fn_ = llvm::Function::Create(
fntype, llvm::Function::PrivateLinkage, ""pytorch"", module_.get());
  for (size_t i = 0; i < args.size(); i++) {
if (!args[i].isVar()) {
fn_->addParamAttr(i, llvm::Attribute::NoAlias);
}
"
1127,"return true;
}
REGISTER_CPU_OPERATOR(
MulGradient,
BinaryElementwiseGradientOp<
","return true;
}
// Used in fallback ops
template bool MulFunctor<CPUContext>::Backward<float, float, float>(
    const std::vector<int>& A_dims,
    const std::vector<int>& B_dims,
    const float* dC,
    const float* A,
    const float* B,
    const float* /* C */,
    float* dA,
    float* dB,
    CPUContext* context) const;

template bool MulFunctor<CPUContext>::Backward<int32_t, int32_t, int32_t>(
    const std::vector<int>& A_dims,
    const std::vector<int>& B_dims,
    const int* dC,
    const int* A,
    const int* B,
    const int* /* C */,
    int* dA,
    int* dB,
    CPUContext* context) const;

template bool MulFunctor<CPUContext>::Backward<double, double, double>(
    const std::vector<int>& A_dims,
    const std::vector<int>& B_dims,
    const double* dC,
    const double* A,
    const double* B,
    const double* /* C */,
    double* dA,
    double* dB,
    CPUContext* context) const;

template bool MulFunctor<CPUContext>::Backward<int64_t, int64_t, int64_t>(
    const std::vector<int>& A_dims,
    const std::vector<int>& B_dims,
    const int64_t* dC,
    const int64_t* A,
    const int64_t* B,
    const int64_t* /* C */,
    int64_t* dA,
    int64_t* dB,
    CPUContext* context) const;

REGISTER_CPU_OPERATOR(
MulGradient,
BinaryElementwiseGradientOp<
"
1128,"},
c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
});
} // namespace jit
} // namespace torch
","},
c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
});
}
} // namespace jit
} // namespace torch
"
1129,"return expr_value;
}
  Value* emitToBool(Value* v) {
    SourceRange loc = v->node()->sourceRange();
Value* out;
try {
auto bool_cast = environment_stack->getSugaredVar(""bool"", loc);
","return expr_value;
}
  Value* emitToBool(const SourceRange& loc, Value* v) {
Value* out;
try {
auto bool_cast = environment_stack->getSugaredVar(""bool"", loc);
"
1130,"#include <torch/csrc/jit/runtime/custom_operator.h>
#include <torch/csrc/jit/codegen/cuda/interface.h>
#include <ATen/core/dispatch/OperatorOptions.h>
namespace torch {
namespace jit {
","#include <torch/csrc/jit/codegen/cuda/interface.h>
#include <ATen/core/dispatch/OperatorOptions.h>
#include <torch/csrc/jit/runtime/custom_operator.h>
namespace torch {
namespace jit {
"
1131,"} // namespace cuda
} // namespace fuser

RegisterOperators reg(
    {Operator(
         prim::CudaFusionGroup,
         [](const Node* node) -> Operation {
           return [node](Stack& stack) {
             fuser::cuda::runFusionGroup(node, stack);
             return 0;
           };
         },
         c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
	}
);

    } // namespace jit
} // namespace torch
","} // namespace cuda
} // namespace fuser
RegisterOperators reg({
    Operator(
        prim::CudaFusionGroup,
        [](const Node* node) -> Operation {
          return [node](Stack& stack) {
            fuser::cuda::runFusionGroup(node, stack);
            return 0;
          };
        },
        c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
});

} // namespace jit
} // namespace torch
"
1132,"c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
c10::ReplaceAll(doc, ""{extra}"", extra);
schema.SetDoc(doc);
    schema.Arg(""broadcast"", ""*(type: int; default: 0)* Pass 1 to enable broadcasting."");
schema.Arg(
""axis"",
""*(type: int; default: -1)* Axis to concatenate on. If set, defines the broadcast dimensions."");
","c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
c10::ReplaceAll(doc, ""{extra}"", extra);
schema.SetDoc(doc);
    schema.Arg(
        ""broadcast"",
        ""*(type: int; default: 0)* Pass 1 to enable broadcasting."");
schema.Arg(
""axis"",
""*(type: int; default: -1)* Axis to concatenate on. If set, defines the broadcast dimensions."");
"
1133,"c10::ReplaceAll(doc, ""{name}"", name);
c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
schema.SetDoc(doc);
    schema.Arg(""broadcast"", ""*(type: int; default: 0)* Pass 1 to enable broadcasting."");
schema.Arg(
""axis"",
""*(type: int; default: -1)* Axis to concatenate on. If set, defines the broadcast dimensions."");
","c10::ReplaceAll(doc, ""{name}"", name);
c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
schema.SetDoc(doc);
    schema.Arg(
        ""broadcast"",
        ""*(type: int; default: 0)* Pass 1 to enable broadcasting."");
schema.Arg(
""axis"",
""*(type: int; default: -1)* Axis to concatenate on. If set, defines the broadcast dimensions."");
"
1134,".NumOutputs(1)
.AllowInplace({{1, 0}})
.Arg(""alpha"", ""Coefficient of leakage"")
    .InheritOnnxSchema();
class GetLeakyReluGradient : public GradientMakerBase {
using GradientMakerBase::GradientMakerBase;
",".NumOutputs(1)
.AllowInplace({{1, 0}})
.Arg(""alpha"", ""Coefficient of leakage"")
    .InheritOnnxSchema()
    .IdenticalTypeAndShapeOfInput(1);
class GetLeakyReluGradient : public GradientMakerBase {
using GradientMakerBase::GradientMakerBase;
"
1135,"""element-wise"")
.InheritOnnxSchema();
OPERATOR_SCHEMA(TanhGradient).NumInputs(2).NumOutputs(1).AllowInplace({{1, 0}});
} // namespace caffe2
","""element-wise"")
.InheritOnnxSchema();
OPERATOR_SCHEMA(TanhGradient)
    .NumInputs(2)
    .NumOutputs(1)
    .IdenticalTypeAndShapeOfInput(1)
    .AllowInplace({{1, 0}});
} // namespace caffe2
"
1136,"auto x_nearest = x.round();
auto y_nearest = y.round();
    auto i_x_nearest = convert_to_int_of_same_size<scalar_t>(x_nearest);
    auto i_y_nearest = convert_to_int_of_same_size<scalar_t>(y_nearest);
auto i_mask = must_in_bound ? iVec(-1)
: (i_x_nearest > iVec(-1)) & (i_x_nearest < iVec(inp_W)) &
","auto x_nearest = x.round();
auto y_nearest = y.round();
    auto i_x_nearest = convert_to_int_of_same_size(x_nearest);
    auto i_y_nearest = convert_to_int_of_same_size(y_nearest);
auto i_mask = must_in_bound ? iVec(-1)
: (i_x_nearest > iVec(-1)) & (i_x_nearest < iVec(inp_W)) &
"
1137,"}
// Erase the pending sends that we added since we have returned from wait.
  pendingSendGuard.lock();
  // NB: We cannot just erase all of currentPendingSends[dst], since this might
  // preemptively remove sends from other threads.
  auto& set = currentPendingSends_[dst];
  for (auto& p : pendingSends) {
    set.erase(p);
}
}
","}
// Erase the pending sends that we added since we have returned from wait.
  {
    std::lock_guard<std::mutex> pendingSendGuard(pendingSendMutex_);
    // NB: We cannot just erase all of currentPendingSends[dst], since this
    // might preemptively remove sends from other threads.
    auto& set = currentPendingSends_[dst];
    for (auto& p : pendingSends) {
      set.erase(p);
    }
}
}
"
1138,"TORCH_CHECK(n->outputs().size() == 1, ""We only support dequantize swapping for ops""
"" with one output right now"");
auto* output = n->output();
        WithInsertPoint ins(n->next());
std::vector<Use> uses = output->uses();
// Insert new dequantize node for each use of the output
insertDeQuantCall(graph.get(), output, output, uses);
","TORCH_CHECK(n->outputs().size() == 1, ""We only support dequantize swapping for ops""
"" with one output right now"");
auto* output = n->output();
std::vector<Use> uses = output->uses();
// Insert new dequantize node for each use of the output
insertDeQuantCall(graph.get(), output, output, uses);
"
1139,"application has called a graceful shutdown. Invoking methods on a
deleted RRef leads to undefined behaviors. RRef implementation only
offers best-effort error detection, and applications should not use
          ``UserRRef``s after ``rpc.shutdown()``.
.. warning::
RRefs can only be serialized and deserialized by the RPC module.
","application has called a graceful shutdown. Invoking methods on a
deleted RRef leads to undefined behaviors. RRef implementation only
offers best-effort error detection, and applications should not use
          ``UserRRefs`` after ``rpc.shutdown()``.
.. warning::
RRefs can only be serialized and deserialized by the RPC module.
"
1140,">>      loss = t1 + t2
>>      dist_autograd.backward(context_id, [loss.sum()])
>>      grads = dist_autograd.get_gradients(context_id)
    >>      print (grads[t1])
    >>      print (grads[t2])
)"",
py::arg(""context_id""));
",">>      loss = t1 + t2
>>      dist_autograd.backward(context_id, [loss.sum()])
>>      grads = dist_autograd.get_gradients(context_id)
    >>      print(grads[t1])
    >>      print(grads[t2])
)"",
py::arg(""context_id""));
"
1141,"PyObject* tensor_to_numpy(const at::Tensor& tensor) {
if (tensor.device().type() != DeviceType::CPU) {
throw TypeError(
      ""can't convert non-cpu tensor to numpy. Use Tensor.cpu() to ""
      ""copy the tensor to host memory first."");
}
if (tensor.layout() != Layout::Strided) {
throw TypeError(
        ""can't convert non-strided tensor to numpy.""
        ""convert the tensor to a strided layout first."");
}
if (tensor.requires_grad()) {
throw std::runtime_error(
","PyObject* tensor_to_numpy(const at::Tensor& tensor) {
if (tensor.device().type() != DeviceType::CPU) {
throw TypeError(
      ""can't convert %s device type tensor to numpy. Use Tensor.cpu() to ""
      ""copy the tensor to host memory first."", tensor.device().str().c_str());
}
if (tensor.layout() != Layout::Strided) {
throw TypeError(
        ""can't convert %s layout tensor to numpy.""
        ""convert the tensor to a strided layout first."", c10::str(tensor.layout()).c_str());
}
if (tensor.requires_grad()) {
throw std::runtime_error(
"
1142,"filename = '{}/{}'.format(self.install_dir, filename)
if isinstance(s, CodeTemplate):
assert env is not None
            env['generated_comment'] = ""@"" + ""generated by aten/src/ATen/gen.py""
s = s.substitute(env)
self._write_if_changed(filename, s)
if filename not in self.filenames:
","filename = '{}/{}'.format(self.install_dir, filename)
if isinstance(s, CodeTemplate):
assert env is not None
            comment = ""@"" + ""generated by aten/src/ATen/gen.py""
            if s.filename:
                comment += "" from {}"".format(os.path.basename(s.filename))
            env['generated_comment'] = comment
s = s.substitute(env)
self._write_if_changed(filename, s)
if filename not in self.filenames:
"
1143,".Arg(
""max_mismatched_ratio"",
""An error is raised when ratio of mismatched ranges exceeds this."")
.TensorInferenceFunction([](const OperatorDef& def,
const vector<TensorShape>& in) {
ArgumentHelper helper(def);
",".Arg(
""max_mismatched_ratio"",
""An error is raised when ratio of mismatched ranges exceeds this."")
    .Arg(
        ""max_empty_ratio"",
        ""An error is raised when ratio of empty ranges exceeds this (default is""
        "" 1, which means by default no error will be triggered)."")
.TensorInferenceFunction([](const OperatorDef& def,
const vector<TensorShape>& in) {
ArgumentHelper helper(def);
"
1144,"#define _USE_MATH_DEFINES
#include <ATen/native/Activation.h>
","#ifndef _USE_MATH_DEFINES
#define _USE_MATH_DEFINES
#endif
#include <ATen/native/Activation.h>
"
1145,"error_stack(e.error_stack.begin(), e.error_stack.end()) {}
#ifndef C10_MOBILE
ErrorReport::ErrorReport()
    : context(c10::nullopt), error_stack(calls.begin(), calls.end()) {}
ErrorReport::ErrorReport(SourceRange r)
: context(std::move(r)), error_stack(calls.begin(), calls.end()) {}
","error_stack(e.error_stack.begin(), e.error_stack.end()) {}
#ifndef C10_MOBILE
ErrorReport::ErrorReport(SourceRange r)
: context(std::move(r)), error_stack(calls.begin(), calls.end()) {}
"
1146,"}
Tensor normal_cpu(const Tensor& mean, const Tensor& std, Generator* gen) {
  Tensor ret = at::empty_like(mean, MemoryFormat::Contiguous);
normal_out_cpu(ret, mean, std, gen);
return ret;
}
","}
Tensor normal_cpu(const Tensor& mean, const Tensor& std, Generator* gen) {
  Tensor ret = at::empty({0}, mean.options(), MemoryFormat::Contiguous);
normal_out_cpu(ret, mean, std, gen);
return ret;
}
"
1147,"The context can be used to store tensors that can be then retrieved
during the backward pass.
""""""
        raise NotImplementedError
@staticmethod
def backward(ctx, *grad_outputs):
","The context can be used to store tensors that can be then retrieved
during the backward pass.
""""""
        raise NotImplementedError(""You must implement the forward function for custom""
                                  "" autograd.Function."")
@staticmethod
def backward(ctx, *grad_outputs):
"
1148,"return '\n'.join(names)
def wrap_output(call):
        # Returns a 2-tuple `(wrapped_call, extra_wrapping_stmts)`, where
        # `wrapped_call` is to drop-in replace `call`, and
        # `extra_wrapping_stmts` is a list of extra statements to run after
        # `call`.
if 'Tensor' not in declaration['return_type']:
            return call, []
elif view_info is not None:
# See NOTE [ Autograd View Variables ] in variable.h for details.
differentiable_output_vars = {r['name'] for r in differentiable_outputs}
            tensor_output_vars = {r['name'] for r in returns if 'Tensor' in r['type']}
            if not isinstance(view_info, dict):
                if len(differentiable_output_vars) == len(tensor_output_vars):
                    # all outputs are differentiable
                    return 'as_view({}, {}, true)'.format(view_info, call), []
                elif len(differentiable_output_vars) == 0:
                    # no output is differentiable
                    return 'as_view({}, {}, false)'.format(view_info, call), []
                else:
                    # some of the outputs are differentiable
                    # need to expand to dict mode, i.e., one entry per output
                    base_name = view_info
                    view_info_dict = {}
                    for i, return_info in enumerate(returns):
                        if 'Tensor' in return_info['type']:
                            view_info_dict[i] = base_name
else:
                view_info_dict = view_info

            def wrap_view_single(output_var, base_var):
                fmt = '{output_var} = as_view({base_var}, {output_var}, {is_differentiable});'
                if output_var in differentiable_output_vars:
                    # If `GradMode::is_enabled()` is False, this is a
                    # non-differentiable view. Gradients should not flow through.
                    is_differentiable = 'true'
                else:
                    # This output is non-differentiable, so it is a
                    # non-differentiable view. Gradients should not flow through.
                    is_differentiable = 'false'
                return fmt.format(output_var=output_var, base_var=base_var,
                                  is_differentiable=is_differentiable)

            extra_wrapping_stmts = []
            for output_idx, return_info in enumerate(returns):
                if 'Tensor' not in return_info['type']:
                    assert output_idx not in view_info_dict, 'Can not wrap non-Tensor output as a view'
                    continue
                output_var = return_info['name']
                if output_idx in view_info_dict:
                    stmt = wrap_view_single(output_var, view_info_dict[output_idx])
                extra_wrapping_stmts.append(stmt)
            return call, extra_wrapping_stmts
else:
            return 'std::move({})'.format(call), []
def enforce_same_tensorimpl_and_storage(env, call):
save_ptrs_stmts = []
","return '\n'.join(names)
def wrap_output(call):
        # Returns `wrapped_call` which is a drop-in replacement for `call`
if 'Tensor' not in declaration['return_type']:
            return call
elif view_info is not None:
# See NOTE [ Autograd View Variables ] in variable.h for details.
differentiable_output_vars = {r['name'] for r in differentiable_outputs}

            if not isinstance(view_info, str):
                raise TypeError(""The view info should be a string for {}, but it is: {}"".format(base_name, view_info))

            if len(differentiable_output_vars) == 0:
                # no output is differentiable (.indices() for SparseTensors for example)
                return 'as_view({}, {}, /* is_differentiable */ false)'.format(view_info, call)
            elif len(differentiable_output_vars) == 1:
                # Single differentiable output (Tensor or Tensor[])
                return_info = differentiable_outputs[0]
                # We only support simple Tensor or a TensorList for functions that return views
                if not return_info['dynamic_type'] in ['Tensor', 'TensorList']:
                    raise RuntimeError(""{} that return differentiable views can only return Tensor or Tensor[]"".format(base_name))
                # Only allow rebasing of the history if we return a single Tensor
                allow_rebase_history = 'true'
                if return_info['dynamic_type'] == 'TensorList':
                    allow_rebase_history = 'false'
                wrapped_call = (""as_view(/* base */{}, /* output */ {}, /* is_differentiable */ true, ""
                                ""/* allow_rebase_history */ {})"").format(view_info, call, allow_rebase_history)
                return wrapped_call
else:
                # This could be supported but we don't need it at the moment, so keeping things simple.
                raise RuntimeError(""Function that return multiple differentiable output ""
                                   ""when at least one of them is view is not supported."")
else:
            return 'std::move({})'.format(call)
def enforce_same_tensorimpl_and_storage(env, call):
save_ptrs_stmts = []
"
1149,"for key in sorted(serialized_storages.keys()):
name = 'data/{}'.format(key)
storage = serialized_storages[key]
        num_bytes = storage.size() * storage.element_size()
        zip_file.write_record(name, storage.data_ptr(), num_bytes)
def load(f, map_location=None, pickle_module=pickle, **pickle_load_args):
","for key in sorted(serialized_storages.keys()):
name = 'data/{}'.format(key)
storage = serialized_storages[key]
        if storage.device.type == 'cpu':
            # If it's on the CPU we can directly copy it into the zip file
            num_bytes = storage.size() * storage.element_size()
            buf = io.BytesIO()
            zip_file.write_record(name, storage.data_ptr(), num_bytes)
        else:
            # Copy to a buffer, then serialize that
            buf = io.BytesIO()
            storage._write_file(buf, _should_read_directly(buf))
            buf_value = buf.getvalue()
            zip_file.write_record(name, buf_value, len(buf_value))
def load(f, map_location=None, pickle_module=pickle, **pickle_load_args):
"
1150,"&& weight.defined() && bias.defined()
&& ((running_mean.defined() && running_var.defined())
|| (!running_mean.defined() && !running_var.defined() && training))
               && input.size(0) <= 131070
&& detail::getCUDAHooks().compiledWithCuDNN()
&& cudnn_enabled && detail::getCUDAHooks().versionCuDNN() >= 5110L);
","&& weight.defined() && bias.defined()
&& ((running_mean.defined() && running_var.defined())
|| (!running_mean.defined() && !running_var.defined() && training))
               && ((input.dim() == 2 && input.size(0) <= 131070 && training) // per-activation, training
                 || (input.dim() == 2 && input.size(0) <= 262136 && !training) // per-activation, eval
                 || (input.dim() >= 3 && input.size(0) <= 880801 && training) // spatial, training
                 || (input.dim() >= 3 && input.size(0) <= 65535 && !training)) //spatial, eval
&& detail::getCUDAHooks().compiledWithCuDNN()
&& cudnn_enabled && detail::getCUDAHooks().versionCuDNN() >= 5110L);
"
1151,"added_names = set()
for name, item in nn_module._parameters.items():
        if item is None:
            # TODO special case: parameters can be None. The JIT assumes
            # parameters are Tensor types, so in this case just add it as a
            # attribute.
            # The ""correct"" fix here is to add the parameter as a NoneType
            # attribute, but NoneType refinemenet is currently wonky
            continue
        assert isinstance(item, torch.Tensor)
attr_type = infer_type(name, item)
        concrete_type_builder.add_attribute(name, attr_type, True)
added_names.add(name)
for name, item in nn_module._buffers.items():
        if item is None:
            # TODO special case: parameters can be None. The JIT assumes
            # parameters are Tensor types, so in this case just add it as a
            # attribute.
            # The ""correct"" fix here is to add the parameter as a NoneType
            # attribute, but NoneType refinemenet is currently wonky
            continue
        assert isinstance(item, torch.Tensor)
attr_type = infer_type(name, item)
concrete_type_builder.add_attribute(name, attr_type, False)
added_names.add(name)
","added_names = set()
for name, item in nn_module._parameters.items():
        assert item is None or isinstance(item, torch.Tensor)
attr_type = infer_type(name, item)
        # We currently have the invariant in various places in our code
        # that parameters must be Tensors. However, the nn.Module API also
        # allows NoneType parameters. These parameters are not returned as
        # part of `parameters()` and its variants, but are available
        # through direct attribute access.
        #
        # So to achieve the nn.Module behavior, add the NoneType parameters
        # as an attribute.
        should_register_as_parameter = item is not None
        concrete_type_builder.add_attribute(name, attr_type, should_register_as_parameter)
added_names.add(name)
for name, item in nn_module._buffers.items():
        assert item is None or isinstance(item, torch.Tensor)
attr_type = infer_type(name, item)
concrete_type_builder.add_attribute(name, attr_type, False)
added_names.add(name)
"
1152,"}
auto current_version = self._version();
if (diff_view_meta->attr_version != current_version) {
auto fn = std::make_shared<torch::autograd::generated::AsStridedBackward>();
fn->self_geometry = at::TensorGeometry(diff_view_meta->base_);
fn->size = self.sizes().vec();
","}
auto current_version = self._version();
if (diff_view_meta->attr_version != current_version) {
      AT_ASSERT(diff_view_meta->output_nr_ == 0);
auto fn = std::make_shared<torch::autograd::generated::AsStridedBackward>();
fn->self_geometry = at::TensorGeometry(diff_view_meta->base_);
fn->size = self.sizes().vec();
"
1153,"Args:
config:         contains number of warmup and benchmark iterations.
module_config:  module_config which contains op, number of parameters that op takes
                    and wether graph mode is enabled or not.
module_type:    Type of the module to be wrapped. e.g. SimpleAddModule for add op.
result:         dictionary instance to be populated with the benchmark result (latency per iter).
""""""
","Args:
config:         contains number of warmup and benchmark iterations.
module_config:  module_config which contains op, number of parameters that op takes
                    and whether graph mode is enabled or not.
module_type:    Type of the module to be wrapped. e.g. SimpleAddModule for add op.
result:         dictionary instance to be populated with the benchmark result (latency per iter).
""""""
"
1154,"void run() {
// Run the pass until no changes are made.
    // This is neccessary, because the algorithm can miss out on certain fusion
// opportunities if ran only once. Consider this graph:
//
// %1 = f(...)
","void run() {
// Run the pass until no changes are made.
    // This is necessary, because the algorithm can miss out on certain fusion
// opportunities if ran only once. Consider this graph:
//
// %1 = f(...)
"
1155,"base_type_call = CALL_DISPATCH_VIA_METHOD.substitute(
combined, unpacked_method_args=unpacked_method_args)
if not modifies_arguments and not returns_void:
                rhs_value = wrap_output('tmp')
call = DISPATCH_TO_NON_VAR_TYPE_WITH_RETURN_VALUES.substitute(
base_type_call=base_type_call,
return_values=tie_return_values(),
","base_type_call = CALL_DISPATCH_VIA_METHOD.substitute(
combined, unpacked_method_args=unpacked_method_args)
if not modifies_arguments and not returns_void:
                rhs_value, extra_wrapping_stmts = wrap_output('tmp')
call = DISPATCH_TO_NON_VAR_TYPE_WITH_RETURN_VALUES.substitute(
base_type_call=base_type_call,
return_values=tie_return_values(),
"
1156,"}
// <NON_GENERATED_CODE>
  auto result = make_variable_non_differentiable_view(self, self, /*allow_tensor_metadata_change=*/false);
namedinference::propagate_names(result, self);
// </NON_GENERATED_CODE>
if (jit::tracer::isTracing()) {
","}
// <NON_GENERATED_CODE>
  auto result = make_variable_view(self, self, /*is_differentiable=*/false, /*allow_tensor_metadata_change=*/false);
namedinference::propagate_names(result, self);
// </NON_GENERATED_CODE>
if (jit::tracer::isTracing()) {
"
1157,"namespace autograd {
DifferentiableViewMeta::DifferentiableViewMeta(at::TensorImpl* self_impl, Variable base, bool allow_rebase_history)
    : AutogradMeta(self_impl), allow_rebase_history(allow_rebase_history) {
base_ = std::move(base);
TORCH_CHECK(base_.defined(), ""base is undefined"");
if (base_.is_view()) {
","namespace autograd {
DifferentiableViewMeta::DifferentiableViewMeta(at::TensorImpl* self_impl, Variable base)
    : AutogradMeta(self_impl, false) {
base_ = std::move(base);
TORCH_CHECK(base_.defined(), ""base is undefined"");
if (base_.is_view()) {
"
1158,"* in_len: the dimension_size of input matrix
* Basically, in_len / out_len gives the number of
* elements in each average computation.
   * This functin computes the start index on input matrix.
*/
return (int)std::floor((float)(out_idx * in_len) / out_len);
}
","* in_len: the dimension_size of input matrix
* Basically, in_len / out_len gives the number of
* elements in each average computation.
   * This function computes the start index on input matrix.
*/
return (int)std::floor((float)(out_idx * in_len) / out_len);
}
"
1159,"*
* To avoid full 64-bit shift, we leverage the fact that shift >= 32, and do
* it in two steps:
     * - Shift by 32, which can be implemented by extacting the high 32-bit word
* on 32-bit systems.
* - Shift by (shift - 32), which can be implemented as a 32-bit shift of
* high word of addition result.
","*
* To avoid full 64-bit shift, we leverage the fact that shift >= 32, and do
* it in two steps:
     * - Shift by 32, which can be implemented by extracting the high 32-bit word
* on 32-bit systems.
* - Shift by (shift - 32), which can be implemented as a 32-bit shift of
* high word of addition result.
"
1160,"}
/*
 * This struct's constructor initalizes the dispatch tables. It simply checks
* what SIMD extensions are available, and then walks the dispatch table
* to choose the best function.
* NOTE: As implemented, it will initialize the dispatch pointer to the first supported function.
","}
/*
 * This struct's constructor initializes the dispatch tables. It simply checks
* what SIMD extensions are available, and then walks the dispatch table
* to choose the best function.
* NOTE: As implemented, it will initialize the dispatch pointer to the first supported function.
"
1161,"""""""
This module constructs a net with 'op_name' operator. The net consist
a series of such operator.
    It intializes the workspace with input blob equal to the number of parameters
needed for the op.
Provides forward method to run the net niter times.
""""""
","""""""
This module constructs a net with 'op_name' operator. The net consist
a series of such operator.
    It initializes the workspace with input blob equal to the number of parameters
needed for the op.
Provides forward method to run the net niter times.
""""""
"
1162,"}
/**
 * This is a helper function which attemtps to get a base value depending on the
* # of nodes. Larger the base the better performance (up to 4) is what we have
* observed in gloo benchmarks. At the moment bcube works only if # nodes = base
* ^ x. Where x is some constant. So, if # node don't match our expectation
","}
/**
 * This is a helper function which attempts to get a base value depending on the
* # of nodes. Larger the base the better performance (up to 4) is what we have
* observed in gloo benchmarks. At the moment bcube works only if # nodes = base
* ^ x. Where x is some constant. So, if # node don't match our expectation
"
1163,"OpSchema()
.SetDoc(""Mirror Caffe2 BatchMatMul operator"")
.Input(0, ""X"", ""tensor of shape (dim0, dim1 ... M, K)"", ""T"")
        .Input(1, ""Y"", ""tensor of shpae (dim0, dim2 ... K, N)"", ""T"")
.Output(0, ""Z"", ""tensor of shape (dim0, dim1 ... M, N)"", ""T"")
.TypeConstraint(
""T"",
","OpSchema()
.SetDoc(""Mirror Caffe2 BatchMatMul operator"")
.Input(0, ""X"", ""tensor of shape (dim0, dim1 ... M, K)"", ""T"")
        .Input(1, ""Y"", ""tensor of shape (dim0, dim2 ... K, N)"", ""T"")
.Output(0, ""Z"", ""tensor of shape (dim0, dim1 ... M, N)"", ""T"")
.TypeConstraint(
""T"",
"
1164,")DOC"")
.Input(0, ""input"", ""Float32 input data"")
.Output(0, ""output"", ""Fused bitwidth, tail, min, max and quantized data"")
    .Arg(""bitwidth"", ""How many bits to quantiz per data (defaults to 8)."")
.Arg(""random"", ""random or not (True). False is set up for unittest."");
NO_GRADIENT(FloatToFusedRandRowwiseQuantized);
",")DOC"")
.Input(0, ""input"", ""Float32 input data"")
.Output(0, ""output"", ""Fused bitwidth, tail, min, max and quantized data"")
    .Arg(""bitwidth"", ""How many bits to quantize per data (defaults to 8)."")
.Arg(""random"", ""random or not (True). False is set up for unittest."");
NO_GRADIENT(FloatToFusedRandRowwiseQuantized);
"
1165,".Input(
5,
""OBJECT_TO_POS_MAP_IN"",
        ""(Optional) Auxillary bookkeeping map. This should be created from ""
"" `CreateMap` with keys of type int64 and values of type int32"")
.Input(
6,
",".Input(
5,
""OBJECT_TO_POS_MAP_IN"",
        ""(Optional) Auxiliary bookkeeping map. This should be created from ""
"" `CreateMap` with keys of type int64 and values of type int32"")
.Input(
6,
"
1166,"[], value=0, shape=[self.num_classes_padded], dtype=core.DataType.INT32
)
        # Compute the accumlated total score of all the paths
accum_score = self.model.net.SortedSegmentRangeLogSumExp(
[out_last, zero_segment_id]
)
","[], value=0, shape=[self.num_classes_padded], dtype=core.DataType.INT32
)
        # Compute the accumulated total score of all the paths
accum_score = self.model.net.SortedSegmentRangeLogSumExp(
[out_last, zero_segment_id]
)
"
1167,"candidate_scope = scope.CurrentNameScope()
best_scope = self._resolve_scope_overrides(candidate_scope)
if best_scope != candidate_scope:
            logger.info(""Overwiting scope {0} with scope {1}"".format(
candidate_scope, best_scope))
return best_scope + name
","candidate_scope = scope.CurrentNameScope()
best_scope = self._resolve_scope_overrides(candidate_scope)
if best_scope != candidate_scope:
            logger.info(""Overwriting scope {0} with scope {1}"".format(
candidate_scope, best_scope))
return best_scope + name
"
1168,"reg_lambda: parameter to scale regularization by
alpha:      hyper parameter to tune that is only used in the calculation
                    of approxiamte L0 norm
budget:     desired number of features. If the number of features is greater
than the budget amount, then the least important features will
","reg_lambda: parameter to scale regularization by
alpha:      hyper parameter to tune that is only used in the calculation
                    of approximate L0 norm
budget:     desired number of features. If the number of features is greater
than the budget amount, then the least important features will
"
1169,"def FetchInt8Blob(name):
""""""Fetches an Int8 blob from the workspace. It shared backend implementation
    with FetchBlob but it is recommened when fetching Int8 Blobs
Inputs:
name: the name of the Int8 blob - a string or a BlobReference
","def FetchInt8Blob(name):
""""""Fetches an Int8 blob from the workspace. It shared backend implementation
    with FetchBlob but it is recommended when fetching Int8 Blobs
Inputs:
name: the name of the Int8 blob - a string or a BlobReference
"
1170,"return;
}
if (lda == N && ldb == N) {
    // can coalese to a single memcpy of size M * N
if (copy) {
copy(static_cast<const char*>(A), static_cast<char*>(B), N * M);
} else {
","return;
}
if (lda == N && ldb == N) {
    // can coalesce to a single memcpy of size M * N
if (copy) {
copy(static_cast<const char*>(A), static_cast<char*>(B), N * M);
} else {
"
1171,"def unpack_variable(name, unpack_expr, typename):
# optional<ArrayRef<T>> are special. The PythonArgParser returns an
            # optional<vector<T>>, which cannot be implictly converted to
# optional<ArrayRef<T>>. One needs to unwrap the optional and rewrap.
if typename == 'c10::optional<DimnameList>':
result = """"""\
","def unpack_variable(name, unpack_expr, typename):
# optional<ArrayRef<T>> are special. The PythonArgParser returns an
            # optional<vector<T>>, which cannot be implicitly converted to
# optional<ArrayRef<T>>. One needs to unwrap the optional and rewrap.
if typename == 'c10::optional<DimnameList>':
result = """"""\
"
1172,"if(state == ProfilerState::CUDA) {
// event recording appears to have some startup overhead, so we need to
    // to generate some dummy events first before recording syncrhonization events
for(int i = 0; i < 5; i++) {
cuda_stubs->onEachDevice([](int d) {
mark(""__cuda_startup"");
","if(state == ProfilerState::CUDA) {
// event recording appears to have some startup overhead, so we need to
    // to generate some dummy events first before recording synchronization events
for(int i = 0; i < 5; i++) {
cuda_stubs->onEachDevice([](int d) {
mark(""__cuda_startup"");
"
1173,"continue;
} else {
if (assign.rhs().present()) {
                  // This is a constant assignemnt, of the form:
// foo : Final[int] = 3
constants.push_back(assign);
} else {
","continue;
} else {
if (assign.rhs().present()) {
                  // This is a constant assignment, of the form:
// foo : Final[int] = 3
constants.push_back(assign);
} else {
"
1174,"// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisted and redesigned if we want to
// suppot backward compatibility in future.
namespace torch {
namespace jit {
","// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisted and redesigned if we want to
// support backward compatibility in future.
namespace torch {
namespace jit {
"
1175,"std::mutex lock;
OperatorMap operators;
// list of operators whose schema have not yet been parsed, and must
  // be registered before any call to lookup an opeator
std::vector<std::shared_ptr<Operator>> to_register;
// Those two maps are used to implement lookupByLiteral, which is needed for
// the n->match(...) calls. Basically, every function schema is assigned a
","std::mutex lock;
OperatorMap operators;
// list of operators whose schema have not yet been parsed, and must
  // be registered before any call to lookup an operator
std::vector<std::shared_ptr<Operator>> to_register;
// Those two maps are used to implement lookupByLiteral, which is needed for
// the n->match(...) calls. Basically, every function schema is assigned a
"
1176,"const ExtraFilesMap& _extra_files = ExtraFilesMap()) {
Module module(""__torch__.PlaceholderModule"");
// [issue 27343]
            // Modules have 'training' attributes by defualt, but due to
// https://github.com/pytorch/pytorch/issues/27343, functions end
// up having a training attribute when they are loaded. This adds
// a fake 'training' attribute that shouldn't be used, but prevents
","const ExtraFilesMap& _extra_files = ExtraFilesMap()) {
Module module(""__torch__.PlaceholderModule"");
// [issue 27343]
            // Modules have 'training' attributes by default, but due to
// https://github.com/pytorch/pytorch/issues/27343, functions end
// up having a training attribute when they are loaded. This adds
// a fake 'training' attribute that shouldn't be used, but prevents
"
1177,"# the old ones alives.
# See [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html]
#
    # This is fine, because all we need to do is to save our position in the allocaiton,
# and reconstruct storage and tensor from it.
# 0xA000 ->  -------CUDA Allocation------
#           |                            |
","# the old ones alives.
# See [https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html]
#
    # This is fine, because all we need to do is to save our position in the allocation,
# and reconstruct storage and tensor from it.
# 0xA000 ->  -------CUDA Allocation------
#           |                            |
"
1178,"have some subtle differences. :class:`InstanceNorm1d` is applied
on each channel of channeled data like multidimensional time series, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionaly, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm1d` usually don't apply affine
transform.
","have some subtle differences. :class:`InstanceNorm1d` is applied
on each channel of channeled data like multidimensional time series, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm1d` usually don't apply affine
transform.
"
1179,"if cuda:
cuda_home_include = _join_cuda_home('include')
# if we have the Debian/Ubuntu packages for cuda, we get /usr as cuda home.
        # but gcc dosn't like having /usr/include passed explicitly
if cuda_home_include != '/usr/include':
paths.append(cuda_home_include)
if CUDNN_HOME is not None:
","if cuda:
cuda_home_include = _join_cuda_home('include')
# if we have the Debian/Ubuntu packages for cuda, we get /usr as cuda home.
        # but gcc doesn't like having /usr/include passed explicitly
if cuda_home_include != '/usr/include':
paths.append(cuda_home_include)
if CUDNN_HOME is not None:
"
1180,"#include <ATen/ATen.h>
#include <ATen/quantized/Quantizer.h>
#include <c10/core/Allocator.h>
#include <ATen/Dispatch.h>
#include <ATen/NativeFunctions.h>
#include <ATen/native/TensorFactories.h>
","#include <ATen/ATen.h>
#include <ATen/quantized/Quantizer.h>
#include <c10/core/Allocator.h>
#include <c10/core/CPUAllocator.h>
#include <ATen/Dispatch.h>
#include <ATen/NativeFunctions.h>
#include <ATen/native/TensorFactories.h>
"
1181,"MemoryFormat memory_format=MemoryFormat::Contiguous) {
AT_ASSERT(options.device().is_cpu());
native::check_size_nonnegative(sizes);
  auto* allocator = at::getCPUAllocator();
int64_t nelements = at::prod_intlist(sizes);
auto dtype = options.dtype();
TORCH_CHECK(isQIntType(typeMetaToScalarType(dtype)),
","MemoryFormat memory_format=MemoryFormat::Contiguous) {
AT_ASSERT(options.device().is_cpu());
  at::Allocator* allocator = at::getCPUAllocator();

#ifdef USE_PYTORCH_QNNPACK
  if (at::globalContext().qEngine() == at::QEngine::QNNPACK) {
    static QAllocator qallocator;
    allocator = &qallocator;
  }
#endif

native::check_size_nonnegative(sizes);
int64_t nelements = at::prod_intlist(sizes);
auto dtype = options.dtype();
TORCH_CHECK(isQIntType(typeMetaToScalarType(dtype)),
"
1182,"self.activation_post_process = observer(**observer_kwargs)
assert torch.iinfo(self.activation_post_process.dtype).min <= quant_min, 'quant_min out of bound'
assert quant_max <= torch.iinfo(self.activation_post_process.dtype).max, 'quant_max out of bound'
        self.scale = None
        self.zero_point = None
self.dtype = self.activation_post_process.dtype
self.qscheme = self.activation_post_process.qscheme
self.ch_axis = self.activation_post_process.ch_axis if hasattr(self.activation_post_process, 'ch_axis') else None
","self.activation_post_process = observer(**observer_kwargs)
assert torch.iinfo(self.activation_post_process.dtype).min <= quant_min, 'quant_min out of bound'
assert quant_max <= torch.iinfo(self.activation_post_process.dtype).max, 'quant_max out of bound'
        self.scale = torch.tensor([1.0])
        self.zero_point = torch.tensor([0])
self.dtype = self.activation_post_process.dtype
self.qscheme = self.activation_post_process.qscheme
self.ch_axis = self.activation_post_process.ch_axis if hasattr(self.activation_post_process, 'ch_axis') else None
"
1183,"assert not (axis == 0 and add_axis == 1), \
""It's not allowed to add axis=0""
assert isinstance(input_record, schema.Struct),\
            ""Incorrect input type. Excpected Struct, but received: {0}"".\
format(input_record)
shapes = []
for field_name, field_type in viewitems(input_record.fields):
assert isinstance(field_type, schema.Scalar),\
                ""Incorrect input type for {}. Excpected Scalar, but got: {}"".\
format(field_name, field_type)
# Assume that first dimension is batch, so actual axis in shape is
# axis - 1
","assert not (axis == 0 and add_axis == 1), \
""It's not allowed to add axis=0""
assert isinstance(input_record, schema.Struct),\
            ""Incorrect input type. Expected Struct, but received: {0}"".\
format(input_record)
shapes = []
for field_name, field_type in viewitems(input_record.fields):
assert isinstance(field_type, schema.Scalar),\
                ""Incorrect input type for {}. Expected Scalar, but got: {}"".\
format(field_name, field_type)
# Assume that first dimension is batch, so actual axis in shape is
# axis - 1
"
1184,"auto one = copy_graph_->insertConstant({1});
updated_max_trip_count =
copy_graph_->insert(aten::sub, {updated_max_trip_count, one});
    TORCH_INTERNAL_ASSERT(old_to_new_.count(outer_node->inputs()[0]) != 0);
auto cur_plus_one = copy_graph_->insert(aten::add, {one, cur_iter});
// We need to be careful when mapping `block_outputs` to continuation
","auto one = copy_graph_->insertConstant({1});
updated_max_trip_count =
copy_graph_->insert(aten::sub, {updated_max_trip_count, one});
auto cur_plus_one = copy_graph_->insert(aten::add, {one, cur_iter});
// We need to be careful when mapping `block_outputs` to continuation
"
1185,"// ""{LIVE_IN} or {GEN}"" or ""{LIVE_OUT} - {KILL}""
struct LivenessAnalyzer {
explicit LivenessAnalyzer(std::shared_ptr<Graph> graph)
      : graph_(std::move(graph)) {}
std::unordered_map<Node*, std::vector<Value*>> run() {
    processBlock(graph_->block(), SparseBitVector{});
std::unordered_map<Node*, std::vector<Value*>> result;
for (const auto& e : liveness_sets_) {
","// ""{LIVE_IN} or {GEN}"" or ""{LIVE_OUT} - {KILL}""
struct LivenessAnalyzer {
explicit LivenessAnalyzer(std::shared_ptr<Graph> graph)
      : graph_(std::move(graph)), changed_(false) {}
std::unordered_map<Node*, std::vector<Value*>> run() {
    // we implement the canonical fixed-point liveness
    // the analysis is run until there are no more changes
    // to liveness sets for each node
    do {
      changed_ = false;
      processBlock(graph_->block(), SparseBitVector{});
    } while (changed_);

std::unordered_map<Node*, std::vector<Value*>> result;
for (const auto& e : liveness_sets_) {
"
1186,"r""""""Pass the input through the encoder layer.
Args:
            src: the sequnce to the encoder layer (required).
src_mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).
","r""""""Pass the input through the encoder layer.
Args:
            src: the sequence to the encoder layer (required).
src_mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).
"
1187,"W = I.shape[3]
ncols = min(nimg, ncols)
nrows = int(np.ceil(float(nimg) / ncols))
    canvas = np.zeros((3, H * nrows, W * ncols))
i = 0
for y in range(nrows):
for x in range(ncols):
","W = I.shape[3]
ncols = min(nimg, ncols)
nrows = int(np.ceil(float(nimg) / ncols))
    canvas = np.zeros((3, H * nrows, W * ncols), dtype=I.dtype)
i = 0
for y in range(nrows):
for x in range(ncols):
"
1188,"}
Tensor cumsum_backward(const Tensor & x, int64_t dim) {
  if (x.dim() == 0) {
return x;
}
auto ret = at::cumsum(-x, dim);
","}
Tensor cumsum_backward(const Tensor & x, int64_t dim) {
  // Need to check numel to see if there are no values (such as shape [0,2], and dim to see if x is a scalar.
  if (x.dim() == 0 || x.numel() == 0) {
return x;
}
auto ret = at::cumsum(-x, dim);
"
1189,"std::unordered_map<std::string, std::string> RRefContext::getDebugInfo() {
std::unordered_map<std::string, std::string> info;
  std::lock_guard<std::mutex> lock(mutex_);
  info[""num_owner_rrefs""] = c10::to_string(owners_.size());
return info;
}
","std::unordered_map<std::string, std::string> RRefContext::getDebugInfo() {
std::unordered_map<std::string, std::string> info;
  std::unique_lock<std::mutex> lock(mutex_);
  auto ownerSize = owners_.size();
  auto numPendingUsers = pendingUsers_.size();
  lock.unlock();
  info[kNumOwnerRRefs] = c10::to_string(ownerSize);
  info[kNumPendingUsers] = c10::to_string(numPendingUsers);
return info;
}
"
