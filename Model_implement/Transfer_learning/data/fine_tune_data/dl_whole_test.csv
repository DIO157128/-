,source,target
0,".def(
""define_constant"",
[](nvfuser::FusionDefinition& self,
             std::complex<double> val) -> nvfuser::Scalar* {
nvfuser::Scalar* out = self.defineScalar();
self.defineRecord(new nvfuser::ConstantRecord<
torch::jit::fuser::cuda::ComplexDouble,
                              c10::complex<double>>(
                {out->index}, static_cast<c10::complex<double>>(val)));
return out;
},
py::return_value_policy::reference)
",".def(
""define_constant"",
[](nvfuser::FusionDefinition& self,
             c10::complex<double> val) -> nvfuser::Scalar* {
nvfuser::Scalar* out = self.defineScalar();
self.defineRecord(new nvfuser::ConstantRecord<
torch::jit::fuser::cuda::ComplexDouble,
                              c10::complex<double>>({out->index}, val));
return out;
},
py::return_value_policy::reference)
"
1,"with preserve_rng_state(), fake_mode as mode:
def process_inputs(flat_args):
            flat_args = pytree.tree_map(
                lambda x: x.detach().requires_grad_(x.requires_grad)
                if isinstance(x, Tensor)
                else x,
                flat_args,
            )
            fake_flat_tensor_args = pytree.tree_map(
                lambda x: mode.from_tensor(x)
                if mode
                else x
                if isinstance(x, Tensor)
                else x,
                flat_args,
            )
return fake_flat_tensor_args
fake_flat_tensor_args = process_inputs(flat_args)
","with preserve_rng_state(), fake_mode as mode:
def process_inputs(flat_args):
            if mode:
                fake_flat_tensor_args = pytree.tree_map_only(
                    Tensor, mode.from_tensor, flat_args
                )
            else:
                # The detach().requires_grad_() pattern can cause some subtle bugs.
                # These will be fixed once FakeTensor is always-on for AOTAutograd.
                #
                # For models that might resize their inputs, the input tensors
                # must have allow_tensor_metadata_change() set to true.
                # detach() returns a view tensor, but with that field set to false.
                #
                # Specifically, this breaks quantized models
                # (resnet50_quantized_qat and mobilenet_v2_quantized_qat)
                # because they use a ""running-mean"" style op that requires
                # resizing the running counter buffers stored on the module.
                fake_flat_tensor_args = pytree.tree_map_only(
                    Tensor,
                    lambda x: x.detach().requires_grad_(x.requires_grad),
                    flat_args,
                )
return fake_flat_tensor_args
fake_flat_tensor_args = process_inputs(flat_args)
"
2,"cache_.emplace(cacheKey, compileFn);
}
  const int64_t size() const { return cache_.size(); }
/// Clear the cache.
void clear() { cache_.clear(); }
","cache_.emplace(cacheKey, compileFn);
}
  int64_t size() const { return cache_.size(); }
/// Clear the cache.
void clear() { cache_.clear(); }
"
3,"_onnx_unsupported(f""{op}, {msg}"")
def _onnx_unsupported(op_name):
raise RuntimeError(
f""Unsupported: ONNX export of operator {op_name}. ""
""Please feel free to request support or submit a pull request on PyTorch GitHub.""
)
def _onnx_opset_unsupported(op_name, current_opset, supported_opset):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in opset {current_opset}. ""
f""Please try opset version {supported_opset}.""
)
def _onnx_opset_unsupported_detailed(op_name, current_opset, supported_opset, reason):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in ""
f""opset {current_opset}. {reason}. Please try opset version {supported_opset}.""
)
def _block_list_in_opset(name):
def symbolic_fn(*args, **kwargs):
raise RuntimeError(
f""ONNX export failed on {name}, which is not implemented for opset ""
","_onnx_unsupported(f""{op}, {msg}"")
def _onnx_unsupported(op_name: str):
raise RuntimeError(
f""Unsupported: ONNX export of operator {op_name}. ""
""Please feel free to request support or submit a pull request on PyTorch GitHub.""
)
def _onnx_opset_unsupported(op_name: str, current_opset: int, supported_opset: int):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in opset {current_opset}. ""
f""Please try opset version {supported_opset}.""
)
def _onnx_opset_unsupported_detailed(
    op_name: str, current_opset: int, supported_opset: int, reason: str
):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in ""
f""opset {current_opset}. {reason}. Please try opset version {supported_opset}.""
)
def _block_list_in_opset(name: str):
def symbolic_fn(*args, **kwargs):
raise RuntimeError(
f""ONNX export failed on {name}, which is not implemented for opset ""
"
4,"dim_desc = ""is"" if allow_multi_dim_support else ""i""
        @symbolic_helper.parse_args(""v"", dim_desc, ""i"", ""none"")
def reduce_dim(g, self, dim, keepdim, dtype):
if dtype.node().kind() == ""onnx::Constant"":
dtype = symbolic_helper._get_const(dtype, ""i"", ""dtype"")
","dim_desc = ""is"" if allow_multi_dim_support else ""i""
        @symbolic_helper.parse_args(""v"", dim_desc, ""i"", ""none"")  # type: ignore[arg-type]
def reduce_dim(g, self, dim, keepdim, dtype):
if dtype.node().kind() == ""onnx::Constant"":
dtype = symbolic_helper._get_const(dtype, ""i"", ""dtype"")
"
5,"new_shape = g.op(""Concat"", g.op(""Shape"", boundaries), g.op(""Shape"", self), axis_i=0)
# Unsqueeze step is performed to respect ONNX's numpy style broadcasting for comparison ops
# https://github.com/onnx/onnx/blob/main/docs/Broadcasting.md
    unsqueeze_axes = list(range(1, symbolic_helper._get_tensor_rank(self) + 1))
expanded_boundaries = expand(
g,
symbolic_helper._unsqueeze_helper(g, boundaries, unsqueeze_axes),
","new_shape = g.op(""Concat"", g.op(""Shape"", boundaries), g.op(""Shape"", self), axis_i=0)
# Unsqueeze step is performed to respect ONNX's numpy style broadcasting for comparison ops
# https://github.com/onnx/onnx/blob/main/docs/Broadcasting.md
    tensor_rank = symbolic_helper._get_tensor_rank(self)
    assert tensor_rank is not None
    unsqueeze_axes = list(range(1, tensor_rank + 1))
expanded_boundaries = expand(
g,
symbolic_helper._unsqueeze_helper(g, boundaries, unsqueeze_axes),
"
6,"const T* X_ptr = X_data + i * inner_size;
T mean_val;
T rstd_val;
      std::tie(mean_val, rstd_val) = utils::RowwiseMoments(X_ptr, inner_size);
rstd_val = T(1) / std::sqrt(std::max(rstd_val, T(0)) + eps);
if (gamma_null && beta_null) {
T* Y_ptr = Y_data + i * inner_size;
","const T* X_ptr = X_data + i * inner_size;
T mean_val;
T rstd_val;
      std::tie(mean_val, rstd_val) = RowwiseMoments(X_ptr, inner_size);
rstd_val = T(1) / std::sqrt(std::max(rstd_val, T(0)) + eps);
if (gamma_null && beta_null) {
T* Y_ptr = Y_data + i * inner_size;
"
7,"T* Y_ptr = Y_data + i * N;
T mean_val;
T rstd_val;
      std::tie(mean_val, rstd_val) = utils::RowwiseMoments(X_ptr, N);
rstd_val = T(1) / std::sqrt(rstd_val + eps);
const T_ACC scale = rstd_val;
const T_ACC bias = -rstd_val * mean_val;
","T* Y_ptr = Y_data + i * N;
T mean_val;
T rstd_val;
      std::tie(mean_val, rstd_val) = RowwiseMoments(X_ptr, N);
rstd_val = T(1) / std::sqrt(rstd_val + eps);
const T_ACC scale = rstd_val;
const T_ACC bias = -rstd_val * mean_val;
"
8,"return self.squeeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
  TORCH_CHECK(batched && batched->bdim() == 0);
auto logical_dim = self.dim();
  auto dim_physical = 1 + maybe_wrap_dim(dim, logical_dim);
  batched->value().squeeze_(dim_physical);
  // Also need to change some metadata...
batched->refreshTensorMetadata();
return self;
}
","return self.squeeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
  const auto bdim = batched->bdim();
auto logical_dim = self.dim();
  // If logically a scalar tensor, then Tensor.squeeze_(dim) is a no-op
  if (logical_dim == 0) {
    return self;
  }

  dim = maybe_wrap_dim(dim, logical_dim);
  if (dim >= bdim) {
    dim = dim + 1;
    batched->value().squeeze_(dim);
    batched->refreshTensorMetadata();
    return self;
  }

  // Tensor.squeeze_(0) is a no-op if dim 0 has a size other than 1
  if (batched->value().size(dim) != 1) {
    return self;
  }

  // dim < bdim, so we need to adjust bdim
  batched->value().squeeze_(dim);
  batched->unsafe_set_bdim(bdim - 1);
  batched->refreshTensorMetadata();
  return self;
}

Tensor& squeeze__batching_rule(Tensor& self) {
  if (!participatesInCurrentLevel(self)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return self.squeeze_();
  }
  auto* batched = maybeGetBatchedImpl(self);

  // Need to find out how many dimensions of size 1 are before the bdim
  const auto bdim = batched->bdim();
  const auto physical_shape = batched->value().sizes();
  auto how_many_dims_of_size_1_before_bdim = 0;
  for (const auto i : c10::irange(0, physical_shape.size())) {
    if ((int64_t)i == bdim) {
      break;
    }
    if (physical_shape[i] == 1) {
      how_many_dims_of_size_1_before_bdim++;
    }
  }

  int64_t new_bdim = bdim - how_many_dims_of_size_1_before_bdim;
  if (physical_shape[bdim] != 1) {
    // if bdim is not 1, can just call squeeze_()
    batched->value().squeeze_();
  } else {
    // otherwise, squeeze_() is going to get rid of the bdim too.
    // We ""fix it up"" by calling unsqueeze_.
    batched->value().squeeze_();
    batched->value().unsqueeze(new_bdim);
  }

  // Refresh metadata
  batched->unsafe_set_bdim(new_bdim);
batched->refreshTensorMetadata();
return self;
}
"
9,"const auto key = getKeyFromDevices(devices);
auto& ncclComms = getNCCLComm(key, devices, opType);
// Used many times below, so we stash the unordered_map lookup
auto& ncclStreams = ncclStreams_[key];
","const auto key = getKeyFromDevices(devices);
auto& ncclComms = getNCCLComm(key, devices, opType);
  if (coalescing_active_) {
    coalescedDevices_.push_back(devices);
  }

// Used many times below, so we stash the unordered_map lookup
auto& ncclStreams = ncclStreams_[key];
"
10,"""to be of type ``torch.distributed.P2POp``.""
)
    backend = get_backend(p2p_op_list[0].group)
    if not all(backend == get_backend(p2p_op.group) for p2p_op in p2p_op_list):
        raise RuntimeError(""All groups need to use the same backend."")
def is_mpi_available():
","""to be of type ``torch.distributed.P2POp``.""
)
    group = p2p_op_list[0].group
    if not all(group == p2p_op.group for p2p_op in p2p_op_list):
        raise RuntimeError(""All ops need to use the same group."")
def is_mpi_available():
"
11,"return self.unsqueeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
  TORCH_CHECK(batched && batched->bdim() == 0);
auto logical_dim = self.dim();
  auto dim_physical = 1 + maybe_wrap_dim(dim, logical_dim + 1);
batched->value().unsqueeze_(dim_physical);
// Also need to change some metadata...
","return self.unsqueeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
auto logical_dim = self.dim();
  int64_t dim_physical = maybe_wrap_dim(dim, logical_dim + 1);
  if (dim_physical >= batched->bdim()) {
    dim_physical = 1 + dim_physical;
  } else {
    batched->unsafe_set_bdim(batched->bdim() + 1);
  }
batched->value().unsqueeze_(dim_physical);
// Also need to change some metadata...
"
12,"int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Helper to convert 1d tensors or scalar tensor to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1), strides(input_ndim, 0);
  auto as_nd = [&](const Tensor& t) {
    TORCH_INTERNAL_ASSERT(t.defined() && (t.dim() == 1 || t.dim() == 0));
    sizes[1] = t.dim() == 1 ? t.sizes()[0] : 1;
    strides[1] = t.dim() == 1 ? t.strides()[0] : 0;
    return t.as_strided(sizes, strides);
  };

  auto qw_nd = as_nd(qw);
auto iter = TensorIteratorConfig()
.add_output(out)
","int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Weight should be a 1d or scalar tensor
  // Reshape it to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1);
  if (input_ndim > 1) {
    sizes[1] = qw.numel();
  }
  auto qw_nd = qw.reshape(sizes);
auto iter = TensorIteratorConfig()
.add_output(out)
"
13,"int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Helper to convert 1d tensors or scalar tensor to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1), strides(input_ndim, 0);
  auto as_nd = [&](const Tensor& t) {
    TORCH_INTERNAL_ASSERT(t.defined() && (t.dim() == 1 || t.dim() == 0));
    sizes[1] = t.dim() == 1 ? t.sizes()[0] : 1;
    strides[1] = t.dim() == 1 ? t.strides()[0] : 0;
    return t.as_strided(sizes, strides);
  };

  auto qw_nd = as_nd(qw);
auto iter = TensorIteratorConfig()
.add_output(out)
","int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Weight should be a 1d or scalar tensor
  // Reshape it to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1);
  if (input_ndim > 1) {
    sizes[1] = qw.numel();
  }
  auto qw_nd = qw.reshape(sizes);
auto iter = TensorIteratorConfig()
.add_output(out)
"
14,"from typing import Dict
class CheckpointException(BaseException):
""""""
Exception raised if failure was detected as part of a checkpoint load or save.
""""""
    def __init__(self, msg: str, failures: Dict[int, BaseException]):
super().__init__(msg, failures)
self._failures = failures
@property
    def failures(self) -> Dict[int, BaseException]:
""""""
Returns:
Dict of failed nodes and their associated exception.
Keys are node ranks and values are exceptions
""""""
return self._failures
","from typing import Dict, Tuple, Any
import traceback as tb

WRAPPED_EXCEPTION = Tuple[BaseException, tb.StackSummary]

def _wrap_exception(exc: BaseException) -> WRAPPED_EXCEPTION:
    return (exc, tb.extract_tb(exc.__traceback__))

def _is_wrapped_exception(obj: Any) -> bool:
    if not isinstance(obj, tuple):
        return False
    if len(obj) != 2:
        return False
    return isinstance(obj[0], BaseException) and isinstance(obj[1], tb.StackSummary)
class CheckpointException(BaseException):
""""""
Exception raised if failure was detected as part of a checkpoint load or save.
""""""
    def __init__(self, msg: str, failures: Dict[int, WRAPPED_EXCEPTION]):
super().__init__(msg, failures)
self._failures = failures
@property
    def failures(self) -> Dict[int, WRAPPED_EXCEPTION]:
""""""
Returns:
Dict of failed nodes and their associated exception.
Keys are node ranks and values are exceptions
""""""
return self._failures

    def __str__(self):
        str = f""CheckpointException ranks:{self._failures.keys()}\n""
        for rank, exc_pair in self._failures.items():
            exc, trace = exc_pair
            str += f""Traceback (most recent call last): (RANK {rank})\n""
            if trace is not None:
                str += """".join(tb.format_list(trace))
            str += """".join(tb.format_exception_only(type(exc), value=exc))
        return str
"
15,"}
}
}}
","}
}
static void handleScalarTypePromotion(Tensor& logical_scalar_tensor, Tensor& second) {
  auto result_type = at::native::result_type(logical_scalar_tensor[0], second);
  if (logical_scalar_tensor.scalar_type() != result_type) {
    logical_scalar_tensor = logical_scalar_tensor.to(result_type);
  }
  if (second.scalar_type() != result_type) {
    second = second.to(result_type);
  }
}

std::tuple<Tensor, Tensor> _binary_pointwise_helper(
    const Tensor& tensor, optional<int64_t> tensor_batch_dim,
    const Tensor& other, optional<int64_t> other_batch_dim,
    bool do_type_promotion) {
  // compute max logical rank
  auto tensor_logical_rank = rankWithoutBatchDim(tensor, tensor_batch_dim);
  auto other_logical_rank = rankWithoutBatchDim(other, other_batch_dim);
  auto max_logical_rank = std::max(tensor_logical_rank, other_logical_rank);

  auto tensor_ = moveBatchDimToFront(tensor, tensor_batch_dim);
  auto other_ = moveBatchDimToFront(other, other_batch_dim);

  // In the (0D, ND) case, type promotion semantics are different :/
  if (do_type_promotion) {
    auto tensor_is_logical_scalar = (tensor_logical_rank == 0 && tensor_batch_dim.has_value());
    auto other_is_logical_scalar = (other_logical_rank == 0 && other_batch_dim.has_value());
    if (tensor_is_logical_scalar && !other_is_logical_scalar) {
      handleScalarTypePromotion(tensor_, other_);
    }
    if (other_is_logical_scalar && !tensor_is_logical_scalar) {
      handleScalarTypePromotion(other_, tensor_);
    }
  }

  // If the dimensions aren't aligned, we need to line them up.
  // Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]
  // Note that only tensors that have a batch dim need to be modified.
  // Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed
  tensor_ = maybePadToLogicalRank(tensor_, tensor_batch_dim, max_logical_rank);
  other_ = maybePadToLogicalRank(other_, other_batch_dim, max_logical_rank);

  return std::make_tuple(tensor_, other_);
}

}}
"
16,"VMAP_SUPPORT(mm, mm_batch_rule);
m.impl(""linear"", linear_decomp);
VMAP_SUPPORT(linalg_householder_product, householder_product_batch_rule);
VMAP_SUPPORT(_linalg_check_errors, _linalg_check_errors_batch_rule);

  VARIADIC_BDIMS_BOXED(cholesky_solve);
  VARIADIC_BDIMS_BOXED(linalg_cholesky_ex);
  VARIADIC_BDIMS_BOXED(linalg_eig);
  VARIADIC_BDIMS_BOXED(linalg_eigh);
  VARIADIC_BDIMS_BOXED(linalg_inv_ex);
  VARIADIC_BDIMS(linalg_pinv);
  VARIADIC_BDIMS_BOXED(linalg_qr);
  VARIADIC_BDIMS_BOXED(linalg_slogdet);

  VARIADIC_BDIMS(cholesky);
  VARIADIC_BDIMS(cholesky_inverse);
  VARIADIC_BDIMS_BOXED(geqrf);
  VARIADIC_BDIMS(logdet);
  VARIADIC_BDIMS(matrix_exp);
  VARIADIC_BDIMS(pinverse);
  VARIADIC_BDIMS(inverse);
  VARIADIC_BDIMS_BOXED(slogdet);
  VARIADIC_BDIMS_BOXED(_linalg_svd);
  VARIADIC_BDIMS_BOXED(solve);
  VARIADIC_BDIMS_BOXED(symeig);
  VARIADIC_BDIMS_BOXED(triangular_solve);

  VARIADIC_BDIMS_BOXED(_linalg_det);
  VARIADIC_BDIMS_BOXED(_lu_with_info);
}
}}
","VMAP_SUPPORT(mm, mm_batch_rule);
m.impl(""linear"", linear_decomp);
VMAP_SUPPORT(linalg_householder_product, householder_product_batch_rule);
  VMAP_SUPPORT(cholesky_solve, cholesky_solve_batch_rule);  // custom dim error
  VMAP_SUPPORT(linalg_lu_factor_ex, linalg_lu_factor_ex_batch_rule);
  VMAP_SUPPORT(linalg_matrix_exp, matrix_exp_batch_rule);
VMAP_SUPPORT(_linalg_check_errors, _linalg_check_errors_batch_rule);
}
}}
"
17,"OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
  OP_DECOMPOSE(inverse);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
","OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
"
18,"OP_DECOMPOSE2(not_equal, Tensor );
OP_DECOMPOSE(outer);
OP_DECOMPOSE(pairwise_distance);
  OP_DECOMPOSE(pinverse);
OP_DECOMPOSE(poisson_nll_loss);
OP_DECOMPOSE(qr);
OP_DECOMPOSE(ravel);
","OP_DECOMPOSE2(not_equal, Tensor );
OP_DECOMPOSE(outer);
OP_DECOMPOSE(pairwise_distance);
OP_DECOMPOSE(poisson_nll_loss);
OP_DECOMPOSE(qr);
OP_DECOMPOSE(ravel);
"
19,"namespace at { namespace functorch {
// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
","namespace at { namespace functorch {
typedef std::tuple<Tensor, optional<int64_t>> oneOutput;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>> twoOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> threeOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> fourOutputs;

// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
"
20,"}
Tensor expand_symint_batching_rule(const Tensor& self, SymIntArrayRef psize, bool implicit) {
  return expand_batching_rule(self, asIntArrayRefSlow(psize), implicit);
}
Tensor sum_symint_batching_rule(const Tensor& input_t, c10::SymIntArrayRef dim, bool keepdim, optional<ScalarType> opt_dtype) {
  return sum_batching_rule(input_t, c10::asIntArrayRefSlow(dim), keepdim, opt_dtype);
}
std::vector<Tensor> chunk_batching_rule(const Tensor& self, int64_t chunks, int64_t dim) {
","}
Tensor expand_symint_batching_rule(const Tensor& self, SymIntArrayRef psize, bool implicit) {
  return self.expand(asIntArrayRefSlow(psize), implicit);
}
Tensor sum_symint_batching_rule(const Tensor& input_t, c10::SymIntArrayRef dim, bool keepdim, optional<ScalarType> opt_dtype) {
  return input_t.sum(c10::asIntArrayRefSlow(dim), keepdim, opt_dtype);
}
std::vector<Tensor> chunk_batching_rule(const Tensor& self, int64_t chunks, int64_t dim) {
"
21,"working as expected with ``torch.autograd.grad`` and support for
keyword arguments input into the checkpointed function. Note that future
versions of PyTorch will default to ``use_reentrant=False``.
args: tuple containing inputs to the :attr:`function`
Returns:
","working as expected with ``torch.autograd.grad`` and support for
keyword arguments input into the checkpointed function. Note that future
versions of PyTorch will default to ``use_reentrant=False``.
            Default: ``True``
args: tuple containing inputs to the :attr:`function`
Returns:
"
22,"true);
auto grad_output = grad_output_.contiguous(memory_format);
  Tensor finput = compute_columns2d(input, padding, stride, kernel_size);
const int64_t batch_size = input.size(0);
","true);
auto grad_output = grad_output_.contiguous(memory_format);
  Tensor finput = compute_columns2d(input, padding, stride, kernel_size, use_channels_last);
const int64_t batch_size = input.size(0);
"
23,"unwrapped_arg_list += [f""{arg}_value"", f""{arg}_bdim""]
else:
unwrapped_arg_list.append(arg)
    return unwraps, unwrapped_arg_list
def get_aten_op_call(schema) -> str:
    if schema.name.overload_name:
        return f""ATEN_FN2({schema.name.name}, {schema.name.overload_name})""
    return f""ATEN_FN({schema.name.name})""


def gen_case_where_all_bdims_are_none(schema, cur_level_var) -> str:
conditions = []
flat_args = schema.arguments.flat_all
for arg in flat_args:
","unwrapped_arg_list += [f""{arg}_value"", f""{arg}_bdim""]
else:
unwrapped_arg_list.append(arg)
    return unwrap_code, unwrapped_arg_list
def gen_case_where_all_bdims_are_none(
    schema: FunctionSchema, cur_level_var: str
) -> str:
conditions = []
flat_args = schema.arguments.flat_all
for arg in flat_args:
"
24,"vTensor v_output{
context,
      broadcast_first_input(v_self, v_other) ? v_other.sizes() : v_self.sizes(),
self.options().dtype(c10::kQUInt8),
scale,
zero_point};
","vTensor v_output{
context,
      broadcast_size(self_arg, other_arg),
self.options().dtype(c10::kQUInt8),
scale,
zero_point};
"
25,"def is_symbolic_op(func):
    return func in [aten.sym_size.default, aten.dim.default, aten.is_contiguous.default, aten.stride]
def handle_symbolic_op(func, args, kwargs):
","def is_symbolic_op(func):
    return func in [aten.sym_size.default, aten.dim.default, aten.is_contiguous.default, aten.stride.default]
def handle_symbolic_op(func, args, kwargs):
"
26,"[](SchemaInfo& self,
const std::string& name,
const py::object& value) {
// For normalization purposes there is an inconsistency within
// torch.fx that turns all arguments named ""self"" into ""input"". Thus
// this check ensures that those arguments are checked correctly.
","[](SchemaInfo& self,
const std::string& name,
const py::object& value) {
            if (isEmptyContainer(value)) {
              return;
            }
// For normalization purposes there is an inconsistency within
// torch.fx that turns all arguments named ""self"" into ""input"". Thus
// this check ensures that those arguments are checked correctly.
"
27,"# TODO: might need to update according to supported input types
""torch.ops.aten.add"": None,
""torch.ops.aten.sub"": None,
            ""torch.ops.aten.rsub"": None,
""torch.ops.aten.div"": None,
""torch.ops.aten.atan2"": None,
""torch.ops.aten.mul"": None,
","# TODO: might need to update according to supported input types
""torch.ops.aten.add"": None,
""torch.ops.aten.sub"": None,
            # ""torch.ops.aten.rsub"": None,    # rsub decomp is supported at aten2aten level
""torch.ops.aten.div"": None,
""torch.ops.aten.atan2"": None,
""torch.ops.aten.mul"": None,
"
28,"""torch.ops.aten.isneginf"": None,
""torch.ops.aten.isposinf"": None,
""torch.ops.aten.isreal"": None,
            ""torch.ops.aten.rand_like"": None,
""torch.ops.aten.softplus"": None,
""torch.ops.aten.threshold"": None,
# relying on aten->aten->prim decomp, aten2aten is using unsupported aten.new_zero op
","""torch.ops.aten.isneginf"": None,
""torch.ops.aten.isposinf"": None,
""torch.ops.aten.isreal"": None,
            # ""torch.ops.aten.rand_like"": None,  # causing Node empty_like_default does not support nvfuser
""torch.ops.aten.softplus"": None,
""torch.ops.aten.threshold"": None,
# relying on aten->aten->prim decomp, aten2aten is using unsupported aten.new_zero op
"
29,"# Overriding fused_module's __call__() function with lower_to_prims_and_execute()
for node in fused_graph_module.graph.nodes:
# TODO: use a better way to identify fused submodule
            if ""fused_"" in node.name:
fused_module = getattr(fused_graph_module, node.name)
fused_module._wrapped_call = self.lower_to_prims_and_execute
","# Overriding fused_module's __call__() function with lower_to_prims_and_execute()
for node in fused_graph_module.graph.nodes:
# TODO: use a better way to identify fused submodule
            if node.op == ""call_module"" and ""fused_"" in node.name:
fused_module = getattr(fused_graph_module, node.name)
fused_module._wrapped_call = self.lower_to_prims_and_execute
"
30,"at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
} // namespace utils
} // namespace torch
#else
","at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}

void warn_numpy_not_writeable() {
  throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
} // namespace utils
} // namespace torch
#else
"
31,"return std::make_tuple(at::diag_embed(self_, offset, dim1, dim2), 0);
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(diag, diag_batch_rule);
VMAP_SUPPORT(chunk, chunk_batching_rule);
","return std::make_tuple(at::diag_embed(self_, offset, dim1, dim2), 0);
}
// We need to write a real batching rule to fully support symint.
// This requires symint variants of other operations, like `view`,
// which don't exist yet.
Tensor expand_symint_decomp_hack(const Tensor& self, SymIntArrayRef packed_size, bool implicit) {
   auto size = asIntArrayRefSlow(packed_size);
   return self.expand(size, implicit);
}

TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(diag, diag_batch_rule);
VMAP_SUPPORT(chunk, chunk_batching_rule);
"
32,"static_argnums = list(static_argnums)
static_argnums.sort()
def returned_function(*args, **kwargs):
global compile_cache
nonlocal cached_res
","static_argnums = list(static_argnums)
static_argnums.sort()
    @wraps(fn)
def returned_function(*args, **kwargs):
global compile_cache
nonlocal cached_res
"
33,"FALLBACK_WITH_ID(nll_loss2d_backward);
FALLBACK_WITH_ID(mse_loss_backward);
FALLBACK_WITH_ID(l1_loss_backward);
  // FALLBACK_WITH_ID(_log_softmax_backward_data);
  // FALLBACK_WITH_ID(_softmax_backward_data);
}
","FALLBACK_WITH_ID(nll_loss2d_backward);
FALLBACK_WITH_ID(mse_loss_backward);
FALLBACK_WITH_ID(l1_loss_backward);
  FALLBACK_WITH_ID(_log_softmax_backward_data);
  FALLBACK_WITH_ID(_softmax_backward_data);
}
"
34,"with open('count_ops.txt', 'r') as f:
opinfo_counts = [i.strip() for i in f.readlines()]
opinfo_counts = defaultdict(int, {k: v for k, v in zip(opinfo_ops, opinfo_counts)})
        count_fn = lambda x: opinfo_counts[x['full_name']]
with open('run_decompositions.txt', 'r') as f:
decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]
gen_data([full_name_check(opinfo_ops), full_name_check(decomposed_ops), count_fn], 'decompositions.txt')
","with open('count_ops.txt', 'r') as f:
opinfo_counts = [i.strip() for i in f.readlines()]
opinfo_counts = defaultdict(int, {k: v for k, v in zip(opinfo_ops, opinfo_counts)})

        def count_fn(x):
            return opinfo_counts[x['full_name']]

with open('run_decompositions.txt', 'r') as f:
decomposed_ops = [remove_suffix(i.strip(), '.default') for i in f.readlines()]
gen_data([full_name_check(opinfo_ops), full_name_check(decomposed_ops), count_fn], 'decompositions.txt')
"
35,"SUM = 2
@register_decomposition(aten.tanh_backward)
def tanh_backward(out_grad: Tensor, y: Tensor):
return out_grad * (1 - y * y)
","SUM = 2
# This expands x until x.dim() == dim. Might be useful as an operator
def _unsqueeze_to_dim(x: Tensor, dim: int):
    for _ in range(dim - x.dim()):
        x = x.unsqueeze(-1)
    return x


@register_decomposition(aten.tanh_backward)
def tanh_backward(out_grad: Tensor, y: Tensor):
return out_grad * (1 - y * y)
"
36,"r = torch.Tensor._make_subclass(cls, elem, elem.requires_grad)
r.proxy = proxy
        if not elem.is_sparse:
proxy.node.meta['tensor_meta'] = _extract_tensor_metadata(r)
return r
","r = torch.Tensor._make_subclass(cls, elem, elem.requires_grad)
r.proxy = proxy
        if elem.is_sparse:
            proxy.node.meta['tensor_meta'] = {}
        else:
proxy.node.meta['tensor_meta'] = _extract_tensor_metadata(r)
return r
"
37,"return std::make_tuple(output, mask);
}
template <typename A, A a, typename C>
struct RandomBatchRuleHelper;
","return std::make_tuple(output, mask);
}
Tensor multinomial_batching_rule(const Tensor& self, const int64_t num_samples, const bool replacement, const c10::optional<Generator> generator) {
  c10::impl::ExcludeDispatchKeyGuard guard(kVmapModeKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  const auto cur_level = maybe_layer->layerId();

  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  self_value = moveBatchDimToFront(self_value, self_bdim);

  RandomnessType randomness = maybe_layer->randomness();
  check_randomness(randomness, self_bdim.has_value());

  if (randomness == RandomnessType::Different && !self_bdim) {
    auto shape = self_value.sizes();
    VmapDimVector shapeVec(1, maybe_layer->batchSize());
    shapeVec.reserve(shape.size() + 1);
    shapeVec.insert(shapeVec.end(), shape.begin(), shape.end());
    self_value = self_value.expand(shapeVec);
  }
  if (self_value.dim() == 3 && (self_bdim || randomness == RandomnessType::Different)) {
    self_value = reshape_dim_into(1, 0, self_value);
  }
  auto out = multinomial(self_value, num_samples, replacement, generator);
  if (randomness == RandomnessType::Same && !self_bdim) {
    return out;
  }
  if(self_value.dim() == 3 && self_bdim) {
    out = out.reshape(self.sizes());
  }
  return makeBatched(out, 0, cur_level);
}

template <typename A, A a, typename C>
struct RandomBatchRuleHelper;
"
38,"if (dynamicLayerStack.size() == 0) {
sanityCheckStack(op, stack);
c10::impl::ExcludeDispatchKeyGuard guard(all_dynlayer_keyset);
op.callBoxed(stack);
return;
}
","if (dynamicLayerStack.size() == 0) {
sanityCheckStack(op, stack);
c10::impl::ExcludeDispatchKeyGuard guard(all_dynlayer_keyset);
    auto local_keyset = c10::impl::tls_local_dispatch_key_set();
    local_keyset.excluded_ = local_keyset.excluded_.remove(DispatchKey::PythonTLSSnapshot);
    c10::impl::ForceDispatchKeyGuard guard2(local_keyset);
op.callBoxed(stack);
return;
}
"
39,"else:
d_bias = grad_out
else:
        d_bias = None
return (d_input, d_weight, d_bias)
# @register_decomposition(aten.addmm)
","else:
d_bias = grad_out
else:
        d_bias = aten.new_empty(input, (0,))
return (d_input, d_weight, d_bias)
# @register_decomposition(aten.addmm)
"
40,"import torch.nn as nn
from functools import partial
from typing import Callable, Iterable, Optional, Tuple, Union
from .aot_autograd import aot_function, aot_module
from .decompositions import decomposition_table
from .partitioners import draw_graph, min_cut_rematerialization_partition
import time
","import torch.nn as nn
from functools import partial
from typing import Callable, Iterable, Optional, Tuple, Union

from .aot_autograd import aot_function, aot_module
from .decompositions import get_decompositions
from .partitioners import draw_graph, min_cut_rematerialization_partition
import time
"
41,"#include <functorch/csrc/PlumbingHelper.h>
#include <functorch/csrc/Constants.h>
#include <functorch/csrc/DynamicLayer.h>

namespace at {
namespace functorch {
","#include <functorch/csrc/PlumbingHelper.h>
#include <functorch/csrc/Constants.h>
#include <functorch/csrc/DynamicLayer.h>
#include <ATen/core/dispatch/Dispatcher.h>
namespace at {
namespace functorch {
"
42,"m.fallback(torch::CppFunction::makeFallthrough());
}
#define UNSUPPORTED_RANDOM(op) \
m.impl(#op, torch::CppFunction::makeFromBoxedFunction<&unsupportedRandomOp>());
","m.fallback(torch::CppFunction::makeFallthrough());
}
void nyiRandomOp(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  TORCH_CHECK(false, ""vmap: we do not yet support "", op.schema().operator_name(),
              "". Please file an issue"");
}
#define UNSUPPORTED_RANDOM(op) \
m.impl(#op, torch::CppFunction::makeFromBoxedFunction<&unsupportedRandomOp>());
"
43,"TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
","TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  if (!areAnyBatchedAtLevel({input, weight_opt, bias_opt}, cur_level)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return at::native_group_norm(input, weight_opt, bias_opt, N, C, HxW, group, eps);
  }

Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
"
44,"auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor self_value, values_value;
optional<int64_t> self_bdim, values_bdim;
std::vector<optional<Tensor>> indices_value;
","auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  if (!isBatchedAtLevel(self, cur_level) && !isBatchedAtLevel(indices, cur_level) && !isBatchedAtLevel(values, cur_level)) {
    return self.index_put(indices, values, accumulate);
  }
Tensor self_value, values_value;
optional<int64_t> self_bdim, values_bdim;
std::vector<optional<Tensor>> indices_value;
"
45,"optional<Layout> layout,
optional<Device> device,
optional<bool> pin_memory) {
auto physical_view = MultiBatchVmapTransform::logicalToPhysical(self);
auto physical_size = physical_view.getPhysicalShape(size);
","optional<Layout> layout,
optional<Device> device,
optional<bool> pin_memory) {
  if (!participatesInCurrentLevel(self)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return self.new_empty_strided(
        size, stride, dtype, layout, device, pin_memory);
  }

auto physical_view = MultiBatchVmapTransform::logicalToPhysical(self);
auto physical_size = physical_view.getPhysicalShape(size);
"
46,"return std::make_tuple(tensor, nullopt);
}
}}
","return std::make_tuple(tensor, nullopt);
}
bool isBatchedAtLevel(const Tensor& tensor, int64_t level) {
  auto result = unwrapTensorAtLevel(tensor, level);
  return std::get<1>(result).has_value();
}

bool isBatchedAtLevel(const c10::optional<Tensor>& maybe_tensor, int64_t level) {
  if (!maybe_tensor.has_value()) {
    return false;
  }
  return isBatchedAtLevel(*maybe_tensor, level);
}

bool isBatchedAtLevel(TensorList tensors, int64_t level) {
  for (const auto& tensor : tensors) {
    if (isBatchedAtLevel(tensor, level)) {
      return true;
    }
  }
  return false;
}

bool isBatchedAtLevel(const c10::List<c10::optional<Tensor>> maybe_tensors, int64_t level) {
  for (const auto idx : c10::irange(0, maybe_tensors.size())) {
    const auto& maybe_tensor = maybe_tensors.get(idx);
    if (isBatchedAtLevel(maybe_tensor, level)) {
      return true;
    }
  }
  return false;
}

bool areAnyBatchedAtLevel(ArrayRef<optional<Tensor>> maybe_tensors, int64_t level) {
  for (const auto& maybe_tensor : maybe_tensors) {
    if (isBatchedAtLevel(maybe_tensor, level)) {
      return true;
    }
  }
  return false;
}


}}
"
47,"m.impl(""linear"", linear_hack);
m.impl(""binary_cross_entropy_with_logits_backward"", binary_cross_entropy_with_logits_backward_hack);
m.impl(""binary_cross_entropy_with_logits"", binary_cross_entropy_with_logits_hack);
}
}}
","m.impl(""linear"", linear_hack);
m.impl(""binary_cross_entropy_with_logits_backward"", binary_cross_entropy_with_logits_backward_hack);
m.impl(""binary_cross_entropy_with_logits"", binary_cross_entropy_with_logits_hack);
  m.impl(""trace_backward"", trace_backward_decomp);
}
}}
"
48,"@register_decomposition(aten.prelu_backward)
def prelu_backward(grad_output: Tensor, self: Tensor, weight: Tensor) -> Tuple[Tensor, Tensor]:
    spatial_dims = list(range(2, grad_output.dim()))
    for _ in range(len(spatial_dims)):
        weight = weight.unsqueeze(-1)
    input_grad = aten.where(self > 0, grad_output, weight * grad_output)
weight_grad_collector = aten.where(self > 0, aten.new_zeros(grad_output, ()), self * grad_output)
    return (input_grad, aten.sum(weight_grad_collector, [0] + spatial_dims))
@register_decomposition(aten.rrelu_with_noise_backward)
","@register_decomposition(aten.prelu_backward)
def prelu_backward(grad_output: Tensor, self: Tensor, weight: Tensor) -> Tuple[Tensor, Tensor]:
    # Logic is more complicated than I would like.  Basically, weight can either
    # be a scalar or a vector of size [C], and in the forward pass it's
    # broadcast against [N, C, ...]. So now, we need to do the corresponding
    # reduction, which is harder than we'd like...
    cur_weight = weight
    for _ in range(2, grad_output.dim()):
        cur_weight = cur_weight.unsqueeze(-1)
    input_grad = aten.where(self > 0, grad_output, cur_weight * grad_output)
weight_grad_collector = aten.where(self > 0, aten.new_zeros(grad_output, ()), self * grad_output)
    out = aten.sum_to_size(weight_grad_collector, cur_weight.shape)
    while out.dim() > weight.dim():
        out = out.squeeze(-1)
    return (input_grad, out)
@register_decomposition(aten.rrelu_with_noise_backward)
"
49,"}
// ['_masked_scale', 'native_dropout_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_14_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double);
template <>
Tensor lowerToNextLayer<batch_rule_14_t,Tensor,const Tensor &, const Tensor &, double>(
  batch_rule_14_t batch_rule,
const Tensor & self, const Tensor & mask, double scale
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_masked_scale', 'native_dropout_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_15_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double);
template <>
Tensor lowerToNextLayer<batch_rule_15_t,Tensor,const Tensor &, const Tensor &, double>(
  batch_rule_15_t batch_rule,
const Tensor & self, const Tensor & mask, double scale
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
50,"}
// ['addmv', 'addr', 'baddbmm', 'sspaddmm', '_sparse_addmm', 'sparse_sampled_addmm', 'addmm', 'addbmm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_23_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_23_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &>(
  batch_rule_23_t batch_rule,
const Tensor & self, const Tensor & mat, const Tensor & vec, const Scalar & beta, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['addmv', 'addr', 'baddbmm', 'sspaddmm', '_sparse_addmm', 'sparse_sampled_addmm', 'addmm', 'addbmm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_24_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_24_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &>(
  batch_rule_24_t batch_rule,
const Tensor & self, const Tensor & mat, const Tensor & vec, const Scalar & beta, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
51,"}
// ['binary_cross_entropy']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_37_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_37_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_37_t batch_rule,
const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['binary_cross_entropy']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_38_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_38_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_38_t batch_rule,
const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
52,"}
// ['constant_pad_nd']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_48_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_48_t,Tensor,const Tensor &, IntArrayRef, const Scalar &>(
  batch_rule_48_t batch_rule,
const Tensor & self, IntArrayRef pad, const Scalar & value
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['constant_pad_nd']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_49_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_49_t,Tensor,const Tensor &, IntArrayRef, const Scalar &>(
  batch_rule_49_t batch_rule,
const Tensor & self, IntArrayRef pad, const Scalar & value
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
53,"if (bias) {
std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
}
  auto results = batch_rule(input_value, input_bdim, weight_value, weight_bdim, bias_value, bias_bdim, stride, padding, dilation, transposed, output_padding);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_convolution_double_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_56_t)(const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, bool, bool, bool, bool, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_56_t,std::tuple<Tensor,Tensor,Tensor>,const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, bool, bool, bool, bool, ::std::array<bool,3>>(
  batch_rule_56_t batch_rule,
  const c10::optional<Tensor> & ggI, const c10::optional<Tensor> & ggW, const c10::optional<Tensor> & ggb, const Tensor & gO, const Tensor & weight, const Tensor & self, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","if (bias) {
std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
}
  auto results = batch_rule(input_value, input_bdim, weight_value, weight_bdim, bias_value, bias_bdim, stride, padding, dilation, groups);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_convolution_double_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_57_t)(const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_57_t,std::tuple<Tensor,Tensor,Tensor>,const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, ::std::array<bool,3>>(
  batch_rule_57_t batch_rule,
  const c10::optional<Tensor> & ggI, const c10::optional<Tensor> & ggW, const c10::optional<Tensor> & ggb, const Tensor & gO, const Tensor & weight, const Tensor & self, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
54,"return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['cudnn_convolution.deprecated', 'miopen_convolution', 'miopen_depthwise_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_68_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_68_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
  batch_rule_68_t batch_rule,
  const Tensor & self, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, bias_value, bias_bdim, padding, stride, dilation, groups, benchmark, deterministic);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['cudnn_convolution.deprecated2', 'miopen_convolution_transpose_backward_input']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_69_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_69_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
batch_rule_69_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['cudnn_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_69_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_69_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool>(
batch_rule_69_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
55,"Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, bias_value, bias_bdim, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution_transpose.deprecated2']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_74_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_74_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
batch_rule_74_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  auto results = batch_rule(self_value, self_bdim, dim);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level));
}
// ['cumprod', 'cumsum', 'log_softmax.int', 'softmax.int', '_sparse_softmax.int', '_sparse_log_softmax.int', 'special_log_softmax', 'special_softmax']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_74_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_74_t,Tensor,const Tensor &, int64_t, c10::optional<ScalarType>>(
batch_rule_74_t batch_rule,
  const Tensor & self, int64_t dim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
56,"}
// ['gradient.scalarrayint']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_94_t)(const Tensor &, c10::optional<int64_t>, ArrayRef<Scalar>, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_94_t,::std::vector<Tensor>,const Tensor &, ArrayRef<Scalar>, c10::optional<int64_t>, int64_t>(
  batch_rule_94_t batch_rule,
const Tensor & self, ArrayRef<Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['gradient.scalarrayint']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_88_t)(const Tensor &, c10::optional<int64_t>, ArrayRef<Scalar>, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_88_t,::std::vector<Tensor>,const Tensor &, ArrayRef<Scalar>, c10::optional<int64_t>, int64_t>(
  batch_rule_88_t batch_rule,
const Tensor & self, ArrayRef<Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
57,"}
// ['div.Tensor_mode', 'divide.Tensor_mode']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_96_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_96_t,Tensor,const Tensor &, const Tensor &, c10::optional<c10::string_view>>(
  batch_rule_96_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<c10::string_view> rounding_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['div.Tensor_mode', 'divide.Tensor_mode']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_90_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_90_t,Tensor,const Tensor &, const Tensor &, c10::optional<c10::string_view>>(
  batch_rule_90_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<c10::string_view> rounding_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
58,"}
// ['embedding']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_98_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_98_t,Tensor,const Tensor &, const Tensor &, int64_t, bool, bool>(
  batch_rule_98_t batch_rule,
const Tensor & weight, const Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['embedding']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_92_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_92_t,Tensor,const Tensor &, const Tensor &, int64_t, bool, bool>(
  batch_rule_92_t batch_rule,
const Tensor & weight, const Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
59,"}
// ['flatten.using_names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_116_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_116_t,Tensor,const Tensor &, Dimname, Dimname, Dimname>(
  batch_rule_116_t batch_rule,
const Tensor & self, Dimname start_dim, Dimname end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['flatten.using_names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_110_t)(const Tensor &, c10::optional<int64_t>, Dimname, Dimname, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_110_t,Tensor,const Tensor &, Dimname, Dimname, Dimname>(
  batch_rule_110_t batch_rule,
const Tensor & self, Dimname start_dim, Dimname end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
60,"}
// ['group_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_124_t)(const Tensor &, c10::optional<int64_t>, int64_t, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_124_t,Tensor,const Tensor &, int64_t, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double, bool>(
  batch_rule_124_t batch_rule,
const Tensor & input, int64_t num_groups, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps, bool cudnn_enabled
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['group_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_118_t)(const Tensor &, c10::optional<int64_t>, int64_t, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_118_t,Tensor,const Tensor &, int64_t, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double, bool>(
  batch_rule_118_t batch_rule,
const Tensor & input, int64_t num_groups, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps, bool cudnn_enabled
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
61,"}
// ['logsumexp.names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_156_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_156_t,Tensor,const Tensor &, DimnameList, bool>(
  batch_rule_156_t batch_rule,
const Tensor & self, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['logsumexp.names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_150_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_150_t,Tensor,const Tensor &, DimnameList, bool>(
  batch_rule_150_t batch_rule,
const Tensor & self, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
62,"}
// ['max_pool1d_with_indices', 'max_pool2d_with_indices', 'max_pool3d_with_indices']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_162_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_162_t,std::tuple<Tensor,Tensor>,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool>(
  batch_rule_162_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['max_pool1d_with_indices', 'max_pool2d_with_indices', 'max_pool3d_with_indices']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_156_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_156_t,std::tuple<Tensor,Tensor>,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool>(
  batch_rule_156_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
63,"}
// ['batch_norm_update_stats']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_185_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_185_t,std::tuple<Tensor,Tensor>,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_185_t batch_rule,
const Tensor & input, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, double momentum
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['batch_norm_update_stats']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_175_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_175_t,std::tuple<Tensor,Tensor>,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_175_t batch_rule,
const Tensor & input, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, double momentum
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
64,"}
// ['randint_like']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_199_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_199_t,Tensor,const Tensor &, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>>(
  batch_rule_199_t batch_rule,
const Tensor & self, int64_t high, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['randint_like']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_186_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_186_t,Tensor,const Tensor &, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>>(
  batch_rule_186_t batch_rule,
const Tensor & self, int64_t high, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
65,"}
// ['repeat_interleave.self_Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_201_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_201_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_201_t batch_rule,
const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['repeat_interleave.self_Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_188_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_188_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_188_t batch_rule,
const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
66,"}
// ['repeat_interleave.self_int']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_202_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_202_t,Tensor,const Tensor &, int64_t, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_202_t batch_rule,
const Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['repeat_interleave.self_int']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_189_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_189_t,Tensor,const Tensor &, int64_t, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_189_t batch_rule,
const Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
67,"}
// ['logit', 'special_logit']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_205_t)(const Tensor &, c10::optional<int64_t>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_205_t,Tensor,const Tensor &, c10::optional<double>>(
  batch_rule_205_t batch_rule,
const Tensor & self, c10::optional<double> eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['logit', 'special_logit']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_194_t)(const Tensor &, c10::optional<int64_t>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_194_t,Tensor,const Tensor &, c10::optional<double>>(
  batch_rule_194_t batch_rule,
const Tensor & self, c10::optional<double> eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
68,"}
// ['hsplit.int', 'vsplit.int', 'dsplit.int', 'unbind.int']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_213_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_213_t,::std::vector<Tensor>,const Tensor &, int64_t>(
  batch_rule_213_t batch_rule,
const Tensor & self, int64_t sections
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['hsplit.int', 'vsplit.int', 'dsplit.int', 'unbind.int']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_202_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_202_t,::std::vector<Tensor>,const Tensor &, int64_t>(
  batch_rule_202_t batch_rule,
const Tensor & self, int64_t sections
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
69,"}
// ['stft']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_215_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<int64_t>, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, c10::optional<bool>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_215_t,Tensor,const Tensor &, int64_t, c10::optional<int64_t>, c10::optional<int64_t>, const c10::optional<Tensor> &, bool, c10::optional<bool>, c10::optional<bool>>(
  batch_rule_215_t batch_rule,
const Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['stft']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_204_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::optional<int64_t>, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, c10::optional<bool>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_204_t,Tensor,const Tensor &, int64_t, c10::optional<int64_t>, c10::optional<int64_t>, const c10::optional<Tensor> &, bool, c10::optional<bool>, c10::optional<bool>>(
  batch_rule_204_t batch_rule,
const Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
70,"}
// ['std.correction_names', 'var.correction_names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_225_t)(const Tensor &, c10::optional<int64_t>, DimnameList, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_225_t,Tensor,const Tensor &, DimnameList, c10::optional<int64_t>, bool>(
  batch_rule_225_t batch_rule,
const Tensor & self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['std.correction_names', 'var.correction_names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_214_t)(const Tensor &, c10::optional<int64_t>, DimnameList, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_214_t,Tensor,const Tensor &, DimnameList, c10::optional<int64_t>, bool>(
  batch_rule_214_t batch_rule,
const Tensor & self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
71,"}
// ['prod.dim_int']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_226_t)(const Tensor &, c10::optional<int64_t>, int64_t, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_226_t,Tensor,const Tensor &, int64_t, bool, c10::optional<ScalarType>>(
  batch_rule_226_t batch_rule,
const Tensor & self, int64_t dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['prod.dim_int']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_215_t)(const Tensor &, c10::optional<int64_t>, int64_t, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_215_t,Tensor,const Tensor &, int64_t, bool, c10::optional<ScalarType>>(
  batch_rule_215_t batch_rule,
const Tensor & self, int64_t dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
72,"}
// ['tensordot']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_228_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_228_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef>(
  batch_rule_228_t batch_rule,
const Tensor & self, const Tensor & other, IntArrayRef dims_self, IntArrayRef dims_other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['tensordot']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_217_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_217_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef>(
  batch_rule_217_t batch_rule,
const Tensor & self, const Tensor & other, IntArrayRef dims_self, IntArrayRef dims_other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
73,"}
// ['trapz.dx', '_make_per_tensor_quantized_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_231_t)(const Tensor &, c10::optional<int64_t>, double, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_231_t,Tensor,const Tensor &, double, int64_t>(
  batch_rule_231_t batch_rule,
const Tensor & y, double dx, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['trapz.dx', '_make_per_tensor_quantized_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_220_t)(const Tensor &, c10::optional<int64_t>, double, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_220_t,Tensor,const Tensor &, double, int64_t>(
  batch_rule_220_t batch_rule,
const Tensor & y, double dx, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
74,"}
// ['where.ScalarSelf']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_239_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_239_t,Tensor,const Tensor &, const Scalar &, const Tensor &>(
  batch_rule_239_t batch_rule,
const Tensor & condition, const Scalar & self, const Tensor & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['where.ScalarSelf']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_228_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_228_t,Tensor,const Tensor &, const Scalar &, const Tensor &>(
  batch_rule_228_t batch_rule,
const Tensor & condition, const Scalar & self, const Tensor & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
75,"}
// ['_weight_norm_cuda_interface_backward', '_weight_norm_differentiable_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_242_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_242_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_242_t batch_rule,
const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_weight_norm_cuda_interface_backward', '_weight_norm_differentiable_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_231_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_231_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_231_t batch_rule,
const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
76,"}
// ['native_norm.ScalarOpt_dim_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_244_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_244_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool, c10::optional<ScalarType>>(
  batch_rule_244_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['native_norm.ScalarOpt_dim_dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_233_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, IntArrayRef, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_233_t,Tensor,const Tensor &, const c10::optional<Scalar> &, IntArrayRef, bool, c10::optional<ScalarType>>(
  batch_rule_233_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
77,"}
// ['_sparse_sum.dtype', 'view.dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_245_t)(const Tensor &, c10::optional<int64_t>, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_245_t,Tensor,const Tensor &, ScalarType>(
  batch_rule_245_t batch_rule,
const Tensor & self, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_sparse_sum.dtype', 'view.dtype']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_234_t)(const Tensor &, c10::optional<int64_t>, ScalarType);
template <>
Tensor lowerToNextLayer<batch_rule_234_t,Tensor,const Tensor &, ScalarType>(
  batch_rule_234_t batch_rule,
const Tensor & self, ScalarType dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
78,"}
// ['norm.names_ScalarOpt_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_252_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_252_t,Tensor,const Tensor &, const c10::optional<Scalar> &, DimnameList, bool>(
  batch_rule_252_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['norm.names_ScalarOpt_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_241_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_241_t,Tensor,const Tensor &, const c10::optional<Scalar> &, DimnameList, bool>(
  batch_rule_241_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & p, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
79,"}
// ['_sparse_coo_tensor_with_dims_and_tensors']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_258_t)(int64_t, int64_t, IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_258_t,Tensor,int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_258_t batch_rule,
int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_sparse_coo_tensor_with_dims_and_tensors']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_247_t)(int64_t, int64_t, IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_247_t,Tensor,int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_247_t batch_rule,
int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
80,"}
// ['_fake_quantize_learnable_per_channel_affine']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_275_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, double);
template <>
Tensor lowerToNextLayer<batch_rule_275_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, double>(
  batch_rule_275_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_fake_quantize_learnable_per_channel_affine']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_264_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, double);
template <>
Tensor lowerToNextLayer<batch_rule_264_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, double>(
  batch_rule_264_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
81,"}
// ['to.device']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_283_t)(const Tensor &, c10::optional<int64_t>, Device, ScalarType, bool, bool, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_283_t,Tensor,const Tensor &, Device, ScalarType, bool, bool, c10::optional<MemoryFormat>>(
  batch_rule_283_t batch_rule,
const Tensor & self, Device device, ScalarType dtype, bool non_blocking, bool copy, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['to.device']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_272_t)(const Tensor &, c10::optional<int64_t>, Device, ScalarType, bool, bool, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_272_t,Tensor,const Tensor &, Device, ScalarType, bool, bool, c10::optional<MemoryFormat>>(
  batch_rule_272_t batch_rule,
const Tensor & self, Device device, ScalarType dtype, bool non_blocking, bool copy, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
82,"}
// ['item', '_local_scalar_dense']
typedef std::tuple<Scalar> (*batch_rule_286_t)(const Tensor &, c10::optional<int64_t>);
template <>
Scalar lowerToNextLayer<batch_rule_286_t,Scalar,const Tensor &>(
  batch_rule_286_t batch_rule,
const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['item', '_local_scalar_dense']
typedef std::tuple<Scalar> (*batch_rule_275_t)(const Tensor &, c10::optional<int64_t>);
template <>
Scalar lowerToNextLayer<batch_rule_275_t,Scalar,const Tensor &>(
  batch_rule_275_t batch_rule,
const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
83,"}
// ['_thnn_differentiable_lstm_cell_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_292_t)(const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_292_t,std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor>,const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &>(
  batch_rule_292_t batch_rule,
const c10::optional<Tensor> & grad_hy, const c10::optional<Tensor> & grad_cy, const Tensor & input_gates, const Tensor & hidden_gates, const c10::optional<Tensor> & input_bias, const c10::optional<Tensor> & hidden_bias, const Tensor & cx, const Tensor & cy
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_thnn_differentiable_lstm_cell_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_281_t)(const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_281_t,std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor>,const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const Tensor &, const Tensor &>(
  batch_rule_281_t batch_rule,
const c10::optional<Tensor> & grad_hy, const c10::optional<Tensor> & grad_cy, const Tensor & input_gates, const Tensor & hidden_gates, const c10::optional<Tensor> & input_bias, const c10::optional<Tensor> & hidden_bias, const Tensor & cx, const Tensor & cy
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
84,"}
// ['put']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_301_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_301_t,Tensor,const Tensor &, const Tensor &, const Tensor &, bool>(
  batch_rule_301_t batch_rule,
const Tensor & self, const Tensor & index, const Tensor & source, bool accumulate
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['put']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_290_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_290_t,Tensor,const Tensor &, const Tensor &, const Tensor &, bool>(
  batch_rule_290_t batch_rule,
const Tensor & self, const Tensor & index, const Tensor & source, bool accumulate
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
85,"}
// ['index_fill.Dimname_Scalar', 'scatter.dimname_value']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_305_t)(const Tensor &, c10::optional<int64_t>, Dimname, const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_305_t,Tensor,const Tensor &, Dimname, const Tensor &, const Scalar &>(
  batch_rule_305_t batch_rule,
const Tensor & self, Dimname dim, const Tensor & index, const Scalar & value
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['index_fill.Dimname_Scalar', 'scatter.dimname_value']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_294_t)(const Tensor &, c10::optional<int64_t>, Dimname, const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_294_t,Tensor,const Tensor &, Dimname, const Tensor &, const Scalar &>(
  batch_rule_294_t batch_rule,
const Tensor & self, Dimname dim, const Tensor & index, const Scalar & value
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
86,"}
// ['scatter.value_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_307_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, const Scalar &, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_307_t,Tensor,const Tensor &, int64_t, const Tensor &, const Scalar &, c10::string_view>(
  batch_rule_307_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, const Scalar & value, c10::string_view reduce
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['scatter.value_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_296_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, const Scalar &, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_296_t,Tensor,const Tensor &, int64_t, const Tensor &, const Scalar &, c10::string_view>(
  batch_rule_296_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, const Scalar & value, c10::string_view reduce
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
87,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_scatter_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_308_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, c10::string_view, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_308_t,Tensor,const Tensor &, int64_t, const Tensor &, c10::string_view, c10::optional<int64_t>>(
  batch_rule_308_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, c10::string_view reduce, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['scatter_reduce.two']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_297_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, c10::string_view, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_297_t,Tensor,const Tensor &, int64_t, const Tensor &, c10::string_view, c10::optional<int64_t>>(
  batch_rule_297_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, c10::string_view reduce, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
88,"}
// ['ormqr']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_323_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_323_t,Tensor,const Tensor &, const Tensor &, const Tensor &, bool, bool>(
  batch_rule_323_t batch_rule,
const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['ormqr']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_312_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_312_t,Tensor,const Tensor &, const Tensor &, const Tensor &, bool, bool>(
  batch_rule_312_t batch_rule,
const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
89,"}
// ['lu_unpack']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_324_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_324_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_324_t batch_rule,
const Tensor & LU_data, const Tensor & LU_pivots, bool unpack_data, bool unpack_pivots
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['lu_unpack']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_313_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_313_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_313_t batch_rule,
const Tensor & LU_data, const Tensor & LU_pivots, bool unpack_data, bool unpack_pivots
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
90,"}
// ['multinomial']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_325_t)(const Tensor &, c10::optional<int64_t>, int64_t, bool, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_325_t,Tensor,const Tensor &, int64_t, bool, c10::optional<Generator>>(
  batch_rule_325_t batch_rule,
const Tensor & self, int64_t num_samples, bool replacement, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['multinomial']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_314_t)(const Tensor &, c10::optional<int64_t>, int64_t, bool, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_314_t,Tensor,const Tensor &, int64_t, bool, c10::optional<Generator>>(
  batch_rule_314_t batch_rule,
const Tensor & self, int64_t num_samples, bool replacement, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
91,"}
// ['fractional_max_pool2d', 'fractional_max_pool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_359_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_359_t,std::tuple<Tensor,Tensor>,const Tensor &, IntArrayRef, IntArrayRef, const Tensor &>(
  batch_rule_359_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fractional_max_pool2d', 'fractional_max_pool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_345_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_345_t,std::tuple<Tensor,Tensor>,const Tensor &, IntArrayRef, IntArrayRef, const Tensor &>(
  batch_rule_345_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
92,"}
// ['fractional_max_pool2d_backward', 'fractional_max_pool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_360_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_360_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, const Tensor &>(
  batch_rule_360_t batch_rule,
const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fractional_max_pool2d_backward', 'fractional_max_pool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_346_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_346_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, const Tensor &>(
  batch_rule_346_t batch_rule,
const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
93,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['max_unpool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_362_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_362_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_362_t batch_rule,
const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['max_unpool2d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_348_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_348_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef>(
  batch_rule_348_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor indices_value;
  optional<int64_t> indices_bdim;
  std::tie(indices_value, indices_bdim) = unwrapTensorAtLevel(indices, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, indices_value, indices_bdim, output_size);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['max_unpool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_349_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_349_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_349_t batch_rule,
const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
94,"}
// ['upsample_nearest1d_backward', '_upsample_nearest_exact1d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_375_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_375_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>>(
  batch_rule_375_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_nearest1d_backward', '_upsample_nearest_exact1d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_362_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_362_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>>(
  batch_rule_362_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
95,"}
// ['upsample_nearest3d_backward', '_upsample_nearest_exact3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_379_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_379_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>>(
  batch_rule_379_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_nearest3d_backward', '_upsample_nearest_exact3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_366_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_366_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>>(
  batch_rule_366_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
96,"}
// ['linalg_norm.ord_str']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_403_t)(const Tensor &, c10::optional<int64_t>, c10::string_view, c10::optional<IntArrayRef>, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_403_t,Tensor,const Tensor &, c10::string_view, c10::optional<IntArrayRef>, bool, c10::optional<ScalarType>>(
  batch_rule_403_t batch_rule,
const Tensor & self, c10::string_view ord, c10::optional<IntArrayRef> dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['linalg_norm.ord_str']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_383_t)(const Tensor &, c10::optional<int64_t>, c10::string_view, c10::optional<IntArrayRef>, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_383_t,Tensor,const Tensor &, c10::string_view, c10::optional<IntArrayRef>, bool, c10::optional<ScalarType>>(
  batch_rule_383_t batch_rule,
const Tensor & self, c10::string_view ord, c10::optional<IntArrayRef> dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
97,"}
// ['linalg_tensorsolve']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_411_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>);
template <>
Tensor lowerToNextLayer<batch_rule_411_t,Tensor,const Tensor &, const Tensor &, c10::optional<IntArrayRef>>(
  batch_rule_411_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<IntArrayRef> dims
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['linalg_tensorsolve']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_391_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>);
template <>
Tensor lowerToNextLayer<batch_rule_391_t,Tensor,const Tensor &, const Tensor &, c10::optional<IntArrayRef>>(
  batch_rule_391_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<IntArrayRef> dims
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
98,".def(""at"",
[](CompileCache &self, int64_t id, int64_t fw_compiler_id,
int64_t bw_compiler_id, int numTensorArgs,
              const std::string hasherType, py::args args) {
return self.at(id, fw_compiler_id, bw_compiler_id, numTensorArgs,
hasherType, args.ptr());
})
.def(""insert"",
[](CompileCache &self, int64_t id, int64_t fw_compiler_id,
int64_t bw_compiler_id, int numTensorArgs,
              const std::string hasherType, const py::object &compileFn,
py::args args, py::kwargs kwargs) {
self.insert(id, fw_compiler_id, bw_compiler_id, numTensorArgs,
hasherType, compileFn, args.ptr());
",".def(""at"",
[](CompileCache &self, int64_t id, int64_t fw_compiler_id,
int64_t bw_compiler_id, int numTensorArgs,
              const std::string &hasherType, py::args args) {
return self.at(id, fw_compiler_id, bw_compiler_id, numTensorArgs,
hasherType, args.ptr());
})
.def(""insert"",
[](CompileCache &self, int64_t id, int64_t fw_compiler_id,
int64_t bw_compiler_id, int numTensorArgs,
              const std::string &hasherType, const py::object &compileFn,
py::args args, py::kwargs kwargs) {
self.insert(id, fw_compiler_id, bw_compiler_id, numTensorArgs,
hasherType, compileFn, args.ptr());
"
99,"VARIADIC_BDIMS(pinverse);
VARIADIC_BDIMS(inverse);
VARIADIC_BDIMS_BOXED(slogdet);
  VARIADIC_BDIMS_BOXED(_svd_helper);
VARIADIC_BDIMS_BOXED(solve);
VARIADIC_BDIMS_BOXED(symeig);
VARIADIC_BDIMS_BOXED(triangular_solve);
","VARIADIC_BDIMS(pinverse);
VARIADIC_BDIMS(inverse);
VARIADIC_BDIMS_BOXED(slogdet);
  VARIADIC_BDIMS_BOXED(_linalg_svd);
VARIADIC_BDIMS_BOXED(solve);
VARIADIC_BDIMS_BOXED(symeig);
VARIADIC_BDIMS_BOXED(triangular_solve);
"
100,"f = torch.jit.script(fx_g)
    # Works around alias analysis issues in TS
    # graph = f.graph
    # outputs = list(graph.outputs())
    # output = outputs[0]
    # graph.eraseOutput(0)
    # outputs = list(output.node().inputs())
    # for inp in output.node().inputs():
    #     graph.registerOutput(inp)
    # output.node().destroy()
    # torch._C._jit_pass_remove_mutation(graph)
    # for i in range(len(list(graph.outputs()))):
    #     graph.eraseOutput(0)
    # node = graph.create(""prim::ListConstruct"", outputs)
    # graph.appendNode(node)
    # node.output().setType(torch._C.ListType.ofTensors())
    # graph.registerOutput(node.output())
torch._C._jit_pass_remove_mutation(f.graph)
f = torch.jit.freeze(f.eval())
","f = torch.jit.script(fx_g)
torch._C._jit_pass_remove_mutation(f.graph)
f = torch.jit.freeze(f.eval())
"
101,"def draw_graph(traced: torch.fx.GraphModule, fname: str, figname: str = ""fx_graph"", clear_meta=True):
if clear_meta:
        traced = copy.deepcopy(traced)
for node in traced.graph.nodes:
node.meta = {}
base, ext = os.path.splitext(fname)
","def draw_graph(traced: torch.fx.GraphModule, fname: str, figname: str = ""fx_graph"", clear_meta=True):
if clear_meta:
        new_graph = copy.deepcopy(traced.graph)
        traced = fx.GraphModule(traced, new_graph)
for node in traced.graph.nodes:
node.meta = {}
base, ext = os.path.splitext(fname)
"
102,"nx_graph.add_edge(""source"", node.name+""_in"", capacity=math.inf)
is_input = True
        if node.op == 'call_function' and node.target not in recomputable_ops:
        # if node.op == 'call_function' and node.target in unrecomputable_ops:
            nx_graph.add_edge(""source"", node.name+""_in"", capacity=math.inf)
if 'tensor_meta' not in node.meta:
weight = math.inf
","nx_graph.add_edge(""source"", node.name+""_in"", capacity=math.inf)
is_input = True
        if AGGRESSIVE_RECOMPUTATION:
            if node.op == 'call_function' and node.target in unrecomputable_ops:
                nx_graph.add_edge(""source"", node.name+""_in"", capacity=math.inf)
        else:
            if node.op == 'call_function' and node.target not in recomputable_ops:
                nx_graph.add_edge(""source"", node.name+""_in"", capacity=math.inf)
if 'tensor_meta' not in node.meta:
weight = math.inf
"
103,"fw_compiler,
bw_compiler,
partition_fn,
        decompose=True,
hasher_type=hasher_type,
)
","fw_compiler,
bw_compiler,
partition_fn,
        decompositions=decomposition_table,
hasher_type=hasher_type,
)
"
104,"try:
real_out = func(*args, **kwargs)
except NotImplementedError:
            args = pytree.tree_map(lambda x: torch.ones_like(x, device=output_device)
if isinstance(x, torch.Tensor) else x, args)
            kwargs = pytree.tree_map(lambda x: torch.ones_like(x, device=output_device)
if isinstance(x, torch.Tensor) else x, kwargs)
real_out = func(*args, **kwargs)
","try:
real_out = func(*args, **kwargs)
except NotImplementedError:
            # Hardcoding in running in cuda if meta-tracing fails for now.
            args = pytree.tree_map(lambda x: torch.ones_like(x, device='cuda')
if isinstance(x, torch.Tensor) else x, args)
            kwargs = pytree.tree_map(lambda x: torch.ones_like(x, device='cuda')
if isinstance(x, torch.Tensor) else x, kwargs)
real_out = func(*args, **kwargs)
"
105,"OP_DECOMPOSE2(conv2d, padding);
OP_DECOMPOSE2(conv3d, padding);
OP_DECOMPOSE(_convolution_mode);
OP_DECOMPOSE(type_as);
DECOMPOSE_FUNCTIONAL(diag_embed);
DECOMPOSE_FUNCTIONAL(block_diag);
","OP_DECOMPOSE2(conv2d, padding);
OP_DECOMPOSE2(conv3d, padding);
OP_DECOMPOSE(_convolution_mode);
  OP_DECOMPOSE(frobenius_norm);
OP_DECOMPOSE(type_as);
DECOMPOSE_FUNCTIONAL(diag_embed);
DECOMPOSE_FUNCTIONAL(block_diag);
"
106,"/// Cache type mapping specialization keys to compiled kernels.
class vector_hasher {
public:
    std::size_t operator()(std::vector<int> const& vec) const {
std::size_t seed = vec.size();
for(auto& i : vec) {
seed ^= i + 0x9e3779b9 + (seed << 6) + (seed >> 2);
","/// Cache type mapping specialization keys to compiled kernels.
class vector_hasher {
public:
    std::size_t operator()(hash_key_t const& vec) const {
std::size_t seed = vec.size();
for(auto& i : vec) {
seed ^= i + 0x9e3779b9 + (seed << 6) + (seed >> 2);
"
107,"&fn,\
c10::guts::function_traits<decltype(fn)>::parameter_types>::apply)
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""ones_like"", BASIC_UNARY_BATCH_RULE(ATEN_FN(ones_like)));
","&fn,\
c10::guts::function_traits<decltype(fn)>::parameter_types>::apply)
std::tuple<Tensor,optional<int64_t>> _new_zeros_with_same_feature_meta_batch_rule(
    const Tensor& self, optional<int64_t> self_bdim,
    const Tensor& other, optional<int64_t> other_bdim,
    int64_t self_num_batch_dims) {
  TORCH_CHECK(!other_bdim.has_value(),
      ""NYI: vmap over jvp of the primal. Please file an issue."");
  auto self_ = moveBatchDimToFront(self, self_bdim);
  auto result = at::_new_zeros_with_same_feature_meta(self, other, self_num_batch_dims + 1);
  return std::make_tuple(result, 0);
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""ones_like"", BASIC_UNARY_BATCH_RULE(ATEN_FN(ones_like)));
"
108,"from enum import Enum
import warnings
from contextlib import contextmanager
decomposition_table = {}
","from enum import Enum
import warnings
from contextlib import contextmanager
aten = torch.ops.aten
decomposition_table = {}
"
109,"return f
return decomposition_decorator
@register_decomposition(torch.ops.aten.tanh_backward)
def tanh_backward_decomposition(out_grad, y):
    return torch.sub(out_grad, out_grad * y * y)
@register_decomposition(torch.ops.aten.sigmoid_backward)
def sigmoid_backward_decomposition(out_grad, y):
return out_grad * (y * (1 - y))
@register_decomposition(torch.ops.aten._s_where)
def _s_where_decomposition(a, b, c):
    return torch.where(a, b, c)
@register_decomposition(torch.ops.aten.detach)
def noop(x):
return x
@register_decomposition(torch.ops.aten.softplus_backward)
def softplus_backward_decomposition(out_grad, x, beta, threshold, out):
    return out_grad * torch.sigmoid(x)
USE_DECOMPOSE = False
","return f
return decomposition_decorator
@register_decomposition(aten.tanh_backward)
def tanh_backward_decomposition(out_grad, y):
    return aten.sub(out_grad, out_grad * y * y)
@register_decomposition(aten.sigmoid_backward)
def sigmoid_backward_decomposition(out_grad, y):
return out_grad * (y * (1 - y))
@register_decomposition(aten._s_where)
def _s_where_decomposition(a, b, c):
    return aten.where(a, b, c)
@register_decomposition(aten.detach)
def noop(x):
return x
@register_decomposition(aten.softplus_backward)
def softplus_backward_decomposition(out_grad, x, beta, threshold, out):
    z = (x * beta).exp()
    return aten.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
USE_DECOMPOSE = False
"
110,"const Tensor& index, optional<int64_t> index_bdim,
const Scalar& src,
const c10::string_view reduce) {
  using scatter_reduce_value_sig = Tensor (*)(const Tensor&, int64_t, const Tensor&, const Scalar&, const c10::string_view reduce);
return scatter_batch_rule(ATEN_FN2(scatter, value_reduce),
self, self_bdim, dim, index, index_bdim, src, reduce);
}
","const Tensor& index, optional<int64_t> index_bdim,
const Scalar& src,
const c10::string_view reduce) {
return scatter_batch_rule(ATEN_FN2(scatter, value_reduce),
self, self_bdim, dim, index, index_bdim, src, reduce);
}
"
111,"DynamicLayerStackHolder() {}
virtual ~DynamicLayerStackHolder() {}
  std::vector<DynamicLayer> dynamicLayerStack = { DynamicLayer(DispatchKey::Autograd, 1) };
};
thread_local std::shared_ptr<DynamicLayerStackHolder> kDynamicLayerStack;
","DynamicLayerStackHolder() {}
virtual ~DynamicLayerStackHolder() {}
  std::vector<DynamicLayer> dynamicLayerStack = { DynamicLayer(DispatchKey::Autograd, 1, nullopt, true) };
};
thread_local std::shared_ptr<DynamicLayerStackHolder> kDynamicLayerStack;
"
112,"pushDynamicLayer(std::move(layer_));
}
  bool prev_grad_enabled_;
DynamicLayer layer_;
};
","pushDynamicLayer(std::move(layer_));
}
DynamicLayer layer_;
};
"
113,"def draw_joint_graph(graph, joint_inputs, file_name=""full_graph.png""):
draw_graph(graph, file_name)
    return partition_backwards(graph, joint_inputs)
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn):
joint_forward_backward = create_joint_forward_backward(flat_fn)
","def draw_joint_graph(graph, joint_inputs, file_name=""full_graph.png""):
draw_graph(graph, file_name)
    return default_partition(graph, joint_inputs)
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn):
joint_forward_backward = create_joint_forward_backward(flat_fn)
"
114,"// simplicity. When that is not the case, this code should be updated.
const auto& argument = (*stack)[arguments_begin + arg_idx];
if (batched_tensor_inputs_pos_iter == batched_tensor_inputs_position.end()
          || arg_idx != *batched_tensor_inputs_pos_iter) {
// argument isn't a BatchedTensor
torch::jit::push(stack, argument);
continue;
","// simplicity. When that is not the case, this code should be updated.
const auto& argument = (*stack)[arguments_begin + arg_idx];
if (batched_tensor_inputs_pos_iter == batched_tensor_inputs_position.end()
          || (int64_t)arg_idx != *batched_tensor_inputs_pos_iter) {
// argument isn't a BatchedTensor
torch::jit::push(stack, argument);
continue;
"
115,"define_macros = []
extra_link_args = []
    extra_compile_args = {""cxx"": [""-O3"", ""-g"", ""-std=c++14"", ""-fdiagnostics-color=always""]}
debug_mode = os.getenv('DEBUG', '0') == '1'
if debug_mode:
print(""Compiling in debug mode"")
extra_compile_args = {
            ""cxx"": [""-O0"", ""-fno-inline"", ""-g"", ""-std=c++14"", ""-fdiagnostics-color=always""]}
extra_link_args = [""-O0"", ""-g""]
this_dir = os.path.dirname(os.path.abspath(__file__))
","define_macros = []
extra_link_args = []
    extra_compile_args = {""cxx"": [
        ""-O3"",
        ""-g"",
        ""-std=c++14"",
        ""-fdiagnostics-color=always"",
        # PyTorch SmallVector has this
        '-Wno-array-bounds',
        '-Werror',
    ]}
debug_mode = os.getenv('DEBUG', '0') == '1'
if debug_mode:
print(""Compiling in debug mode"")
extra_compile_args = {
            ""cxx"": [
                ""-O0"",
                ""-fno-inline"",
                ""-g"",
                ""-std=c++14"",
                ""-fdiagnostics-color=always"",
                # PyTorch SmallVector has this
                '-Wno-array-bounds',
                '-Werror',
            ]}
extra_link_args = [""-O0"", ""-g""]
this_dir = os.path.dirname(os.path.abspath(__file__))
"
116,"elif n.op == 'output':
output_node = n
bw_graph = fx.Graph()
value_remap = {}
for saved_node in saved_nodes:
value_remap[saved_node] = bw_graph.placeholder(saved_node.name)
for node in fx_module.graph.nodes:
        if node in bw_nodes:
value_remap[node] = bw_graph.node_copy(node, lambda n : value_remap[n])
    num_fwd_outputs = fx_module._out_spec.children_specs[0].num_leaves
    num_bwd_outputs = fx_module._out_spec.children_specs[1].num_leaves
assert(num_fwd_outputs + num_bwd_outputs == len(output_node.args[0]))
    bwd_outputs = [value_remap[i] for i in output_node.args[0][num_fwd_outputs:]]
if len(bwd_outputs) == 1:
bwd_outputs = bwd_outputs[0]
bw_graph.output(bwd_outputs)
","elif n.op == 'output':
output_node = n
    num_fwd_outputs = fx_module._out_spec.children_specs[0].num_leaves
    num_bwd_outputs = fx_module._out_spec.children_specs[1].num_leaves
    bw_outputs = output_node.args[0][num_fwd_outputs:]

bw_graph = fx.Graph()
value_remap = {}
for saved_node in saved_nodes:
value_remap[saved_node] = bw_graph.placeholder(saved_node.name)
for node in fx_module.graph.nodes:
        if node in bw_nodes or node in bw_outputs:
value_remap[node] = bw_graph.node_copy(node, lambda n : value_remap[n])
assert(num_fwd_outputs + num_bwd_outputs == len(output_node.args[0]))
    bwd_outputs = [value_remap[i] for i in bw_outputs]
if len(bwd_outputs) == 1:
bwd_outputs = bwd_outputs[0]
bw_graph.output(bwd_outputs)
"
117,"dims = {arguments[dim_arg_pos].toInt()};
} else if (arguments[dim_arg_pos].isNone())  {
reduction_case = ReductionCase::DimArray;
    auto all_dims = range(0, self.dim() - 1);
    dims = std::vector<int64_t>(all_dims.begin(), all_dims.end());
} else{
TORCH_INTERNAL_ASSERT(false, ""Unexpected dtype found at dims"");
// auto all_dims = range(0, self.dim() - 1);
","dims = {arguments[dim_arg_pos].toInt()};
} else if (arguments[dim_arg_pos].isNone())  {
reduction_case = ReductionCase::DimArray;
    if (logical_dim == 0) {
      dims = {0};
    } else {
      auto all_dims = range(0, self.dim() - 1);
      dims = std::vector<int64_t>(all_dims.begin(), all_dims.end());
    }
} else{
TORCH_INTERNAL_ASSERT(false, ""Unexpected dtype found at dims"");
// auto all_dims = range(0, self.dim() - 1);
"
118,"m.impl(""cudnn_convolution"", cudnn_convolution_plumbing);
OP_DECOMPOSE(dropout);
  VMAP_SUPPORT(""_softmax_backward_data"", _softmax_backward_batch_rule);

VMAP_SUPPORT(""constant_pad_nd"", BASIC_UNARY_BATCH_RULE(at::constant_pad_nd));
VMAP_SUPPORT(""reflection_pad1d"", EXISTING_BDIM_BATCH_RULE(at::reflection_pad1d));
VMAP_SUPPORT(""reflection_pad2d"", EXISTING_BDIM_BATCH_RULE(at::reflection_pad2d));
","m.impl(""cudnn_convolution"", cudnn_convolution_plumbing);
OP_DECOMPOSE(dropout);
VMAP_SUPPORT(""constant_pad_nd"", BASIC_UNARY_BATCH_RULE(at::constant_pad_nd));
VMAP_SUPPORT(""reflection_pad1d"", EXISTING_BDIM_BATCH_RULE(at::reflection_pad1d));
VMAP_SUPPORT(""reflection_pad2d"", EXISTING_BDIM_BATCH_RULE(at::reflection_pad2d));
"
119,"from functools import partial
import time
a = torch.randn(10000, 1, 4, requires_grad=True)
b = torch.randn(1, 10000, 4)
def f(a):
    return (a * b).sin().sum(dim=0)
fw_compiler = partial(tvm_compile, name='fw_keops')
bw_compiler = partial(tvm_compile, name='bw_keops')
","from functools import partial
import time
a = torch.randn(2000, 1, 4, requires_grad=True)
b = torch.randn(1, 2000, 4)
def f(a):
    return (a * b).sum(dim=0)
fw_compiler = partial(tvm_compile, name='fw_keops')
bw_compiler = partial(tvm_compile, name='bw_keops')
"
120,"def bench(func):
begin = time.time()
for _ in range(iters):
        out = func(a)
out.sum().backward()
        mod.zero_grad()
print(time.time()-begin)
def bench_jax():
","def bench(func):
begin = time.time()
for _ in range(iters):
        out = func(a).sin()
out.sum().backward()
        a.grad = None
print(time.time()-begin)
def bench_jax():
"
121,"#define TENSOROPTIONSPARAMS c10::optional<c10::ScalarType> dtype, c10::optional<c10::Layout> layout, c10::optional<c10::Device> device, c10::optional<bool> pin_memory
#define TENSOROPTIONSARGS dtype, layout, device, pin_memory
Tensor randn_mbatching_rule(IntArrayRef shape, TENSOROPTIONSPARAMS) {
","#define TENSOROPTIONSPARAMS c10::optional<c10::ScalarType> dtype, c10::optional<c10::Layout> layout, c10::optional<c10::Device> device, c10::optional<bool> pin_memory
#define UNSUPPORTED_RANDOM(op) \
  m.impl(#op, torch::CppFunction::makeFromBoxedFunction<&unsupportedRandomOp2>());

#define UNSUPPORTED_RANDOM2(op, overload) \
  m.impl(#op"".""#overload, torch::CppFunction::makeFromBoxedFunction<&unsupportedRandomOp2>());

#define TENSOROPTIONSARGS dtype, layout, device, pin_memory
Tensor randn_mbatching_rule(IntArrayRef shape, TENSOROPTIONSPARAMS) {
"
122,"VMAP_SUPPORT(""threshold_backward"", SINGLE_ARG(
binary_pointwise_batch_rule<decltype(&at::threshold_backward), &at::threshold_backward, const Scalar&>));
using TensorScalarInplaceT = Tensor& (Tensor::*)(const Tensor&, const Scalar&) const;
using ScalarScalarInplaceT = Tensor& (Tensor::*)(const Scalar&, const Scalar&) const;
using TensorInplaceT = Tensor& (Tensor::*)(const Tensor&) const;
","VMAP_SUPPORT(""threshold_backward"", SINGLE_ARG(
binary_pointwise_batch_rule<decltype(&at::threshold_backward), &at::threshold_backward, const Scalar&>));
  m.impl(""where.self"", static_cast<decltype(&ATEN_FN2(where, self))>(native::where));
  VMAP_SUPPORT(""_s_where"", _s_where_batch_rule);

using TensorScalarInplaceT = Tensor& (Tensor::*)(const Tensor&, const Scalar&) const;
using ScalarScalarInplaceT = Tensor& (Tensor::*)(const Scalar&, const Scalar&) const;
using TensorInplaceT = Tensor& (Tensor::*)(const Tensor&) const;
"
123,"TORCH_INTERNAL_ASSERT(false, ""Can't set_storage_offset for PythonTensorImpl"");
}
bool isPythonTensor(at::Tensor tensor) {
return tensor.unsafeGetTensorImpl()->key_set().has(
c10::DispatchKey::FuncTorchPython);
}
PythonTensorImpl* getPythonImpl(at::Tensor tensor) {
return static_cast<PythonTensorImpl*>(tensor.unsafeGetTensorImpl());
}
at::Tensor addPythonKey(const py::object& tensor) {
return at::detail::make_tensor<PythonTensorImpl>(tensor);
}
bool hasPythonKey(at::Tensor tensor) {
return isPythonTensor(tensor);
}
py::object removePythonKey(at::Tensor tensor) {
assert(isPythonTensor(tensor));
return getPythonImpl(tensor)->value_;
}
","TORCH_INTERNAL_ASSERT(false, ""Can't set_storage_offset for PythonTensorImpl"");
}
bool isPythonTensor(const at::Tensor& tensor) {
return tensor.unsafeGetTensorImpl()->key_set().has(
c10::DispatchKey::FuncTorchPython);
}
PythonTensorImpl* getPythonImpl(const at::Tensor& tensor) {
return static_cast<PythonTensorImpl*>(tensor.unsafeGetTensorImpl());
}
at::Tensor addPythonKey(const py::object& tensor) {
return at::detail::make_tensor<PythonTensorImpl>(tensor);
}
bool hasPythonKey(const at::Tensor& tensor) {
return isPythonTensor(tensor);
}
py::object removePythonKey(const at::Tensor& tensor) {
assert(isPythonTensor(tensor));
return getPythonImpl(tensor)->value_;
}
"
124,"continue
shapes = get_te_shapes(node.meta['tensor_meta'].shape)
placeholder = te.Placeholder(node.name, get_te_type(node), shapes)
            env[node.name] = placeholder.data()
inputs.append(placeholder)
elif node.op == 'call_function':
# This does the bulk of the work - we call `lower_function`, which
","continue
shapes = get_te_shapes(node.meta['tensor_meta'].shape)
placeholder = te.Placeholder(node.name, get_te_type(node), shapes)
            env[node.name] = placeholder
inputs.append(placeholder)
elif node.op == 'call_function':
# This does the bulk of the work - we call `lower_function`, which
"
125,"std::tuple<Tensor, optional<int64_t>> dot_batch_rule(const Tensor& A, optional<int64_t> A_bdim, const Tensor& B, optional<int64_t> B_bdim) {
auto A_ = moveBatchDimToFront(A, A_bdim);
auto B_ = moveBatchDimToFront(B, B_bdim);
return {at::matmul(A_, B_.t()), 0};
}
","std::tuple<Tensor, optional<int64_t>> dot_batch_rule(const Tensor& A, optional<int64_t> A_bdim, const Tensor& B, optional<int64_t> B_bdim) {
auto A_ = moveBatchDimToFront(A, A_bdim);
auto B_ = moveBatchDimToFront(B, B_bdim);
  if (A_bdim && B_bdim) {
    return {at::matmul(A_.unsqueeze(-2), B_.unsqueeze(-1)).squeeze(-1).squeeze(-1), 0};
  }
return {at::matmul(A_, B_.t()), 0};
}
"
126,"int64_t dim) {
auto self_ = moveBatchDimToFront(self, self_bdim);
auto rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, rank + 1) + 1;
  return { self.unsqueeze(dim), valIfNonempty(self_bdim, 0) };
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
","int64_t dim) {
auto self_ = moveBatchDimToFront(self, self_bdim);
auto rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, rank + 1);
  if (self_bdim) {
    dim += 1;
  }
  return { self_.unsqueeze(dim), valIfNonempty(self_bdim, 0) };
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
"
127,"castPyCFunctionWithKeywords(THPVariable_nonzero),
METH_VARARGS | METH_KEYWORDS | METH_STATIC,
nullptr},
    {""randint"",
     castPyCFunctionWithKeywords(THPVariable_randint),
     METH_VARARGS | METH_KEYWORDS | METH_STATIC,
     nullptr},
{""range"",
castPyCFunctionWithKeywords(THPVariable_range),
METH_VARARGS | METH_KEYWORDS | METH_STATIC,
","castPyCFunctionWithKeywords(THPVariable_nonzero),
METH_VARARGS | METH_KEYWORDS | METH_STATIC,
nullptr},
{""range"",
castPyCFunctionWithKeywords(THPVariable_range),
METH_VARARGS | METH_KEYWORDS | METH_STATIC,
"
128,"# The out variant (e.g. conv2d_out)
outplace: Optional[NativeFunction]
# C++ function dispatch is wrapped in a lambda function. The lambda function
# has almost the same signature as the C++ function, only with some small
","# The out variant (e.g. conv2d_out)
outplace: Optional[NativeFunction]
    @classmethod
    def from_pairs(
        cls,
        functional: PythonSignatureNativeFunctionPair,
        out: Optional[PythonSignatureNativeFunctionPair],
    ) -> ""PythonSignatureGroup"":
        if out is None:
            return PythonSignatureGroup(
                signature=functional.signature,
                base=functional.function,
                outplace=None,
            )

        # prefer the signature with optional out=... arguments because it's the
        # superset that can be used to parse input for both base and outplace.
        signature_kwargs = out.signature.__dict__.copy()

        # Out overloads in C++ don't have TensorOptions arguments,
        # so take these from the functional variant
        signature_kwargs[
            ""tensor_options_args""
        ] = functional.signature.tensor_options_args

        return PythonSignatureGroup(
            signature=type(out.signature)(**signature_kwargs),
            base=functional.function,
            outplace=out.function,
        )

# C++ function dispatch is wrapped in a lambda function. The lambda function
# has almost the same signature as the C++ function, only with some small
"
129,"'get_all_sharing_strategies']
# Compat from older version of python to make sure we get the objects
# compatible with the default context.
import multiprocessing as mp

_default_context = mp.get_context()
__all__ctx = [x for x in dir(_default_context) if not x.startswith('_')]
__all__mp = [
    x for x in dir(mp) if x not in __all__ctx and not x.startswith('_')
]
globals().update((name, getattr(_default_context, name)) for name in __all__ctx)
globals().update((name, getattr(mp, name)) for name in __all__mp)

__all__ += __all__ctx + __all__mp
# This call adds a Linux specific prctl(2) wrapper function to this module.
# See https://github.com/pytorch/pytorch/pull/14391 for more information.
","'get_all_sharing_strategies']
from multiprocessing import *  # noqa: F403


__all__ += multiprocessing.__all__  # type: ignore[attr-defined]

# This call adds a Linux specific prctl(2) wrapper function to this module.
# See https://github.com/pytorch/pytorch/pull/14391 for more information.
"
130,"template <typename scalar_t>
static void apply_eig(const Tensor& self, bool eigenvectors, Tensor& out_eigvals, Tensor& out_eigvecs,
                      int64_t *info_ptr) {
#if !AT_MAGMA_ENABLED()
TORCH_CHECK(false, ""Calling torch.eig on a CUDA tensor requires compiling PyTorch with MAGMA. ""
""Either transfer the tensor to the CPU before calling torch.eig or recompile with MAGMA."");
","template <typename scalar_t>
static void apply_eig(const Tensor& self, bool eigenvectors, Tensor& out_eigvals, Tensor& out_eigvecs,
                      int* info_ptr) {
#if !AT_MAGMA_ENABLED()
TORCH_CHECK(false, ""Calling torch.eig on a CUDA tensor requires compiling PyTorch with MAGMA. ""
""Either transfer the tensor to the CPU before calling torch.eig or recompile with MAGMA."");
"
131,"? at::empty_strided({n, n}, {1, n}, options)
: Tensor();
  int64_t info;
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""eig_cuda"", [&]{
    apply_eig<scalar_t>(self_working_copy, eigenvectors, out_eigvals, out_eigvecs, &info);
});
  singleCheckErrors(info, ""eig_cuda"");
return std::tuple<Tensor, Tensor>(out_eigvals, out_eigvecs);
}
","? at::empty_strided({n, n}, {1, n}, options)
: Tensor();
  auto infos = at::zeros({}, self_working_copy.options().dtype(kInt));
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""eig_cuda"", [&]{
    apply_eig<scalar_t>(self_working_copy, eigenvectors, out_eigvals, out_eigvecs, infos.data_ptr<int>());
});
  at::_linalg_check_errors(infos, ""eig"", /*is_matrix*/true);
return std::tuple<Tensor, Tensor>(out_eigvals, out_eigvecs);
}
"
132,"padding=(0, padding[0]),
stride=(1, stride[0])),
lambda: F.unfold(input, kernel_size, dilation=dilation, padding=padding, stride=stride),
        lambda: unfold3d(input, kernel_size, dilation, padding, stride)
)
input = unfold_func()
","padding=(0, padding[0]),
stride=(1, stride[0])),
lambda: F.unfold(input, kernel_size, dilation=dilation, padding=padding, stride=stride),
        lambda: unfold3d(input, kernel_size, padding, stride, dilation)
)
input = unfold_func()
"
133,"output, mean, rstd = forward_helper(torch.native_layer_norm, expanded_args, expanded_kwargs)
ctx.args = expanded_args
        if input.requires_grad or isinstance(ExpandedWeight, expanded_kwargs['weight']):
ctx.weight = expanded_kwargs['weight']
        if input.requires_grad or isinstance(ExpandedWeight, expanded_kwargs['bias']):
ctx.bias = expanded_kwargs['bias']
ctx.eps = expanded_kwargs['eps']
ctx.mean, ctx.rstd = mean, rstd
","output, mean, rstd = forward_helper(torch.native_layer_norm, expanded_args, expanded_kwargs)
ctx.args = expanded_args
        if input.requires_grad or isinstance(expanded_kwargs['weight'], ExpandedWeight):
ctx.weight = expanded_kwargs['weight']
        if input.requires_grad or isinstance(expanded_kwargs['bias'], ExpandedWeight):
ctx.bias = expanded_kwargs['bias']
ctx.eps = expanded_kwargs['eps']
ctx.mean, ctx.rstd = mean, rstd
"
134,"// TODO: Use re2.
void ValueCache::trimPrefixes() {
  static auto prefixes = py::module::import(""torch.profiler.python_tracer"")
                             .attr(""_prefix_regex"")()
                             .cast<std::vector<std::string>>();
for (auto& it : std::get<CallType::PyCall>(state_)) {
std::string filename = it.second.filename_.str();
","// TODO: Use re2.
void ValueCache::trimPrefixes() {
  static const auto prefixes = []() {
    pybind11::gil_scoped_acquire gil;
    return py::module::import(""torch.profiler.python_tracer"")
        .attr(""_prefix_regex"")()
        .cast<std::vector<std::string>>();
  }();
for (auto& it : std::get<CallType::PyCall>(state_)) {
std::string filename = it.second.filename_.str();
"
135,"ObserverBase,
)
from ..qconfig import (
    _partial_wrapper_equals,
float16_dynamic_qconfig,
float16_static_qconfig,
is_reuse_input_qconfig,
","ObserverBase,
)
from ..qconfig import (
    obs_or_fq_ctr_equals,
float16_dynamic_qconfig,
float16_static_qconfig,
is_reuse_input_qconfig,
"
136,"# Program the BackendIndex for the implicit dispatch entry from ufunc
if ufunc_inner_loop:
assert structured, ""ufunc must be structured""
for dispatch_key in UFUNC_DISPATCH_KEYS:
assert (
dispatch_key not in dispatch
","# Program the BackendIndex for the implicit dispatch entry from ufunc
if ufunc_inner_loop:
assert structured, ""ufunc must be structured""

            # Delay import ufunc here to avoid circular import issue
            # See: https://github.com/pytorch/pytorch/issues/81294
            import torchgen.api.ufunc as ufunc

for dispatch_key in UFUNC_DISPATCH_KEYS:
assert (
dispatch_key not in dispatch
"
137,"void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& lambd) {
AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.dtype(), ""hardshrink_cpu"", [&] {
auto lambd_val = lambd.to<scalar_t>();
cpu_kernel_vec(
iter,
[=](scalar_t self_val) {
return (self_val >= -lambd_val && self_val <= lambd_val) ? scalar_t(0)
: self_val;
},
        [=](Vectorized<scalar_t> self_val) {
          return ((self_val < -lambd_val) | (self_val > lambd_val)) & self_val;
});
});
}
","void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& lambd) {
AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.dtype(), ""hardshrink_cpu"", [&] {
auto lambd_val = lambd.to<scalar_t>();
    using Vec = Vectorized<scalar_t>;
cpu_kernel_vec(
iter,
[=](scalar_t self_val) {
return (self_val >= -lambd_val && self_val <= lambd_val) ? scalar_t(0)
: self_val;
},
        [=](Vec self_val) {
          return Vec::blendv(self_val, Vec(0), (self_val >= -lambd_val) & (self_val <= lambd_val));
});
});
}
"
138,"# hardshrink(x) = x if x > lambd
#               = x if x < -lambd
#               = 0 otherwise
    return refs.where(abs(a) > abs(lambd), a, 0)
@register_decomposition(torch.ops.aten.softshrink)
","# hardshrink(x) = x if x > lambd
#               = x if x < -lambd
#               = 0 otherwise
    return refs.where(refs.logical_and(a >= -lambd, a <= lambd), 0, a)
@register_decomposition(torch.ops.aten.softshrink)
"
139,"expanded_args, expanded_kwargs = conv_args_and_kwargs(kwarg_names, expanded_args_and_kwargs)
output = forward_helper(conv_fn, expanded_args, expanded_kwargs)
input, weight = expanded_args
ctx.conv_fn = conv_fn
","expanded_args, expanded_kwargs = conv_args_and_kwargs(kwarg_names, expanded_args_and_kwargs)
output = forward_helper(conv_fn, expanded_args, expanded_kwargs)
input, weight = expanded_args
        batched_dim_size = conv_picker(conv_fn, 3, 4, 5)
        if input.dim() != batched_dim_size:
            raise RuntimeError(f""Expanded Weights only support convolution with batched input, got {conv_fn} with an""
                               f""unbatched input of dim {input.dim()}, expected input of dim {batched_dim_size}"")
ctx.conv_fn = conv_fn
"
140,"mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
","mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
    virtual ~MergeThenSplitAxes() = default;

private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
"
141,"auto grad_v_data = grad_v.data_ptr<scalar_t>();
auto grad_g_data = grad_g.data_ptr<scalar_t>();
int num_threads = at::get_num_threads();
  Tensor buffer = at::empty({num_threads, N}, saved_norm.options()).zero_();
auto buffer_data = buffer.data_ptr<accscalar_t>();
// vertical parallel reduction
","auto grad_v_data = grad_v.data_ptr<scalar_t>();
auto grad_g_data = grad_g.data_ptr<scalar_t>();
  // the temp buffer will be used twice:
  // 1. vertical reduction from [M, N] to [T, N]
  // 2. store the intermediate data of `sum`, `a` and `b`,
  //    so need to make sure it has at least 3 rows
  //
int num_threads = at::get_num_threads();
  int K = std::max(3, num_threads);
  Tensor buffer = at::empty({K, N}, saved_norm.options()).zero_();
auto buffer_data = buffer.data_ptr<accscalar_t>();
// vertical parallel reduction
"
142,"mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
    virtual ~MergeThenSplitAxes() = default;

private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
","mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
"
143,"at::Tensor tmp_output;
{
at::AutoDispatchSkipFunctionalize guard;
    tmp_output = at::resize_functional(self_, size, memory_format);
}
auto itemsize = self.dtype().itemsize();
","at::Tensor tmp_output;
{
at::AutoDispatchSkipFunctionalize guard;
    tmp_output = at::resize(self_, size, memory_format);
}
auto itemsize = self.dtype().itemsize();
"
144,"return {Shape(grad_output.scalar_type(), grad_output.sizes().vec())};
}
std::vector<Shape> compute_shape_random_functional(
const at::Tensor& self,
c10::optional<at::Generator> generator) {
return {Shape(self.scalar_type(), self.sizes().vec())};
}
std::vector<Shape> compute_shape_random_functional(
const at::Tensor& self,
int64_t to,
c10::optional<at::Generator> generator) {
  return compute_shape_random_functional(self, generator);
}
std::vector<Shape> compute_shape_random_functional(
const at::Tensor& self,
int64_t from,
c10::optional<int64_t> to,
c10::optional<at::Generator> generator) {
  return compute_shape_random_functional(self, generator);
}
std::vector<Shape> compute_shape_relu(const at::Tensor& self) {
","return {Shape(grad_output.scalar_type(), grad_output.sizes().vec())};
}
std::vector<Shape> compute_shape_random(
const at::Tensor& self,
c10::optional<at::Generator> generator) {
return {Shape(self.scalar_type(), self.sizes().vec())};
}
std::vector<Shape> compute_shape_random(
const at::Tensor& self,
int64_t to,
c10::optional<at::Generator> generator) {
  return compute_shape_random(self, generator);
}
std::vector<Shape> compute_shape_random(
const at::Tensor& self,
int64_t from,
c10::optional<int64_t> to,
c10::optional<at::Generator> generator) {
  return compute_shape_random(self, generator);
}
std::vector<Shape> compute_shape_relu(const at::Tensor& self) {
"
145,"assert weight_qscheme in [None, torch.per_tensor_affine, torch.per_channel_affine], \
Exception(f""qscheme: {weight_qscheme} is not support in {self._get_name()}"")
if weight_qscheme is not None:
                self.register_buffer(
                    key + ""_scale"",
                    torch.tensor(weight_qparams[""scale""], dtype=torch.float, device=device))
                self.register_buffer(
                    key + ""_zero_point"",
                    torch.tensor(weight_qparams[""zero_point""], dtype=torch.int, device=device))
if weight_qscheme == torch.per_channel_affine:
                    self.register_buffer(
                        key + ""_axis"",
                        torch.tensor(weight_qparams[""axis""], dtype=torch.int, device=device))
else:
# added for TorchScriptability, not used
self.register_buffer(
","assert weight_qscheme in [None, torch.per_tensor_affine, torch.per_channel_affine], \
Exception(f""qscheme: {weight_qscheme} is not support in {self._get_name()}"")
if weight_qscheme is not None:
                scale = weight_qparams[""scale""]
                scale_tensor = scale.clone().detach() \
                    if isinstance(scale, torch.Tensor) else \
                    torch.tensor(scale, dtype=torch.float, device=device)
                self.register_buffer(key + ""_scale"", scale_tensor)
                zp = weight_qparams[""zero_point""]
                zp_tensor = zp.clone().detach() \
                    if isinstance(zp, torch.Tensor) else \
                    torch.tensor(zp, dtype=torch.int, device=device)
                self.register_buffer(key + ""_zero_point"", zp_tensor)
if weight_qscheme == torch.per_channel_affine:
                    axis = weight_qparams[""axis""]
                    axis_tensor = axis.clone().detach() \
                        if isinstance(axis, torch.Tensor) else \
                        torch.tensor(axis, dtype=torch.int, device=device)
                    self.register_buffer(key + ""_axis"", axis_tensor)
else:
# added for TorchScriptability, not used
self.register_buffer(
"
146,"T = TypeVar('T', bound='Union[_StorageBase, _TypedStorage]')
class _StorageBase(object):
_cdata: Any
    is_cuda: bool = False
is_sparse: bool = False
is_sparse_csr: bool = False
device: torch.device
","T = TypeVar('T', bound='Union[_StorageBase, _TypedStorage]')
class _StorageBase(object):
_cdata: Any
is_sparse: bool = False
is_sparse_csr: bool = False
device: torch.device
"
147,"true /* is_index_merge_rhs */, true /* is_last_axis_rfactor */);
}
private:
MergeAdjacentSingletonAxes(
AnalyzeViewTransformation* avt,
","true /* is_index_merge_rhs */, true /* is_last_axis_rfactor */);
}
    virtual ~MergeAdjacentSingletonAxes() = default;

private:
MergeAdjacentSingletonAxes(
AnalyzeViewTransformation* avt,
"
148,"repo.fetch(orig_ref, orig_ref)
repo._run_git(""rebase"", onto_branch, orig_ref)
os.environ[""OAUTH_TOKEN""] = os.environ[""GITHUB_TOKEN""]
with open('.ghstackrc', 'w+') as f:
f.write('[ghstack]\n' +
","repo.fetch(orig_ref, orig_ref)
repo._run_git(""rebase"", onto_branch, orig_ref)
    # steal the identity of the committer of the commit on the orig branch
    email = repo._run_git(""log"", orig_ref, ""--pretty=format:%ae"", ""-1"")
    name = repo._run_git(""log"", orig_ref, ""--pretty=format:%an"", ""-1"")
    repo._run_git(""config"", ""--global"", ""user.name"", name)
    repo._run_git(""config"", ""--global"", ""user.email"", email)

os.environ[""OAUTH_TOKEN""] = os.environ[""GITHUB_TOKEN""]
with open('.ghstackrc', 'w+') as f:
f.write('[ghstack]\n' +
"
149,"activation_is_memoryless)
from torch.nn.utils.parametrize import type_before_parametrizations
def is_activation_post_process(module):
return (isinstance(module, torch.ao.quantization.ObserverBase) or
isinstance(module, torch.ao.quantization.FakeQuantizeBase))
","activation_is_memoryless)
from torch.nn.utils.parametrize import type_before_parametrizations
_DEFAULT_CUSTOM_CONFIG_DICT = {
    'float_to_observed_custom_module_class': {
        nn.LSTM: nn.quantizable.LSTM,
        nn.MultiheadAttention: nn.quantizable.MultiheadAttention,
    },
    'observed_to_quantized_custom_module_class': {
        nn.quantizable.LSTM: nn.quantized.LSTM,
        nn.quantizable.MultiheadAttention: nn.quantized.MultiheadAttention,
    }
}

def get_default_custom_config_dict():
    r""""""Defines the default custom config dict.
    """"""
    return _DEFAULT_CUSTOM_CONFIG_DICT

def is_activation_post_process(module):
return (isinstance(module, torch.ao.quantization.ObserverBase) or
isinstance(module, torch.ao.quantization.FakeQuantizeBase))
"
150,"mapping = get_default_static_quant_reference_module_mappings() if is_reference \
else get_default_static_quant_module_mappings()
if convert_custom_config_dict is None:
        convert_custom_config_dict = {}
custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", {})
if not inplace:
","mapping = get_default_static_quant_reference_module_mappings() if is_reference \
else get_default_static_quant_module_mappings()
if convert_custom_config_dict is None:
        convert_custom_config_dict = get_default_custom_config_dict()
custom_module_class_mapping = convert_custom_config_dict.get(""observed_to_quantized_custom_module_class"", {})
if not inplace:
"
151,"instance to the hook as the first parameter.
""""""
handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)
        if with_module:
            hook = functools.partial(hook, self)
        self._load_state_dict_pre_hooks[handle.id] = hook
return handle
def register_load_state_dict_post_hook(self, hook):
","instance to the hook as the first parameter.
""""""
handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)
        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(hook, self if with_module else None)
return handle
def register_load_state_dict_post_hook(self, hook):
"
152,"import types
import torch.jit
import torch._utils_internal as torch_utils_internal
# Query `hasattr` only once.
_SET_GLOBAL_FLAGS = hasattr(sys, 'getdlopenflags') and hasattr(sys, 'setdlopenflags')
","import types
import torch.jit
from torch import _utils_internal
# Query `hasattr` only once.
_SET_GLOBAL_FLAGS = hasattr(sys, 'getdlopenflags') and hasattr(sys, 'setdlopenflags')
"
153,"(torch.nn.functional, torch._refs.nn.functional),
(torch.special, torch._refs.special),
]
    r = {}
for mod_torch, mod_refs in modules:
for s in mod_refs.__all__:  # type: ignore[attr-defined]
r[mod_torch.__dict__.get(s)] = mod_refs.__dict__.get(s)
","(torch.nn.functional, torch._refs.nn.functional),
(torch.special, torch._refs.special),
]
    r: Dict[Any, Any] = {
        torch.Tensor.__invert__: torch._refs.bitwise_not,
        torch.Tensor.__xor__: torch._refs.bitwise_xor,
        torch.Tensor.__and__: torch._refs.bitwise_and,
        torch.Tensor.__or__: torch._refs.bitwise_or,
        torch.Tensor.__eq__: torch._refs.eq,
    }
for mod_torch, mod_refs in modules:
for s in mod_refs.__all__:  # type: ignore[attr-defined]
r[mod_torch.__dict__.get(s)] = mod_refs.__dict__.get(s)
"
154,"// Computes equality closeness
Tensor close = self == other;
if (equal_nan && (self.is_floating_point() || self.is_complex())) {
close.__ior__(self.isnan().__iand__(other.isnan()));
}
// In case of zero tolerances the closeness inequality degenerates to an equality check.
","// Computes equality closeness
Tensor close = self == other;
if (equal_nan && (self.is_floating_point() || self.is_complex())) {
    // For CompositeCompliance, if `other` is a CCT and `self` is a regular Tensor,
    // then we can't perform inplace op into `self` with `other`.
    // NOTE: Inplacing into `close` is fine because it is generated from
    // out-of-place with args `self` and `other`. So if either of them is
    // a CCT then `close` will also be a `CCT`.
    if (isTensorSubclassLike(other)) {
      close.__ior__(self.isnan().bitwise_and(other.isnan()));
    } else {
close.__ior__(self.isnan().__iand__(other.isnan()));
    }
}
// In case of zero tolerances the closeness inequality degenerates to an equality check.
"
155,"const std::string& domain_name,
const std::string& func_name) {
const auto& func_scope = func_ctx.scope_key_;
  const auto& func_scope_ctx = func_ctx.scope_ctxs_[func_scope];
GRAPH_DEBUG(
""Create and insert local function for scope: "",
func_scope->namesFromRoot());
","const std::string& domain_name,
const std::string& func_name) {
const auto& func_scope = func_ctx.scope_key_;
GRAPH_DEBUG(
""Create and insert local function for scope: "",
func_scope->namesFromRoot());
"
156,"}
return name;
}
} // namespace
const std::vector<at::Tensor> Module::parameters() const {
","}
return name;
}
#endif

} // namespace
const std::vector<at::Tensor> Module::parameters() const {
"
157,"std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
","std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::Backend get_backend(bool is_cuda, bool is_sparse) {
  if (is_cuda) {
    if (is_sparse){
      return at::Backend::SparseCUDA;
    } else {
      return at::Backend::CUDA;
    }
  } else {
    if (is_sparse){
      return at::Backend::SparseCPU;
    } else {
      return at::Backend::CPU;
    }
  }
}

at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
"
158,"return outnames;
}
// tensor_dotted_dim and other_dotted_dim are the dimensions of the two
// tensors that we contract together. Usually other_dotted_dim is 0
// and tensor_dotted_dim is the last dim of tensor, but there are some special
// cases like einsum and tensordot where one can contract arbitrary dims.
// NOLINTNEXTLINE(clang-diagnostic-unused-function)
static std::vector<Dimname> compute_dot_product_outnames(
    DimnameList tensor_names,
    int64_t tensor_dotted_dim,
    DimnameList other_names,
    int64_t other_dotted_dim) {
  int64_t num_outnames = tensor_names.size() + other_names.size() - 2;
  if (num_outnames == 0) {
    return {};
  }
  std::vector<Dimname> outnames(num_outnames, Dimname::wildcard());
  int64_t index = 0;
  for (const auto j : c10::irange(static_cast<int64_t>(tensor_names.size()))) {
    if (j == tensor_dotted_dim) continue;
    outnames[index++] = tensor_names[j];
  }
  for (const auto j : c10::irange(static_cast<int64_t>(other_names.size()))) {
    if (j == other_dotted_dim) continue;
    outnames[index++] = other_names[j];
  }
  return outnames;
}

static void check_feature_names_are_distinct(
DimnameList self_names,
DimnameList other_names,
","return outnames;
}
static void check_feature_names_are_distinct(
DimnameList self_names,
DimnameList other_names,
"
159,"}
}
// Copy all content from reader to stringstream
void get_model_stream(PyTorchStreamReader& reader, std::stringstream& out) {
  auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
    out.write(static_cast<const char*>(buf), nbytes);
    return !out ? 0 : nbytes;
  };
  PyTorchStreamWriter writer(writer_func);

  selective_copy(
      reader,
      writer,
      std::unordered_set<std::string>(),
      std::unordered_set<std::string>());
}

// The write_archive_current function is used for bytecode from version v5 to
// v7 (the latest bytecode version). pre-v5 we serialized things differently.
// This write archive function may change in export_module.cpp, however we don't
","}
}
// The write_archive_current function is used for bytecode from version v5 to
// v7 (the latest bytecode version). pre-v5 we serialized things differently.
// This write archive function may change in export_module.cpp, however we don't
"
160,"return f""https://github.com/{suffix_str}""
def merge_on_green(pr_num: int, repo: GitRepo,
                   dry_run: bool = False,
                   mandatory_only: bool = False,
                   timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
start_time = time.time()
last_exception = ''
elapsed_time = 0.0
while elapsed_time < timeout_minutes * 60:
current_time = time.time()
elapsed_time = current_time - start_time

print(f""Attempting merge of https://github.com/{org}/{project}/pull/{pr_num} ({elapsed_time / 60} minutes elapsed)"")
pr = GitHubPR(org, project, pr_num)
try:
find_matching_merge_rule(pr, repo)
pending = pr_get_pending_checks(pr)
failing = pr_get_failed_checks(pr)
            if not mandatory_only and len(failing) > 0:
raise RuntimeError(f""{len(failing)} additional jobs have failed, first few of them are: "" +
' ,'.join(f""[{x[0]}]({x[1]})"" for x in failing[:5]))
            if not mandatory_only and len(pending) > 0:
raise MandatoryChecksMissingError(f""Still waiting for {len(pending)} additional jobs to finish, "" +
f""first few of them are: {' ,'.join(x[0] for x in pending[:5])}"")
            return pr.merge_into(repo, dry_run=dry_run)
except MandatoryChecksMissingError as ex:
last_exception = str(ex)
print(f""Merge of https://github.com/{org}/{project}/pull/{pr_num} failed due to: {ex}. Retrying in 5 min"")
","return f""https://github.com/{suffix_str}""
def merge(pr_num: int, repo: GitRepo,
          dry_run: bool = False,
          force: bool = False,
          comment_id: Optional[int] = None,
          mandatory_only: bool = False,
          on_green: bool = False,
          timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
    if force:
        pr = GitHubPR(org, project, pr_num)
        pr.merge_into(repo, dry_run=dry_run, force=force, comment_id=comment_id)

start_time = time.time()
last_exception = ''
elapsed_time = 0.0
while elapsed_time < timeout_minutes * 60:
current_time = time.time()
elapsed_time = current_time - start_time
print(f""Attempting merge of https://github.com/{org}/{project}/pull/{pr_num} ({elapsed_time / 60} minutes elapsed)"")
pr = GitHubPR(org, project, pr_num)
try:
find_matching_merge_rule(pr, repo)
pending = pr_get_pending_checks(pr)
failing = pr_get_failed_checks(pr)
            if (not mandatory_only and on_green) and len(failing) > 0:
raise RuntimeError(f""{len(failing)} additional jobs have failed, first few of them are: "" +
' ,'.join(f""[{x[0]}]({x[1]})"" for x in failing[:5]))
            if (not mandatory_only and on_green) and len(pending) > 0:
raise MandatoryChecksMissingError(f""Still waiting for {len(pending)} additional jobs to finish, "" +
f""first few of them are: {' ,'.join(x[0] for x in pending[:5])}"")
            return pr.merge_into(repo, dry_run=dry_run, force=force, comment_id=comment_id)
except MandatoryChecksMissingError as ex:
last_exception = str(ex)
print(f""Merge of https://github.com/{org}/{project}/pull/{pr_num} failed due to: {ex}. Retrying in 5 min"")
"
161,"return;
}
  CAFFE_ENFORCE(op_id >= 0 && op_id < op_start_times_run_.size());
op_start_times_run_[op_id] = timer_.MilliSeconds();
}
","return;
}
  CAFFE_ENFORCE(op_id < op_start_times_run_.size());
op_start_times_run_[op_id] = timer_.MilliSeconds();
}
"
162,""" producer: "",
producer_);
TORCH_INTERNAL_ASSERT(
      reference_position_ >= 0 && reference_position_ <= reference_->nDims(),
""Invalid computeAt axis, received "",
reference_position_,
"" but should be > -"",
",""" producer: "",
producer_);
TORCH_INTERNAL_ASSERT(
      reference_position_ <= reference_->nDims(),
""Invalid computeAt axis, received "",
reference_position_,
"" but should be > -"",
"
163,"out_args.push_back(stack.back());
stack.pop_back();
}
        size_t start_index = num_specified_args.value() - out_args.size();
TORCH_CHECK(
            start_index >= 0,
""The number of output arguments is: "",
out_args.size(),
"", which is more then the number of specified arguments: "",
num_specified_args.value());
for (size_t i = start_index; i < (args.size() - out_args.size()); ++i) {
TORCH_CHECK(
args[i].default_value().has_value(),
","out_args.push_back(stack.back());
stack.pop_back();
}
TORCH_CHECK(
            num_specified_args.value() >= out_args.size(),
""The number of output arguments is: "",
out_args.size(),
"", which is more then the number of specified arguments: "",
num_specified_args.value());
        size_t start_index = num_specified_args.value() - out_args.size();
for (size_t i = start_index; i < (args.size() - out_args.size()); ++i) {
TORCH_CHECK(
args[i].default_value().has_value(),
"
164,"}
Tensor linalg_matrix_rank(const Tensor& input, const optional<Tensor>& atol, const optional<Tensor>& rtol, bool hermitian) {
  Tensor result = at::empty({0}, input.options().dtype(ScalarType::Long));
  result = at::linalg_matrix_rank_outf(input, atol, rtol, hermitian, result);
  return result;
}
Tensor linalg_matrix_rank(const Tensor& input, optional<double> atol, optional<double> rtol, bool hermitian) {
  Tensor result = at::empty({0}, input.options().dtype(ScalarType::Long));
  result = at::linalg_matrix_rank_outf(input, atol, rtol, hermitian, result);
  return result;
}
Tensor& linalg_matrix_rank_out(const Tensor& input, const Tensor& tol, bool hermitian, Tensor& result) {
","}
Tensor linalg_matrix_rank(const Tensor& input, const optional<Tensor>& atol, const optional<Tensor>& rtol, bool hermitian) {
  auto result = get_matrix_rank_result_tensor(input);
  return matrix_rank_impl(input, atol, rtol, hermitian, result);
}
Tensor linalg_matrix_rank(const Tensor& input, optional<double> atol, optional<double> rtol, bool hermitian) {
  auto result = get_matrix_rank_result_tensor(input);

  Tensor atol_tensor, rtol_tensor;
  std::tie(atol_tensor, rtol_tensor) = get_atol_rtol(input, atol, rtol);

  return matrix_rank_impl(input, atol_tensor, rtol_tensor, hermitian, result);
}
Tensor& linalg_matrix_rank_out(const Tensor& input, const Tensor& tol, bool hermitian, Tensor& result) {
"
165,"m.impl(""unfold"", unfold_batching_rule);
m.impl(""unsqueeze"", unsqueeze_batching_rule);
m.impl(""view"", view_batching_rule);
m.impl(""view_as"", native::view_as); // composite wrt autograd
// clamp operations
","m.impl(""unfold"", unfold_batching_rule);
m.impl(""unsqueeze"", unsqueeze_batching_rule);
m.impl(""view"", view_batching_rule);
  // From the perspective of vmap, view and _unsafe_view are the same.
  // (they are only different w.r.t. autograd).
  m.impl(""_unsafe_view"", view_batching_rule);
m.impl(""view_as"", native::view_as); // composite wrt autograd
// clamp operations
"
166,"env_callable=lambda fn: {
""unboxed_ops"": [ComputeCodegenUnboxedKernels(selector)(fn)]
},
        num_shards=1 if selected_op_num < sharding_threshold else 10,
sharded_keys={""unboxed_ops""},
)
","env_callable=lambda fn: {
""unboxed_ops"": [ComputeCodegenUnboxedKernels(selector)(fn)]
},
        num_shards=10,
sharded_keys={""unboxed_ops""},
)
"
167,"Tensor hinge_embedding_loss(const Tensor& self, const Tensor& target, double margin, int64_t reduction) {
auto zeros = at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
  auto margin_clamp = (margin - self).clamp_min_(0);
auto output_margin = at::where(target != 1, margin_clamp, zeros);
auto output_self = at::where(target != -1, self, zeros);
auto output = output_margin + output_self;
","Tensor hinge_embedding_loss(const Tensor& self, const Tensor& target, double margin, int64_t reduction) {
auto zeros = at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
  auto margin_diff = (margin - self);
  // For Composite Compliance,
  // In Forward AD, if `margin_diff` is a CCT but its tangent isn't,
  // using inplace clamp_min doesn't work because we end up writing
  // the CCT in-place to the tangent
  auto margin_clamp = (margin_diff._fw_grad(/*level*/ 0).defined() &&
                       isTensorSubclassLike(margin_diff))
      ? margin_diff.clamp_min(0)
      : margin_diff.clamp_min_(0);
auto output_margin = at::where(target != 1, margin_clamp, zeros);
auto output_self = at::where(target != -1, self, zeros);
auto output = output_margin + output_self;
"
168,"Raises:
ImportError: If the package will use a disallowed module.
""""""
self.zip_reader: Any
if isinstance(file_or_buffer, torch._C.PyTorchFileReader):
self.filename = ""<pytorch_file_reader>""
","Raises:
ImportError: If the package will use a disallowed module.
""""""
        torch._C._log_api_usage_once(""torch.package.PackageImporter"")

self.zip_reader: Any
if isinstance(file_or_buffer, torch._C.PyTorchFileReader):
self.filename = ""<pytorch_file_reader>""
"
169,"cur_node = subgraph_a.end_node
while cur_node != subgraph_a.start_node:
nodes.append(cur_node)
        cur_node = _get_normalized_nth_input(cur_node, gm_a, 0)  # type: ignore[assignment]
nodes.append(cur_node)
nodes.reverse()
","cur_node = subgraph_a.end_node
while cur_node != subgraph_a.start_node:
nodes.append(cur_node)
        cur_node = get_normalized_nth_input(cur_node, gm_a, 0)  # type: ignore[assignment]
nodes.append(cur_node)
nodes.reverse()
"
170,"const auto& b_ih = params_cpu[i * 4 + 2];
const auto& b_hh = params_cpu[i * 4 + 3];
    const auto&  w_i_rzn = w_ih.split(h_in);
    const auto&  w_h_rzn = w_hh.split(h_in);
    const auto&  b_i_rzn = b_ih.split(h_in);
    const auto&  b_h_rzn = b_hh.split(h_in);
const auto&  w_ir = w_i_rzn[0];
const auto&  w_iz = w_i_rzn[1];
","const auto& b_ih = params_cpu[i * 4 + 2];
const auto& b_hh = params_cpu[i * 4 + 3];
    const auto&  w_i_rzn = w_ih.split(hidden_size);
    const auto&  w_h_rzn = w_hh.split(hidden_size);
    const auto&  b_i_rzn = b_ih.split(hidden_size);
    const auto&  b_h_rzn = b_hh.split(hidden_size);
const auto&  w_ir = w_i_rzn[0];
const auto&  w_iz = w_i_rzn[1];
"
171,"env_callable=lambda fn: {
""definitions"": [ComputeUnboxingFunctions(Target.DEFINITION, selector)(fn)]
},
        num_shards=5,
sharded_keys={""definitions""},
)
cpu_fm.write(
","env_callable=lambda fn: {
""definitions"": [ComputeUnboxingFunctions(Target.DEFINITION, selector)(fn)]
},
        num_shards=1 if selected_op_num < sharding_threshold else 5,
sharded_keys={""definitions""},
)
cpu_fm.write(
"
172,"self.iterator = iterator
self.source_dp = source_dp
self.iterator_id = iterator_id
def __iter__(self):
return self
","self.iterator = iterator
self.source_dp = source_dp
self.iterator_id = iterator_id
            self._profiler_enabled = torch.autograd._profiler_enabled()
def __iter__(self):
return self
"
173,"def _wrap_torch_function(f):
@functools.wraps(f)
def wrapped(self, *args, **kwargs):
inner = getattr(self, ""inner"", None)
with enable_torch_function_mode(inner):
","def _wrap_torch_function(f):
@functools.wraps(f)
def wrapped(self, *args, **kwargs):
        if isinstance(f, classmethod):
            raise RuntimeError(""TorchFunctionMode's torch_function function "" +
                               ""should be a normal method not a class method"")
inner = getattr(self, ""inner"", None)
with enable_torch_function_mode(inner):
"
174,"add_docstr(torch.as_strided,
r""""""
as_strided(input, size, stride, storage_offset=0) -> Tensor
Create a view of an existing `torch.Tensor` :attr:`input` with specified
:attr:`size`, :attr:`stride` and :attr:`storage_offset`.
","add_docstr(torch.as_strided,
r""""""
as_strided(input, size, stride, storage_offset=None) -> Tensor
Create a view of an existing `torch.Tensor` :attr:`input` with specified
:attr:`size`, :attr:`stride` and :attr:`storage_offset`.
"
175,"import copy
import torch
from torch.distributed._shard.sharded_tensor import (
    sharded_op_impl,
Shard,
ShardedTensor,
)
","import copy
import torch
from torch.distributed._shard.sharded_tensor import (
    _sharded_op_impl,
Shard,
ShardedTensor,
)
"
176,") -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
input_shape = input.shape
input_ndim = input.dim()
axis = input_ndim - len(normalized_shape)
inner_dims = input_shape[axis:]
",") -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
input_shape = input.shape
input_ndim = input.dim()
    computation_dtype = utils.get_computation_dtype(input.dtype)
    grad_out_cast, input_cast, weight_cast, bias_cast = [
        x.to(computation_dtype) if x is not None else x
        for x in (grad_out, input, weight, bias)
    ]
    assert grad_out_cast is not None
axis = input_ndim - len(normalized_shape)
inner_dims = input_shape[axis:]
"
177,"# This doesn't strictly match eager's numerics, which accumulates var sum and then directly applies the correction
# But... that would require re-implementing var here, for negligible numerics gain on a tensor whose
# numerics probably don't matter.
            unbiased_var = biased_var * (n / (n - 1))
running_var.copy_(momentum * unbiased_var + (1 - momentum) * running_var)
        mean = save_mean
        invstd = save_invstd
else:
assert running_mean is not None and running_var is not None
mean = running_mean
invstd = 1 / (torch.sqrt(running_var + eps))
# Very annoying inconsistency where CPU and CUDA give different shapes
        if input.device.type == ""cuda"":
save_mean = running_mean
            save_invstd = invstd
else:
save_mean = input.new_zeros((0,))
            save_invstd = input.new_zeros((0,))
if weight is None:
weight = input.new_ones(())
","# This doesn't strictly match eager's numerics, which accumulates var sum and then directly applies the correction
# But... that would require re-implementing var here, for negligible numerics gain on a tensor whose
# numerics probably don't matter.
            unbiased_var = torch.var(input, reduction_dims, unbiased=False) * (n / (n - 1))
running_var.copy_(momentum * unbiased_var + (1 - momentum) * running_var)
else:
assert running_mean is not None and running_var is not None
        running_mean = running_mean.to(dtype=computation_dtype)
        running_var = running_var.to(dtype=computation_dtype)
mean = running_mean
invstd = 1 / (torch.sqrt(running_var + eps))
# Very annoying inconsistency where CPU and CUDA give different shapes
        if input.device.type != ""cpu"":
save_mean = running_mean
            save_rstd = invstd
else:
save_mean = input.new_zeros((0,))
            save_rstd = input.new_zeros((0,))
        mean = _unsqueeze_to_dim(mean, input.dim() - 1)
        invstd = _unsqueeze_to_dim(invstd, input.dim() - 1)
        output = ((input - mean) * invstd)
if weight is None:
weight = input.new_ones(())
"
178,"if bias is None:
bias = input.new_zeros(())
    mean = _unsqueeze_to_dim(mean, input.dim() - 1)
    invstd = _unsqueeze_to_dim(invstd, input.dim() - 1)
weight = _unsqueeze_to_dim(weight, input.dim() - 1)
bias = _unsqueeze_to_dim(bias, input.dim() - 1)
    output = ((input - mean) * invstd) * weight + bias
    return output, save_mean, save_invstd
@register_decomposition(aten.clamp_min)
","if bias is None:
bias = input.new_zeros(())
weight = _unsqueeze_to_dim(weight, input.dim() - 1)
bias = _unsqueeze_to_dim(bias, input.dim() - 1)
    output = output * weight + bias
    if input.device.type == 'cpu':
        save_mean = save_mean.to(dtype=input.dtype)
        save_rstd = save_rstd.to(dtype=input.dtype)
    return output.to(dtype=input.dtype), save_mean, save_rstd
@register_decomposition(aten.clamp_min)
"
179,"func (Callable): registered implementation for sharded op for
``__torch_function__`` dispatch.
""""""
    @_sharded_op_impl(op)
@_sharded_op_common(op, early_stop_func, extra_check)
def sharded_tensor_op_on_local_shards(types, args=(), kwargs=None, pg=None):
st = args[0]
","func (Callable): registered implementation for sharded op for
``__torch_function__`` dispatch.
""""""
    @sharded_op_impl(op)
@_sharded_op_common(op, early_stop_func, extra_check)
def sharded_tensor_op_on_local_shards(types, args=(), kwargs=None, pg=None):
st = args[0]
"
180,"import torch
from torch.distributed._shard.sharded_tensor import (
    _sharded_op_impl,
ShardedTensor,
)
from torch.distributed._shard.sharding_spec import ChunkShardingSpec
def register_chunk_op(op):
    @_sharded_op_impl(op)
def sharded_chunk(types, args=(), kwargs=None, pg=None):
""""""
Handles ``__torch_function__`` dispatch for the chunk op.
","import torch
from torch.distributed._shard.sharded_tensor import (
    sharded_op_impl,
ShardedTensor,
)
from torch.distributed._shard.sharding_spec import ChunkShardingSpec
def register_chunk_op(op):
    @sharded_op_impl(op)
def sharded_chunk(types, args=(), kwargs=None, pg=None):
""""""
Handles ``__torch_function__`` dispatch for the chunk op.
"
181,"cast,
)
import copy
from functools import reduce
import weakref
import threading
import torch
","cast,
)
import copy
import weakref
import math
import threading
import torch
"
182,"Default: ``None``
""""""
def shard_size(shard_md):
            return reduce((lambda x, y: x * y), shard_md.shard_sizes)  # type: ignore[attr-defined]
rank = dist.get_rank(self._process_group)
full_size = self.metadata().size
","Default: ``None``
""""""
def shard_size(shard_md):
            return math.prod(shard_md.shard_sizes)  # type: ignore[attr-defined]
rank = dist.get_rank(self._process_group)
full_size = self.metadata().size
"
183,"@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
            # Dispatch to custom user provided op first if it exists.
            if func in _CUSTOM_SHARDED_OPS:
                return _CUSTOM_SHARDED_OPS[func](types, args, kwargs, st._process_group)

# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
","@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
"
184,"""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
","""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
        self.enable_nested_tensor = enable_nested_tensor
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
"
185,"if (is_concretized) {
// Keep track of all the origin domains as concretized
for (auto origin : producer_origins) {
            // concretized_root_domains_.insert(origin);
            markAsConcretized(origin);
}
} else {
// Not concretized yet. Propagate forward the origin info.
","if (is_concretized) {
// Keep track of all the origin domains as concretized
for (auto origin : producer_origins) {
            markAsConcretized(origin, c_id);
}
} else {
// Not concretized yet. Propagate forward the origin info.
"
186,"""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
        self.enable_nested_tensor = enable_nested_tensor
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
","""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
"
187,"# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf
        if (self.is_fastpath() and src.dim() == 3 and
((src_mask is None and src_key_padding_mask is None)
if src.is_nested
else (src_mask is None or src_key_padding_mask is None))):
","# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf
        if (not self.norm_first and not self.training and
            self.self_attn.batch_first and src.dim() == 3 and self.self_attn._qkv_same_embed_dim and
            self.activation_relu_or_gelu and self.norm1.eps == self.norm2.eps and
((src_mask is None and src_key_padding_mask is None)
if src.is_nested
else (src_mask is None or src_key_padding_mask is None))):
"
188,"""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
","""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
        self.enable_nested_tensor = enable_nested_tensor
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
"
189,"# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf
        if (not self.norm_first and not self.training and
            self.self_attn.batch_first and src.dim() == 3 and self.self_attn._qkv_same_embed_dim and
            self.activation_relu_or_gelu and self.norm1.eps == self.norm2.eps and
((src_mask is None and src_key_padding_mask is None)
if src.is_nested
else (src_mask is None or src_key_padding_mask is None))):
","# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf
        if (self.is_fastpath() and src.dim() == 3 and
((src_mask is None and src_key_padding_mask is None)
if src.is_nested
else (src_mask is None or src_key_padding_mask is None))):
"
190,"
import io
from typing import Any, Dict, List, Tuple
import torch
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
ShardedTensorMetadata
","import io
from typing import Any, Dict, List, Tuple, Optional, cast
import torch
import torch.distributed as dist
from torch.distributed._shard.sharded_tensor import (
ShardedTensor,
ShardedTensorMetadata
"
191,"If using the `process_group` argument, make sure that only its ranks
call `save_state_dict` and that all data in state_dict belong to it.
Args:
state_dict (Dict[str, Any]) : A state_dict
storage_writer (StorageWriter): Instance of StorageWrite use to perform writes.
process_group (ProcessGroup): ProcessGroup to be used for cross-rank synchronization
Example:
>>> my_model = MyModule()
","If using the `process_group` argument, make sure that only its ranks
call `save_state_dict` and that all data in state_dict belong to it.
    This function can be used to save a state_dict with an intialized process
    group by passing ``no_dist=True``. This can be used to produce a checkpoint
    that can consumed by load_state_dict is a SPMD fashion.

Args:
state_dict (Dict[str, Any]) : A state_dict
storage_writer (StorageWriter): Instance of StorageWrite use to perform writes.
process_group (ProcessGroup): ProcessGroup to be used for cross-rank synchronization
        coordinator_rank (int): Rank to use to coordinate the checkpoint, rank0 is used by default
        no_dist (bool): Don't attempt to save in SPMD style. Default to False
Example:
>>> my_model = MyModule()
"
192,"def __new__(cls, data=None, requires_grad=True):
if data is None:
data = torch.empty(0)
        if type(data) is torch.Tensor:
# For ease of BC maintenance, keep this path for standard Tensor.
# Eventually (tm), we should change the behavior for standard Tensor to match.
return torch.Tensor._make_subclass(cls, data, requires_grad)
# Path for custom tensors: set a flag on the instance to indicate parameter-ness.
t = data.detach().requires_grad_(requires_grad)
t._is_param = True
return t
","def __new__(cls, data=None, requires_grad=True):
if data is None:
data = torch.empty(0)
        if type(data) is torch.Tensor or type(data) is Parameter:
# For ease of BC maintenance, keep this path for standard Tensor.
# Eventually (tm), we should change the behavior for standard Tensor to match.
return torch.Tensor._make_subclass(cls, data, requires_grad)
# Path for custom tensors: set a flag on the instance to indicate parameter-ness.
t = data.detach().requires_grad_(requires_grad)
        if type(t) is not type(data):
            raise RuntimeError(f""Creating a Parameter from an instance of type {type(data).__name__} ""
                               ""requires that detach() returns an instance of the same type, but return ""
                               f""type {type(t).__name__} was found instead. To use the type as a ""
                               ""Parameter, please correct the detach() semantics defined by ""
                               ""its __torch_dispatch__() implementation."")
t._is_param = True
return t
"
193,"stride[0], // vertical stride
stride[1])); // horizontal stride
  auto dataType = getCudnnDataType(input);
float one{1};
float zero{0.0};
TensorDescriptor xDesc;
","stride[0], // vertical stride
stride[1])); // horizontal stride
float one{1};
float zero{0.0};
TensorDescriptor xDesc;
"
194,"self.mod = mod
self.checkpoint_impl = checkpoint_impl
self.offload_to_cpu = offload_to_cpu
def forward(self, *args, **kwargs):
offload_mgr = save_on_cpu(pin_memory=True) if self.offload_to_cpu else suppress()
","self.mod = mod
self.checkpoint_impl = checkpoint_impl
self.offload_to_cpu = offload_to_cpu
        # state_dict post hook to remove prefix to allow loading into a
        # non-checkpoint wrapped module.
        self._register_state_dict_hook(self._post_state_dict_hook)
        # load_state_dict pre-hook to allow loading back into
        # checkpoint-wrapped module.
        self._register_load_state_dict_pre_hook(
            self._pre_load_state_dict_hook, with_module=True
        )
def forward(self, *args, **kwargs):
offload_mgr = save_on_cpu(pin_memory=True) if self.offload_to_cpu else suppress()
"
195,"return resize_named_tensor_(self, size, optional_memory_format);
}
auto* self_ = self.unsafeGetTensorImpl();
  //std::cout << ""resize mps  size "" << size << std::endl;
resize_impl_mps_(self_, size, /*strides=*/c10::nullopt);
if (optional_memory_format.has_value()) {
auto memory_format =
","return resize_named_tensor_(self, size, optional_memory_format);
}
auto* self_ = self.unsafeGetTensorImpl();
resize_impl_mps_(self_, size, /*strides=*/c10::nullopt);
if (optional_memory_format.has_value()) {
auto memory_format =
"
196,"parseKind(kind),
std::move(name),
std::string(dispatch) == """" ? c10::nullopt : c10::make_optional(c10::parseDispatchKey(dispatch)),
      ""<unknown>"", // temporary workaround
linenum);
END_HANDLE_TH_ERRORS_PYBIND
}, """", py::arg(""kind""), py::arg(""name""), py::arg(""dispatch""), py::arg(""file"")=""/dev/null"", py::arg(""linenum"")=0)
","parseKind(kind),
std::move(name),
std::string(dispatch) == """" ? c10::nullopt : c10::make_optional(c10::parseDispatchKey(dispatch)),
      ""/dev/null"", // temporary workaround
linenum);
END_HANDLE_TH_ERRORS_PYBIND
}, """", py::arg(""kind""), py::arg(""name""), py::arg(""dispatch""), py::arg(""file"")=""/dev/null"", py::arg(""linenum"")=0)
"
197,"// For now I'm retroactively setting this in functorch,
// but once Open Multiple Dispatch lands we should be able to calculate this in core.
level_ = -1;
  // shallow_copy_from overwrites the storage and dispatch keyset...
  auto functional_storage = storage_;
  shallow_copy_from(value_.getIntrusivePtr());
  storage_ = functional_storage;
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
// All of the keys corresponding to functorch transforms should not be copied over.
","// For now I'm retroactively setting this in functorch,
// but once Open Multiple Dispatch lands we should be able to calculate this in core.
level_ = -1;
  // mirror all of the generic tensor metadata onto the wrapper
  copy_generic_tensor_metadata(value_.getIntrusivePtr().get(), this);
  refresh_numel();
  refresh_contiguous();
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
// All of the keys corresponding to functorch transforms should not be copied over.
"
198,"#include <torch/library.h>
#include <c10/util/irange.h>
#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/NativeFunctions.h>
#else
#include <ATen/ops/to_native.h>
#endif

namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
","#include <torch/library.h>
#include <c10/util/irange.h>
namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
"
199,"func(Callable): The op to override (ex: torch.bmm)
""""""
def decorator_sharded_func(wrapped_func):
        _register_custom_op(sharding_spec_class, func, wrapped_func)
@functools.wraps(wrapped_func)
        def wrapper(*args, **kwargs):
            return wrapped_func(*args, **kwargs)
return wrapper
return decorator_sharded_func
","func(Callable): The op to override (ex: torch.bmm)
""""""
def decorator_sharded_func(wrapped_func):
        from torch.distributed._shard.sharded_tensor._ops._common import _basic_validation
@functools.wraps(wrapped_func)
        def wrapper(types, args, kwargs):
            _basic_validation(func, args, kwargs)
            return wrapped_func(types, args, kwargs)

        _register_custom_op(sharding_spec_class, func, wrapper)
return wrapper

return decorator_sharded_func
"
200,"that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
","that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
            Note that if ``mixed_precision`` is enabled for FSDP model that
            contains ``BatchNorm`` with ``auto_wrap_policy``, FSDP will take
            care to disable mixed precision for ``BatchNorm`` units by wrapping
            them separately in their own FSDP unit with ``mixed_precision=None``.
            This is done because several ``BatchNorm`` kernels do not implement
            reduced type support at the moment. If individually wrapping the model,
            users must take care to set ``mixed_precision=None`` for
            ``BatchNorm`` units.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
"
201,"""""""
return True
def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
","""""""
return True

def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
"
202,"# if not recursing, decide whether we should wrap for the leaf node or reminder
return isinstance(module, tuple(transformer_layer_cls))
def _wrap_batchnorm_individually(
    module: nn.Module,
    recurse: bool,
    *args,
    **kwargs,
) -> bool:
    """"""
    A policy that wraps ``BatchNorm`` instances in their own FSDP unit.
    """"""
    if recurse:
        # always recurse
        return True
    else:
        # if not recursing, decide whether we should wrap based on whether it is a
        # BN layer or not.
        return isinstance(module, _BatchNorm)

def _or_policy(
    module: nn.Module,
    recurse: bool,
    unwrapped_params: int,
    policies,
) -> bool:
    """"""
    A policy that wraps ``module`` if any policy in the passed in iterable of
    ``policies`` returns ``True``.
    """"""
    return any(
        policy(module, recurse, unwrapped_params) for policy in policies
    )

def size_based_auto_wrap_policy(
module: nn.Module,
","# if not recursing, decide whether we should wrap for the leaf node or reminder
return isinstance(module, tuple(transformer_layer_cls))
def size_based_auto_wrap_policy(
module: nn.Module,
"
203,"check_fn=lambda mod: not isinstance(mod, FullyShardedDataParallel),
err_fn=lambda mod: f""Expected {mod} to NOT be FullyShardedDataParallel if auto_wrap is enabled."",
)
_recursive_wrap(
module,
                auto_wrap_policy=auto_wrap_policy,
wrapper_cls=FullyShardedDataParallel,
ignored_modules=ignored_modules,
ignored_params=ignored_params,
","check_fn=lambda mod: not isinstance(mod, FullyShardedDataParallel),
err_fn=lambda mod: f""Expected {mod} to NOT be FullyShardedDataParallel if auto_wrap is enabled."",
)
            if mixed_precision is not None and _contains_batchnorm(module):
                _override_batchnorm_mixed_precision(module)
                policy_to_use = functools.partial(
                    _or_policy,
                    policies=[_wrap_batchnorm_individually, auto_wrap_policy]
                )
                warnings.warn(
                    ""Mixed precision was specified for FSDP module with""
                    "" batchnorm submodules wrapped via ``auto_wrap_policy``.""
                    "" BatchNorm units will be wrapped as a separate FSDP unit,""
                    "" with mixed_precision disabled (i.e. set to ``None``)""
                    "" as several BatchNorm kernels would raise errors when""
                    "" operating on reduced precision inputs.""
                )
            else:
                policy_to_use = auto_wrap_policy
_recursive_wrap(
module,
                auto_wrap_policy=policy_to_use,
wrapper_cls=FullyShardedDataParallel,
ignored_modules=ignored_modules,
ignored_params=ignored_params,
"
204,"from typing import Dict, List
def get_first_dim(t: torch.Tensor) -> int:
""""""
    A free function primarily for use in the merge_matmul graph transformation below
    that returns the first dimension of a Tensor. This is necessary because torch.Tensor.shape
    is an attribute (and cannot be the target of a call_function node) and also helps save
    a getitem op in the graph.
Arguments:
        t: The tensor to get the first dimension of.
Returns:
        The first dimension of t.
""""""
    return t.shape[0]
def may_depend_on(a: Node, b: Node, search_depth: int = 6):
","from typing import Dict, List
def split_result_tensors(result: torch.Tensor, inputs: List[torch.Tensor]) -> List[torch.Tensor]:
""""""
    A free function for use in the merge_matmul graph transformation below that
    splits the output from a merged matmul into the individual results for each
    input tensor.
Arguments:
        result: The merged matmul result tensor.
        inputs: The list of inputs that were merged into one for the matmul.
Returns:
        List of matmul results for each input tensor.
""""""
    # When fx tracer is running, x.shape[0] will be torch.fx.Attribute but we
    # need an int even when tracing
    if isinstance(result, torch.fx.Proxy):
        splits = [0] * len(inputs)
    else:
        splits = [x.shape[0] for x in inputs]

    return torch.split(result, splits)
def may_depend_on(a: Node, b: Node, search_depth: int = 6):
"
205,"if (LD.is_complex()) {
TORCH_CHECK(
hermitian,
        ""torch.linalg.ldl_factor: complex tensors with hermitian=False flag are not supported."");
}
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(
LD.scalar_type(), ""ldl_factor_magma"", [&] {
","if (LD.is_complex()) {
TORCH_CHECK(
hermitian,
        ""torch.linalg.ldl_factor: complex tensors with hermitian=False flag are not supported with MAGMA backend. "",
        ""Currently preferred backend is "",
        at::globalContext().linalgPreferredBackend(),
        "", please set 'default' or 'cusolver' backend with torch.backends.cuda.preferred_linalg_library"");
}
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(
LD.scalar_type(), ""ldl_factor_magma"", [&] {
"
206,"namespace {
// TODO: Consider representing debug info as a struct instead so you
// don't have to allocate strings all the time
  std::string debugString(std::string file, uint32_t line) {
#ifdef STRIP_ERROR_MESSAGES
return std::string();
#else
","namespace {
// TODO: Consider representing debug info as a struct instead so you
// don't have to allocate strings all the time
  std::string debugString(const std::string& file, uint32_t line) {
#ifdef STRIP_ERROR_MESSAGES
return std::string();
#else
"
207,"@register_decomposition(aten.masked_fill.Scalar)
def masked_fill_Scalar(self: Tensor, mask: Tensor, value: float) -> Tensor:
    return torch.where(mask, self.new_full((), value), self)
@register_decomposition(aten.masked_fill.Tensor)
","@register_decomposition(aten.masked_fill.Scalar)
def masked_fill_Scalar(self: Tensor, mask: Tensor, value: float) -> Tensor:
    return torch.where(mask, utils.dtype_to_type(self.dtype)(value), self)
@register_decomposition(aten.masked_fill.Tensor)
"
208,"return weight.index_select(0, indices.reshape(-1)).view(size)

@register_decomposition(aten.embedding_dense_backward)
@cast_for_opmath
def embedding_dense_backward(
grad_output: Tensor,
indices: Tensor,
","return weight.index_select(0, indices.reshape(-1)).view(size)
# TODO: Correct the type promotion semantics
@register_decomposition(aten.embedding_dense_backward)
def embedding_dense_backward(
grad_output: Tensor,
indices: Tensor,
"
209,"#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/NumericUtils.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
","#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
"
210,"public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = at::_isnan<scalar_t>(*src_data) ? *src_data : std::max(*self_data, *src_data);
}
};
static ReduceMaximum reduce_maximum;
","public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = std::max(*self_data, *src_data);
}
};
static ReduceMaximum reduce_maximum;
"
211,"{""_set_warnAlways"", THPModule_setWarnAlways, METH_O,  nullptr},
{""_get_cublas_allow_tf32"", THPModule_allowTF32CuBLAS, METH_NOARGS,     nullptr},
{""_set_cublas_allow_tf32"", THPModule_setAllowTF32CuBLAS, METH_O,  nullptr},
{""_get_cublas_allow_fp16_reduced_precision_reduction"", THPModule_allowFP16ReductionCuBLAS, METH_NOARGS, nullptr},
{""_set_cublas_allow_fp16_reduced_precision_reduction"", THPModule_setAllowFP16ReductionCuBLAS, METH_O, nullptr},
{""_vmapmode_increment_nesting"", THPModule_vmapmode_increment_nesting, METH_NOARGS, nullptr},
","{""_set_warnAlways"", THPModule_setWarnAlways, METH_O,  nullptr},
{""_get_cublas_allow_tf32"", THPModule_allowTF32CuBLAS, METH_NOARGS,     nullptr},
{""_set_cublas_allow_tf32"", THPModule_setAllowTF32CuBLAS, METH_O,  nullptr},
  {""_get_float32_matmul_precision"", THPModule_float32MatmulPrecision, METH_NOARGS,     nullptr},
  {""_set_float32_matmul_precision"", THPModule_setFloat32MatmulPrecision, METH_O,  nullptr},
{""_get_cublas_allow_fp16_reduced_precision_reduction"", THPModule_allowFP16ReductionCuBLAS, METH_NOARGS, nullptr},
{""_set_cublas_allow_fp16_reduced_precision_reduction"", THPModule_setAllowFP16ReductionCuBLAS, METH_O, nullptr},
{""_vmapmode_increment_nesting"", THPModule_vmapmode_increment_nesting, METH_NOARGS, nullptr},
"
212,"import torch.nn.qat as nnqat
import torch.nn.quantized._reference as nnqr
from ..observer import (
    default_affine_fixed_qparams_observer,
    default_symmetric_fixed_qparams_observer,
)
from ..fake_quantize import FixedQParamsFakeQuantize
from ..fuser_method_mappings import (
","import torch.nn.qat as nnqat
import torch.nn.quantized._reference as nnqr
from ..observer import (
    default_fixed_qparams_range_0to1_observer,
    default_fixed_qparams_range_neg1to1_observer,
)
from ..fake_quantize import FixedQParamsFakeQuantize
from ..fuser_method_mappings import (
"
213,"Default dynamic fake_quant for activations.
""""""
default_symmetric_fixed_qparams_fake_quant = FixedQParamsFakeQuantize.with_args(observer=default_symmetric_fixed_qparams_observer)
default_affine_fixed_qparams_fake_quant = FixedQParamsFakeQuantize.with_args(observer=default_affine_fixed_qparams_observer)
default_per_channel_weight_fake_quant = FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,
quant_min=-128,
","Default dynamic fake_quant for activations.
""""""
default_fixed_qparams_range_neg1to1_fake_quant = (
    FixedQParamsFakeQuantize.with_args(observer=default_fixed_qparams_range_neg1to1_observer)
)
default_fixed_qparams_range_0to1_fake_quant = (
    FixedQParamsFakeQuantize.with_args(observer=default_fixed_qparams_range_0to1_observer)
)
# TODO: the following 2 variables are kept for backwards compatibility; remove after a few releases
default_symmetric_fixed_qparams_fake_quant = default_fixed_qparams_range_neg1to1_fake_quant
default_affine_fixed_qparams_fake_quant = default_fixed_qparams_range_0to1_fake_quant
default_per_channel_weight_fake_quant = FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,
quant_min=-128,
"
214,"import torch.ao.nn as ao_nn
from torch.ao.quantization.stubs import QuantStub, DeQuantStub
from torch.ao.quantization.fake_quantize import (
    default_affine_fixed_qparams_fake_quant,
    default_symmetric_fixed_qparams_fake_quant,
)
from torch.ao.quantization.utils import get_combined_dict
from torch.nn.utils.parametrize import type_before_parametrizations
","import torch.ao.nn as ao_nn
from torch.ao.quantization.stubs import QuantStub, DeQuantStub
from torch.ao.quantization.fake_quantize import (
    default_fixed_qparams_range_0to1_fake_quant,
    default_fixed_qparams_range_neg1to1_fake_quant,
)
from torch.ao.quantization.utils import get_combined_dict
from torch.nn.utils.parametrize import type_before_parametrizations
"
215,"else:
raise AssertionError(
            f""handling of node with op {node_a.op} is not implemented"")
def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
","else:
raise AssertionError(
            f""handling of node {node_a.format_node()} with op {node_a.op} is not implemented"")
def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
"
216,"def _post_state_dict_hook(
    module: nn.Module, state_dict: ""OrderedDict[str, Tensor]"", prefix: str, *args: Any
) -> ""OrderedDict[str, Tensor]"":
""""""
_post_state_dict_hook() is called after the state_dict() is executed
and before returning the state_dict to the users.
","def _post_state_dict_hook(
    module: nn.Module, state_dict: Dict[str, Any], prefix: str, *args: Any
) -> Dict[str, Any]:
""""""
_post_state_dict_hook() is called after the state_dict() is executed
and before returning the state_dict to the users.
"
217,"return result
@register_decomposition(aten.slice_backward)
def slice_backward(
grad_output: Tensor,
","return result
@register_decomposition(aten._euclidean_dist)
def _euclidean_dist(x1: Tensor, x2: Tensor) -> Tensor:
    x1_norm = x1.pow(2).sum(-1, True)
    x1_pad = torch.ones_like(x1_norm, memory_format=torch.contiguous_format)
    x2_norm = x2.pow(2).sum(-1, True)
    x2_pad = torch.ones_like(x2_norm, memory_format=torch.contiguous_format)
    x1_ = torch.cat([x1.mul(-2), x1_norm, x1_pad], -1)
    x2_ = torch.cat([x2, x2_pad, x2_norm], -1)
    result = x1_.matmul(x2_.mT)
    return result.clamp_min(0).sqrt()


@register_decomposition(aten.slice_backward)
def slice_backward(
grad_output: Tensor,
"
218,"""torch/include/*"",
]
# Check if the compiler is hip-clang.
def is_hip_clang() -> bool:
try:
","""torch/include/*"",
]
ignores = [os.path.join(proj_dir, ignore) for ignore in ignores]

# Check if the compiler is hip-clang.
def is_hip_clang() -> bool:
try:
"
219,"mutable_input_post_processing = ""\n"".join(
[
f""""""
      auto {a.name}_functional = at::functionalization::impl::unsafeGetFunctionalWrapper({a.name});
      {a.name}_functional->replace_({'std::get<' + str(i) + '>(tmp_output)' if len(f.func.returns) > 1 else 'tmp_output'});
      {a.name}_functional->commit_update();""""""
for (i, a) in enumerate(f.func.arguments.out)
if a.annotation and a.annotation.is_write and a.type.is_tensor_like()
]
","mutable_input_post_processing = ""\n"".join(
[
f""""""
      at::functionalization::impl::replace_(
        {a.name}, {'std::get<' + str(i) + '>(tmp_output)' if len(f.func.returns) > 1 else 'tmp_output'});
      at::functionalization::impl::commit_update({a.name});""""""
for (i, a) in enumerate(f.func.arguments.out)
if a.annotation and a.annotation.is_write and a.type.is_tensor_like()
]
"
220,"flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
names_offset = 0;
c10::QualifiedName setstate_name(*class_ptr->name(), ""__setstate__"");
const mobile::Function* setstate = mcu_->find_function(setstate_name);
  if (setstate != nullptr) {
typetype = mobile::serialization::TypeType::CLASS_WITH_SETSTATE;
  } else if (class_ptr->findMethod(""__setstate__"")) {
typetype = mobile::serialization::TypeType::CUSTOM_CLASS;
} else {
size_t num_attr = class_ptr->numAttributes();
","flatbuffers::Vector<flatbuffers::Offset<flatbuffers::String>>>
names_offset = 0;
c10::QualifiedName setstate_name(*class_ptr->name(), ""__setstate__"");
  c10::QualifiedName getstate_name(*class_ptr->name(), ""__getstate__"");
const mobile::Function* setstate = mcu_->find_function(setstate_name);
  const mobile::Function* getstate = mcu_->find_function(getstate_name);
  if (setstate != nullptr && getstate != nullptr) {
typetype = mobile::serialization::TypeType::CLASS_WITH_SETSTATE;
  } else if (
      class_ptr->findMethod(""__setstate__"") &&
      class_ptr->findMethod(""__getstate__"")) {
typetype = mobile::serialization::TypeType::CUSTOM_CLASS;
} else {
size_t num_attr = class_ptr->numAttributes();
"
221,"@functional_datapipe('read_from_stream')
class StreamReaderIterDataPipe(IterDataPipe[Tuple[str, bytes]]):
r""""""
    Given IO streams and their label names, yields bytes with label name in a tuple.
Args:
datapipe: Iterable DataPipe provides label/URL and byte stream
","@functional_datapipe('read_from_stream')
class StreamReaderIterDataPipe(IterDataPipe[Tuple[str, bytes]]):
r""""""
    Given IO streams and their label names, yields bytes with label
    name in a tuple (functional name: ``read_from_stream``).
Args:
datapipe: Iterable DataPipe provides label/URL and byte stream
"
222,"[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
""""""
functions: Dict[str, Callable] = {}
def __getattr__(self, attribute_name):
if attribute_name in MapDataPipe.functions:
","[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
""""""
functions: Dict[str, Callable] = {}
    str_hook: Optional[Callable] = None
    repr_hook: Optional[Callable] = None
def __getattr__(self, attribute_name):
if attribute_name in MapDataPipe.functions:
"
223,"persistent_workers (bool, optional): If ``True``, the data loader will not shutdown
the worker processes after a dataset has been consumed once. This allows to
maintain the workers `Dataset` instances alive. (default: ``False``)
.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`
","persistent_workers (bool, optional): If ``True``, the data loader will not shutdown
the worker processes after a dataset has been consumed once. This allows to
maintain the workers `Dataset` instances alive. (default: ``False``)
        pin_memory_device (str, optional): the data loader will copy Tensors
            into device pinned memory before returning them if pin_memory is set to true.
.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`
"
224,"obj = getattr(_C._VariableFunctions, name)
obj.__module__ = 'torch'
globals()[name] = obj
    __all__.append(name)
################################################################################
# Import interface functions defined in Python
","obj = getattr(_C._VariableFunctions, name)
obj.__module__ = 'torch'
globals()[name] = obj
    if not name.startswith(""_""):
        __all__.append(name)
################################################################################
# Import interface functions defined in Python
"
225,"#include <torch/csrc/jit/python/pybind_utils.h>
#include <ATen/autocast_mode.h>
#include <ATen/record_function.h>
#include <torch/csrc/autograd/profiler.h>
#include <torch/csrc/autograd/profiler_python.h>
#include <torch/csrc/autograd/python_function.h>
","#include <torch/csrc/jit/python/pybind_utils.h>
#include <ATen/autocast_mode.h>
#include <ATen/record_function.h>
#include <ATen/core/PythonFallbackKernel.h>
#include <torch/csrc/autograd/profiler.h>
#include <torch/csrc/autograd/profiler_python.h>
#include <torch/csrc/autograd/python_function.h>
"
226,"#include <torch/csrc/jit/serialization/source_range_serialization.h>
#include <torch/csrc/jit/serialization/source_range_serialization_impl.h>
#include <torch/csrc/jit/mobile/type_parser.h>
#include <torch/csrc/jit/serialization/pickle.h>
namespace torch {
namespace jit {
class SourceRangeSerializer {
public:
// Serialize SourceRange as Tuple[SourceType, int, int]
  // where SourceType = Tuple[str, Optional[str], int, List[int]],
// the serialized form of Source
c10::IValue serialize(const SourceRange& sr);
private:
// Serialize Source as Tuple[str, Optional[str], int, List[int]]
// This caches serialized sources, since many SourceRanges can
// refer to the same one.
  c10::IValue serialize_source(const std::shared_ptr<SourceView>& s);
  std::unordered_map<std::shared_ptr<SourceView>, c10::IValue>
      serialized_sources;
};
SourceRange SourceRangeDeserializer::deserialize(const c10::IValue& iv) {
const auto& tup_elems = iv.toTupleRef().elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::shared_ptr<SourceView> source_ = deserialize_source(tup_elems[0]);
int64_t start_ = tup_elems[1].toInt();
int64_t end_ = tup_elems[2].toInt();
return SourceRange(source_, start_, end_);
}
std::shared_ptr<SourceView> SourceRangeDeserializer::deserialize_source(
const c10::IValue& iv) {
auto tup = iv.toTuple();
auto it = cached_sources.find(tup);
if (it != cached_sources.end()) {
return it->second;
}

const auto& tup_elems = tup->elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::string text_ = tup_elems[0].toString()->string();
  c10::optional<std::string> filename_ = tup_elems[1].toOptional<std::string>();
  int64_t starting_line_no_ = tup_elems[2].toInt();
  auto source = std::make_shared<Source>(
      std::move(text_), std::move(filename_), starting_line_no_);
cached_sources[tup] = source;
return source;
}
","#include <torch/csrc/jit/serialization/source_range_serialization.h>
#include <torch/csrc/jit/serialization/source_range_serialization_impl.h>
#include <c10/util/Exception.h>
#include <c10/util/Flags.h>
#include <torch/csrc/jit/mobile/type_parser.h>
#include <torch/csrc/jit/serialization/pickle.h>
#include <algorithm>
namespace torch {
namespace jit {
// ""Whether to emit compact debug_pkl when saving a model to .pt file.""
// ""Compact file is smaller but cannot be loaded by old torch binaries.""
// TODO(qihan) remove when all binaries are using string table.
thread_local bool should_use_format_with_string_table_ = false;

class SourceRangeSerializer {
public:
// Serialize SourceRange as Tuple[SourceType, int, int]
  // where SourceType = Tuple[int, int, int, List[int]],
  // The first 2 ints are positions into the vector returned by textSaved
  // after all the Ranges are processed. textSaved() returns a vector of str
// the serialized form of Source
c10::IValue serialize(const SourceRange& sr);
  const std::vector<c10::IValue>& texts_saved() {
    return texts_;
  }

  SourceRangeSerializer() {
    texts_.emplace_back("""");
    text_to_idx_[texts_.back().toStringRef()] = 0;
  }

private:
// Serialize Source as Tuple[str, Optional[str], int, List[int]]
// This caches serialized sources, since many SourceRanges can
// refer to the same one.
  c10::IValue serialize_source(const std::shared_ptr<Source>& s);
  std::unordered_map<std::shared_ptr<Source>, c10::IValue> serialized_sources;

  int64_t store_text_and_get_index(const std::string& text_view);
  std::vector<c10::IValue> texts_;
  std::unordered_map<c10::string_view, int64_t> text_to_idx_;
};
SourceRange SourceRangeDeserializer::deserialize(const c10::IValue& iv) {
const auto& tup_elems = iv.toTupleRef().elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::shared_ptr<Source> source_ = deserialize_source(tup_elems[0]);
int64_t start_ = tup_elems[1].toInt();
int64_t end_ = tup_elems[2].toInt();
return SourceRange(source_, start_, end_);
}
std::shared_ptr<Source> SourceRangeDeserializer::deserialize_source(
const c10::IValue& iv) {
auto tup = iv.toTuple();
auto it = cached_sources.find(tup);
if (it != cached_sources.end()) {
return it->second;
}
  std::shared_ptr<Source> source;
const auto& tup_elems = tup->elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  if (!text_table_.empty()) {
    const auto& textIndex = tup_elems[0].toIntList();
    int64_t fnameIndex = tup_elems[1].toInt();
    int64_t starting_line_no_ = tup_elems[2].toInt();
    c10::optional<std::string> filename = c10::nullopt;

    filename = *text_table_[fnameIndex];
    std::vector<c10::string_view> pieces;
    std::vector<std::shared_ptr<std::string>> strs;

    for (int64_t i : textIndex) {
      pieces.emplace_back(*text_table_[i]);
      strs.emplace_back(text_table_[i]);
    }

    StringCordView str_cord(std::move(pieces), std::move(strs));

    source = std::make_shared<Source>(str_cord, filename, starting_line_no_);
  } else {
    std::string text_ = tup_elems[0].toString()->string();
    c10::optional<std::string> filename_ =
        tup_elems[1].toOptional<std::string>();
    int64_t starting_line_no_ = tup_elems[2].toInt();
    source = std::make_shared<Source>(
        std::move(text_), std::move(filename_), starting_line_no_);
  }
cached_sources[tup] = source;
return source;
}
"
227,"auto t_new = c10::IValue(at::functionalization::impl::from_functional_tensor(tensors));
(*stack)[arguments_begin + idx] = t_new;
}
}
}
// we should wrap the output if any inputs were wrapped,
","auto t_new = c10::IValue(at::functionalization::impl::from_functional_tensor(tensors));
(*stack)[arguments_begin + idx] = t_new;
}
      } else if (ivalue.isOptionalTensorList()) {
        any_tensor_inputs = true;
        auto opt_tensors = ivalue.toOptionalTensorList();
        if (at::functionalization::impl::isFunctionalTensor(opt_tensors)) {
          any_functional_inputs = true;
          at::functionalization::impl::sync(opt_tensors);
          auto t_new = c10::IValue(at::functionalization::impl::from_functional_tensor(opt_tensors));
          (*stack)[arguments_begin + idx] = t_new;
        }
}
}
// we should wrap the output if any inputs were wrapped,
"
228,"import argparse
import concurrent.futures
import json
import logging
import os
","import argparse
import json
import logging
import os
"
229,"import argparse
import concurrent.futures
import json
import logging
import os
","import argparse
import json
import logging
import os
"
230,"# flake8: noqa C101
import itertools
from typing import Iterator
import torch
import torch.distributed as dist

def average_parameters(
params: Iterator[torch.nn.Parameter], process_group: dist.ProcessGroup
):
","# flake8: noqa C101
import itertools
from typing import Union, Iterable, Dict, Iterator
import torch
import torch.distributed as dist
def average_parameters(
params: Iterator[torch.nn.Parameter], process_group: dist.ProcessGroup
):
"
231,"continue
iteration_guard = 0
while not is_activation_post_process_node(input_arg, modules):
input_arg = input_arg.args[0]
iteration_guard += 1
if iteration_guard > 10000:
","continue
iteration_guard = 0
while not is_activation_post_process_node(input_arg, modules):
                # failed to trace back since no input arg for the current node
                if len(input_arg.args) < 1:
                    return False
input_arg = input_arg.args[0]
iteration_guard += 1
if iteration_guard > 10000:
"
232,")
return torch.unique_consecutive(self, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
    @_wrap_type_error_to_not_implemented
def __rsub__(self, other):
        if has_torch_function_variadic(self, other):
            return handle_torch_function(Tensor.__rsub__, (self, other), self, other)
return _C._VariableFunctions.rsub(self, other)
    @_wrap_type_error_to_not_implemented
def __rdiv__(self, other):
        if has_torch_function_variadic(self, other):
            return handle_torch_function(Tensor.__rdiv__, (self, other), self, other)
return self.reciprocal() * other
__rtruediv__ = __rdiv__
__itruediv__ = _C._TensorBase.__idiv__
    __pow__ = _wrap_type_error_to_not_implemented(_C._TensorBase.pow)
    @_wrap_type_error_to_not_implemented
def __rmod__(self, other):
        if has_torch_function_variadic(self, other):
            return handle_torch_function(Tensor.__rmod__, (self, other), self, other)
return torch.remainder(other, self)
def __format__(self, format_spec):
",")
return torch.unique_consecutive(self, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
def __rsub__(self, other):
return _C._VariableFunctions.rsub(self, other)
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
def __rdiv__(self, other):
return self.reciprocal() * other
__rtruediv__ = __rdiv__
__itruediv__ = _C._TensorBase.__idiv__
    __pow__ = _handle_torch_function_and_wrap_type_error_to_not_implemented(_C._TensorBase.pow)
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
def __rmod__(self, other):
return torch.remainder(other, self)
def __format__(self, format_spec):
"
233,"create_graph=create_graph,
only_inputs=only_inputs,
allow_unused=allow_unused,
)
if not only_inputs:
","create_graph=create_graph,
only_inputs=only_inputs,
allow_unused=allow_unused,
            is_grads_batched=is_grads_batched,
)
if not only_inputs:
"
234,"has_torch_function = _add_docstr(
_has_torch_function,
    r""""""Check for __torch_function__ implementations in the elements of an iterable.
    Considers exact ``Tensor`` s and ``Parameter`` s non-dispatchable.
Arguments
--------
relevant_args : iterable
","has_torch_function = _add_docstr(
_has_torch_function,
    r""""""Check for __torch_function__ implementations in the elements of an iterable
    or if a __torch_function__ mode is enabled.  Considers exact ``Tensor`` s
    and ``Parameter`` s non-dispatchable.  Use this to guard a call to
    :func:`handle_torch_function`; don't use it to test if something
    is Tensor-like, use :func:`is_tensor_like` instead.
Arguments
relevant_args : iterable
"
235,"}
bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {
  return t1.dtype() == t2.dtype() && t1.equal(t2);
}
void DeduplicateInitializers(
","}
bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {
  if (t1.dtype() != t2.dtype() || !t1.sizes().equals(t2.sizes()) ||
      !t1.strides().equals(t2.strides())) {
    return false;
  }

  if (t1.device() != t2.device()) {
    return t1.to(""cpu"").equal(t2.to(""cpu""));
  }

  return t1.equal(t2);
}
void DeduplicateInitializers(
"
236,"from abc import ABC, abstractmethod
from typing import List, Union
from dataclasses import dataclass
from tools.codegen.context import method_with_native_function
","from abc import ABC
from typing import List, Union
from dataclasses import dataclass
from tools.codegen.context import method_with_native_function
"
237,"zip(chain.from_iterable((g['params'] for g in saved_groups)),
chain.from_iterable((g['params'] for g in groups)))}
        def cast(param, value):
r""""""Make a deep copy of value, casting all tensors to device of param.""""""
if isinstance(value, torch.Tensor):
# Floating-point types are a bit special here. They are the only ones
# that are assumed to always match the type of params.
                if param.is_floating_point():
                    value = value.to(param.dtype)
                value = value.to(param.device)
return value
elif isinstance(value, dict):
                return {k: cast(param, v) for k, v in value.items()}
elif isinstance(value, container_abcs.Iterable):
return type(value)(cast(param, v) for v in value)
else:
","zip(chain.from_iterable((g['params'] for g in saved_groups)),
chain.from_iterable((g['params'] for g in groups)))}
        def cast(param, value, key=None):
r""""""Make a deep copy of value, casting all tensors to device of param.""""""
if isinstance(value, torch.Tensor):
# Floating-point types are a bit special here. They are the only ones
# that are assumed to always match the type of params.
                # Make sure state['step'] is not casted https://github.com/pytorch/pytorch/issues/74424
                if (key != ""step""):
                    if param.is_floating_point():
                        value = value.to(param.dtype)
                    value = value.to(param.device)
return value
elif isinstance(value, dict):
                return {k: cast(param, v, key=k) for k, v in value.items()}
elif isinstance(value, container_abcs.Iterable):
return type(value)(cast(param, v) for v in value)
else:
"
238,"# so, we don't export then to it
from_c.extend(['hardtanh', 'leaky_relu', 'hardsigmoid'])
dispatch_code = [""{}: Callable"".format(_) for _ in (dispatches + from_c)]
    fm.write_with_template('torch/_C/_nn.pyi', respath(""_nn.pyi.in""), lambda: {
'imported_hints': import_code,
'dispatched_hints': dispatch_code,
})
","# so, we don't export then to it
from_c.extend(['hardtanh', 'leaky_relu', 'hardsigmoid'])
dispatch_code = [""{}: Callable"".format(_) for _ in (dispatches + from_c)]
    fm.write_with_template('torch/_C/_nn.pyi', 'torch/_C/_nn.pyi.in', lambda: {
'imported_hints': import_code,
'dispatched_hints': dispatch_code,
})
"
239,"self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
else:
            # added for TorchScriptability, not used
self.register_buffer(""weight_scale"", torch.tensor(1.0, dtype=torch.float, device=device))
self.register_buffer(""weight_zero_point"", torch.tensor(0, dtype=torch.int, device=device))
self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))

def get_weight(self):
""""""
Fake quantize (quantize and dequantize) the weight with
","self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
else:
            # added for TorchScriptability, and for torch.float
self.register_buffer(""weight_scale"", torch.tensor(1.0, dtype=torch.float, device=device))
self.register_buffer(""weight_zero_point"", torch.tensor(0, dtype=torch.int, device=device))
self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
def get_weight(self):
""""""
Fake quantize (quantize and dequantize) the weight with
"
240,"weight = torch.quantize_per_tensor(weight, weight_scale, weight_zero_point, weight_dtype)
return weight
elif weight_qscheme in [torch.per_channel_affine, torch.per_channel_affine_float_qparams]:
        if weight_dtype in [torch.quint8, torch.qint8, torch.quint4x2]:
weight = torch.quantize_per_channel(
weight, weight_scale,
weight_zero_point, weight_axis.item(), weight_dtype)  # type: ignore[arg-type]
","weight = torch.quantize_per_tensor(weight, weight_scale, weight_zero_point, weight_dtype)
return weight
elif weight_qscheme in [torch.per_channel_affine, torch.per_channel_affine_float_qparams]:
        if weight_dtype in [torch.quint8, torch.qint8, torch.quint4x2, torch.qint32]:
weight = torch.quantize_per_channel(
weight, weight_scale,
weight_zero_point, weight_axis.item(), weight_dtype)  # type: ignore[arg-type]
"
241,"if (comms) {
for(const auto i : c10::irange(ndevices)) {
int dummy_var;
        if (C10_CUDA_ERROR_HANDLED(cudaGetDevice(&dummy_var)) != cudaSuccess) {
/* there are cases when this destructor is called after the
CUDA driver is already unloaded from the process.
In these cases, skip ncclCommDestroy */
","if (comms) {
for(const auto i : c10::irange(ndevices)) {
int dummy_var;
        if (cudaGetDevice(&dummy_var) != cudaSuccess) {
/* there are cases when this destructor is called after the
CUDA driver is already unloaded from the process.
In these cases, skip ncclCommDestroy */
"
242,"throw std::runtime_error(ss.str());
}
}
#define TORCH_CUDA_CHECK(result) torch_cuda_check_impl(result,__FILE__,__LINE__);
struct CUDAMethods : public CUDAStubs {
void record(int* device, CUDAEventStub* event, int64_t* cpu_ns) const override {
","throw std::runtime_error(ss.str());
}
}
#define TORCH_CUDA_CHECK(result) cudaCheck(result,__FILE__,__LINE__);
struct CUDAMethods : public CUDAStubs {
void record(int* device, CUDAEventStub* event, int64_t* cpu_ns) const override {
"
243,"quantized_dict = get_logger_dict(q_module)
act_dict: Dict[str, Dict] = {}
for key in quantized_dict:
match_key = _find_match(sorted(float_dict, reverse=True), key, ""stats"")
if match_key is not None:
act_dict[key] = {}
","quantized_dict = get_logger_dict(q_module)
act_dict: Dict[str, Dict] = {}
for key in quantized_dict:
        if len(quantized_dict[key][""tensor_val""]) == 0:
            continue
match_key = _find_match(sorted(float_dict, reverse=True), key, ""stats"")
if match_key is not None:
act_dict[key] = {}
"
244,"if (comms) {
for(const auto i : c10::irange(ndevices)) {
int dummy_var;
        if (cudaGetDevice(&dummy_var) != cudaSuccess) {
/* there are cases when this destructor is called after the
CUDA driver is already unloaded from the process.
In these cases, skip ncclCommDestroy */
","if (comms) {
for(const auto i : c10::irange(ndevices)) {
int dummy_var;
        if (C10_CUDA_ERROR_HANDLED(cudaGetDevice(&dummy_var)) != cudaSuccess) {
/* there are cases when this destructor is called after the
CUDA driver is already unloaded from the process.
In these cases, skip ncclCommDestroy */
"
245,"c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    cudaMemGetInfo(&device_free, &device_total);
return {device_free, device_total};
});
}
","c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    C10_CUDA_CHECK(cudaMemGetInfo(&device_free, &device_total));
return {device_free, device_total};
});
}
"
246,"if (*largest ==
0) { // make an initial guess if a zero *largest is passed in
size_t tmp_bytes;
      cudaMemGetInfo(
largest, // Use free memory as an optimistic initial guess of *largest
          &tmp_bytes);
}
cache_info_aux(large_blocks, total, largest);
cache_info_aux(small_blocks, total, largest);
","if (*largest ==
0) { // make an initial guess if a zero *largest is passed in
size_t tmp_bytes;
      C10_CUDA_CHECK(cudaMemGetInfo(
largest, // Use free memory as an optimistic initial guess of *largest
          &tmp_bytes));
}
cache_info_aux(large_blocks, total, largest);
cache_info_aux(small_blocks, total, largest);
"
247,"""""""
A wrapper for sharding Module parameters across data parallel workers. This
is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.
    FullyShardedDataParallel is commonly shorten to FSDP.
.. _`Xu et al.`: https://arxiv.org/abs/2004.13336
.. _DeepSpeed: https://www.deepspeed.ai/
","""""""
A wrapper for sharding Module parameters across data parallel workers. This
is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.
    FullyShardedDataParallel is commonly shortened to FSDP.
.. _`Xu et al.`: https://arxiv.org/abs/2004.13336
.. _DeepSpeed: https://www.deepspeed.ai/
"
248,"def lp_pool2d(
    input: Tensor, norm_type: float,
    kernel_size: int,
stride: Optional[BroadcastingList2[int]] = None,
ceil_mode: bool = False
) -> Tensor:
","def lp_pool2d(
    input: Tensor, norm_type: Union[int, float],
    kernel_size: BroadcastingList2[int],
stride: Optional[BroadcastingList2[int]] = None,
ceil_mode: bool = False
) -> Tensor:
"
249,"return torch.affine_grid_generator(theta, size, align_corners)
def _pad(input: Tensor, pad: List[int], mode: str = ""constant"", value: float = 0.0) -> Tensor:
r""""""Pads tensor.
Padding size:
","return torch.affine_grid_generator(theta, size, align_corners)
def _pad(input: Tensor, pad: BroadcastingList1[int], mode: str = ""constant"", value: Union[int, float] = 0.0) -> Tensor:
r""""""Pads tensor.
Padding size:
"
250,"} // namespace
void applyUpgrader(mobile::Function* function, uint64_t operator_version) {
  const Code& code = function->get_code();
auto& operator_version_map = getOperatorVersionMapForMobile();
  for (size_t i = 0; i < function->get_code().instructions_.size(); i++) {
    Instruction& inst = function->get_code().instructions_[i];
if (inst.op == OpCode::OP) {
      std::string op_name = function->get_code().op_names_[inst.X].name;
      std::string operator_name = function->get_code().op_names_[inst.X].name +
          (function->get_code().op_names_[inst.X].overload_name.empty()
? """"
               : ""."" + function->get_code().op_names_[inst.X].overload_name);
auto it = operator_version_map.find(operator_name);
// Find out if there is an upgrader for this operator
","} // namespace
void applyUpgrader(mobile::Function* function, uint64_t operator_version) {
  Code& code = function->get_code();
auto& operator_version_map = getOperatorVersionMapForMobile();
  for (size_t i = 0; i < code.instructions_.size(); i++) {
    Instruction& inst = code.instructions_[i];
if (inst.op == OpCode::OP) {
      std::string op_name = code.op_names_[inst.X].name;
      std::string operator_name = code.op_names_[inst.X].name +
          (code.op_names_[inst.X].overload_name.empty()
? """"
               : ""."" + code.op_names_[inst.X].overload_name);
auto it = operator_version_map.find(operator_name);
// Find out if there is an upgrader for this operator
"
251,"// new_inst.X = upgrader.index;
// code->instructions_[i] = new_inst;
TORCH_CHECK(
                upgrader.index < function->get_code().functions_.size(),
""upgrader index is, "",
upgrader.index,
"" and it's larger than the upgrader function list length "",
                function->get_code().functions_.size());
inst.op = OpCode::CALL;
inst.X = upgrader.index;
}
","// new_inst.X = upgrader.index;
// code->instructions_[i] = new_inst;
TORCH_CHECK(
                upgrader.index < code.functions_.size(),
""upgrader index is, "",
upgrader.index,
"" and it's larger than the upgrader function list length "",
                code.functions_.size());
inst.op = OpCode::CALL;
inst.X = upgrader.index;
}
"
252,"Config sharding algorithm, different sharding algorithm has trade off
between memory saving and communication overhead. 'FULL_SHARD' will
be chose if sharding_strategy is not specified.
        cpu_offload (Optional[CPUOffload]):
CPU offloading config. Currently, only parameter and gradient CPU
offload is supported. It can be enabled via passing in
``cpu_offload=CPUOffload(offload_params=True)``. Note that this
","Config sharding algorithm, different sharding algorithm has trade off
between memory saving and communication overhead. 'FULL_SHARD' will
be chose if sharding_strategy is not specified.
        cpu_offload (Optional [CPUOffload]):
CPU offloading config. Currently, only parameter and gradient CPU
offload is supported. It can be enabled via passing in
``cpu_offload=CPUOffload(offload_params=True)``. Note that this
"
253,"``state_dict`` on every rank, which could result in OOM if the model
cannot fit on a single GPU. As a result, :func:`state_dict_type` API is
available to configure between `state_dict` implementations. User can
        thus use `with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`
context manager to perform a local checkpoint that will store only local
shards of the module. Currently, the only supported implementations are
``StateDictType.LOCAL_STATE_DICT`` and ``StateDictType.FULL_STATE_DICT``
","``state_dict`` on every rank, which could result in OOM if the model
cannot fit on a single GPU. As a result, :func:`state_dict_type` API is
available to configure between `state_dict` implementations. User can
        thus use ``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)``
context manager to perform a local checkpoint that will store only local
shards of the module. Currently, the only supported implementations are
``StateDictType.LOCAL_STATE_DICT`` and ``StateDictType.FULL_STATE_DICT``
"
254,"void Module::_save_for_mobile(
std::ostream& out,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info) const {
ExportModule(
*this,
out,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info);
}
void Module::_save_for_mobile(
const std::string& filename,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info) const {
ExportModule(
*this,
filename,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info);
}
} // namespace jit
","void Module::_save_for_mobile(
std::ostream& out,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info,
    bool use_flatbuffer) const {
ExportModule(
*this,
out,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info,
      use_flatbuffer);
}
void Module::_save_for_mobile(
const std::string& filename,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info,
    bool use_flatbuffer) const {
ExportModule(
*this,
filename,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info,
      use_flatbuffer);
}
} // namespace jit
"
255,"#include <ATen/core/ivalue.h>
#include <ATen/core/qualified_name.h>
#include <c10/core/CPUAllocator.h>
#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <c10/util/ScopeExit.h>
","#include <ATen/core/ivalue.h>
#include <ATen/core/qualified_name.h>
#include <c10/core/CPUAllocator.h>
#include <c10/core/impl/alloc_cpu.h>
#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <c10/util/ScopeExit.h>
"
256,"c10::optional<at::Device> device,
ExtraFilesMap& extra_files,
uint64_t module_load_options) {
  std::unique_ptr<FileAdapter> rai = std::make_unique<FileAdapter>(filename);
  auto module = _load_for_mobile_impl(
      std::move(rai), device, extra_files, module_load_options);
  return module;
}
mobile::Module _load_for_mobile(
","c10::optional<at::Device> device,
ExtraFilesMap& extra_files,
uint64_t module_load_options) {
  auto format = getFileFormat(filename);
  switch (format) {
    case FileFormat::ZipFileFormat: {
      std::unique_ptr<FileAdapter> rai =
          std::make_unique<FileAdapter>(filename);
      auto module = _load_for_mobile_impl(
          std::move(rai), device, extra_files, module_load_options);
      return module;
    }
#if defined(ENABLE_FLATBUFFER)
    case FileFormat::FlatbufferFileFormat: {
      std::shared_ptr<char> data;
      size_t size = 0;
      std::tie(data, size) = get_file_content(filename.c_str());
      auto* flatbuffer_module =
          mobile::serialization::GetMutableModule(data.get());
      mobile::Module m = initialize_mobile_module(flatbuffer_module);
      parseExtraFiles(flatbuffer_module, extra_files);
      return m;
    }
#else
    case FileFormat::FlatbufferFileFormat: {
      TORCH_CHECK(
          false,
          ""Flatbuffer input file but the build hasn't enabled flatbuffer"");
    }
#endif
    default: {
      TORCH_CHECK(false, ""Format error"");
    }
  }
}
mobile::Module _load_for_mobile(
"
257,"#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/callstack_debug_info_serialization.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_functions.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
","#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/callstack_debug_info_serialization.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#if defined(ENABLE_FLATBUFFER)
#include <torch/csrc/jit/serialization/flatbuffer_serializer.h>
#endif
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_functions.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
"
258,"Fused version of `default_per_channel_weight_fake_quant`, with improved performance.
""""""
def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
","Fused version of `default_per_channel_weight_fake_quant`, with improved performance.
""""""
fused_wt_fake_quant_range_neg_127_to_127 = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                                   quant_min=-127,
                                                                                   quant_max=127,
                                                                                   dtype=torch.qint8,
                                                                                   qscheme=torch.per_tensor_symmetric,
                                                                                   eps=2 ** -12)
""""""
Fused version of `default_weight_fake_quant`, with the 8-bit values restricted to [-127, +127], excluding -128.
""""""

fused_per_channel_wt_fake_quant_range_neg_127_to_127 = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                                               quant_min=-127,
                                                                                               quant_max=127,
                                                                                               dtype=torch.qint8,
                                                                                               qscheme=torch.per_channel_symmetric,
                                                                                               eps=2 ** -12)
""""""
Fused version of `default_per_channel_weight_fake_quant`, with the 8-bit values restricted to [-127, +127], excluding -128.
""""""


def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
"
259,"const std::string& filename,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info,
    bool use_flatbuffer) {
  if (use_flatbuffer) {
#if defined(ENABLE_FLATBUFFER)
    auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
      std::fstream ofile(filename, std::ios::binary | std::ios::out);
      ofile.write(static_cast<const char*>(buf), nbytes);
      ofile.close();
      return !ofile ? 0 : nbytes;
    };
    save_mobile_module_to(
        module, extra_files, save_mobile_debug_info, writer_func);
#else
    TORCH_CHECK(
        false,
        ""Trying to export as flatbuffer file but the build hasn't enabled flatbuffer"");
#endif
  } else {
    caffe2::serialize::PyTorchStreamWriter writer(filename);
    ScriptModuleSerializer serializer(writer);
    serializer.serialize(
        module, extra_files, bytecode_format, save_mobile_debug_info);
  }
}
void ExportModule(
","const std::string& filename,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info) {
  caffe2::serialize::PyTorchStreamWriter writer(filename);
  ScriptModuleSerializer serializer(writer);
  serializer.serialize(
      module, extra_files, bytecode_format, save_mobile_debug_info);
}
void ExportModule(
"
260,"}
bool ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const {
  for (const auto i : c10::irange(devices_.size())) {
    // Checking the work's corresponding CUDA events' status
    if (!(*ncclEndEvents_)[i].query()) {
      return false;
}
}
return true;
}
","}
bool ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const {
  try {
    for (const auto i : c10::irange(devices_.size())) {
      // Checking the work's corresponding CUDA events' status
      if (!(*ncclEndEvents_)[i].query()) {
        return false;
      }
}
  } catch (const std::exception& e) {
    if (std::string(e.what()).find(""driver shutting down"") == std::string::npos) {
      throw;
    }
    LOG(INFO) << ""[Rank "" << rank_
              << ""] Event query failed with exception: ""
              << e.what();
}
return true;
}
"
261,"quant_min=None,
quant_max=None,
factory_kwargs=None,
) -> None:
# bins: The number of bins used for histogram calculation.
super(HistogramObserver, self).__init__(
","quant_min=None,
quant_max=None,
factory_kwargs=None,
        eps=torch.finfo(torch.float32).eps,
) -> None:
# bins: The number of bins used for histogram calculation.
super(HistogramObserver, self).__init__(
"
262,"y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
The input channels are separated into :attr:`num_groups` groups, each containing
    ``num_channels / num_groups`` channels. The mean and standard-deviation are calculated
separately over the each group. :math:`\gamma` and :math:`\beta` are learnable
per-channel affine transform parameter vectors of size :attr:`num_channels` if
:attr:`affine` is ``True``.
","y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
The input channels are separated into :attr:`num_groups` groups, each containing
    ``num_channels / num_groups`` channels. :attr:`num_channels` must be divisible by
    :attr:`num_groups`. The mean and standard-deviation are calculated
separately over the each group. :math:`\gamma` and :math:`\beta` are learnable
per-channel affine transform parameter vectors of size :attr:`num_channels` if
:attr:`affine` is ``True``.
"
263,"#include <c10/util/Exception.h>
#include <torch/csrc/deploy/deploy.h>
#include <torch/csrc/deploy/elf_file.h>
#include <torch/cuda.h>
","#include <torch/csrc/deploy/Exception.h>
#include <torch/csrc/deploy/deploy.h>
#include <torch/csrc/deploy/elf_file.h>
#include <torch/cuda.h>
"
264,"}
at::optional<Section> ElfFile::findSection(const char* name) const {
  TORCH_CHECK(name != nullptr, ""Null name"");
at::optional<Section> found = at::nullopt;
for (const auto& section : sections_) {
if (strcmp(name, section.name) == 0) {
","}
at::optional<Section> ElfFile::findSection(const char* name) const {
  MULTIPY_CHECK(name != nullptr, ""Null name"");
at::optional<Section> found = at::nullopt;
for (const auto& section : sections_) {
if (strcmp(name, section.name) == 0) {
"
265,"#include <torch/csrc/deploy/loader.h>
#include <vector>
using torch::deploy::CustomLibrary;
","#include <torch/csrc/deploy/loader.h>
#include <sstream>
#include <vector>
using torch::deploy::CustomLibrary;
"
266,"}
void XarEnvironment::setupPythonApp() {
  TORCH_CHECK(
!alreadySetupPythonApp_,
""Already setup the python application. It should only been done once!"");
","}
void XarEnvironment::setupPythonApp() {
  MULTIPY_CHECK(
!alreadySetupPythonApp_,
""Already setup the python application. It should only been done once!"");
"
267,"""""""
Initializes internal Module state, shared by both nn.Module and ScriptModule.
""""""
        super().__init__()

torch._C._log_api_usage_once(""python.nn_module"")
self.training = True
","""""""
Initializes internal Module state, shared by both nn.Module and ScriptModule.
""""""
torch._C._log_api_usage_once(""python.nn_module"")
self.training = True
"
268,"def _get_tensor_label(self, t: torch.Tensor) -> str:
return str(t.dtype) + str(list(t.shape)) + r""\n""
        def _to_dot(self, graph_module: torch.fx.GraphModule, name: str, ignore_getattr: bool) -> pydot.Dot:
""""""
Actual interface to visualize a fx.Graph. Note that it takes in the GraphModule instead of the Graph
""""""
","def _get_tensor_label(self, t: torch.Tensor) -> str:
return str(t.dtype) + str(list(t.shape)) + r""\n""
        def _to_dot(
            self,
            graph_module: torch.fx.GraphModule,
            name: str,
            ignore_getattr: bool,
            skip_node_names_in_args: bool,
        ) -> pydot.Dot:
""""""
Actual interface to visualize a fx.Graph. Note that it takes in the GraphModule instead of the Graph
""""""
"
269,"Return:
qconfig
""""""

    if backend == 'fbgemm':
        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
                          weight=default_per_channel_weight_observer)
    elif backend == 'qnnpack':
        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),
                          weight=default_weight_observer)
else:
        qconfig = default_qconfig
return qconfig
default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
","Return:
qconfig
""""""
    if version == 0:
        if backend == 'fbgemm':
            qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
                              weight=default_per_channel_weight_observer)
        elif backend == 'qnnpack':
            qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),
                              weight=default_weight_observer)
        else:
            qconfig = default_qconfig
else:
        raise AssertionError(""Version number: "" + str(version) +
                             "" in get_default_qconfig is not supported. Version number must be 0"")

return qconfig
default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
"
270,"continue
if global_sharded_tensor_metadata is None:
            global_sharded_tensor_metadata = rank_metadata
global_metadata_rank = rank
else:
_raise_if_mismatch(global_sharded_tensor_metadata.size,
","continue
if global_sharded_tensor_metadata is None:
            global_sharded_tensor_metadata = copy.deepcopy(rank_metadata)
global_metadata_rank = rank
else:
_raise_if_mismatch(global_sharded_tensor_metadata.size,
"
271,"py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
  py::class_<DisableTorchDispatch>(_C_m, ""_DisableTorchDispatch"")
.def(py::init<>());
py::class_<torch::autograd::SavedVariable>(m, ""SavedTensor"")
","py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
  // TODO: line up this binding with DisableTorchFunction
  py::class_<torch::DisableTorchDispatch>(_C_m, ""_DisableTorchDispatch"")
.def(py::init<>());
py::class_<torch::autograd::SavedVariable>(m, ""SavedTensor"")
"
272,"void set_disabled_torch_function_impl(PyObject* value) {
disabled_torch_function = value;
}
}
typedef struct {
","void set_disabled_torch_function_impl(PyObject* value) {
disabled_torch_function = value;
}

  PyObject* disabled_torch_dispatch_impl() {
    return disabled_torch_dispatch;
  }

  void set_disabled_torch_dispatch_impl(PyObject* value) {
    disabled_torch_dispatch = value;
  }
}
typedef struct {
"
273,"return per_bucket_variable_indices;
}
std::vector<int> Logger::get_bucket_sizes() {
  std::vector<int> bucket_sizes;
for (const auto& bucket : reducer_->buckets_) {
const auto& variables = bucket.variables;
    int bucket_size = 0;
for (const auto& v : variables) {
bucket_size += v.numel() * v.element_size();
}
","return per_bucket_variable_indices;
}
std::vector<int64_t> Logger::get_bucket_sizes() {
  std::vector<int64_t> bucket_sizes;
for (const auto& bucket : reducer_->buckets_) {
const auto& variables = bucket.variables;
    int64_t bucket_size = 0;
for (const auto& v : variables) {
bucket_size += v.numel() * v.element_size();
}
"
274,"TORCH_INTERNAL_ASSERT(
running_mean.has_value() && running_var.has_value(),
""Expect running_mean and running_var to have value when train=false"");
mean_p = running_mean.value().view(view_size);
invstd_p = (1 / at::sqrt(running_var.value() + at::Scalar(eps))).view(view_size);
result_t = input_t * invstd_p;
","TORCH_INTERNAL_ASSERT(
running_mean.has_value() && running_var.has_value(),
""Expect running_mean and running_var to have value when train=false"");
    TORCH_CHECK(
        !running_mean.value()._fw_grad(/*level=*/0).defined() && !running_var.value()._fw_grad(/*level=*/0).defined(),
        ""batch_norm is not differentiable wrt running_mean and running_var, they cannot have forward grad defined"");
mean_p = running_mean.value().view(view_size);
invstd_p = (1 / at::sqrt(running_var.value() + at::Scalar(eps))).view(view_size);
result_t = input_t * invstd_p;
"
275,"at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalues =
          std::move(*jit::unpickle(
                         reinterpret_cast<const char*>(debug_data.get()),
                         debug_size,
                         nullptr,
                         {},
                         c10::parseType)
                         .toTuple())
              .elements();
      SourceRangeDeserializer deserializer;
      for (auto& val : ivalues) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
","at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalueTuple = jit::unpickle(
          reinterpret_cast<const char*>(debug_data.get()),
          debug_size,
          nullptr,
          {},
          c10::parseType);
      const auto& ivalues = ivalueTuple.toTuple()->elements();
      IValue lines;
      std::unique_ptr<SourceRangeDeserializer> deserializer;
      if (ivalues.size() == 3 && ivalues[0].isString() &&
          kFormatWithStringTable == ivalues[0].toStringRef()) {
        // new format
        deserializer = std::make_unique<SourceRangeDeserializer>(ivalues[1]);
        lines = ivalues[2];
      } else {
        deserializer = std::make_unique<SourceRangeDeserializer>();
        lines = ivalueTuple;
      }

      for (auto& val : lines.toTuple()->elements()) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
"
276,"if (it != source_range_tags.end()) {
source_range_tag = it->second;
}
ivalues.emplace_back(c10::ivalue::Tuple::create(
{(int64_t)range.bytes,
srs->serialize(range.range),
static_cast<int64_t>(source_range_tag)}));
}
std::vector<at::Tensor> table;
auto ivalue = c10::ivalue::Tuple::create(std::move(ivalues));
  auto result = jit::pickle(ivalue, &table);
TORCH_CHECK(table.size() == 0, ""Expected 0 tensors to be written"");
return result;
}
","if (it != source_range_tags.end()) {
source_range_tag = it->second;
}

ivalues.emplace_back(c10::ivalue::Tuple::create(
{(int64_t)range.bytes,
srs->serialize(range.range),
static_cast<int64_t>(source_range_tag)}));
}

std::vector<at::Tensor> table;
  auto textTable = c10::ivalue::Tuple::create(srs->texts_saved());
auto ivalue = c10::ivalue::Tuple::create(std::move(ivalues));
  std::vector<char> result;
  if (should_use_format_with_string_table_) {
    result = jit::pickle(
        c10::ivalue::Tuple::create({kFormatWithStringTable, textTable, ivalue}),
        &table);
  } else {
    result = jit::pickle(ivalue, &table);
  }
TORCH_CHECK(table.size() == 0, ""Expected 0 tensors to be written"");
return result;
}
"
277,"{},
c10::parseType)
.toTuple();
  const auto& ivalues = ivaluesTuple->elements();
unpickled_records = std::make_shared<SourceRangeRecords>();
  for (auto& val : ivalues) {
const auto& tup_elems = val.toTupleRef().elements();
int64_t offset = tup_elems[kByteOffsetIndex].toInt();
auto source_range = deserializer->deserialize(tup_elems[kSourceRangeIndex]);
","{},
c10::parseType)
.toTuple();
  const auto& ivalues = ivaluesTuple->elements();
unpickled_records = std::make_shared<SourceRangeRecords>();
  IValue lines;
  if (ivalues[0].isString() &&
      kFormatWithStringTable == ivalues[0].toStringRef()) {
    deserializer.reset(new SourceRangeDeserializer(ivalues[1]));
    lines = ivalues[2];
  } else {
    deserializer.reset(new SourceRangeDeserializer());
    lines = ivaluesTuple;
  }
  for (auto& val : lines.toTuple()->elements()) {
const auto& tup_elems = val.toTupleRef().elements();
int64_t offset = tup_elems[kByteOffsetIndex].toInt();
auto source_range = deserializer->deserialize(tup_elems[kSourceRangeIndex]);
"
278,"column_offset_ptr = &column_offset_temp;
}
for (int i = 0; i < M; ++i) {
      (*b_quantized_)[i] -= in_qparams_[0].zero_point * (*column_offset_ptr)[i];
}
}
}
","column_offset_ptr = &column_offset_temp;
}
for (int i = 0; i < M; ++i) {
      (*b_quantized_)[i] -= in_qparams_[INPUT].zero_point * (*column_offset_ptr)[i];
}
}
}
"
279,"ScalarType scalar_type = declared_types[i].second;
set_type(tensor_type, backend, scalar_type);
set_name(tensor_type, get_name(backend, scalar_type));

    // Use torch.float32 as the default tensor type
    if (backend == Backend::CPU && scalar_type == at::kFloat) {
      set_default_tensor_type(&tensor_type);
    }
}
}
void initialize_python_bindings() {
","ScalarType scalar_type = declared_types[i].second;
set_type(tensor_type, backend, scalar_type);
set_name(tensor_type, get_name(backend, scalar_type));
}

  set_default_tensor_type(Backend::CPU, ScalarType::Float);
}
void initialize_python_bindings() {
"
280,"torch.sigmoid,
torch.squeeze,
torch.stack,
torch.tanh,
torch.unsqueeze,
torch.cat,
","torch.sigmoid,
torch.squeeze,
torch.stack,
    torch.sum,
torch.tanh,
torch.unsqueeze,
torch.cat,
"
281,"mod.num_features, mod.weight, mod.bias, float(scale), int(zero_point),
mod.eps, mod.affine)
return new_mod
","mod.num_features, mod.weight, mod.bias, float(scale), int(zero_point),
mod.eps, mod.affine)
return new_mod

    @classmethod
    def from_reference(cls, mod, scale, zero_point):
        return cls(
            mod.num_features, mod.weight, mod.bias, float(scale), int(zero_point),
            mod.eps, mod.affine)
"
282,"std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.push_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.push_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
","std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.emplace_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.emplace_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
"
283,"Args:
*datapipes: Map DataPipes being aggregated
""""""
datapipes: Tuple[MapDataPipe[T_co], ...]
length: int
","Args:
*datapipes: Map DataPipes being aggregated

    Example:
        >>> from torchdata.datapipes.map import SequenceWrapper
        >>> dp1 = SequenceWrapper(range(3))
        >>> dp2 = SequenceWrapper(range(10, 13))
        >>> zip_dp = dp1.zip(dp2)
        >>> list(zip_dp)
        [(0, 10), (1, 11), (2, 12)]
""""""
datapipes: Tuple[MapDataPipe[T_co], ...]
length: int
"
284,"datapipe: Iterable DataPipe being batched
batch_size: The size of each batch
drop_last: Option to drop the last batch if it's not full
""""""
datapipe: MapDataPipe
batch_size: int
","datapipe: Iterable DataPipe being batched
batch_size: The size of each batch
drop_last: Option to drop the last batch if it's not full

    Example:
        >>> from torchdata.datapipes.map import SequenceWrapper
        >>> dp = SequenceWrapper(range(10))
        >>> batch_dp = dp.batch(batch_size=2)
        >>> list(batch_dp)
        [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]
""""""
datapipe: MapDataPipe
batch_size: int
"
285,"SharedParserData& shared;
};
Parser::Parser(const std::shared_ptr<Source>& src)
: pImpl(new ParserImpl(src)) {}
Parser::~Parser() = default;
","SharedParserData& shared;
};
Parser::Parser(const std::shared_ptr<SourceView>& src)
: pImpl(new ParserImpl(src)) {}
Parser::~Parser() = default;
"
286,"// expression and base type names.
if (resolver_) {
if (auto typePtr =
            resolver_->resolveType(expr.range().text().str(), expr.range())) {
return typePtr;
}
}
","// expression and base type names.
if (resolver_) {
if (auto typePtr =
            resolver_->resolveType(expr.range().text(), expr.range())) {
return typePtr;
}
}
"
287,"at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalueTuple = jit::unpickle(
          reinterpret_cast<const char*>(debug_data.get()),
          debug_size,
          nullptr,
          {},
          c10::parseType);
      const auto& ivalues = ivalueTuple.toTuple()->elements();
      IValue lines;
      std::unique_ptr<SourceRangeDeserializer> deserializer;
      if (ivalues.size() == 3 && ivalues[0].isString() &&
          kFormatWithStringTable == ivalues[0].toStringRef()) {
        // new format
        deserializer = std::make_unique<SourceRangeDeserializer>(ivalues[1]);
        lines = ivalues[2];
      } else {
        deserializer = std::make_unique<SourceRangeDeserializer>();
        lines = ivalueTuple;
      }

      for (auto& val : lines.toTuple()->elements()) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
","at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalues =
          std::move(*jit::unpickle(
                         reinterpret_cast<const char*>(debug_data.get()),
                         debug_size,
                         nullptr,
                         {},
                         c10::parseType)
                         .toTuple())
              .elements();
      SourceRangeDeserializer deserializer;
      for (auto& val : ivalues) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
"
288,"#include <torch/csrc/jit/serialization/source_range_serialization.h>
#include <torch/csrc/jit/serialization/source_range_serialization_impl.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/mobile/type_parser.h>
#include <torch/csrc/jit/serialization/pickle.h>
#include <algorithm>
namespace torch {
namespace jit {
","#include <torch/csrc/jit/serialization/source_range_serialization.h>
#include <torch/csrc/jit/serialization/source_range_serialization_impl.h>
#include <torch/csrc/jit/mobile/type_parser.h>
#include <torch/csrc/jit/serialization/pickle.h>
namespace torch {
namespace jit {
"
289,"class SourceRangeSerializer {
public:
// Serialize SourceRange as Tuple[SourceType, int, int]
  // where SourceType = Tuple[str, Optional[str], int, List[int]],
// the serialized form of Source
c10::IValue serialize(const SourceRange& sr);
private:
// Serialize Source as Tuple[str, Optional[str], int, List[int]]
// This caches serialized sources, since many SourceRanges can
// refer to the same one.
  c10::IValue serialize_source(const std::shared_ptr<SourceView>& s);
  std::unordered_map<std::shared_ptr<SourceView>, c10::IValue>
      serialized_sources;
};
SourceRange SourceRangeDeserializer::deserialize(const c10::IValue& iv) {
const auto& tup_elems = iv.toTupleRef().elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::shared_ptr<SourceView> source_ = deserialize_source(tup_elems[0]);
int64_t start_ = tup_elems[1].toInt();
int64_t end_ = tup_elems[2].toInt();
return SourceRange(source_, start_, end_);
}
std::shared_ptr<SourceView> SourceRangeDeserializer::deserialize_source(
const c10::IValue& iv) {
auto tup = iv.toTuple();
auto it = cached_sources.find(tup);
if (it != cached_sources.end()) {
return it->second;
}

const auto& tup_elems = tup->elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::string text_ = tup_elems[0].toString()->string();
  c10::optional<std::string> filename_ = tup_elems[1].toOptional<std::string>();
  int64_t starting_line_no_ = tup_elems[2].toInt();
  auto source = std::make_shared<Source>(
      std::move(text_), std::move(filename_), starting_line_no_);
cached_sources[tup] = source;
return source;
}
","class SourceRangeSerializer {
public:
// Serialize SourceRange as Tuple[SourceType, int, int]
  // where SourceType = Tuple[int, int, int, List[int]],
  // The first 2 ints are positions into the vector returned by textSaved
  // after all the Ranges are processed. textSaved() returns a vector of str
// the serialized form of Source
c10::IValue serialize(const SourceRange& sr);
  const std::vector<c10::IValue>& texts_saved() {
    return texts_;
  }

  SourceRangeSerializer() {
    texts_.emplace_back("""");
    text_to_idx_[texts_.back().toStringRef()] = 0;
  }

private:
// Serialize Source as Tuple[str, Optional[str], int, List[int]]
// This caches serialized sources, since many SourceRanges can
// refer to the same one.
  c10::IValue serialize_source(const std::shared_ptr<Source>& s);
  std::unordered_map<std::shared_ptr<Source>, c10::IValue> serialized_sources;
  int64_t store_text_and_get_index(const std::string& text_view);

  std::vector<c10::IValue> texts_;
  std::unordered_map<c10::string_view, int64_t> text_to_idx_;
};
SourceRange SourceRangeDeserializer::deserialize(const c10::IValue& iv) {
const auto& tup_elems = iv.toTupleRef().elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  std::shared_ptr<Source> source_ = deserialize_source(tup_elems[0]);
int64_t start_ = tup_elems[1].toInt();
int64_t end_ = tup_elems[2].toInt();
return SourceRange(source_, start_, end_);
}
std::shared_ptr<Source> SourceRangeDeserializer::deserialize_source(
const c10::IValue& iv) {
auto tup = iv.toTuple();
auto it = cached_sources.find(tup);
if (it != cached_sources.end()) {
return it->second;
}
  std::shared_ptr<Source> source;
const auto& tup_elems = tup->elements();
TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);
  if (!text_table_.empty()) {
    const auto& textIndex = tup_elems[0].toIntList();
    int64_t fnameIndex = tup_elems[1].toInt();
    int64_t starting_line_no_ = tup_elems[2].toInt();
    c10::optional<std::string> filename = c10::nullopt;

    filename = *text_table_[fnameIndex];
    std::vector<c10::string_view> pieces;
    std::vector<std::shared_ptr<std::string>> strs;

    for (int64_t i : textIndex) {
      pieces.emplace_back(*text_table_[i]);
      strs.emplace_back(text_table_[i]);
    }

    StringCordView str_cord(std::move(pieces), std::move(strs));

    source = std::make_shared<Source>(str_cord, filename, starting_line_no_);
  } else {
    std::string text_ = tup_elems[0].toString()->string();
    c10::optional<std::string> filename_ =
        tup_elems[1].toOptional<std::string>();
    int64_t starting_line_no_ = tup_elems[2].toInt();
    source = std::make_shared<Source>(
        std::move(text_), std::move(filename_), starting_line_no_);
  }
cached_sources[tup] = source;
return source;
}
"
290,"# Parse debug info and add begin/end markers if not present
# to ensure that we cover the entire source code.
debug_info_t = pickle.loads(raw_debug)
            assert isinstance(debug_info_t, tuple)
debug_info = list(debug_info_t)
if not debug_info:
debug_info.append((0, (('', '', 0), 0, 0)))
","# Parse debug info and add begin/end markers if not present
# to ensure that we cover the entire source code.
debug_info_t = pickle.loads(raw_debug)
            text_table = None

            if (len(debug_info_t) == 3 and
                    isinstance(debug_info_t[0], str) and
                    debug_info_t[0] == 'FORMAT_WITH_STRING_TABLE'):
                _, text_table, content = debug_info_t

                def parse_new_format(line):
                    # (0, (('', '', 0), 0, 0))
                    num, ((text_indexes, fname_idx, offset), start, end), tag = line
                    text = ''.join(text_table[x] for x in text_indexes)  # type: ignore
                    fname = text_table[fname_idx]  # type: ignore
                    return num, ((text, fname, offset), start, end), tag

                debug_info_t = map(parse_new_format, content)

debug_info = list(debug_info_t)
if not debug_info:
debug_info.append((0, (('', '', 0), 0, 0)))
"
291,"from typing import Callable, Iterator, Sized, TypeVar
from torch.utils.data import IterDataPipe, _utils, functional_datapipe
from torch.utils.data.datapipes.utils.common import DILL_AVAILABLE, check_lambda_fn

if DILL_AVAILABLE:
    import dill
    dill.extend(use_dill=False)
T_co = TypeVar(""T_co"", covariant=True)
","from typing import Callable, Iterator, Sized, TypeVar
from torch.utils.data import IterDataPipe, _utils, functional_datapipe
from torch.utils.data.datapipes.utils.common import check_lambda_fn
T_co = TypeVar(""T_co"", covariant=True)
"
292,"res = buffer_elements.pop(key)
buffer_size -= len(res)
yield self.wrapper_class(res)

    def __getstate__(self):
        if IterDataPipe.getstate_hook is not None:
            return IterDataPipe.getstate_hook(self)

        if DILL_AVAILABLE:
            dill_function = dill.dumps(self.group_key_fn)
        else:
            dill_function = self.group_key_fn
        state = (
            self.datapipe,
            dill_function,
            self.buffer_size,
            self.group_size,
            self.guaranteed_group_size,
            self.drop_remaining,
        )
        return state

    def __setstate__(self, state):
        (
            self.datapipe,
            dill_function,
            self.buffer_size,
            self.group_size,
            self.guaranteed_group_size,
            self.drop_remaining,
        ) = state
        if DILL_AVAILABLE:
            self.group_key_fn = dill.loads(dill_function)  # type: ignore[assignment]
        else:
            self.group_key_fn = dill_function  # type: ignore[assignment]
        self.wrapper_class = DataChunk
","res = buffer_elements.pop(key)
buffer_size -= len(res)
yield self.wrapper_class(res)
"
293,"const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qx);
  auto ResultBuf = makeQBufHandleNLC(
""quantized_conv1d"",
outputShape,
Dtype(out_qdtype),
","const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qx);
  auto ResultBuf = makeQBufHandleChannelsLast(
""quantized_conv1d"",
outputShape,
Dtype(out_qdtype),
"
294,"const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
  auto ResultBuf = makeQBufHandleNCHW(
""quantized_mul"", outputShape, Dtype(out_qdtype), out_qscale, out_qzero);
StmtPtr s = ExternalCall::make(
ResultBuf,
","const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
  auto ResultBuf = makeQBufHandleContiguous(
""quantized_mul"", outputShape, Dtype(out_qdtype), out_qscale, out_qzero);
StmtPtr s = ExternalCall::make(
ResultBuf,
"
295,"at::Device device) {
const BufHandle& qa = c10::get<BufHandle>(inputs[0]);
const auto out_qdtype = immQDType(qa);
  const bool isQAChannelsLast = isNHWC(qa);
  auto ResultBuf = isQAChannelsLast ? makeQBufHandleNHWC(
""quantized_relu"",
outputShape,
Dtype(out_qdtype),
immQScale(qa),
immQZero(qa))
                                    : makeQBufHandleNCHW(
""quantized_relu"",
outputShape,
Dtype(out_qdtype),
","at::Device device) {
const BufHandle& qa = c10::get<BufHandle>(inputs[0]);
const auto out_qdtype = immQDType(qa);
  const bool isQAChannelsLast = isChannelsLast(qa);
  auto ResultBuf = isQAChannelsLast ? makeQBufHandleChannelsLast(
""quantized_relu"",
outputShape,
Dtype(out_qdtype),
immQScale(qa),
immQZero(qa))
                                    : makeQBufHandleContiguous(
""quantized_relu"",
outputShape,
Dtype(out_qdtype),
"
296,"q_data.store(&q_k_v_data
[0 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
k_data.store(&q_k_v_data
[1 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
v_data.store(&q_k_v_data
[2 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
}
}
","q_data.store(&q_k_v_data
[0 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
k_data.store(&q_k_v_data
[1 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
v_data.store(&q_k_v_data
[2 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
}
}
"
297,"#endif
}
#if defined(USE_DIRECT_NVRTC)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, at::cuda::load_nvrtc());
}
#elif !defined(USE_ROCM)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, &at::cuda::detail::lazyNVRTC);
}
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
#if defined(_WIN32)
","#endif
}
#if !defined(USE_ROCM)
#if defined(USE_DIRECT_NVRTC)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, at::cuda::load_nvrtc());
}
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, &at::cuda::detail::lazyNVRTC);
}
#endif
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
#if defined(_WIN32)
"
298,"param_size /= elem_size;
if(linear_id == 0 || linear_id == num_linear_layers / 2) {
                std::initializer_list<int64_t> size = { param_size * num_linear_layers / 2, 1};
Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), offset, size);
params.emplace_back(std::move(param));
layer_params_count++;
","param_size /= elem_size;
if(linear_id == 0 || linear_id == num_linear_layers / 2) {
                const auto size = { static_cast<int64_t>(param_size * num_linear_layers / 2), 1L};
Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), offset, size);
params.emplace_back(std::move(param));
layer_params_count++;
"
299,"register_size,
schema_offset,
debug_info_offset,
      0);
return function_offset;
}
","register_size,
schema_offset,
debug_info_offset,
      class_index);
return function_offset;
}
"
300,"#include ""torch/csrc/autograd/python_return_types.h""
#include <ATen/core/Tensor.h>
#include ""c10/util/Optional.h""
#include ""c10/core/Stream.h""
","#include ""torch/csrc/autograd/python_return_types.h""
#include <ATen/core/Tensor.h>
#include <ATen/FuncTorchTLS.h>
#include ""c10/util/Optional.h""
#include ""c10/core/Stream.h""
"
301,"LINUX_CPU_TEST_RUNNER = ""linux.2xlarge""
# contains 1 gpu
LINUX_CUDA_TEST_RUNNER = ""linux.4xlarge.nvidia.gpu""
# contains 4 gpus
LINUX_ROCM_TEST_RUNNER = ""linux.rocm.gpu""
LINUX_RUNNERS = {
LINUX_CPU_TEST_RUNNER,
","LINUX_CPU_TEST_RUNNER = ""linux.2xlarge""
# contains 1 gpu
LINUX_CUDA_TEST_RUNNER = ""linux.4xlarge.nvidia.gpu""
# contains at least 2 gpus
LINUX_ROCM_TEST_RUNNER = ""linux.rocm.gpu""
LINUX_RUNNERS = {
LINUX_CPU_TEST_RUNNER,
"
302,"} // namespace native
} // namespace at
#endif // AT_MKLDNN_EBABLED
","} // namespace native
} // namespace at
#endif // AT_MKLDNN_ENABLED
"
303,"} // namespace native
} // namespace at
#else // AT_MKLDNN_EBABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
#include <ATen/native/mkldnn/Utils.h>
","} // namespace native
} // namespace at
#else // AT_MKLDNN_ENABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
#include <ATen/native/mkldnn/Utils.h>
"
304,"for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
    if (select_indices_data[i] != padding_idx) {
at::native::cpublas::axpy<data_t>(ddim, 1,
              src_data + src_stride0 * select_indices_data[i], src_stride1,
output_data + output_stride0 * add_indices_data[i], output_stride1);
} else if (bag_size.defined()) {
// Decrement bag_size to reflect that the index is padded
","for (const auto i : c10::irange(numel)) {
// We can skip indices equal to padding_idx so they are not included in
// the reduction
    auto idx = select_indices_data[i];
    TORCH_CHECK(
        idx >= 0 && idx < vocab_size,
        ""embedding_bag: Expected idx >= 0 && idx < num_embeddings but found idx to be "",
        idx);
    if (idx != padding_idx) {
at::native::cpublas::axpy<data_t>(ddim, 1,
              src_data + src_stride0 * idx, src_stride1,
output_data + output_stride0 * add_indices_data[i], output_stride1);
} else if (bag_size.defined()) {
// Decrement bag_size to reflect that the index is padded
"
305,"if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
    auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
306,"bag_size_data = bag_size.data_ptr<index_t>();
}
auto numel = add_indices.numel();
  int64_t ddim = src.sizes()[1];
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","bag_size_data = bag_size.data_ptr<index_t>();
}
auto numel = add_indices.numel();
  int64_t ddim = src.size(1);
  auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
307,"for (const auto i : c10::irange(numIndices)) {
auto bag = offset2bag_data[i];
auto word_idx = indices_data[i];

if (word_idx != static_cast<index_t>(padding_idx)) {
bool is_first_for_bag = bag_empty[bag];
for (const auto dim : c10::irange(featureSize)) {
","for (const auto i : c10::irange(numIndices)) {
auto bag = offset2bag_data[i];
auto word_idx = indices_data[i];
      TORCH_CHECK(
          word_idx >= 0 && word_idx < vocab_size,
          ""embedding_bag: Expected idx >= 0 && idx < num_embeddings but found idx to be "",
          word_idx);
if (word_idx != static_cast<index_t>(padding_idx)) {
bool is_first_for_bag = bag_empty[bag];
for (const auto dim : c10::irange(featureSize)) {
"
308,"std::vector<ExprHandle> newAxes(axes.begin(), axes.end());
ExprHandle load = promoteToDtype(
tensorOrConstant(nonEmptyInputs[0], newAxes), highType);
        auto offset = *intValue(nonEmptyInputs[0].node()->dim(dim));
        newAxes[dim] = newAxes[dim] - ExprHandle(immLike(newAxes[dim], offset));
for (size_t ii = 1; ii < nonEmptyInputs.size(); ++ii) {
auto input = nonEmptyInputs[ii];
","std::vector<ExprHandle> newAxes(axes.begin(), axes.end());
ExprHandle load = promoteToDtype(
tensorOrConstant(nonEmptyInputs[0], newAxes), highType);
        auto offset = ExprHandle(nonEmptyInputs[0].node()->dim(dim));
        newAxes[dim] = newAxes[dim] - offset;
for (size_t ii = 1; ii < nonEmptyInputs.size(); ++ii) {
auto input = nonEmptyInputs[ii];
"
309,"dispatch = str(backend_index.dispatch_key).lower()
empty_impl = f""at::detail::empty_{dispatch}""
empty_strided_impl = f""at::detail::empty_strided_{dispatch}""
elif backend_index.dispatch_key == DispatchKey.CompositeExplicitAutograd:
empty_impl = ""at::empty""
empty_strided_impl = ""at::empty_strided""
else:
return []
return [f""""""
Tensor create_out(IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {{
if (strides.empty()) {{
return {empty_impl}(sizes, {empty_options});
}} else {{
","dispatch = str(backend_index.dispatch_key).lower()
empty_impl = f""at::detail::empty_{dispatch}""
empty_strided_impl = f""at::detail::empty_strided_{dispatch}""
        runtime_empty_supported_check = """"
elif backend_index.dispatch_key == DispatchKey.CompositeExplicitAutograd:
empty_impl = ""at::empty""
empty_strided_impl = ""at::empty_strided""
        runtime_empty_supported_check = """"""\
  if (!c10::detail::backend_supports_empty_operator(options)) {{
    // The main purpose of this CompositeExplicitAutograd kernel is to provide
    // a ""free"" implementation of out-of-place operators.
    // If a backend hasn't implemented an out-of-place op but has implemented
    // the out= variant, then this kernel will call their out= variant.
    // It does that by using at::empty() to create the tensor to pass to the out= variant though,
    // so this ""default"" kernel doesn't actually handle backends that don't support at::empty
    // (e.g. quantized backends).
    // Returning an undefined tensor here allows us to reach the out= kernel and give a better error.
    // Longer term, this could be better fixed by https://github.com/pytorch/pytorch/issues/52680
    return at::Tensor();
  }}
""""""
else:
return []
return [f""""""
Tensor create_out(IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {{
  {runtime_empty_supported_check}
if (strides.empty()) {{
return {empty_impl}(sizes, {empty_options});
}} else {{
"
310,"output.squeeze_(0);
}
return output;
}
Tensor max_pool1d(
const Tensor& self,
IntArrayRef kernel_size,
","output.squeeze_(0);
}
  guard.reset();
  namedinference::propagate_names(output, self);

return output;
}
} // namespace

Tensor max_pool1d(
const Tensor& self,
IntArrayRef kernel_size,
"
311,"""of the TensorRT region!""
)
layer = network.add_activation(input_val, operation_type)
    if alpha:
layer.alpha = alpha
    if beta:
layer.beta = beta
set_layer_name(layer, target, name)
return layer.get_output(0)
","""of the TensorRT region!""
)
layer = network.add_activation(input_val, operation_type)
    if alpha is not None:
layer.alpha = alpha
    if beta is not None:
layer.beta = beta
set_layer_name(layer, target, name)
return layer.get_output(0)
"
312,"void printConstant(TaggedStringStream& stmt, const IValue& v) {
const auto customFormatter = [&](std::ostream& ss, const IValue& v) {
if (v.isTensor() || containsNonASCIIString(v) || v.isObject()) {
        TORCH_INTERNAL_ASSERT(!v.type()->is_module());
ss << ""CONSTANTS.c"" << getOrAddConstant(v);
return true;
}
      if (v.isTuple() && v.type()->expectRef<TupleType>().schema()) {
// print the namedtuple constructor and let rest of tuple printing
// continue
        ss << v.type()->expectRef<TupleType>().annotation_str(type_printer_);
}
return false;
};
","void printConstant(TaggedStringStream& stmt, const IValue& v) {
const auto customFormatter = [&](std::ostream& ss, const IValue& v) {
if (v.isTensor() || containsNonASCIIString(v) || v.isObject()) {
        TORCH_INTERNAL_ASSERT(!v.type<c10::Type>()->is_module());
ss << ""CONSTANTS.c"" << getOrAddConstant(v);
return true;
}
      auto type = v.type();
      if (auto dyn = type->castRaw<c10::DynamicType>()) {
        type = dyn->fallback();
      }
      if (v.isTuple() && type->expectRef<TupleType>().schema()) {
// print the namedtuple constructor and let rest of tuple printing
// continue
        ss << type->expectRef<TupleType>().annotation_str(type_printer_);
}
return false;
};
"
313,"#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/mobile/type_parser.h>
namespace torch {
namespace jit {
// From operator_versions_map
${operator_version_map}
","#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/mobile/type_parser.h>
namespace c10 {
TypePtr parseType(const std::string& pythonStr);
} // namespace c10

namespace torch {
namespace jit {
// clang-format off

// From operator_versions_map
${operator_version_map}
"
314,"N=instruction[2],
)
)
    return INSTRUCTION_LIST.substitute(instruction_list="""".join(instruction_list_part))
def construct_constants(constants_list_from_yaml: List[Any]) -> str:
constants_list_part = []
","N=instruction[2],
)
)
    return INSTRUCTION_LIST.substitute(instruction_list="""".join(instruction_list_part).lstrip(""\n""))
def construct_constants(constants_list_from_yaml: List[Any]) -> str:
constants_list_part = []
"
315,"# TODO: remove the skip after these two operators schemas are fixed
if op_name in EXCLUDED_OP_SET:
continue
        for upgrader_entry in sorted_version_map[op_name]:
            # Split a string by ""_"" and filter empty string in the list
            # For example: ""div__Scalar_0_3"" => ['div', 'Scalar', '0', '3']
            upgrader_info = list(filter(lambda token: token != """", upgrader_entry.upgrader_name.split('_')))
            upgrader_min_version = upgrader_info[2]
            upgrader_max_version = upgrader_info[3]
upgrader_name = upgrader_entry.upgrader_name

bytecode_function_index = upgrader_bytecode_function_to_index_map[upgrader_name]
upgraders_in_version_map_part.append(
ONE_UPGRADER_IN_VERSION_MAP.substitute(
                    upgrader_min_version=upgrader_min_version,
                    upgrader_max_version=upgrader_max_version,
upgrader_name=upgrader_name,
bytecode_func_index=bytecode_function_index,
)
","# TODO: remove the skip after these two operators schemas are fixed
if op_name in EXCLUDED_OP_SET:
continue
        upgrader_ranges = torch._C._get_upgrader_ranges(op_name)
        upgrader_entries = sorted_version_map[op_name]
        assert len(upgrader_ranges) == len(upgrader_entries)
        for idx, upgrader_entry in enumerate(upgrader_entries):
upgrader_name = upgrader_entry.upgrader_name
bytecode_function_index = upgrader_bytecode_function_to_index_map[upgrader_name]
upgraders_in_version_map_part.append(
ONE_UPGRADER_IN_VERSION_MAP.substitute(
                    upgrader_min_version=upgrader_ranges[idx].min_version,
                    upgrader_max_version=upgrader_ranges[idx].max_version,
upgrader_name=upgrader_name,
bytecode_func_index=bytecode_function_index,
)
"
316,"REDUCER_CHECK(
local_used_map_[variable_index].item<int>() == 0,
logger_,
            ""Encountered gradient which is undefined, but still allreduced by DDP reducer. This indicates a bug in DDP implementation, please report a bug with a repro to PyTorch."");
}
bucket_view.zero_();
}
","REDUCER_CHECK(
local_used_map_[variable_index].item<int>() == 0,
logger_,
            ""Encountered gradient which is undefined, but still allreduced by ""
            ""DDP reducer. This indicates a bug in DDP implementation, please ""
            ""report a bug with a repro to PyTorch.""
        );
}
bucket_view.zero_();
}
"
317,"Module& module,
const std::vector<std::string>& other_methods) {
// if not frozen yet
if (module._ivalue()->type()->hasAttribute(""training"")) {
    auto mod = freeze(module, {}, true);
}
  optimize_for_inference(module.get_method(""forward"").graph());
for (const auto& method : other_methods) {
    optimize_for_inference(module.get_method(method).graph());
}
  return module;
}
buffer_list Module::buffers(bool recurse) const {
","Module& module,
const std::vector<std::string>& other_methods) {
// if not frozen yet
  Module frozen_mod;
if (module._ivalue()->type()->hasAttribute(""training"")) {
    frozen_mod = freeze(module, {}, true);
  }
  else {
    frozen_mod = module;
}
  optimize_for_inference(frozen_mod.get_method(""forward"").graph());
for (const auto& method : other_methods) {
    optimize_for_inference(frozen_mod.get_method(method).graph());
}
  return frozen_mod;
}
buffer_list Module::buffers(bool recurse) const {
"
318,"return_vec.reserve(returns.size());
for (const auto& arg : args) {
int index = storeIValueAndGetIndex(fbb, arg.default_value());
arg_vec.emplace_back(CreateArg(
fbb,
fbb.CreateSharedString(arg.name()),
","return_vec.reserve(returns.size());
for (const auto& arg : args) {
int index = storeIValueAndGetIndex(fbb, arg.default_value());
    TORCH_INTERNAL_ASSERT(arg.type()->kind() != c10::DynamicType::Kind);
arg_vec.emplace_back(CreateArg(
fbb,
fbb.CreateSharedString(arg.name()),
"
319,"values_below.unsqueeze_(0).transpose_(0, -1).squeeze_(-1);
}
  out.copy_(values_below);
}
std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cpu(
","values_below.unsqueeze_(0).transpose_(0, -1).squeeze_(-1);
}
  return values_below;
}

} // namespace

void quantile_out_impl(
    Tensor& out,
    const Tensor& self,
    const Tensor& q,
    const optional<int64_t> original_dim,
    const bool keepdim,
    const QUANTILE_INTERPOLATION_MODE& interpolation,
    const bool ignore_nan) {
  quantile_checks(self, q);
  TORCH_CHECK(
      self.scalar_type() == out.scalar_type(),
      ""quantile() out tensor must be same dtype as the input tensor"");
  TORCH_CHECK(
      self.device() == out.device(),
      ""quantile() out tensor must be on the same device as the input tensor"");

  int64_t wrapped_dim = at::maybe_wrap_dim(original_dim.value_or(0), self.dim());

  auto out_shape = quantile_output_shape(original_dim, self, q, keepdim, wrapped_dim);
  resize_output(out, out_shape);

  auto quantile = quantile_compute(
      self, q, original_dim, keepdim, interpolation, ignore_nan, wrapped_dim, out_shape);
  out.copy_(quantile);
}

Tensor quantile_impl(
    const Tensor& self,
    const Tensor& q,
    const optional<int64_t> original_dim,
    const bool keepdim,
    const QUANTILE_INTERPOLATION_MODE& interpolation,
    const bool ignore_nan) {
  quantile_checks(self, q);

  int64_t wrapped_dim = at::maybe_wrap_dim(original_dim.value_or(0), self.dim());

  auto out_shape = quantile_output_shape(original_dim, self, q, keepdim, wrapped_dim);

  return quantile_compute(
      self, q, original_dim, keepdim, interpolation, ignore_nan, wrapped_dim, out_shape);
}
std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cpu(
"
320,"bool keepdim,
const c10::string_view interpolation,
Tensor& out) {
  quantile_impl(
out,
self,
q,
","bool keepdim,
const c10::string_view interpolation,
Tensor& out) {
  quantile_out_impl(
out,
self,
q,
"
321,"continue
conv = modules[node.args[0].target]
bn = modules[node.target]
fused_conv = fuse_conv_bn_eval(conv, bn)
replace_node_module(node.args[0], modules, fused_conv)
node.replace_all_uses_with(node.args[0])
","continue
conv = modules[node.args[0].target]
bn = modules[node.target]
                if not bn.track_running_stats:
                    continue
fused_conv = fuse_conv_bn_eval(conv, bn)
replace_node_module(node.args[0], modules, fused_conv)
node.replace_all_uses_with(node.args[0])
"
322,"convert_dict_to_ordered_dict(qconfig_dict)
convert_dict_to_ordered_dict(equalization_qconfig_dict)
flattened_qconfig_dict = get_flattened_qconfig_dict(qconfig_dict)
# TODO: support regex as well
propagate_qconfig_(model, flattened_qconfig_dict)
","convert_dict_to_ordered_dict(qconfig_dict)
convert_dict_to_ordered_dict(equalization_qconfig_dict)
    qconfig_dict = update_qconfig_for_fusion(model, qconfig_dict)
    equalization_qconfig_dict = update_qconfig_for_fusion(model, equalization_qconfig_dict)
flattened_qconfig_dict = get_flattened_qconfig_dict(qconfig_dict)
# TODO: support regex as well
propagate_qconfig_(model, flattened_qconfig_dict)
"
323,"nnqd.LSTM,
nn.BatchNorm2d,
nn.BatchNorm3d,
nn.ConvTranspose1d,
nn.ConvTranspose2d,
nn.ConvTranspose3d,
","nnqd.LSTM,
nn.BatchNorm2d,
nn.BatchNorm3d,
        nn.Dropout,
nn.ConvTranspose1d,
nn.ConvTranspose2d,
nn.ConvTranspose3d,
"
324,"'Quantize',
'ReLU6',
'Sigmoid',
# Wrapper modules
'FloatFunctional',
'FXFloatFunctional',
","'Quantize',
'ReLU6',
'Sigmoid',
    'Dropout',
# Wrapper modules
'FloatFunctional',
'FXFloatFunctional',
"
325,".def(""_jit_pass_constant_pooling"", ConstantPooling)
// RemoveInplaceOps is used by CoreML so it must be removed with care.
.def(""_jit_pass_propagate_dtype"", DtypePropagation)
.def(
""_jit_pass_remove_inplace_ops"",
[](const std::shared_ptr<Graph>& g) { return RemoveInplaceOps(g); })
",".def(""_jit_pass_constant_pooling"", ConstantPooling)
// RemoveInplaceOps is used by CoreML so it must be removed with care.
.def(""_jit_pass_propagate_dtype"", DtypePropagation)
      .def(""_jit_pass_propagate_device"", DeviceTypePropagation)
.def(
""_jit_pass_remove_inplace_ops"",
[](const std::shared_ptr<Graph>& g) { return RemoveInplaceOps(g); })
"
326,"is_enabled = true;
}
void SavedTensorDefaultHooks::set_hooks(PyObject* pack_hook, PyObject* unpack_hook) {
  if (!is_enabled) {
    TORCH_INTERNAL_ASSERT(pack_hook == nullptr && unpack_hook == nullptr);
    return;
  }
  pack_hook_ = pack_hook;
  unpack_hook_ = unpack_hook;
}
std::pair<PyObject*, PyObject*> SavedTensorDefaultHooks::get_hooks() {
  if (!is_enabled) {
return std::make_pair(nullptr, nullptr);
}
  return std::make_pair(pack_hook_, unpack_hook_);
}
}
","is_enabled = true;
}
void SavedTensorDefaultHooks::push_hooks(PyObject* pack_hook, PyObject* unpack_hook) {
  // Reference counting is handled by the caller of `push_hooks`
  TORCH_INTERNAL_ASSERT(is_enabled);
  TORCH_INTERNAL_ASSERT(pack_hook != nullptr && unpack_hook != nullptr);
  stack.push(std::make_pair(pack_hook, unpack_hook));
}

void SavedTensorDefaultHooks::pop_hooks() {
  // Reference counting is handled by the caller of `pop_hooks`
  TORCH_INTERNAL_ASSERT(is_enabled && !stack.empty());
  stack.pop();
}
std::pair<PyObject*, PyObject*> SavedTensorDefaultHooks::get_hooks() {
  if (!is_enabled || stack.empty()) {
return std::make_pair(nullptr, nullptr);
}
  return stack.top();
}

std::stack<std::pair<PyObject*, PyObject*>> SavedTensorDefaultHooks::get_stack() {
  return stack;
}

void SavedTensorDefaultHooks::set_stack(std::stack<std::pair<PyObject*, PyObject*>> stack_) {
  stack = stack_;
}
}
"
327,"at::set_record_function_tls_(state.rf_tls_);
  SavedTensorDefaultHooks::set_hooks(
      state.saved_tensors_default_hooks_.first,
      state.saved_tensors_default_hooks_.second);
c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo(state.debug_info_);
","at::set_record_function_tls_(state.rf_tls_);
  at::SavedTensorDefaultHooks::set_stack(state.saved_tensors_default_hooks_);
c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo(state.debug_info_);
"
328,"Default weight observer.
""""""
default_histogram_observer = HistogramObserver.with_args(reduce_range=True)
""""""
Default histogram observer, usually used for PTQ.
""""""
","Default weight observer.
""""""
default_histogram_observer = HistogramObserver.with_args(quant_min=0, quant_max=127)
""""""
Default histogram observer, usually used for PTQ.
""""""
"
329,"fun_type->function()->name(),
""' to "",
*cur);
          GRAPH_UPDATE(""Function body: "", graphFunction->optimized_graph());
          inlineCallTo(cur, graphFunction);
}
} break;
case prim::CallMethod: {
","fun_type->function()->name(),
""' to "",
*cur);

          std::shared_ptr<Graph> g = nullptr;
          // inline optimized graph for debugging/testing purposes.
          // we only insert fallback functions in JIT optimized graphs for
          // execution, not on the Graph that is used for serialization
          bool fallback =
              function_constant->hasAttribute(Symbol::attr(""fallback""));
          if (fallback && graphFunction->get_executor().isOptimized()) {
            auto exec_plans =
                graphFunction->get_executor().getDebugState().execution_plans;
            if (exec_plans.size() != 0) {
              g = exec_plans.begin()->second.graph;
              // optimized_graph() calls Inline, so we only need to explicitly
              // invoke inlining on the jit optimized graph with recursive
              // fallback funciton calls
              Inline(*g.get());
            }
          }
          if (g == nullptr) {
            g = graphFunction->optimized_graph();
          }

          GRAPH_UPDATE(""Function body: "", g);
          inlineCallTo(cur, graphFunction, g.get());
}
} break;
case prim::CallMethod: {
"
330,"super(Adam, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('amsgrad', False)
@torch.no_grad()
def step(self, closure=None):
","super(Adam, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('amsgrad', False)
            group.setdefault('maximize', False)
@torch.no_grad()
def step(self, closure=None):
"
331,"namespace at { namespace native {
DECLARE_DISPATCH(void(*)(TensorIterator&, const Scalar&, const Scalar&, const Scalar&), arange_stub);
DECLARE_DISPATCH(void(*)(TensorIterator&, const Scalar&, const Scalar&, int64_t), linspace_stub);
Tensor& linspace_out(const Scalar& start, const Scalar& end, c10::optional<int64_t> optional_steps, Tensor& result) {
const auto steps = optional_steps.value_or(100);
","namespace at { namespace native {
Tensor& linspace_out(const Scalar& start, const Scalar& end, c10::optional<int64_t> optional_steps, Tensor& result) {
const auto steps = optional_steps.value_or(100);
"
332,"&\rule{110mm}{0.4pt}                                                                 \\
&\textbf{input}      : \gamma \text{ (lr)}, \beta_1, \beta_2
\text{ (betas)},\theta_0 \text{ (params)},f(\theta) \text{ (objective)}          \\
            &\hspace{13mm}      \lambda \text{ (weight decay)},  \: \textit{amsgrad},\:          \\
            \textit{maximize}                                                                    \\
&\textbf{initialize} :  m_0 \leftarrow 0 \text{ ( first moment)},
v_0\leftarrow 0 \text{ (second moment)},\: \widehat{v_0}^{max}\leftarrow 0\\[-1.ex]
&\rule{110mm}{0.4pt}                                                                 \\
&\textbf{for} \: t=1 \: \textbf{to} \: \ldots \: \textbf{do}                         \\
            &\hspace{5mm} /textbf{if} \: \textit{maximize}:                                      \\
            &\hspace{10mm}g_t           \leftarrow   -\nabla_{\theta} f_t (\theta_{t-1})           \\
            &\hspace{5mm} /textbf{else}                                                          \\
            &\hspace{10mm}g_t           \leftarrow   \nabla_{\theta} f_t (\theta_{t-1})           \\
&\hspace{5mm}\textbf{if} \: \lambda \neq 0                                           \\
&\hspace{10mm} g_t \leftarrow g_t + \lambda  \theta_{t-1}                            \\
&\hspace{5mm}m_t           \leftarrow   \beta_1 m_{t-1} + (1 - \beta_1) g_t          \\
","&\rule{110mm}{0.4pt}                                                                 \\
&\textbf{input}      : \gamma \text{ (lr)}, \beta_1, \beta_2
\text{ (betas)},\theta_0 \text{ (params)},f(\theta) \text{ (objective)}          \\
            &\hspace{13mm}      \lambda \text{ (weight decay)},  \: \textit{amsgrad},
                \:\textit{maximize}                                                              \\
&\textbf{initialize} :  m_0 \leftarrow 0 \text{ ( first moment)},
v_0\leftarrow 0 \text{ (second moment)},\: \widehat{v_0}^{max}\leftarrow 0\\[-1.ex]
&\rule{110mm}{0.4pt}                                                                 \\
&\textbf{for} \: t=1 \: \textbf{to} \: \ldots \: \textbf{do}                         \\
            &\hspace{5mm}\textbf{if} \: \textit{maximize}:                                       \\
            &\hspace{10mm}g_t           \leftarrow   -\nabla_{\theta} f_t (\theta_{t-1})         \\
            &\hspace{5mm}\textbf{else}                                                           \\
            &\hspace{10mm}g_t           \leftarrow   \nabla_{\theta} f_t (\theta_{t-1})          \\
&\hspace{5mm}\textbf{if} \: \lambda \neq 0                                           \\
&\hspace{10mm} g_t \leftarrow g_t + \lambda  \theta_{t-1}                            \\
&\hspace{5mm}m_t           \leftarrow   \beta_1 m_{t-1} + (1 - \beta_1) g_t          \\
"
333,"import torch
from torch._C import _add_docstr, _linalg  # type: ignore[attr-defined]
Tensor = torch.Tensor
common_notes = {
","import torch
from torch._C import _add_docstr, _linalg  # type: ignore[attr-defined]
LinAlgError = torch._C._LinAlgError  # type: ignore[attr-defined]

Tensor = torch.Tensor
common_notes = {
"
334,"}
}
StaticRuntime::Deallocator::~Deallocator() {
// Assume cleanup cannot throw.
cleanupImpl();
","}
}
bool StaticRuntime::fast_check_and_correct_overlap_with(
    ProcessedNode& n,
    c10::IValue& tensor_ival) {
  auto& tensor = tensor_ival.toTensor();
  if (planner_->overlapWithInternalBuffer(tensor.data_ptr())) {
    DLOG(INFO) << ""Detected alias for node: "" << PrintNode(n.node());
    tensor_ival = at::native::clone(tensor, c10::nullopt);
    n.set_outputs_memory_overlap_detected();
    return true;
  }
  return false;
}

StaticRuntime::Deallocator::~Deallocator() {
// Assume cleanup cannot throw.
cleanupImpl();
"
335,"name = mod_prefix + ('.' if mod_prefix else '') + name
yield name, val
        # find all nn.Parameters
        for name, val in mod.named_parameters():
            yield name, val
","name = mod_prefix + ('.' if mod_prefix else '') + name
yield name, val
    # find all nn.Parameters
    for name, val in module.named_parameters():
        yield name, val
"
336,"auto bias_n = graph->insertNode(graph->create(
prim::add_optional, {n->output(0), unsqueezed_bias->output()}, 1));
bias_n->output()->setType(n->output(0)->type());
// replace later uses
n->output(0)->replaceAllUsesAfterNodeWith(bias_n, bias_n->output());
","auto bias_n = graph->insertNode(graph->create(
prim::add_optional, {n->output(0), unsqueezed_bias->output()}, 1));
bias_n->output()->setType(n->output(0)->type());
    // moving add_optional after conv2d since it uses its output.
    bias_n->moveAfter(n);
// replace later uses
n->output(0)->replaceAllUsesAfterNodeWith(bias_n, bias_n->output());
"
337,"# type here.
@no_type_check
@torch.jit.script
    def full_names_0_4(size: List[int], fill_value: Union[int, float], *,
                       dtype: Optional[int], layout: Optional[int], device: Optional[torch.device],
                       pin_memory: Optional[bool]) -> torch.Tensor:
if dtype is None:
fill_value = float(fill_value)
return torch.full(size, fill_value, dtype=dtype, layout=layout,
","# type here.
@no_type_check
@torch.jit.script
    def full_0_4(size: List[int], fill_value: Union[int, float], *,
                 dtype: Optional[int], layout: Optional[int], device: Optional[torch.device],
                 pin_memory: Optional[bool]) -> torch.Tensor:
if dtype is None:
fill_value = float(fill_value)
return torch.full(size, fill_value, dtype=dtype, layout=layout,
"
338,"'_DatasetKind',
'argument_validation',
'communication',
'functional_datapipe',
'get_worker_info',
'guaranteed_datapipes_determinism',
","'_DatasetKind',
'argument_validation',
'communication',
           'default_collate',
           'default_convert',
'functional_datapipe',
'get_worker_info',
'guaranteed_datapipes_determinism',
"
339,"print('Adjusting learning rate'
' of group {} to {:.4e}.'.format(group, lr))
else:
                print('Epoch {:5d}: adjusting learning rate'
                      ' of group {} to {:.4e}.'.format(epoch, group, lr))
def step(self, epoch=None):
","print('Adjusting learning rate'
' of group {} to {:.4e}.'.format(group, lr))
else:
                epoch_str = (""%.2f"" if isinstance(epoch, float) else
                             ""%.5d"") % epoch
                print('Epoch {}: adjusting learning rate'
                      ' of group {} to {:.4e}.'.format(epoch_str, group, lr))
def step(self, epoch=None):
"
340,"gi.run();
}
void RemoveProfilingNodes(const std::shared_ptr<Graph>& graph) {
  removeProfilingNodes(graph->block());
}

} // namespace jit
} // namespace torch
","gi.run();
}
} // namespace jit
} // namespace torch
"
341,"const bool include_last_offset,
const bool requires_grad) {
if (requires_grad || mode == MODE_MEAN || mode == MODE_MAX) {
    auto num_bags = offsets.size(0) - (include_last_offset ? 1 : 0);
    bag_size_out = at::zeros({num_bags}, offsets.options());
// Compute this for MODE_MEAN and MODE_MAX (latter needed for backwards)
if (num_bags != 1) {
bag_size_out.slice(0, 0, bag_size_out.sizes()[0] - 1, 1) =
","const bool include_last_offset,
const bool requires_grad) {
if (requires_grad || mode == MODE_MEAN || mode == MODE_MAX) {
    auto num_bags = offsets.sizes()[0] - (include_last_offset ? 1 : 0);
    at::native::resize_(bag_size_out, {num_bags}, c10::nullopt);
// Compute this for MODE_MEAN and MODE_MAX (latter needed for backwards)
if (num_bags != 1) {
bag_size_out.slice(0, 0, bag_size_out.sizes()[0] - 1, 1) =
"
342,"if (num_bags > 0) {
bag_size_out[-1] = indices.sizes()[0] - offsets[num_bags - 1];
}
}
}
","if (num_bags > 0) {
bag_size_out[-1] = indices.sizes()[0] - offsets[num_bags - 1];
}
  } else {
    at::native::resize_(bag_size_out, offsets.sizes(), c10::nullopt);
}
}
"
343,"float StaticRuntime::benchmark_model(
const std::vector<std::vector<c10::IValue>>& args_list,
    const std::vector<std::unordered_map<std::string, c10::IValue>>&
        kwargs_list,
const int warmup_runs,
const int main_runs) {
TORCH_CHECK(warmup_runs >= 0 && main_runs >= 1);
","float StaticRuntime::benchmark_model(
const std::vector<std::vector<c10::IValue>>& args_list,
    const std::vector<KeywordArgs>& kwargs_list,
const int warmup_runs,
const int main_runs) {
TORCH_CHECK(warmup_runs >= 0 && main_runs >= 1);
"
344,"double output_scale,
int64_t output_zero_point);
#endif // USE_PYTORCH_QNNPACK
namespace at {
","double output_scale,
int64_t output_zero_point);
template at::Tensor PackedConvWeightsQnnp<2>::apply_impl<false>(
    const at::Tensor& act,
    double output_scale,
    int64_t output_zero_point);

template at::Tensor PackedConvWeightsQnnp<3>::apply_impl<false>(
  const at::Tensor& act,
  double output_scale,
  int64_t output_zero_point);

#endif // USE_PYTORCH_QNNPACK
namespace at {
"
345," ``constraints.real_vector``
 ``constraints.real``
 ``constraints.simplex``
 ``constraints.stack``
 ``constraints.unit_interval``
""""""
","- ``constraints.symmetric``
- ``constraints.square``
- ``constraints.symmetric``
""""""
"
346,"TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!isFunctionalTensor(tensor));
return at::detail::make_tensor<FunctionalTensorWrapper>(tensor);
}
TensorList to_functional_tensor(const c10::List<Tensor>& t_list) {
  std::vector<Tensor> outputs(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
    outputs[i] = to_functional_tensor(t_list[i]);
}
return outputs;
}
","TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!isFunctionalTensor(tensor));
return at::detail::make_tensor<FunctionalTensorWrapper>(tensor);
}
c10::List<Tensor> to_functional_tensor(const c10::List<Tensor>& t_list) {
  c10::List<Tensor> outputs;
  outputs.reserve(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
    outputs.push_back(to_functional_tensor(t_list[i]));
}
return outputs;
}
"
347,"const auto returns_begin = stack->size() - num_returns;
auto returns = torch::jit::last(stack, num_returns);
    for (int64_t idx = 0; idx < num_returns; ++idx) {
const auto& ivalue = returns[idx];
if (ivalue.isTensor()) {
auto t = ivalue.toTensor();
","const auto returns_begin = stack->size() - num_returns;
auto returns = torch::jit::last(stack, num_returns);
    for (const auto idx : c10::irange(num_returns)) {
const auto& ivalue = returns[idx];
if (ivalue.isTensor()) {
auto t = ivalue.toTensor();
"
348,"zero_numel_check_dims(self, dim, ""median()"");
if (self.dim() > 0) {
assert(dim >= 0);
    assert(dim < out_shape.size());
if (keepdim) {
out_shape[dim] = 1;
","zero_numel_check_dims(self, dim, ""median()"");
if (self.dim() > 0) {
assert(dim >= 0);
    assert(dim < static_cast<int64_t>(out_shape.size()));
if (keepdim) {
out_shape[dim] = 1;
"
349,"the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field :attr:`size_average`
is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when :attr:`reduce` is ``False``. Default: ``True``
ignore_index (int, optional): Specifies a target value that is ignored
and does not contribute to the input gradient. When
:attr:`size_average` is ``True``, the loss is averaged over
","the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field :attr:`size_average`
is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when :attr:`reduce` is ``False``. Default: ``None``
ignore_index (int, optional): Specifies a target value that is ignored
and does not contribute to the input gradient. When
:attr:`size_average` is ``True``, the loss is averaged over
"
350,"else:
storage = obj
storage_type = normalize_storage_type(type(obj))
dtype = torch.uint8
storage_numel = cast(Storage, storage).nbytes()
view_metadata: Optional[Tuple[str, int, int]]
storage = cast(Storage, storage)
","else:
storage = obj
                storage_dtype = storage.dtype
storage_type = normalize_storage_type(type(obj))
dtype = torch.uint8
storage_numel = cast(Storage, storage).nbytes()
            if storage.data_ptr() in storage_dtypes:
                if storage_dtype != storage_dtypes[storage.data_ptr()]:
                    raise RuntimeError(
                        'Cannot save multiple tensors or storages that '
                        'view the same data as different types')
            else:
                storage_dtypes[storage.data_ptr()] = storage_dtype

view_metadata: Optional[Tuple[str, int, int]]
storage = cast(Storage, storage)
"
351,"REGISTER_CPU_OPERATOR(
Div,
    BinaryElementwiseBroadcastOp<NumericTypes, CPUContext, DivFunctor<CPUContext>>);
} // namespace caffe2
","REGISTER_CPU_OPERATOR(
Div,
    BinaryElementwiseOp<NumericTypes, CPUContext, DivFunctor<CPUContext>>);
} // namespace caffe2
"
352,"const TIn* B,
TGrad* dA,
TGrad* dB,
    CPUContext* context,
    bool allow_broadcast_fastpath) {
const auto A_size = c10::multiply_integers(A_dims, A_dims + ndim);
const auto B_size = c10::multiply_integers(B_dims, B_dims + ndim);
const auto C_size = c10::multiply_integers(C_dims, C_dims + ndim);
math::Set<TGrad, CPUContext>(A_size, TGrad(0), dA, context);
math::Set<TGrad, CPUContext>(B_size, TGrad(0), dB, context);
if (
      allow_broadcast_fastpath
      && math::can_use_broadcast_fastpath(ndim, A_dims)
&& math::can_use_broadcast_fastpath(ndim, B_dims)) {
ComputeMulGradientFastpath(A_size, B_size, C_size, dC, A, B, dA, dB);
return;
","const TIn* B,
TGrad* dA,
TGrad* dB,
    CPUContext* context) {
const auto A_size = c10::multiply_integers(A_dims, A_dims + ndim);
const auto B_size = c10::multiply_integers(B_dims, B_dims + ndim);
const auto C_size = c10::multiply_integers(C_dims, C_dims + ndim);
math::Set<TGrad, CPUContext>(A_size, TGrad(0), dA, context);
math::Set<TGrad, CPUContext>(B_size, TGrad(0), dB, context);
if (
      math::can_use_broadcast_fastpath(ndim, A_dims)
&& math::can_use_broadcast_fastpath(ndim, B_dims)) {
ComputeMulGradientFastpath(A_size, B_size, C_size, dC, A, B, dA, dB);
return;
"
353,"add_unary_layer,
add_activation_layer,
extend_attr_to_tuple,
)
","add_unary_layer,
add_activation_layer,
extend_attr_to_tuple,
    get_positive_dim,
)
"
354,"dim = kwargs[""dim""]
input_shape = input_val.shape
input_shape_size = len(input_val.shape) + 1 if network.has_implicit_batch_dimension else len(input_val.shape)
    if dim < 0:
        dim = dim % (input_shape_size + 1)
if network.has_implicit_batch_dimension:
assert dim != 0
dim -= 1
","dim = kwargs[""dim""]
input_shape = input_val.shape
input_shape_size = len(input_val.shape) + 1 if network.has_implicit_batch_dimension else len(input_val.shape)
    dim = get_positive_dim(dim, input_shape_size + 1)

if network.has_implicit_batch_dimension:
assert dim != 0
dim -= 1
"
355,"return {reverse_lambda.inner_call()}
}}
);
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
// See  Note [Propagating strides in the functionalization pass]
at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
return out;
","return {reverse_lambda.inner_call()}
}}
);
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, {view_tensor_name}, view_meta);
// See  Note [Propagating strides in the functionalization pass]
at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
return out;
"
356,"import sys
import time
from enum import Enum
from typing import Any, List, NamedTuple, Optional
","import sys
import time
from enum import Enum
from pathlib import Path
from typing import Any, List, NamedTuple, Optional
"
357,"}
// Prepare for memory planning
  AliasDb alias_db(
      graph_, /*isFrozen=*/false, /*enablePreciseTupleContainerAnalysis=*/true);
value_group_.init(graph_, alias_db);
GRAPH_DEBUG(value_group_.toString());
","}
// Prepare for memory planning
value_group_.init(graph_, alias_db);
GRAPH_DEBUG(value_group_.toString());
"
358,"init_method: str = rpc_contants.DEFAULT_INIT_METHOD,
device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,
devices: Optional[List[DeviceType]] = None,
        _transports: List = None,
        _channels: List = None,
):
full_device_maps = (
            {} if device_maps is None else
            {k : _to_device_map(v) for k, v in device_maps.items()}
        )
        full_device_list = (
            [] if devices is None else
            _to_device_list(devices)
)
super().__init__(
num_worker_threads,
_transports,
","init_method: str = rpc_contants.DEFAULT_INIT_METHOD,
device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,
devices: Optional[List[DeviceType]] = None,
        _transports: Optional[List] = None,
        _channels: Optional[List] = None,
):
full_device_maps = (
            {}
            if device_maps is None
            else {k: _to_device_map(v) for k, v in device_maps.items()}
)
        full_device_list = [] if devices is None else _to_device_list(devices)
super().__init__(
num_worker_threads,
_transports,
"
359,">>>
>>> with dist_autograd.context() as context_id:
>>>     pred = ddp_model(rref.to_here())
            >>>     loss = loss_func(pred, loss)
            >>>     dist_autograd.backward(context_id, loss)
            >>>     dist_optim.step()
.. note::
To let a non-DDP model load a state dict from a DDP model,
",">>>
>>> with dist_autograd.context() as context_id:
>>>     pred = ddp_model(rref.to_here())
            >>>     loss = loss_func(pred, target)
            >>>     dist_autograd.backward(context_id, [loss])
            >>>     dist_optim.step(context_id)
.. note::
To let a non-DDP model load a state dict from a DDP model,
"
360,"std::shared_ptr<RpcAgent> RpcAgent::getCurrentRpcAgent() {
std::shared_ptr<RpcAgent> agent = std::atomic_load(&currentRpcAgent_);
  TORCH_INTERNAL_ASSERT(agent, ""Current RPC agent is not set!"");
return agent;
}
","std::shared_ptr<RpcAgent> RpcAgent::getCurrentRpcAgent() {
std::shared_ptr<RpcAgent> agent = std::atomic_load(&currentRpcAgent_);
  TORCH_CHECK(
      agent,
      ""Current RPC agent is not set! Did you initialize the RPC ""
      ""framework (e.g. by calling `rpc.init_rpc`)?"");
return agent;
}
"
361,"**config.rdzv_configs,
)
    agent = None
    rdzv_handler = rdzv_registry.get_rendezvous_handler(rdzv_parameters)
master_addr, master_port = _get_addr_and_port(rdzv_parameters)
    try:
        spec = WorkerSpec(
            role=config.role,
            local_world_size=config.nproc_per_node,
            entrypoint=entrypoint,
            args=tuple(args),
            rdzv_handler=rdzv_handler,
            max_restarts=config.max_restarts,
            monitor_interval=config.monitor_interval,
            redirects=config.redirects,
            tee=config.tee,
            master_addr=master_addr,
            master_port=master_port,
        )
        cfg = metrics.MetricsConfig(config.metrics_cfg) if config.metrics_cfg else None
        metrics.initialize_metrics(cfg)
        agent = LocalElasticAgent(
            spec=spec, start_method=config.start_method, log_dir=config.log_dir
        )
result = agent.run()
        events.record(agent.get_agent_status_event(WorkerState.SUCCEEDED))
if result.is_failed():
# ChildFailedError is treated specially by @record
# if the error files for the failed children exist
","**config.rdzv_configs,
)
master_addr, master_port = _get_addr_and_port(rdzv_parameters)
    spec = WorkerSpec(
        role=config.role,
        local_world_size=config.nproc_per_node,
        entrypoint=entrypoint,
        args=tuple(args),
        rdzv_handler=rdzv_registry.get_rendezvous_handler(rdzv_parameters),
        max_restarts=config.max_restarts,
        monitor_interval=config.monitor_interval,
        redirects=config.redirects,
        tee=config.tee,
        master_addr=master_addr,
        master_port=master_port,
    )
    agent = LocalElasticAgent(
        spec=spec, start_method=config.start_method, log_dir=config.log_dir
    )

    shutdown_rdzv = True
    try:
        metrics.initialize_metrics(metrics.MetricsConfig(config.metrics_cfg))
result = agent.run()
        # records that agent.run() has succeeded NOT that workers have succeeded
        events.record(agent.get_event_succeeded())

if result.is_failed():
# ChildFailedError is treated specially by @record
# if the error files for the failed children exist
"
362,"name=entrypoint_name,
failures=result.failures,
)
        else:
            return result.return_values
except ChildFailedError:
raise
except Exception:
        if agent:
            events.record(agent.get_agent_status_event(WorkerState.FAILED))
        else:
            events.record(_construct_event(config))
raise
finally:
        rdzv_handler.shutdown()
","name=entrypoint_name,
failures=result.failures,
)

        return result.return_values
except ChildFailedError:
raise
    except SignalException:
        # when the agent dies with a signal do NOT shutdown the rdzv_handler
        # since this closes the rendezvous on this rdzv_id permanently and
        # prevents any additional scaling events
        shutdown_rdzv = False
        events.record(agent.get_event_failed())
        raise
except Exception:
        events.record(agent.get_event_failed())
raise
finally:
        if shutdown_rdzv:
            spec.rdzv_handler.shutdown()
"
363,"LLVMCodeGen cg(nest.root_stmt(), buf_args);
std::vector<CodeGen::CallArg> call_args;
for (auto _ : state) {
      output_ = at::empty_like(ref_);
call_args.clear();
call_args.emplace_back(input_.data_ptr<float>());
call_args.emplace_back(output_.data_ptr<float>());
","LLVMCodeGen cg(nest.root_stmt(), buf_args);
std::vector<CodeGen::CallArg> call_args;
    output_ = at::empty_like(ref_);
for (auto _ : state) {
call_args.clear();
call_args.emplace_back(input_.data_ptr<float>());
call_args.emplace_back(output_.data_ptr<float>());
"
364,"}
namespace {
static thread_local DebugHandle exception_debug_handle_{-1};
void createObject(Stack& stack, const at::ClassTypePtr& type) {
auto userObj = c10::ivalue::Object::create(
c10::StrongTypePtr(type->compilation_unit(), type),
","}
namespace {
static thread_local std::vector<DebugHandle> exception_debug_handles_;
void createObject(Stack& stack, const at::ClassTypePtr& type) {
auto userObj = c10::ivalue::Object::create(
c10::StrongTypePtr(type->compilation_unit(), type),
"
365,">getMethod(code.constants_[inst.X].toStringRef());
RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(
method.name(), debug_handle, stack);
          method.run(stack);
          frame.step();
} break;
case LOAD:
stack.emplace_back(reg(inst.X));
","RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(
method.name(), debug_handle, stack);
          callFunction(method, stack);
} break;
case LOAD:
stack.emplace_back(reg(inst.X));
"
366,"importers = sys_importer
def persistent_id(obj):
        # FIXME: the docs say that persistent_id should only return a string
        # but torch store returns tuples. This works only in the binary protocol
        # see
        # https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects
        # https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537
        if torch.is_storage(obj):
serialized_storages.append(obj)
            serialized_dtypes.append(obj.dtype)
return ('storage', len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
","importers = sys_importer
def persistent_id(obj):
        if torch.is_storage(obj) or isinstance(obj, torch.storage.TypedStorage):
            if isinstance(obj, torch.storage.TypedStorage):
                # TODO: Once we decide to break serialization FC, we can
                # remove this case
                storage = obj._storage
                dtype = obj.dtype
            else:
                storage = obj
                dtype = torch.uint8

serialized_storages.append(obj)
            serialized_dtypes.append(dtype)
return ('storage', len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
"
367,"data = saved_id[1:]
if typename == 'storage':
            return serialized_storages[data[0]]
if typename == 'reduce_deploy':
reduce_id, func, args = data
","data = saved_id[1:]
if typename == 'storage':
            # TODO: Once we decide to break serialization FC, we can
            # stop wrapping with TypedStorage
            storage = serialized_storages[data[0]]
            dtype = serialized_dtypes[data[0]]
            return torch.storage.TypedStorage(
                wrap_storage=storage._untyped(),
                dtype=dtype)
if typename == 'reduce_deploy':
reduce_id, func, args = data
"
368,")
outputs.append(output)
bindings[idx] = output.data_ptr()
with torch.autograd.profiler.record_function(""TRTModule:TensorRTRuntime""):
if self.engine.has_implicit_batch_dimension:
",")
outputs.append(output)
bindings[idx] = output.data_ptr()
                for i, idx in enumerate(self.hidden_output_binding_indices_in_order):
                    if self.engine.has_implicit_batch_dimension:
                        shape = (batch_size,) + tuple(
                            self.engine.get_binding_shape(idx)
                        )
                    else:
                        shape = tuple(self.context.get_binding_shape(idx))

                    output = torch.empty(  # type: ignore[call-overload]
                        size=shape,
                        dtype=self.hidden_output_dtypes[i],
                        device=torch.cuda.current_device(),
                    )
                    bindings[idx] = output.data_ptr()
with torch.autograd.profiler.record_function(""TRTModule:TensorRTRuntime""):
if self.engine.has_implicit_batch_dimension:
"
369,"from . import profiler
def register_py_tensor_class_for_device(device, cls):
if not isinstance(cls, type):
raise RuntimeError(""cls isn't a typeinfo object"")
    torch._C._autograd._register_py_class_for_device(device, cls)
","from . import profiler
def _register_py_tensor_class_for_device(device, cls):
if not isinstance(cls, type):
raise RuntimeError(""cls isn't a typeinfo object"")
    torch._C._register_py_class_for_device(device, cls)
"
370,"if isinstance(scale_factor, (list, tuple)):
if len(scale_factor) != dim:
raise ValueError(
                    ""scale_factor shape must match input shape. ""
                    ""Input is {}D, scale_factor is {}"".format(dim, len(scale_factor))
)
scale_factors = scale_factor
else:
","if isinstance(scale_factor, (list, tuple)):
if len(scale_factor) != dim:
raise ValueError(
                    ""Input and scale_factor must have the same number of spatial dimensions, but ""
                    f""got input with spatial dimensions of {list(input.shape[2:])} and ""
                    f""scale_factor of shape {scale_factor}. ""
                    ""Please provide input tensor in (N, C, d1, d2, ...,dK) format and ""
                    ""scale_factor in (s1, s2, ...,sK) format.""
)
scale_factors = scale_factor
else:
"
371,"return at::linalg_det(self);
}
Tensor& linalg_det_out(const Tensor& self, Tensor& out) {
checkSameDevice(""torch.linalg.det"", out, self, ""out"");
checkLinalgCompatibleDtype(""torch.linalg.det"", out, self, ""out"");
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point or complex tensor as input"");
IntArrayRef out_sizes(self.sizes().data(), self.dim() - 2);
at::native::resize_output(out, out_sizes);
  auto det = std::get<0>(at::native::_det_lu_based_helper(self));
out.copy_(det);
return out;
}
Tensor linalg_det(const Tensor& self) {
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point or complex tensor as input"");

  return std::get<0>(at::_det_lu_based_helper(self));
}

Tensor logdet(const Tensor& self) {
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point tensor as input"");
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
","return at::linalg_det(self);
}
Tensor linalg_det(const Tensor& self) {
  squareCheckInputs(self, ""linalg.det"");
  checkFloatingOrComplex(self, ""linalg.det"");

  return std::get<0>(at::_det_lu_based_helper(self));
}

Tensor& linalg_det_out(const Tensor& self, Tensor& out) {
checkSameDevice(""torch.linalg.det"", out, self, ""out"");
checkLinalgCompatibleDtype(""torch.linalg.det"", out, self, ""out"");
IntArrayRef out_sizes(self.sizes().data(), self.dim() - 2);
at::native::resize_output(out, out_sizes);
  auto det = at::native::linalg_det(self);
out.copy_(det);
return out;
}
Tensor logdet(const Tensor& self) {
  squareCheckInputs(self, ""logdet"");
  checkFloatingOrComplex(self, ""logdet"");
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
"
372,"// Mathematics 2019, 7, 1174.
//
Tensor linalg_matrix_exp(const Tensor& a) {
  squareCheckInputs(a);
  TORCH_CHECK((at::isFloatingType(a.scalar_type()) || at::isComplexType(a.scalar_type())),
              ""Expected a floating point or complex tensor as input. Got: "", a.scalar_type());
NoTF32Guard disable_tf32;
","// Mathematics 2019, 7, 1174.
//
Tensor linalg_matrix_exp(const Tensor& a) {
  squareCheckInputs(a, ""linalg.matrix_exp"");
  checkFloatingOrComplex(a, ""matrix_exp"");
NoTF32Guard disable_tf32;
"
373,"};
// Fill in the non-ellipsis dimensions
  for (const auto order_idx : c10::irange(0U, order.size())) {
auto out_idx = order_idx;
if (order_idx >= ellipsis_idx) {
out_idx = order_idx + num_ellipsis_names;
","};
// Fill in the non-ellipsis dimensions
  for (const auto order_idx : c10::irange(static_cast<int64_t>(order.size()))) {
auto out_idx = order_idx;
if (order_idx >= ellipsis_idx) {
out_idx = order_idx + num_ellipsis_names;
"
374,"""of the TensorRT region!"")
dim = kwargs[""dim""]
if network.has_implicit_batch_dimension:
assert dim != 0
dim -= 1
","""of the TensorRT region!"")
dim = kwargs[""dim""]
    input_shape = input_val.shape
    input_shape_size = len(input_val.shape) + 1 if network.has_implicit_batch_dimension else len(input_val.shape)
    if dim < 0:
        dim = dim % (input_shape_size + 1)
if network.has_implicit_batch_dimension:
assert dim != 0
dim -= 1
"
375,"input_quantized_idxs, output_quantized_idxs, please
see docs for prepare_fx for details
""""""
    return _convert_fx(graph_module, is_reference, convert_custom_config_dict, is_standalone_module=True)
","input_quantized_idxs, output_quantized_idxs, please
see docs for prepare_fx for details
""""""
    return _convert_fx(
        graph_module,
        is_reference,
        convert_custom_config_dict,
        is_standalone_module=True,
    )
"
376,"Method method = module.get_method(""forward"");
auto graph = module.get_method(""forward"").graph();
  // graph->dump();
PrepareGraphForStaticModule(graph, opts);
return std::make_pair(graph, module);
","Method method = module.get_method(""forward"");
auto graph = module.get_method(""forward"").graph();
PrepareGraphForStaticModule(graph, opts);
return std::make_pair(graph, module);
"
377,"namespace {
TypePtr toSingleType(const AliasTypeSet& mut_types) {
  return mut_types.size() == 1 ? mut_types[0]
                               : c10::UnionType::create(mut_types);
}
// This class determines whether a type is mutable, and, if so, it maps
","namespace {
c10::MaybeOwned<TypePtr> toSingleType(const AliasTypeSet& mut_types) {
  return mut_types.size() == 1
      ? c10::MaybeOwned<TypePtr>::borrowed(mut_types[0])
      : c10::MaybeOwned<TypePtr>::owned(c10::UnionType::create(mut_types));
}
// This class determines whether a type is mutable, and, if so, it maps
"
378,"import torch
from collections import OrderedDict, defaultdict
from typing import Union, Callable, Any, Dict, Tuple, Set
from torch.ao.quantization.qconfig import add_module_to_qconfig_obs_ctr, QConfigAny
import re
","import torch
from collections import OrderedDict, defaultdict
from typing import Union, Callable, Any, Dict, Tuple, Set, Optional
from torch.ao.quantization.qconfig import add_module_to_qconfig_obs_ctr, QConfigAny
import re
"
379,"parseSourceIfNeeded(name.prefix());
auto it = to_be_defined_.find(name);
if (it != to_be_defined_.end() && it->second->kind() == TK_CLASS_DEF) {
    ClassDef cd(it->second);
to_be_defined_.erase(it);
importNamedType(name.prefix(), cd);
}
","parseSourceIfNeeded(name.prefix());
auto it = to_be_defined_.find(name);
if (it != to_be_defined_.end() && it->second->kind() == TK_CLASS_DEF) {
    ClassDef cd(std::move(it->second));
to_be_defined_.erase(it);
importNamedType(name.prefix(), cd);
}
"
380,"add_docstr_all('index_add_',
r""""""
index_add_(dim, index, tensor, *, alpha=1) -> Tensor
Accumulate the elements of :attr:`alpha` times :attr:`tensor` into the :attr:`self`
tensor by adding to the indices in the order given in :attr:`index`. For example,
if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\ th row of
:attr:`tensor` is subtracted from the ``j``\ th row of :attr:`self`.
The :attr:`dim`\ th dimension of :attr:`tensor` must have the same size as the
length of :attr:`index` (which must be a vector), and all other dimensions must
match :attr:`self`, or an error will be raised.
","add_docstr_all('index_add_',
r""""""
index_add_(dim, index, source, *, alpha=1) -> Tensor
Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`
tensor by adding to the indices in the order given in :attr:`index`. For example,
if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\ th row of
``source`` is subtracted from the ``j``\ th row of :attr:`self`.
The :attr:`dim`\ th dimension of ``source`` must have the same size as the
length of :attr:`index` (which must be a vector), and all other dimensions must
match :attr:`self`, or an error will be raised.
"
381,"add_docstr_all('index_add',
r""""""
index_add(dim, index, tensor2) -> Tensor
Out-of-place version of :meth:`torch.Tensor.index_add_`.
"""""")
","add_docstr_all('index_add',
r""""""
index_add(dim, index, source) -> Tensor
Out-of-place version of :meth:`torch.Tensor.index_add_`.
"""""")
"
382,"isOpSupportedInMobile(op),
toString(op),
"" is not supported in mobile module."");
  code_->instructions_with_handles_.emplace_back(
      Instruction(op, X, N), dbg_handle);
}
bool Function::append_operator(
","isOpSupportedInMobile(op),
toString(op),
"" is not supported in mobile module."");
  code_->instructions_.emplace_back(op, X, N);
  code_->debug_handles_.emplace_back(dbg_handle);
}

void Function::append_instruction(OpCode op, int X, int N) {
  TORCH_CHECK(
      isOpSupportedInMobile(op),
      toString(op),
      "" is not supported in mobile module."");
  code_->instructions_.emplace_back(op, X, N);
}
bool Function::append_operator(
"
383,"pin_memory: bool
drop_last: bool
timeout: float
    sampler: Sampler
prefetch_factor: int
_iterator : Optional['_BaseDataLoaderIter']
__initialized = False
","pin_memory: bool
drop_last: bool
timeout: float
    sampler: Union[Sampler, Iterable]
prefetch_factor: int
_iterator : Optional['_BaseDataLoaderIter']
__initialized = False
"
384,"raise RuntimeError(f""slice_tensor received input {input_val} that is not part ""
""of the TensorRT region!"")
    dims = kwargs[""dims""]
if network.has_implicit_batch_dimension:
if not len(dims):
raise RuntimeError(""dim argument cannot be empty!"")
","raise RuntimeError(f""slice_tensor received input {input_val} that is not part ""
""of the TensorRT region!"")
    ranks = len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0)
    dims = [dim % ranks for dim in kwargs[""dims""]]

if network.has_implicit_batch_dimension:
if not len(dims):
raise RuntimeError(""dim argument cannot be empty!"")
"
385,"for i, dim in enumerate(dims):
start[dim] = starts[i]
stride[dim] = steps[i]
        output_shape[dim] = (stops[i] - start[i]) // steps[i]
layer = network.add_slice(input_val, start=start, shape=output_shape, stride=stride)
layer.name = name
","for i, dim in enumerate(dims):
start[dim] = starts[i]
stride[dim] = steps[i]
        output_shape[dim] = (stops[i] - starts[i]) // steps[i]
layer = network.add_slice(input_val, start=start, shape=output_shape, stride=stride)
layer.name = name
"
386,"if not isinstance(node.kwargs[k], torch.fx.Node):
continue
            kwarg_tensor_meta = node.kwargs[k].meta.get(""tensor_meta"")  # type: ignore[union-attr]
            kwarg_dtype = kwarg_tensor_meta.dtype if kwarg_tensor_meta else None

if kwarg_dtype not in dtypes:
return False
return True
","if not isinstance(node.kwargs[k], torch.fx.Node):
continue
            kwarg_dtype = _get_arg_dtype(node.kwargs[k])  # type: ignore[arg-type]
if kwarg_dtype not in dtypes:
return False
return True


# ======================================================================
# Functional interfaces and utils for defining basic operator support logic
# and composing them into more complex ones
# ======================================================================

IsNodeSupported = t.Callable[[t.Mapping[str, torch.nn.Module], torch.fx.Node], bool]


@compatibility(is_backward_compatible=False)
def create_op_support(is_node_supported: IsNodeSupported) -> OperatorSupportBase:
    """"""Wraps a `IsNodeSupported` function into an `OperatorSupportBase` instance

    `IsNodeSupported` has the same call signature as
    `OperatorSupportBase.is_node_supported`
    """"""
    class FunctionalOperatorSupport(OperatorSupportBase):
        def is_node_supported(
                self, submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node
        ) -> bool:
            return is_node_supported(submodules, node)
    return FunctionalOperatorSupport()


@compatibility(is_backward_compatible=False)
def chain(*op_support: OperatorSupportBase) -> OperatorSupportBase:
    """"""Combines a sequence of `OperatorSupportBase` instances to form a single `OperatorSupportBase`
    instance by evaluating each input `OperatorSupportBase` instance, and returns False if
    any of it reports False.
    """"""
    def _chain(submods, node) -> bool:
        return all(
            x.is_node_supported(submods, node)
            for x in op_support
        )
    return create_op_support(_chain)


@compatibility(is_backward_compatible=False)
class OpSupports:
    """"""A set of atomic `OperatorSupportBase` instances that can be combined together
    to form more complex operator support logic.
    """"""
    @classmethod
    def decline_if_input_dtype(cls, dtype: torch.dtype) -> OperatorSupportBase:
        """"""Report a node as non-supported, if any of its arguments is of dtype""""""

        def _decline_if_input_dtype(
            submodules: t.Mapping[str, torch.nn.Module],
            node: torch.fx.Node,
        ) -> bool:
            for arg in node._input_nodes:
                arg_dtype = _get_arg_dtype(arg)
                if arg_dtype == dtype:
                    return False
            return True
        return create_op_support(_decline_if_input_dtype)


def _get_arg_dtype(arg: torch.fx.Node) -> t.Any:
    assert isinstance(arg, torch.fx.Node)
    tensor_meta = arg.meta.get(""tensor_meta"")  # type: ignore[union-attr]
    dtype = tensor_meta.dtype if isinstance(tensor_meta, TensorMetadata) else arg.meta[""type""]
    return dtype
"
387,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v' + cuda_version
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
import ctypes
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path, cupti_path]))
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
prev_error_mode = kernel32.SetErrorMode(0x0001)
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v' + cuda_version
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
import ctypes
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]))
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
prev_error_mode = kernel32.SetErrorMode(0x0001)
"
388,"}
}
template<typename data_t, typename index_t>
typename std::enable_if<std::is_same<data_t, float>::value, void>::type
index_select_add(const Tensor &select_indices,
","}
}
namespace {
template <typename index_t>
void fbgemm_spmdm_report_error_(
    int64_t output_size,
    int index_size,
    int64_t N,
    const index_t* offsets,
    const index_t* indices) {
  for (int m = 0; m < output_size; ++m) {
    for (index_t i = offsets[m]; i < offsets[m + 1]; ++i) {
      TORCH_CHECK(i < index_size);
      index_t idx = indices[i];
      TORCH_CHECK(
          0 <= idx && idx < N,
          ""Index "",
          i,
          "" is out of bounds: "",
          idx,
          "", range 0 to "",
          N);
    }
  }
  TORCH_CHECK(
      offsets[output_size] == index_size,
      ""Yout input seems to be incorrect: the last offset value should be ""
      ""the size of the indices tensor, but it appears not."");
}
} // namespace

template<typename data_t, typename index_t>
typename std::enable_if<std::is_same<data_t, float>::value, void>::type
index_select_add(const Tensor &select_indices,
"
389,": nullptr,
/*out=*/output_data + start_idx * D);
          TORCH_CHECK(
              success,
              ""FBGEMM GenerateEmbeddingSpMDM kernel failed for 8-bit input"");
});
} else {
// pruned weights
",": nullptr,
/*out=*/output_data + start_idx * D);
          if (!success) {
            fbgemm_spmdm_report_error_(
                end_idx - start_idx,
                offsets_data[end_idx] - offsets_data[start_idx],
                N,
                offsets_data + start_idx,
                indices_data + offsets_data[start_idx]);
          }
});
} else {
// pruned weights
"
390,"@tensorrt_converter(acc_ops.permute)
def acc_ops_permute(network, target, args, kwargs, name):
input_val = kwargs[""input""]
    permutation = kwargs[""permutation""]
if not isinstance(input_val, trt.tensorrt.ITensor):
raise RuntimeError(
","@tensorrt_converter(acc_ops.permute)
def acc_ops_permute(network, target, args, kwargs, name):
input_val = kwargs[""input""]
    ranks = len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0)
    permutation = [i % ranks for i in kwargs[""permutation""]]
if not isinstance(input_val, trt.tensorrt.ITensor):
raise RuntimeError(
"
391,"// resolved at runtime init stage for better operator compatibility.
std::stringstream intermediate_model_stream;
{
    BytecodeEmitDefaultInputsGuard argNumGuard(true);
torch_script._save_for_mobile(
intermediate_model_stream, extra_files, hasBytecodeDebug);
}
// Update the bytecode version (from 6 to 5)
  PyTorchStreamReader reader_bytecode(&intermediate_model_stream);
  std::vector<IValue> bytecode_values = get_bytecode_ivalues(reader_bytecode);
  std::unordered_set<std::string> excluded_files{
      ""constants.pkl"",
      ""bytecode.pkl"",
      ""version"",
  };

  std::unordered_set<std::string> excluded_dirs{
      ""constants"",
      ""bytecode"",
  };

  std::stringstream ouput_model_stream;
  auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
    ouput_model_stream.write(static_cast<const char*>(buf), nbytes);
    return !ouput_model_stream ? 0 : nbytes;
  };
  PyTorchStreamWriter writer_bytecode(writer_func);
  selective_copy(
      reader_bytecode, writer_bytecode, excluded_files, excluded_dirs);
  update_bytecode_version(bytecode_values, kBytecodeVersionV5);
  auto bytecode_tuple = c10::ivalue::Tuple::create(std::move(bytecode_values));
  SerializationStorageContext storage_context;
  writeArchiveV5(
      writer_bytecode,
      c10::ivalue::Tuple::create(constants_values),
      /*archive_name=*/""constants"",
      /*archive_dir=*/"""",
      /*tensor_dir=*/""constants/"",
      /*use_storage_context=*/true,
      storage_context);
  writeArchiveV5(
      writer_bytecode,
      bytecode_tuple,
      /*archive_name=*/""bytecode"",
      /*archive_dir=*/"""",
      /*tensor_dir=*/""constants/"",
      /*use_storage_context=*/true,
      storage_context);
  return ouput_model_stream;
}
} // namespace
// A generic contract for backport logic to the previous bytecode version.
// Args:
// * PyTorchStreamReader has access to the input model from N bytecode version.
","// resolved at runtime init stage for better operator compatibility.
std::stringstream intermediate_model_stream;
{
    BytecodeEmitModeGuard argNumGuard(
        true /*emit_default_input_instructions*/,
        false /*enable_defaults_args_with_out_args*/);
torch_script._save_for_mobile(
intermediate_model_stream, extra_files, hasBytecodeDebug);
}
// Update the bytecode version (from 6 to 5)
  std::stringstream output_model_stream =
      update_bytecode_version(intermediate_model_stream, kBytecodeVersionV5);
  return output_model_stream;
}
/*
Backport function bytecode v7 that introduced support for operators with out
arguments. Previously, in v6, operators with out arguments forced the
serialization of all arguments in the schema, even when optional arguments
were not provided (as they had default values). Currently, in v7, operators
are aware of out arguments being present in the schema (always appended),
allowing the serialization of only required arguments (as default values will
be provided by the runtime).

The bump was needed because the v7 bytecode specifies less arguments for ops
with out arguments in the schema, since the runtime code is now able to query
whether an argument is of type ""out"" and insert the necessary default values in
the right order in the interpreter stack (i.e. before the out arguments).

For example schema is: torch.add(x, h, alpha=1.0, out=x) So, if the program
defines: torch.add(x, h, out=x) Previously, in v6, we serialized the bytecode to
contain all 4 arguments. Currently, in v7, we serialize the bytecode with only 3
arguments, since alpha is optional and has a default value that the runtime will
push in the stack. Thus, the backport is necessary such that the bytecode
contains all the arguments as before.
*/
std::stringstream backport_v7_to_v6(std::stringstream& input_model_stream) {
  std::shared_ptr<IStreamAdapter> rai =
      std::make_shared<IStreamAdapter>(&input_model_stream);
  auto reader = std::make_shared<PyTorchStreamReader>(rai);
  std::vector<IValue> constants_values =
      readArchive(kArchiveNameConstants, *reader.get()).toTuple()->elements();
  // If there are debug info files in the original model file, it should also
  // show up in the backported model
  bool hasBytecodeDebug = reader->hasRecord(""mobile_debug_handles.pkl"");
  // extra_files are kept
  auto records = reader->getAllRecords();
  ExtraFilesMap extra_files;
  for (const auto& record : records) {
    std::size_t found = record.find_last_of(""/\\"");
    auto path = record.substr(0, found);
    if (""extra"" == path) {
      extra_files.emplace(record.substr(found + 1), """");
    }
  }
  // Loading the TS module is required for this backport, because bytecode needs
  // to be re-emitted (refer to the comments below)
  Module torch_script = torch::jit::load(rai, c10::nullopt, extra_files);
  // The RAII guard to change the flag, emit_default_input_instructions, to
  // false to keep the same behavior in bytecode version 6. Change the flag,
  // enable_defaults_args_with_out_args, to deserialized the number of specified
  // operators which allowing both out arguments and default arguments to
  // #all_args, instead of (#all_args - #default_args)
  std::stringstream intermediate_model_stream;
  {
    BytecodeEmitModeGuard argNumGuard(
        false /*emit_default_input_instructions*/,
        false /*enable_defaults_args_with_out_args*/);
    torch_script._save_for_mobile(
        intermediate_model_stream, extra_files, hasBytecodeDebug);
  }
  // Update the bytecode version (from 7 to 6)
  std::stringstream output_model_stream =
      update_bytecode_version(intermediate_model_stream, kBytecodeVersionV6);
  return output_model_stream;
}

} // namespace
/********************** BackportManager **********************/

// A generic contract for backport logic to the previous bytecode version.
// Args:
// * PyTorchStreamReader has access to the input model from N bytecode version.
"
392,"):
break
later_node = later_node.next
        assert later_node.op != ""root""
        node.prepend(later_node)
# Note we do not increment node here, as it still may be in the wrong
# place (we just prepended the ph that should have come before it).
# split_module currently does not use get_attrs for attrs. Instead it passes
# them in as args from the parent module, which used get_attrs. Here we set
# them as get_attrs inside submod_0, allowing for running folding without
","):
break
later_node = later_node.next

        # The placeholder is in split.graph but not in split.submod_1.graph.
        # In this case we add the placeholder to submod_1.graph.
        if later_node.op == ""root"":
            with split.submod_1.graph.inserting_before(node):
                split.submod_1.graph.placeholder(curr_orig_ph_target)
        else:
            node.prepend(later_node)

# Note we do not increment node here, as it still may be in the wrong
# place (we just prepended the ph that should have come before it).
    # There might be more placeholders left in orig_ph_targets.
    last_placeholder = next(n for n in split.submod_1.graph.nodes if n.target == orig_ph_targets[ph_idx - 1])
    while ph_idx < len(orig_ph_targets):
        with split.submod_1.graph.inserting_after(last_placeholder):
            split.submod_1.graph.placeholder(orig_ph_targets[ph_idx])
        ph_idx += 1

# split_module currently does not use get_attrs for attrs. Instead it passes
# them in as args from the parent module, which used get_attrs. Here we set
# them as get_attrs inside submod_0, allowing for running folding without
"
393,"return a, b
def add_binary_elementwise_layer(network, lhs_val, rhs_val, op_type, name):
    lhs_val = get_trt_tensor(network, lhs_val, f""{name}_lhs"")
    rhs_val = get_trt_tensor(network, rhs_val, f""{name}_rhs"")
lhs_val, rhs_val = broadcast(
network, lhs_val, rhs_val, f""{name}_lhs"", f""{name}_rhs""
)
","return a, b
def add_binary_elementwise_layer(network, lhs_val, rhs_val, op_type, name):
    """"""
    This function adds a TensorRT elementwise layer. We only allow at most one
    operand to not be a trt tensor, otherwise, we should const fold it first.
    If any operand is not a trt tensor, we make it a trt constant layer which
    has the same type as the other trt tensor. Then we broadcast these two inputs
    to have the same number of dimensions.

    Limitation:
        If we are using implicit batch dim mode, the operand that is not a trt
    tensor are not allowed to have larger ranks than the trt tensor operand.

    Args:
        network: TensorRT network object.
        lhs_val: Left operand of the binary operation. Could be a TensorRT tensor,
            a PyTorch tensor or a simple value.
        rhs_val: Right operand of the binary operation. Similar to lhs_val.
        op_type: Type of the TensorRT elementwise binary operation.
        name: The name we want to assign to the created TensorRT layer.

    Returns:
        The output of TensorRT elementwise layer.
    """"""
    dtype = None
    is_lhs_trt_tensor = False
    is_rhs_trt_tensor = False
    if isinstance(lhs_val, trt.tensorrt.ITensor):
        dtype = torch_dtype_from_trt(lhs_val.dtype)
        is_lhs_trt_tensor = True
    if isinstance(rhs_val, trt.tensorrt.ITensor):
        dtype = torch_dtype_from_trt(rhs_val.dtype)
        is_rhs_trt_tensor = True
    if not is_lhs_trt_tensor and not is_rhs_trt_tensor:
        raise RuntimeError(f""Both operands of the binary elementwise op {name}""
                           ""are constant. In this case, please consider constant fold the model first."")

    lhs_val = get_trt_tensor(network, lhs_val, f""{name}_lhs"", dtype)
    rhs_val = get_trt_tensor(network, rhs_val, f""{name}_rhs"", dtype)

    # Check the limitation in the doc string.
    if network.has_implicit_batch_dimension:
        if is_lhs_trt_tensor and not is_rhs_trt_tensor:
            assert len(lhs_val.shape) >= len(rhs_val.shape)
        elif not is_lhs_trt_tensor and is_rhs_trt_tensor:
            assert len(rhs_val.shape) >= len(lhs_val.shape)

lhs_val, rhs_val = broadcast(
network, lhs_val, rhs_val, f""{name}_lhs"", f""{name}_rhs""
)
"
394,"#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/tensorexpr/external_functions_registry.h>
#include <c10/util/irange.h>
","#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/tensorexpr/external_functions_registry.h>
#include <c10/util/irange.h>
"
395,"return graph
# We accept dictionnaries and strings as ONNX inputs,
# but they should be only for configuration use.
# we detect here if these inputs are modified, and if so
# we warn the user that the changes won't take effect in the
","return graph
# We accept dictionaries and strings as ONNX inputs,
# but they should be only for configuration use.
# we detect here if these inputs are modified, and if so
# we warn the user that the changes won't take effect in the
"
396,"size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
auto ivalues =
          jit::unpickle(
              reinterpret_cast<const char*>(debug_data.get()), debug_size)
              .toTuple()
              ->elements();
SourceRangeDeserializer deserializer;
for (auto& val : ivalues) {
        auto tup_elems = val.toTuple()->elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
// byte_offset, debug_handle (=source range tag), source range
","size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
auto ivalues =
          std::move(
              *jit::unpickle(
                   reinterpret_cast<const char*>(debug_data.get()), debug_size)
                   .toTuple())
              .elements();
SourceRangeDeserializer deserializer;
for (auto& val : ivalues) {
        auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
// byte_offset, debug_handle (=source range tag), source range
"
397,"for (auto slice : slices) {
IndexBounds newRegion;
newRegion.reserve(A.size());
      TORCH_INTERNAL_ASSERT(remainingOuterBounds.size() == i);
for (size_t j = 0; j < i; ++j) {
newRegion.push_back(remainingOuterBounds[j]);
","for (auto slice : slices) {
IndexBounds newRegion;
newRegion.reserve(A.size());
      TORCH_INTERNAL_ASSERT(
          remainingOuterBounds.size() == i, buildErrorMessage());
for (size_t j = 0; j < i; ++j) {
newRegion.push_back(remainingOuterBounds[j]);
"
398,"std::string buildErrorMessage(const std::string& s) {
static const std::string generic_error_message =
""This error occured in the fuser. You can turn off the fuser with ""
      ""torch._C._jit_override_can_fuse_on_cpu(False)"";
if (s.empty()) {
return generic_error_message;
}
","std::string buildErrorMessage(const std::string& s) {
static const std::string generic_error_message =
""This error occured in the fuser. You can turn off the fuser with ""
      ""torch.jit.enable_fusion(False)."";
if (s.empty()) {
return generic_error_message;
}
"
399,"op.schema().name(),
"". File a bug to add a case for this operator.\n"");
}
    if (!aliasAnalysisHasSpecialCaseFor(s) &&
op.aliasAnalysisKind() == AliasAnalysisKind::CONSERVATIVE) {
AT_ERROR(
          ""Missing special case in alias analysis for non-schematized""
"" operator "",
op.schema().name(),
"". File a bug to add a case for this operator.\n"");
","op.schema().name(),
"". File a bug to add a case for this operator.\n"");
}
    if (aliasAnalysisHasSpecialCaseFor(s) &&
op.aliasAnalysisKind() == AliasAnalysisKind::CONSERVATIVE) {
AT_ERROR(
          ""Conflict in special casing in alias analysis for non-schematized""
"" operator "",
op.schema().name(),
"". File a bug to add a case for this operator.\n"");
"
400,"throw malformed_input(msg);
}
  TORCH_INTERNAL_ASSERT(tt->sizes().concrete_sizes());
auto sizes = *tt->sizes().concrete_sizes();
std::vector<int64_t> default_strides = TensorType::contiguousStridesOf(sizes);
if (!tt->strides().concrete_sizes()) {
return Tensor(buf, nullptr);
}
  TORCH_INTERNAL_ASSERT(tt->strides().concrete_sizes());
const std::vector<int64_t> strides = *tt->strides().concrete_sizes();
// All Tensors in NNC are layed out in default, contiguous layout.
// If the output is also default contiguous we don't need to do anything
","throw malformed_input(msg);
}
  TORCH_INTERNAL_ASSERT(
      tt->sizes().concrete_sizes(),
      buildErrorMessage(""Output shapes are unknown.""));
auto sizes = *tt->sizes().concrete_sizes();
std::vector<int64_t> default_strides = TensorType::contiguousStridesOf(sizes);
if (!tt->strides().concrete_sizes()) {
return Tensor(buf, nullptr);
}
  TORCH_INTERNAL_ASSERT(
      tt->strides().concrete_sizes(),
      buildErrorMessage(""Output strides are unknown.""));
const std::vector<int64_t> strides = *tt->strides().concrete_sizes();
// All Tensors in NNC are layed out in default, contiguous layout.
// If the output is also default contiguous we don't need to do anything
"
401,"// from model. We can use it to handle backward compatibility.
if (num_specified_args &&
num_specified_args.value() < static_cast<int64_t>(args.size())) {
      // Sanity check at load time, to save perf at runtime
      for (size_t i = num_specified_args.value(); i < args.size(); ++i) {
        auto default_val = args[i].default_value();
        TORCH_CHECK(
            default_val.has_value(),
            ""Error happened at preparing for default values for the argument. The "",
            i,
            ""th arguement of operator"",
            opname,
            "" does not have a specified value or default value. "");
      }
fn = [fn, num_specified_args, args](Stack& stack) {
        for (size_t i = num_specified_args.value(); i < args.size(); ++i) {
stack.push_back(args[i].default_value());
}
fn(stack);
};
}
","// from model. We can use it to handle backward compatibility.
if (num_specified_args &&
num_specified_args.value() < static_cast<int64_t>(args.size())) {
fn = [fn, num_specified_args, args](Stack& stack) {
        std::vector<IValue> out_args;
        // The following logic pops and temporarily stores all out arguments
        // from the stack (which can be 0 or more, and always appended to the
        // schema), in order to push the necessary default values. Finally, the
        // out arguments are pushed back into the stack.
        for (size_t i = args.size() - 1; i > 0 && args.at(i).is_out(); i--) {
          out_args.push_back(stack.back());
          stack.pop_back();
        }
        size_t start_index = num_specified_args.value() - out_args.size();
        TORCH_CHECK(
            start_index >= 0,
            ""The number of output arguments is: "",
            out_args.size(),
            "", which is more then the number of specified arguments: "",
            num_specified_args.value());
        for (size_t i = start_index; i < (args.size() - out_args.size()); ++i) {
          TORCH_CHECK(
              args[i].default_value().has_value(),
              ""Error happened at preparing for default values for the argument. The "",
              i,
              ""th argument "",
              args[i].name(),
              "" does not have a specified value or default value. "");

stack.push_back(args[i].default_value());
}
        stack.insert(stack.end(), out_args.rbegin(), out_args.rend());
fn(stack);
};
}
"
402,"%output : Tensor = aten::squeeze(%output_2d, %two)
return (%output) )"";
SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(conv_1d_pattern, conv_2d_pattern);
rewriter.runOnGraph(graph);
}
","%output : Tensor = aten::squeeze(%output_2d, %two)
return (%output) )"";
  std::vector<std::pair<std::string, std::string>> value_mappings(
      {{""zero"", ""res""},
       {""one"", ""res""},
       {""stride_w"", ""res""},
       {""stride_2d"", ""res""},
       {""padding_w"", ""res""},
       {""padding_2d"", ""res""},
       {""dilation_w"", ""res""},
       {""dilation_2d"", ""res""},
       {""two"", ""res""},
       {""input_2d"", ""res""},
       {""weight_2d"", ""res""},
       {""output_2d"", ""res""},
       {""output"", ""res""}});

SubgraphRewriter rewriter;
  rewriter.RegisterRewritePattern(
      conv_1d_pattern, conv_2d_pattern, value_mappings);
rewriter.runOnGraph(graph);
}
"
403,"%packed_weight_bias = prepacked::conv2d_transpose_clamp_prepack(
%weight, %bias, %stride, %padding, %output_padding, %dilation, %groups,
%output_min_max, %output_min_max)
        %r = prepacked::conv2d_transpose_clamp_run(%input, %packed_weight_bias)
        return (%r) )"";
SubgraphRewriter transpose_rewriter;
transpose_rewriter.RegisterRewritePattern(
      conv_2d_transpose_pattern, prepacked_ops_conv2d_transpose_pattern);
transpose_rewriter.runOnGraph(graph);
}
","%packed_weight_bias = prepacked::conv2d_transpose_clamp_prepack(
%weight, %bias, %stride, %padding, %output_padding, %dilation, %groups,
%output_min_max, %output_min_max)
        %res = prepacked::conv2d_transpose_clamp_run(%input, %packed_weight_bias)
        return (%res) )"";

  value_mappings = {
      {""output_min_max"", ""res""}, {""packed_weight_bias"", ""res""}, {""res"", ""res""}};
SubgraphRewriter transpose_rewriter;
transpose_rewriter.RegisterRewritePattern(
      conv_2d_transpose_pattern,
      prepacked_ops_conv2d_transpose_pattern,
      value_mappings);
transpose_rewriter.runOnGraph(graph);
}
"
404,"%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %r = aten::hardtanh(%conv2d_res, %output_min, %output_max)
        return (%r) )"";
rewriter.RegisterRewritePattern(
      conv2d_prepack_run_hardtanh, conv2d_prepack_run_hardtanh_fused);
std::string linear_prepack_run_hardtanh_inplace = R""(
graph(%input, %weight, %bias, %output_min, %output_max, %dummy_min_max):
","%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %res = aten::hardtanh(%conv2d_res, %output_min, %output_max)
        return (%res) )"";

  value_mappings = {
      {""packed_weight_bias"", ""packed_weight_bias""}, {""res"", ""res""}};
rewriter.RegisterRewritePattern(
      conv2d_prepack_run_hardtanh,
      conv2d_prepack_run_hardtanh_fused,
      value_mappings);
std::string linear_prepack_run_hardtanh_inplace = R""(
graph(%input, %weight, %bias, %output_min, %output_max, %dummy_min_max):
"
405,"auto variables_for_bucket = get_variables_for_bucket(next_bucket_, bucket);
GradBucket grad_bucket(
next_bucket_,
tensors[0],
// Since we only support single-process single-device
// mode, there is always only one replica in the bucket.
","auto variables_for_bucket = get_variables_for_bucket(next_bucket_, bucket);
GradBucket grad_bucket(
next_bucket_,
      buckets_.size(),
tensors[0],
// Since we only support single-process single-device
// mode, there is always only one replica in the bucket.
"
406,"return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(old_indices_.size() == v->indices().size());
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
","return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(
        old_indices_.size() == v->indices().size(),
        buildErrorMessage(
            ""Expected ranks to match in RfactorStoreRewriter in the fuser.""));
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
"
407,"//   X[*indexes] = ReduceOp(X[*indexes] + T[*indexes + {reduction_var}],
//                          reduce_axis={reduction_var})
BlockPtr b = outer_reduction_for->body();
  TORCH_INTERNAL_ASSERT(b->nstmts() == 1);
StmtPtr first_reduction_loop = b->stmts().front();
auto rfac_buf_indices = orig_buf_indices;
rfac_buf_indices.emplace_back(reduction_var);
","//   X[*indexes] = ReduceOp(X[*indexes] + T[*indexes + {reduction_var}],
//                          reduce_axis={reduction_var})
BlockPtr b = outer_reduction_for->body();
  TORCH_INTERNAL_ASSERT(
      b->nstmts() == 1,
      buildErrorMessage(
          ""Expected to have a single stmt in the block in rfactor transformation in the fuser.""));
StmtPtr first_reduction_loop = b->stmts().front();
auto rfac_buf_indices = orig_buf_indices;
rfac_buf_indices.emplace_back(reduction_var);
"
408,"memory_format: torch.memory_format = field(default=torch.contiguous_format)
pin_memory: bool = False
@dataclass
class ShardedTensorMetadata(object):
""""""
","memory_format: torch.memory_format = field(default=torch.contiguous_format)
pin_memory: bool = False

class MEM_FORMAT_ENCODING(Enum):
    TORCH_CONTIGUOUS_FORMAT = 0
    TORCH_CHANNELS_LAST = 1
    TORCH_PRESERVE_FORMAT = 2


@dataclass
class ShardedTensorMetadata(object):
""""""
"
409,"for (auto const& input : node->inputs()) {
if (auto tt = input->type()->cast<TensorType>()) {
if (auto inputDevice = tt->device()) {
          TORCH_INTERNAL_ASSERT(!device || *device == *inputDevice);
device = inputDevice;
}
}
}
}
  TORCH_INTERNAL_ASSERT(device);
return device;
}
","for (auto const& input : node->inputs()) {
if (auto tt = input->type()->cast<TensorType>()) {
if (auto inputDevice = tt->device()) {
          TORCH_INTERNAL_ASSERT(
              !device || *device == *inputDevice,
              buildErrorMessage(
                  ""Different devices specified for inputs to the fuser.""));
device = inputDevice;
}
}
}
}
  TORCH_INTERNAL_ASSERT(
      device,
      buildErrorMessage(""Could not find device in fuser graph inputs.""));
return device;
}
"
410,"void batch_norm_cpu_kernel(Tensor& output, const Tensor& input,
const Tensor& weight, const Tensor& bias, const Tensor& save_mean,  const Tensor& save_invstd,
const Tensor& running_mean, const Tensor& running_var, bool train, double eps) {
  switch (input.suggest_memory_format()) {
    case at::MemoryFormat::Contiguous: {
      AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_contiguous"", [&] {
        batch_norm_cpu_contiguous_impl<scalar_t>(output, input, weight, bias,
            save_mean, save_invstd, running_mean, running_var, train, eps);
      });
      break;
    }
    case at::MemoryFormat::ChannelsLast: {
      AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_channels_last"", [&] {
        batch_norm_cpu_channels_last_impl<scalar_t>(output, input, weight, bias,
            save_mean, save_invstd, running_mean, running_var, train, eps);
      });
      break;
    }
    default:
      TORCH_CHECK(false, ""Unsupported memory format. Supports only ChannelsLast, Contiguous"");
}
}
void batch_norm_cpu_collect_stats_kernel(
Tensor& mean, Tensor& var_sum, const Tensor& input) {
int64_t image_size = input.numel() / input.size(0) / input.size(1);
  switch (input.suggest_memory_format()) {
    case at::MemoryFormat::Contiguous: {
      AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_collect_stats_contiguous"", [&] {
        if (image_size == 1) { // NC11 is also channels last
          batch_norm_cpu_collect_stats_channels_last_impl<scalar_t>(mean, var_sum, input);
        } else {
          batch_norm_cpu_collect_stats_contiguous_impl<scalar_t>(mean, var_sum, input);
        }
      });
      break;
    }
    case at::MemoryFormat::ChannelsLast: {
      AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_collect_stats_channels_last"", [&] {
batch_norm_cpu_collect_stats_channels_last_impl<scalar_t>(mean, var_sum, input);
      });
      break;
    }
    default:
      TORCH_CHECK(false, ""Unsupported memory format. Supports only ChannelsLast, Contiguous"");
}
}
","void batch_norm_cpu_kernel(Tensor& output, const Tensor& input,
const Tensor& weight, const Tensor& bias, const Tensor& save_mean,  const Tensor& save_invstd,
const Tensor& running_mean, const Tensor& running_var, bool train, double eps) {
  if (input.is_contiguous()) {
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_contiguous"", [&] {
      batch_norm_cpu_contiguous_impl<scalar_t>(output, input, weight, bias,
          save_mean, save_invstd, running_mean, running_var, train, eps);
    });
  } else if (input.is_contiguous(at::MemoryFormat::ChannelsLast)) {
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_channels_last"", [&] {
      batch_norm_cpu_channels_last_impl<scalar_t>(output, input, weight, bias,
          save_mean, save_invstd, running_mean, running_var, train, eps);
    });
  } else {
    TORCH_CHECK(false, ""batch_norm_cpu_kernel: expecting input to be contiguous."");
}
}
void batch_norm_cpu_collect_stats_kernel(
Tensor& mean, Tensor& var_sum, const Tensor& input) {
int64_t image_size = input.numel() / input.size(0) / input.size(1);
  if (input.is_contiguous()) {
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_collect_stats_contiguous"", [&] {
      if (image_size == 1) { // NC11 is also channels last
batch_norm_cpu_collect_stats_channels_last_impl<scalar_t>(mean, var_sum, input);
      } else {
        batch_norm_cpu_collect_stats_contiguous_impl<scalar_t>(mean, var_sum, input);
      }
    });
  } else if (input.is_contiguous(at::MemoryFormat::ChannelsLast)) {
    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), ""batch_norm_cpu_collect_stats_channels_last"", [&] {
      batch_norm_cpu_collect_stats_channels_last_impl<scalar_t>(mean, var_sum, input);
    });
  } else {
    TORCH_CHECK(false, ""batch_norm_cpu_collect_stats_kernel: expecting input to be contiguous."");
}
}
"
411,"namespace jit {
bool IndexingPatternFinder::IsSameSource(const Node* n, const Node* m) {
  const auto& source_n = n->sourceRange().source();
  const auto& source_m = m->sourceRange().source();
return (
(source_n->text() == source_m->text()) &&
(source_n->starting_line_no() == source_m->starting_line_no()));
","namespace jit {
bool IndexingPatternFinder::IsSameSource(const Node* n, const Node* m) {
  const auto source_n = n->sourceRange().source();
  const auto source_m = m->sourceRange().source();
return (
(source_n->text() == source_m->text()) &&
(source_n->starting_line_no() == source_m->starting_line_no()));
"
412,"#include <c10/util/string_utils.h>
#include <ATen/core/functional.h>
#include <torch/csrc/jit/tensorexpr/analysis.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
","#include <c10/util/string_utils.h>
#include <ATen/core/functional.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/tensorexpr/analysis.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
"
413,"#include <c10d/default_comm_hooks.hpp>
#include <c10d/ProcessGroup.hpp>
#include <c10d/comm.hpp>
","#include <c10d/default_comm_hooks.hpp>
#include <c10/core/ScalarType.h>
#include <c10/util/Exception.h>
#include <c10d/ProcessGroup.hpp>
#include <c10d/comm.hpp>
"
414,"
import collections
from itertools import repeat
from typing import List, Dict, Any
","import collections
from itertools import repeat
from typing import List, Dict, Any
"
415,"auto ctx_ptr = std::make_unique<KinetoObserverContext>();
ctx_ptr->correlationId = corr_id;
ctx_ptr->startThreadId = at::RecordFunction::currentThreadId();
if (config.report_input_shapes) {
ctx_ptr->shapes = inputSizes(fn);
","auto ctx_ptr = std::make_unique<KinetoObserverContext>();
ctx_ptr->correlationId = corr_id;
ctx_ptr->startThreadId = at::RecordFunction::currentThreadId();
          ctx_ptr->debug_handle = fn.debugHandle();
if (config.report_input_shapes) {
ctx_ptr->shapes = inputSizes(fn);
"
416,"import io
import logging
from itertools import chain
from typing import Any, Callable, Dict, List, NamedTuple, Optional, Type, Union
import torch
import torch.distributed as dist
","import io
import logging
from itertools import chain
from typing import Any, Callable, Dict, List, Optional, Set, Type
import torch
import torch.distributed as dist
"
417,"self.zero.step()
class _DDPBucket(NamedTuple):
r""""""
    This contains the model parameters corresponding to a
    :class:`DistributedDataParallel` gradient bucket.
bucket_index (int): index of the bucket determined by the DDP gradient
bucket all-reduce order.
        params (List[torch.Tensor]): model parameters in the bucket.
""""""
    bucket_index: int
    params: List[torch.Tensor]
class _OverlapStatus(enum.IntEnum):
","self.zero.step()
class _DDPBucketAssignment():
r""""""
    This represents a :class:`DistributedDataParallel` bucket assignment,
    meaning a (possibly non-strict) subset of the parameters corresponding to
    a DDP bucket assigned to a rank to update.
    Attributes:
bucket_index (int): index of the bucket determined by the DDP gradient
bucket all-reduce order.
        parameters (List[torch.Tensor]): model parameters in the bucket
            assigned to this rank.
        offset (int): offset into the :class:`GradBucket` 's :meth:`parameters`
            giving the index of the first element in the passed-in
            ``parameters``; this equivalently indexes into the
            :class:`GradBucket` 's :meth:`gradients`.
        device (torch.device): device on which the parameters are stored.
        tensor (torch.Tensor): flattened tensor giving the data of the
            parameter subset assigned to the rank.
""""""
    def __init__(
        self,
        bucket_index: int,
        parameters: List[torch.Tensor],
        offset: int,
    ):
        self.bucket_index = bucket_index
        self.parameters = parameters
        self.offset = offset
        if len(self.parameters) == 0:
            raise ValueError(""Empty bucket assignment"")
        # DDP guarantees all parameters in the bucket have the same device
        self.device: torch.device = self.parameters[0].device
        self.tensor: Optional[torch.Tensor] = None
class _OverlapStatus(enum.IntEnum):
"
418,"self._device_to_params_per_rank_cache[device][rank].append(param)
return self._device_to_params_per_rank_cache
    @property
    def _device_to_buckets(
        self
    ) -> Dict[torch.device, List[List[_DDPBucket]]]:
r""""""
        :class:`dict` mapping each device to a :class:`list` of :class:`list`
        of :class:`_DDPBucket` s.
        ``_device_to_buckets[d][r][i]`` gives the ``i``th bucket
        assigned to rank ``r`` stored on device ``d``, where each bucket
        contains a list of the model parameters associated with the
        corresponding logical :class:`DistributedDataParallel` gradient bucket.
        This is used for constructing the parameter buckets if
        ``overlap_with_ddp=True``.
""""""
        assert self._overlap_with_ddp, \
            ""`_device_to_buckets()` should only be used if "" \
            ""`overlap_with_ddp=True`""
        if len(self._device_to_buckets_cache) > 0:
            return self._device_to_buckets_cache
overlap_info = self._overlap_info
        assert overlap_info.status == _OverlapStatus.INITIALIZED, \
            ""Accessing `_device_to_buckets` before the necessary "" \
            ""information has been collected""
params_per_bucket = overlap_info.params_per_bucket
        for bucket_idx, bucket_params in enumerate(params_per_bucket):
            assert len(bucket_params) > 0, ""Empty bucket""
            rank = self._ddp_bucket_index_to_rank(bucket_idx)
            bucket = _DDPBucket(bucket_idx, bucket_params)
            device = bucket_params[0].device  # assume same device per bucket
            if device not in self._device_to_buckets_cache:
                self._device_to_buckets_cache[device] = [[] for _ in range(self.world_size)]
            self._device_to_buckets_cache[device][rank].append(bucket)
        return self._device_to_buckets_cache
def _local_step(
self,
","self._device_to_params_per_rank_cache[device][rank].append(param)
return self._device_to_params_per_rank_cache
    def _get_min_index(
        self,
        values: List[int],
        disallowed_indices: Optional[Set[int]] = None,
    ) -> int:
r""""""
        Returns ``values.index(min(values))``, except only uses one pass. It
        also excludes any indices in ``disallowed_indices`` if provided.
        Arguments:
            values: (List[int]): :class:`list` of values.
            disallowed_indices (Optional[Set[int]]): indices that are
                disallowed from being the returned min index.
        """"""
        min_index = -1
        min_value = float(""inf"")
        for i, value in enumerate(values):
            if disallowed_indices and i in disallowed_indices:
                continue
            if value < min_value:
                min_value = value
                min_index = i
        assert min_index >= 0, ""All indices are disallowed""
        return min_index

    def _assign_bucket_subset_to_rank(
        self,
        bucket_index: int,
        bucket_params: List[torch.Tensor],
        bucket_offset: int,
        assigned_rank: int,
        assigned_ranks_per_bucket: List[Set[int]],
    ) -> None:
        r""""""
        Assigns the model parameters given by ``bucket_params``, representing a
        (possibly non-strict) subset of the parameters corresponding to a
        :class:`DistributedDataParallel` bucket, to the rank with the least
        size assigned so far and collects relevant information.
        Arguments:
            bucket_index (int): index of the :class:`DistributedDataParallel`
                gradient bucket.
            bucket_params (List[torch.Tensor]): subset of the parameters
                corresponding to the bucket to assign.
            bucket_offset (int): offset giving the index of the first element
                in ``bucket_params`` in the bucket's full parameter list.
            assigned_rank (int): rank to assign to.
            assigned_ranks_per_bucket (List[Set[int]]): :class:`set` of ranks
                assigned to each bucket.
""""""
        overlap_info = self._overlap_info
        if len(bucket_params) == 0:
            raise ValueError(
                ""Empty bucket assignment""
            )
        params_per_rank = overlap_info.params_per_rank
        offsets = overlap_info.offsets

        self._bucket_assignments_per_rank_cache[assigned_rank][bucket_index] = \
            _DDPBucketAssignment(bucket_index, bucket_params, bucket_offset)
        if self.global_rank == assigned_rank:
            offsets[bucket_index] = len(params_per_rank[assigned_rank])
        params_per_rank[assigned_rank].extend(bucket_params)
        assigned_ranks_per_bucket[bucket_index].add(assigned_rank)
        self._overlap_info.num_bucket_assignments += 1

    @property
    def _bucket_assignments_per_rank(
        self
    ) -> List[Dict[int, _DDPBucketAssignment]]:
        r""""""
        :class:`list` of length world size consisting of :class:`dict` s
        mapping bucket indices to :class:`_DDPBucketAssignment` s for each
        rank.
        """"""
        assert self._overlap_with_ddp, ""`_bucket_assignments_per_rank` "" \
            ""only be used if `overlap_with_ddp=True`""
        if len(self._bucket_assignments_per_rank_cache) > 0:
            return self._bucket_assignments_per_rank_cache
overlap_info = self._overlap_info
        assert overlap_info.status == _OverlapStatus.INITIALIZED
        self._bucket_assignments_per_rank_cache = [{} for _ in range(self.world_size)]
params_per_bucket = overlap_info.params_per_bucket
        if overlap_info.shard_buckets:
            # Define the assignment threshold to approximate uniformity
            assert overlap_info.total_size is not None, \
                ""`total_size` was not computed""
            threshold = overlap_info.total_size / self.world_size  # type: ignore[operator]
            size_per_rank = [0 for _ in range(self.world_size)]

        num_buckets = len(params_per_bucket)
        overlap_info.assigned_ranks_per_bucket = [set() for _ in range(num_buckets)]
        assigned_ranks_per_bucket = overlap_info.assigned_ranks_per_bucket
        if not overlap_info.shard_buckets:
            # Assign each DDP bucket entirely to a single rank
            for bucket_index, bucket_params in enumerate(params_per_bucket):
                assert len(bucket_params) > 0, ""Empty bucket""
                assigned_rank = self._get_assigned_rank(bucket_index)
                self._assign_bucket_subset_to_rank(
                    bucket_index,
                    bucket_params,
                    0,
                    assigned_rank,
                    assigned_ranks_per_bucket,
                )
        else:
            # Assign each DDP bucket to possibly multiple ranks
            # Specifically, sort the DDP buckets by increasing size, and for
            # each bucket, iteratively assign the maximal unassigned subset
            # with size less than `threshold` to the rank with the least total
            # size so far -- each such assignment is represented by a
            # `_DDPBucketAssignment` instance and only contains parameters from
            # a single DDP bucket
            params_per_bucket_enum = sorted(
                enumerate(params_per_bucket),
                key=lambda x: sum(p.numel() for p in x[1])
            )
            for bucket_index, bucket_params in params_per_bucket_enum:
                assert len(bucket_params) > 0, ""Empty bucket""
                bucket_offset = 0
                assignment_size = 0
                for param_index, param in enumerate(bucket_params):
                    param_numel = param.numel()
                    if assignment_size + param_numel >= threshold and param_index > bucket_offset:
                        assigned_rank = self._get_min_index(size_per_rank, assigned_ranks_per_bucket[bucket_index])
                        # Include up to but not including the parameter that
                        # exceeded the threshold
                        self._assign_bucket_subset_to_rank(
                            bucket_index,
                            bucket_params[bucket_offset:param_index],
                            bucket_offset,
                            assigned_rank,
                            assigned_ranks_per_bucket,
                        )
                        size_per_rank[assigned_rank] += assignment_size
                        bucket_offset = param_index
                        assignment_size = 0
                    assignment_size += param_numel
                # Assign the remainder of the bucket so that no assignment
                # spans across two buckets
                assigned_rank = self._get_min_index(size_per_rank, assigned_ranks_per_bucket[bucket_index])
                self._assign_bucket_subset_to_rank(
                    bucket_index,
                    bucket_params[bucket_offset:],
                    bucket_offset,
                    assigned_rank,
                    assigned_ranks_per_bucket,
                )
                size_per_rank[assigned_rank] += assignment_size

        return self._bucket_assignments_per_rank_cache
def _local_step(
self,
"
419,"import argparse
import sys
","

import argparse
import sys
"
420,"# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (
        master_doc,
        ""pytorch.tex"",
        ""PyTorch Documentation"",
        ""Torch Contributors"",
        ""manual"",
    ),
]
","# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'pytorch.tex', 'PyTorch Documentation',
     'Torch Contributors', 'manual'),
]
"
421,"# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    ""ELU"",
    ""Hardshrink"",
    ""Hardtanh"",
    ""LeakyReLU"",  # Perhaps we should add text explaining slight slope?
    ""LogSigmoid"",
    ""PReLU"",
    ""ReLU"",
    ""ReLU6"",
    ""RReLU"",
    ""SELU"",
    ""SiLU"",
    ""Mish"",
    ""CELU"",
    ""GELU"",
    ""Sigmoid"",
    ""Softplus"",
    ""Softshrink"",
    ""Softsign"",
    ""Tanh"",
    ""Tanhshrink""
# 'Threshold'  Omit, pending cleanup. See PR5457
]
","# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    'ELU',
    'Hardshrink',
    'Hardtanh',
    'LeakyReLU',  # Perhaps we should add text explaining slight slope?
    'LogSigmoid',
    'PReLU',
    'ReLU',
    'ReLU6',
    'RReLU',
    'SELU',
    'SiLU',
    'Mish',
    'CELU',
    'GELU',
    'Sigmoid',
    'Softplus',
    'Softshrink',
    'Softsign',
    'Tanh',
    'Tanhshrink'
# 'Threshold'  Omit, pending cleanup. See PR5457
]
"
422,"Or if you installed mypy via conda, run this:
conda install -c conda-forge mypy={correct_version}
"""""",
            file=sys.stderr,
        )
return Plugin
","Or if you installed mypy via conda, run this:
conda install -c conda-forge mypy={correct_version}
''', file=sys.stderr)
return Plugin
"
423,"# This future is needed to print Python2 EOL message
from __future__ import print_function

import sys

if sys.version_info < (3,):
print(""Python 2 has reached end-of-life and is no longer supported by PyTorch."")
sys.exit(-1)
if sys.platform == ""win32"" and sys.maxsize.bit_length() == 31:
    print(
        ""32-bit Windows Python runtime is not supported. Please switch to 64-bit Python.""
    )
sys.exit(-1)
import platform

python_min_version = (3, 6, 2)
python_min_version_str = ""."".join(map(str, python_min_version))
if sys.version_info < python_min_version:
    print(
        ""You are using Python {}. Python >={} is required."".format(
            platform.python_version(), python_min_version_str
        )
    )
sys.exit(-1)
import filecmp
import glob
import importlib
import json
import os
import shutil
import subprocess
import sysconfig
import time
from collections import defaultdict
import setuptools.command.build_ext
import setuptools.command.install
import setuptools.command.sdist
from setuptools import setup, Extension, find_packages
from setuptools.dist import Distribution
from tools.build_pytorch_libs import build_caffe2
from tools.generate_torch_version import get_torch_version
from tools.setup_helpers.cmake import CMake
from tools.setup_helpers.env import IS_WINDOWS, IS_DARWIN, IS_LINUX, build_type
################################################################################
# Parameters parsed from environment
","# This future is needed to print Python2 EOL message
from __future__ import print_function
import sys
if sys.version_info < (3,):
print(""Python 2 has reached end-of-life and is no longer supported by PyTorch."")
sys.exit(-1)
if sys.platform == 'win32' and sys.maxsize.bit_length() == 31:
    print(""32-bit Windows Python runtime is not supported. Please switch to 64-bit Python."")
sys.exit(-1)
import platform
python_min_version = (3, 6, 2)
python_min_version_str = '.'.join(map(str, python_min_version))
if sys.version_info < python_min_version:
    print(""You are using Python {}. Python >={} is required."".format(platform.python_version(),
                                                                     python_min_version_str))
sys.exit(-1)
from setuptools import setup, Extension, find_packages
from collections import defaultdict
from setuptools.dist import Distribution
import setuptools.command.build_ext
import setuptools.command.install
import setuptools.command.sdist
import filecmp
import shutil
import subprocess
import os
import json
import glob
import importlib
import time
import sysconfig
from tools.build_pytorch_libs import build_caffe2
from tools.setup_helpers.env import (IS_WINDOWS, IS_DARWIN, IS_LINUX,
                                     build_type)
from tools.setup_helpers.cmake import CMake
from tools.generate_torch_version import get_torch_version
################################################################################
# Parameters parsed from environment
"
424,"entry_points=entry_points,
install_requires=install_requires,
package_data={
            ""torch"": [
                ""py.typed"",
                ""bin/*"",
                ""test/*"",
                ""_C/*.pyi"",
                ""cuda/*.pyi"",
                ""optim/*.pyi"",
                ""autograd/*.pyi"",
                ""utils/data/*.pyi"",
                ""nn/*.pyi"",
                ""nn/modules/*.pyi"",
                ""nn/parallel/*.pyi"",
                ""lib/*.so*"",
                ""lib/*.dylib*"",
                ""lib/*.dll"",
                ""lib/*.lib"",
                ""lib/*.pdb"",
                ""lib/torch_shm_manager"",
                ""lib/*.h"",
                ""include/ATen/*.h"",
                ""include/ATen/cpu/*.h"",
                ""include/ATen/cpu/vec/vec256/*.h"",
                ""include/ATen/cpu/vec/vec512/*.h"",
                ""include/ATen/cpu/vec/*.h"",
                ""include/ATen/core/*.h"",
                ""include/ATen/cuda/*.cuh"",
                ""include/ATen/cuda/*.h"",
                ""include/ATen/cuda/detail/*.cuh"",
                ""include/ATen/cuda/detail/*.h"",
                ""include/ATen/cudnn/*.h"",
                ""include/ATen/hip/*.cuh"",
                ""include/ATen/hip/*.h"",
                ""include/ATen/hip/detail/*.cuh"",
                ""include/ATen/hip/detail/*.h"",
                ""include/ATen/hip/impl/*.h"",
                ""include/ATen/detail/*.h"",
                ""include/ATen/native/*.h"",
                ""include/ATen/native/cpu/*.h"",
                ""include/ATen/native/cuda/*.h"",
                ""include/ATen/native/cuda/*.cuh"",
                ""include/ATen/native/hip/*.h"",
                ""include/ATen/native/hip/*.cuh"",
                ""include/ATen/native/quantized/*.h"",
                ""include/ATen/native/quantized/cpu/*.h"",
                ""include/ATen/quantized/*.h"",
                ""include/caffe2/utils/*.h"",
                ""include/caffe2/utils/**/*.h"",
                ""include/c10/*.h"",
                ""include/c10/macros/*.h"",
                ""include/c10/core/*.h"",
                ""include/ATen/core/boxing/*.h"",
                ""include/ATen/core/boxing/impl/*.h"",
                ""include/ATen/core/dispatch/*.h"",
                ""include/ATen/core/op_registration/*.h"",
                ""include/c10/core/impl/*.h"",
                ""include/c10/util/*.h"",
                ""include/c10/cuda/*.h"",
                ""include/c10/cuda/impl/*.h"",
                ""include/c10/hip/*.h"",
                ""include/c10/hip/impl/*.h"",
                ""include/c10d/*.hpp"",
                ""include/caffe2/**/*.h"",
                ""include/torch/*.h"",
                ""include/torch/csrc/*.h"",
                ""include/torch/csrc/api/include/torch/*.h"",
                ""include/torch/csrc/api/include/torch/data/*.h"",
                ""include/torch/csrc/api/include/torch/data/dataloader/*.h"",
                ""include/torch/csrc/api/include/torch/data/datasets/*.h"",
                ""include/torch/csrc/api/include/torch/data/detail/*.h"",
                ""include/torch/csrc/api/include/torch/data/samplers/*.h"",
                ""include/torch/csrc/api/include/torch/data/transforms/*.h"",
                ""include/torch/csrc/api/include/torch/detail/*.h"",
                ""include/torch/csrc/api/include/torch/detail/ordered_dict.h"",
                ""include/torch/csrc/api/include/torch/nn/*.h"",
                ""include/torch/csrc/api/include/torch/nn/functional/*.h"",
                ""include/torch/csrc/api/include/torch/nn/options/*.h"",
                ""include/torch/csrc/api/include/torch/nn/modules/*.h"",
                ""include/torch/csrc/api/include/torch/nn/modules/container/*.h"",
                ""include/torch/csrc/api/include/torch/nn/parallel/*.h"",
                ""include/torch/csrc/api/include/torch/nn/utils/*.h"",
                ""include/torch/csrc/api/include/torch/optim/*.h"",
                ""include/torch/csrc/api/include/torch/optim/schedulers/*.h"",
                ""include/torch/csrc/api/include/torch/serialize/*.h"",
                ""include/torch/csrc/autograd/*.h"",
                ""include/torch/csrc/autograd/functions/*.h"",
                ""include/torch/csrc/autograd/generated/*.h"",
                ""include/torch/csrc/autograd/utils/*.h"",
                ""include/torch/csrc/cuda/*.h"",
                ""include/torch/csrc/jit/*.h"",
                ""include/torch/csrc/jit/backends/*.h"",
                ""include/torch/csrc/jit/generated/*.h"",
                ""include/torch/csrc/jit/passes/*.h"",
                ""include/torch/csrc/jit/passes/quantization/*.h"",
                ""include/torch/csrc/jit/passes/utils/*.h"",
                ""include/torch/csrc/jit/runtime/*.h"",
                ""include/torch/csrc/jit/ir/*.h"",
                ""include/torch/csrc/jit/frontend/*.h"",
                ""include/torch/csrc/jit/api/*.h"",
                ""include/torch/csrc/jit/serialization/*.h"",
                ""include/torch/csrc/jit/python/*.h"",
                ""include/torch/csrc/jit/testing/*.h"",
                ""include/torch/csrc/jit/tensorexpr/*.h"",
                ""include/torch/csrc/jit/tensorexpr/operators/*.h"",
                ""include/torch/csrc/onnx/*.h"",
                ""include/torch/csrc/utils/*.h"",
                ""include/torch/csrc/tensor/*.h"",
                ""include/pybind11/*.h"",
                ""include/pybind11/detail/*.h"",
                ""include/TH/*.h*"",
                ""include/TH/generic/*.h*"",
                ""include/THC/*.cuh"",
                ""include/THC/*.h*"",
                ""include/THC/generic/*.h"",
                ""include/THCUNN/*.cuh"",
                ""include/THCUNN/generic/*.h"",
                ""include/THH/*.cuh"",
                ""include/THH/*.h*"",
                ""include/THH/generic/*.h"",
                ""share/cmake/ATen/*.cmake"",
                ""share/cmake/Caffe2/*.cmake"",
                ""share/cmake/Caffe2/public/*.cmake"",
                ""share/cmake/Caffe2/Modules_CUDA_fix/*.cmake"",
                ""share/cmake/Caffe2/Modules_CUDA_fix/upstream/*.cmake"",
                ""share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/*.cmake"",
                ""share/cmake/Gloo/*.cmake"",
                ""share/cmake/Tensorpipe/*.cmake"",
                ""share/cmake/Torch/*.cmake"",
                ""utils/benchmark/utils/*.cpp"",
                ""utils/benchmark/utils/valgrind_wrapper/*.cpp"",
                ""utils/benchmark/utils/valgrind_wrapper/*.h"",
                ""utils/model_dump/skeleton.html"",
                ""utils/model_dump/code.js"",
                ""utils/model_dump/*.mjs"",
],
            ""caffe2"": [
                ""python/serialized_test/data/operator_test/*.zip"",
],
},
        url=""https://pytorch.org/"",
        download_url=""https://github.com/pytorch/pytorch/tags"",
        author=""PyTorch Team"",
        author_email=""packages@pytorch.org"",
        python_requires="">={}"".format(python_min_version_str),
# PyPI package information.
classifiers=[
            ""Development Status :: 5 - Production/Stable"",
            ""Intended Audience :: Developers"",
            ""Intended Audience :: Education"",
            ""Intended Audience :: Science/Research"",
            ""License :: OSI Approved :: BSD License"",
            ""Topic :: Scientific/Engineering"",
            ""Topic :: Scientific/Engineering :: Mathematics"",
            ""Topic :: Scientific/Engineering :: Artificial Intelligence"",
            ""Topic :: Software Development"",
            ""Topic :: Software Development :: Libraries"",
            ""Topic :: Software Development :: Libraries :: Python Modules"",
            ""Programming Language :: C++"",
            ""Programming Language :: Python :: 3"",
        ]
        + [
            ""Programming Language :: Python :: 3.{}"".format(i)
            for i in range(python_min_version[1], version_range_max)
        ],
        license=""BSD-3"",
        keywords=""pytorch machine learning"",
)
if EMIT_BUILD_WARNING:
print_box(build_update_message)
","entry_points=entry_points,
install_requires=install_requires,
package_data={
            'torch': [
                'py.typed',
                'bin/*',
                'test/*',
                '_C/*.pyi',
                'cuda/*.pyi',
                'optim/*.pyi',
                'autograd/*.pyi',
                'utils/data/*.pyi',
                'nn/*.pyi',
                'nn/modules/*.pyi',
                'nn/parallel/*.pyi',
                'lib/*.so*',
                'lib/*.dylib*',
                'lib/*.dll',
                'lib/*.lib',
                'lib/*.pdb',
                'lib/torch_shm_manager',
                'lib/*.h',
                'include/ATen/*.h',
                'include/ATen/cpu/*.h',
                'include/ATen/cpu/vec/vec256/*.h',
                'include/ATen/cpu/vec/vec512/*.h',
                'include/ATen/cpu/vec/*.h',
                'include/ATen/core/*.h',
                'include/ATen/cuda/*.cuh',
                'include/ATen/cuda/*.h',
                'include/ATen/cuda/detail/*.cuh',
                'include/ATen/cuda/detail/*.h',
                'include/ATen/cudnn/*.h',
                'include/ATen/hip/*.cuh',
                'include/ATen/hip/*.h',
                'include/ATen/hip/detail/*.cuh',
                'include/ATen/hip/detail/*.h',
                'include/ATen/hip/impl/*.h',
                'include/ATen/detail/*.h',
                'include/ATen/native/*.h',
                'include/ATen/native/cpu/*.h',
                'include/ATen/native/cuda/*.h',
                'include/ATen/native/cuda/*.cuh',
                'include/ATen/native/hip/*.h',
                'include/ATen/native/hip/*.cuh',
                'include/ATen/native/quantized/*.h',
                'include/ATen/native/quantized/cpu/*.h',
                'include/ATen/quantized/*.h',
                'include/caffe2/utils/*.h',
                'include/caffe2/utils/**/*.h',
                'include/c10/*.h',
                'include/c10/macros/*.h',
                'include/c10/core/*.h',
                'include/ATen/core/boxing/*.h',
                'include/ATen/core/boxing/impl/*.h',
                'include/ATen/core/dispatch/*.h',
                'include/ATen/core/op_registration/*.h',
                'include/c10/core/impl/*.h',
                'include/c10/util/*.h',
                'include/c10/cuda/*.h',
                'include/c10/cuda/impl/*.h',
                'include/c10/hip/*.h',
                'include/c10/hip/impl/*.h',
                'include/c10d/*.hpp',
                'include/caffe2/**/*.h',
                'include/torch/*.h',
                'include/torch/csrc/*.h',
                'include/torch/csrc/api/include/torch/*.h',
                'include/torch/csrc/api/include/torch/data/*.h',
                'include/torch/csrc/api/include/torch/data/dataloader/*.h',
                'include/torch/csrc/api/include/torch/data/datasets/*.h',
                'include/torch/csrc/api/include/torch/data/detail/*.h',
                'include/torch/csrc/api/include/torch/data/samplers/*.h',
                'include/torch/csrc/api/include/torch/data/transforms/*.h',
                'include/torch/csrc/api/include/torch/detail/*.h',
                'include/torch/csrc/api/include/torch/detail/ordered_dict.h',
                'include/torch/csrc/api/include/torch/nn/*.h',
                'include/torch/csrc/api/include/torch/nn/functional/*.h',
                'include/torch/csrc/api/include/torch/nn/options/*.h',
                'include/torch/csrc/api/include/torch/nn/modules/*.h',
                'include/torch/csrc/api/include/torch/nn/modules/container/*.h',
                'include/torch/csrc/api/include/torch/nn/parallel/*.h',
                'include/torch/csrc/api/include/torch/nn/utils/*.h',
                'include/torch/csrc/api/include/torch/optim/*.h',
                'include/torch/csrc/api/include/torch/optim/schedulers/*.h',
                'include/torch/csrc/api/include/torch/serialize/*.h',
                'include/torch/csrc/autograd/*.h',
                'include/torch/csrc/autograd/functions/*.h',
                'include/torch/csrc/autograd/generated/*.h',
                'include/torch/csrc/autograd/utils/*.h',
                'include/torch/csrc/cuda/*.h',
                'include/torch/csrc/jit/*.h',
                'include/torch/csrc/jit/backends/*.h',
                'include/torch/csrc/jit/generated/*.h',
                'include/torch/csrc/jit/passes/*.h',
                'include/torch/csrc/jit/passes/quantization/*.h',
                'include/torch/csrc/jit/passes/utils/*.h',
                'include/torch/csrc/jit/runtime/*.h',
                'include/torch/csrc/jit/ir/*.h',
                'include/torch/csrc/jit/frontend/*.h',
                'include/torch/csrc/jit/api/*.h',
                'include/torch/csrc/jit/serialization/*.h',
                'include/torch/csrc/jit/python/*.h',
                'include/torch/csrc/jit/testing/*.h',
                'include/torch/csrc/jit/tensorexpr/*.h',
                'include/torch/csrc/jit/tensorexpr/operators/*.h',
                'include/torch/csrc/onnx/*.h',
                'include/torch/csrc/utils/*.h',
                'include/torch/csrc/tensor/*.h',
                'include/pybind11/*.h',
                'include/pybind11/detail/*.h',
                'include/TH/*.h*',
                'include/TH/generic/*.h*',
                'include/THC/*.cuh',
                'include/THC/*.h*',
                'include/THC/generic/*.h',
                'include/THCUNN/*.cuh',
                'include/THCUNN/generic/*.h',
                'include/THH/*.cuh',
                'include/THH/*.h*',
                'include/THH/generic/*.h',
                'share/cmake/ATen/*.cmake',
                'share/cmake/Caffe2/*.cmake',
                'share/cmake/Caffe2/public/*.cmake',
                'share/cmake/Caffe2/Modules_CUDA_fix/*.cmake',
                'share/cmake/Caffe2/Modules_CUDA_fix/upstream/*.cmake',
                'share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/*.cmake',
                'share/cmake/Gloo/*.cmake',
                'share/cmake/Tensorpipe/*.cmake',
                'share/cmake/Torch/*.cmake',
                'utils/benchmark/utils/*.cpp',
                'utils/benchmark/utils/valgrind_wrapper/*.cpp',
                'utils/benchmark/utils/valgrind_wrapper/*.h',
                'utils/model_dump/skeleton.html',
                'utils/model_dump/code.js',
                'utils/model_dump/*.mjs',
],
            'caffe2': [
                'python/serialized_test/data/operator_test/*.zip',
],
},
        url='https://pytorch.org/',
        download_url='https://github.com/pytorch/pytorch/tags',
        author='PyTorch Team',
        author_email='packages@pytorch.org',
        python_requires='>={}'.format(python_min_version_str),
# PyPI package information.
classifiers=[
            'Development Status :: 5 - Production/Stable',
            'Intended Audience :: Developers',
            'Intended Audience :: Education',
            'Intended Audience :: Science/Research',
            'License :: OSI Approved :: BSD License',
            'Topic :: Scientific/Engineering',
            'Topic :: Scientific/Engineering :: Mathematics',
            'Topic :: Scientific/Engineering :: Artificial Intelligence',
            'Topic :: Software Development',
            'Topic :: Software Development :: Libraries',
            'Topic :: Software Development :: Libraries :: Python Modules',
            'Programming Language :: C++',
            'Programming Language :: Python :: 3',
        ] + ['Programming Language :: Python :: 3.{}'.format(i) for i in range(python_min_version[1], version_range_max)],
        license='BSD-3',
        keywords='pytorch machine learning',
)
if EMIT_BUILD_WARNING:
print_box(build_update_message)
"
425,"introducing torch._VF
""""""
import sys
import types
import torch

class VFModule(types.ModuleType):
vf: types.ModuleType
","introducing torch._VF
""""""
import torch
import sys
import types
class VFModule(types.ModuleType):
vf: types.ModuleType
"
426,"is_loaded = True
if not is_loaded:
if not path_patched:
                os.environ[""PATH""] = "";"".join(dll_paths + [os.environ[""PATH""]])
path_patched = True
res = kernel32.LoadLibraryW(dll)
if res is None:
","is_loaded = True
if not is_loaded:
if not path_patched:
                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])
path_patched = True
res = kernel32.LoadLibraryW(dll)
if res is None:
"
427,"""""""
if system == ""win32"":
path = user_data_dir(appname, appauthor, None, roaming)
    elif system == ""darwin"":
        path = os.path.expanduser(""~/Library/Preferences/"")
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv(""XDG_CONFIG_HOME"", os.path.expanduser(""~/.config""))
if appname:
path = os.path.join(path, appname)
if appname and version:
","""""""
if system == ""win32"":
path = user_data_dir(appname, appauthor, None, roaming)
    elif system == 'darwin':
        path = os.path.expanduser('~/Library/Preferences/')
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv('XDG_CONFIG_HOME', os.path.expanduser(""~/.config""))
if appname:
path = os.path.join(path, appname)
if appname and version:
"
428,"# A_grad = (U^{-1} X^H P^T)^H, or
# A_grad = torch.triangular_solve(X^H P^T, U)^H
X = torch.triangular_solve(phi, L.transpose(-1, -2).conj(), upper=True).solution
        A_grad = (
            torch.triangular_solve(
                X.transpose(-1, -2).conj() @ P.transpose(-1, -2), U, upper=True
            )
            .solution.transpose(-1, -2)
            .conj()
        )
return A_grad, None, None
","# A_grad = (U^{-1} X^H P^T)^H, or
# A_grad = torch.triangular_solve(X^H P^T, U)^H
X = torch.triangular_solve(phi, L.transpose(-1, -2).conj(), upper=True).solution
        A_grad = torch.triangular_solve(X.transpose(-1, -2).conj() @ P.transpose(-1, -2), U, upper=True) \
            .solution.transpose(-1, -2).conj()
return A_grad, None, None
"
429,"You should not use this directly, it should only be used from the other
createResolutionCallbackFrom* functions.
""""""

def lookupInModule(qualified_name, module):
        if ""."" in qualified_name:
            parts = qualified_name.split(""."")
base = parts[0]
            remaining_pieces = ""."".join(parts[1:])
module_value = getattr(module, base)
return lookupInModule(remaining_pieces, module_value)
else:
","You should not use this directly, it should only be used from the other
createResolutionCallbackFrom* functions.
""""""
def lookupInModule(qualified_name, module):
        if '.' in qualified_name:
            parts = qualified_name.split('.')
base = parts[0]
            remaining_pieces = '.'.join(parts[1:])
module_value = getattr(module, base)
return lookupInModule(remaining_pieces, module_value)
else:
"
430,"def parseExpr(expr, module):
try:
value, len_parsed = parseNestedExpr(expr, module)
            assert len_parsed == len(
                expr
            ), ""whole expression was not parsed, falling back to c++ parser""
return value
except Exception:
""""""
","def parseExpr(expr, module):
try:
value, len_parsed = parseNestedExpr(expr, module)
            assert len_parsed == len(expr), ""whole expression was not parsed, falling back to c++ parser""
return value
except Exception:
""""""
"
431,"""""""
# cls is a type here, so `ismethod` is false since the methods on the type
# aren't bound to anything, so Python treats them as regular functions
    fns = [
        getattr(cls, name)
        for name in cls.__dict__
        if inspect.isroutine(getattr(cls, name))
    ]
captures = {}
for fn in fns:
","""""""
# cls is a type here, so `ismethod` is false since the methods on the type
# aren't bound to anything, so Python treats them as regular functions
    fns = [getattr(cls, name) for name in cls.__dict__ if inspect.isroutine(getattr(cls, name))]
captures = {}
for fn in fns:
"
432,"# when modules of the same name are in the same file
# qualified_name => class name => list[overload_functions]
_overloaded_methods: Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484
# (qualified_name, class name) => class_fileno
_overloaded_method_class_fileno = {}

def _overload_method(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
","# when modules of the same name are in the same file
# qualified_name => class name => list[overload_functions]
_overloaded_methods : Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484
# (qualified_name, class name) => class_fileno
_overloaded_method_class_fileno = {}
def _overload_method(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
"
433,"else:
existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]
if existing_lineno != line_no:
            raise RuntimeError(
                ""Cannot currently overload the same method name in two different""
                "" classes with the same name in the same module""
            )
method_overloads.append(func)
return func

def _get_overloaded_methods(method, mod_class):
# TODO: __name__ not set for submodules in recursive script
if not hasattr(method, ""__name__""):
","else:
existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]
if existing_lineno != line_no:
            raise RuntimeError(""Cannot currently overload the same method name in two different""
                               "" classes with the same name in the same module"")
method_overloads.append(func)
return func
def _get_overloaded_methods(method, mod_class):
# TODO: __name__ not set for submodules in recursive script
if not hasattr(method, ""__name__""):
"
434,"def is_rref_instance(obj) -> bool:
return isinstance(obj, PyRRef)

else:

def is_rref_instance(obj) -> bool:
# If the RPC module doesn't exist then RRefs don't exist either.
return False

def is_final(ann) -> bool:
    return ann.__module__ in {""typing"", ""typing_extensions""} and (
        getattr(ann, ""__origin__"", None) is Final or isinstance(ann, type(Final))
    )

# allows BroadcastingList instance to be subscriptable
class BroadcastingListCls(object):
def __getitem__(self, types):
return

# mypy doesn't support parameters on types, so we have to explicitly type each
# list size
BroadcastingList1 = BroadcastingListCls()
","def is_rref_instance(obj) -> bool:
return isinstance(obj, PyRRef)
else:
def is_rref_instance(obj) -> bool:
# If the RPC module doesn't exist then RRefs don't exist either.
return False
def is_final(ann) -> bool:
    return ann.__module__ in {'typing', 'typing_extensions'} and \
        (getattr(ann, '__origin__', None) is Final or isinstance(ann, type(Final)))
# allows BroadcastingList instance to be subscriptable
class BroadcastingListCls(object):
def __getitem__(self, types):
return
# mypy doesn't support parameters on types, so we have to explicitly type each
# list size
BroadcastingList1 = BroadcastingListCls()
"
435,"# Needs more memory, O(... * k^2), but with only O(... * k^2) complexity.
poly_coeffs_new = poly_coeffs.clone() if roots.requires_grad else poly_coeffs
out = poly_coeffs_new.narrow(-1, poly_order - i, i + 1)
        out -= roots.narrow(-1, i - 1, 1) * poly_coeffs.narrow(
            -1, poly_order - i + 1, i + 1
        )
poly_coeffs = poly_coeffs_new
return poly_coeffs.narrow(-1, 1, poly_order + 1)
","# Needs more memory, O(... * k^2), but with only O(... * k^2) complexity.
poly_coeffs_new = poly_coeffs.clone() if roots.requires_grad else poly_coeffs
out = poly_coeffs_new.narrow(-1, poly_order - i, i + 1)
        out -= roots.narrow(-1, i - 1, 1) * poly_coeffs.narrow(-1, poly_order - i + 1, i + 1)
poly_coeffs = poly_coeffs_new
return poly_coeffs.narrow(-1, 1, poly_order + 1)
"
436,"dtype = _utils.get_floating_dtype(A)
device = A.device
if tol is None:
        feps = {torch.float32: 1.2e-07, torch.float64: 2.23e-16}[dtype]
tol = feps ** 0.5
m = A.shape[-1]
k = (1 if X is None else X.shape[-1]) if k is None else k
n = (k if n is None else n) if X is None else X.shape[-1]
    if m < 3 * n:
raise ValueError(
            ""LPBPCG algorithm is not applicable when the number of A rows (={})""
            "" is smaller than 3 x the number of requested eigenpairs (={})"".format(m, n)
        )
    method = ""ortho"" if method is None else method
iparams = {
        ""m"": m,
        ""n"": n,
        ""k"": k,
        ""niter"": 1000 if niter is None else niter,
}
fparams = {
        ""tol"": tol,
}
    bparams = {""largest"": True if largest is None else largest}
    if method == ""ortho"":
if ortho_iparams is not None:
iparams.update(ortho_iparams)
if ortho_fparams is not None:
fparams.update(ortho_fparams)
if ortho_bparams is not None:
bparams.update(ortho_bparams)
        iparams[""ortho_i_max""] = iparams.get(""ortho_i_max"", 3)
        iparams[""ortho_j_max""] = iparams.get(""ortho_j_max"", 3)
        fparams[""ortho_tol""] = fparams.get(""ortho_tol"", tol)
        fparams[""ortho_tol_drop""] = fparams.get(""ortho_tol_drop"", tol)
        fparams[""ortho_tol_replace""] = fparams.get(""ortho_tol_replace"", tol)
        bparams[""ortho_use_drop""] = bparams.get(""ortho_use_drop"", False)
if not torch.jit.is_scripting():
LOBPCG.call_tracker = LOBPCG_call_tracker  # type: ignore[assignment]
","dtype = _utils.get_floating_dtype(A)
device = A.device
if tol is None:
        feps = {torch.float32: 1.2e-07,
                torch.float64: 2.23e-16}[dtype]
tol = feps ** 0.5
m = A.shape[-1]
k = (1 if X is None else X.shape[-1]) if k is None else k
n = (k if n is None else n) if X is None else X.shape[-1]
    if (m < 3 * n):
raise ValueError(
            'LPBPCG algorithm is not applicable when the number of A rows (={})'
            ' is smaller than 3 x the number of requested eigenpairs (={})'
            .format(m, n))
    method = 'ortho' if method is None else method
iparams = {
        'm': m,
        'n': n,
        'k': k,
        'niter': 1000 if niter is None else niter,
}
fparams = {
        'tol': tol,
}
    bparams = {
        'largest': True if largest is None else largest
    }
    if method == 'ortho':
if ortho_iparams is not None:
iparams.update(ortho_iparams)
if ortho_fparams is not None:
fparams.update(ortho_fparams)
if ortho_bparams is not None:
bparams.update(ortho_bparams)
        iparams['ortho_i_max'] = iparams.get('ortho_i_max', 3)
        iparams['ortho_j_max'] = iparams.get('ortho_j_max', 3)
        fparams['ortho_tol'] = fparams.get('ortho_tol', tol)
        fparams['ortho_tol_drop'] = fparams.get('ortho_tol_drop', tol)
        fparams['ortho_tol_replace'] = fparams.get('ortho_tol_replace', tol)
        bparams['ortho_use_drop'] = bparams.get('ortho_use_drop', False)
if not torch.jit.is_scripting():
LOBPCG.call_tracker = LOBPCG_call_tracker  # type: ignore[assignment]
"
437,"if not torch.jit.is_scripting():
if type(A) is not torch.Tensor and has_torch_function((A,)):
            return handle_torch_function(
                pca_lowrank, (A,), A, q=q, center=center, niter=niter
            )
(m, n) = A.shape[-2:]
if q is None:
q = min(6, m, n)
elif not (q >= 0 and q <= min(m, n)):
        raise ValueError(
            ""q(={}) must be non-negative integer""
            "" and not greater than min(m, n)={}"".format(q, min(m, n))
        )
if not (niter >= 0):
        raise ValueError(""niter(={}) must be non-negative integer"".format(niter))
dtype = _utils.get_floating_dtype(A)
","if not torch.jit.is_scripting():
if type(A) is not torch.Tensor and has_torch_function((A,)):
            return handle_torch_function(pca_lowrank, (A,), A, q=q, center=center, niter=niter)
(m, n) = A.shape[-2:]
if q is None:
q = min(6, m, n)
elif not (q >= 0 and q <= min(m, n)):
        raise ValueError('q(={}) must be non-negative integer'
                         ' and not greater than min(m, n)={}'
                         .format(q, min(m, n)))
if not (niter >= 0):
        raise ValueError('niter(={}) must be non-negative integer'
                         .format(niter))
dtype = _utils.get_floating_dtype(A)
"
438,"has_names = len(names) > 0
has_rename_pairs = bool(rename_map)
if has_names and has_rename_pairs:
        raise RuntimeError(
            ""{api_name}: This function takes either positional ""
            ""args or keyword args, but not both. Use tensor.{api_name}(*names) ""
            ""to name dims and tensor.{api_name}(**rename_map) to rename ""
            ""dims."".format(api_name=namer_api_name(inplace))
        )
# Special case for tensor.rename(*[]), which is valid for a 0 dim tensor.
if not has_names and not has_rename_pairs:
","has_names = len(names) > 0
has_rename_pairs = bool(rename_map)
if has_names and has_rename_pairs:
        raise RuntimeError('{api_name}: This function takes either positional '
                           'args or keyword args, but not both. Use tensor.{api_name}(*names) '
                           'to name dims and tensor.{api_name}(**rename_map) to rename '
                           'dims.'.format(api_name=namer_api_name(inplace)))
# Special case for tensor.rename(*[]), which is valid for a 0 dim tensor.
if not has_names and not has_rename_pairs:
"
439,"import re

import torch._C as C
","import re
import torch._C as C
"
440,"""""""
def remove_prefix(text, prefix):
        return text[text.startswith(prefix) and len(prefix) :]
# Find the line and line number containing the function definition
for i, l in enumerate(sourcelines):
","""""""
def remove_prefix(text, prefix):
        return text[text.startswith(prefix) and len(prefix):]
# Find the line and line number containing the function definition
for i, l in enumerate(sourcelines):
"
441,"filename: Optional[str]
file_lineno: int

def parse_def(fn):
    sourcelines, file_lineno, filename = get_source_lines_and_file(
        fn, ErrorReport.call_stack()
    )
sourcelines = normalize_source_lines(sourcelines)
    source = """".join(sourcelines)
dedent_src = dedent(source)
py_ast = ast.parse(dedent_src)
if len(py_ast.body) != 1 or not isinstance(py_ast.body[0], ast.FunctionDef):
        raise RuntimeError(
            f""Expected a single top-level function: {filename}:{file_lineno}""
        )
    leading_whitespace_len = len(source.split(""\n"", 1)[0]) - len(
        dedent_src.split(""\n"", 1)[0]
    )
    ctx = make_source_context(
        source, filename, file_lineno, leading_whitespace_len, True
    )
return ParsedDef(py_ast, ctx, source, filename, file_lineno)
","filename: Optional[str]
file_lineno: int
def parse_def(fn):
    sourcelines, file_lineno, filename = get_source_lines_and_file(fn, ErrorReport.call_stack())
sourcelines = normalize_source_lines(sourcelines)
    source = ''.join(sourcelines)
dedent_src = dedent(source)
py_ast = ast.parse(dedent_src)
if len(py_ast.body) != 1 or not isinstance(py_ast.body[0], ast.FunctionDef):
        raise RuntimeError(f""Expected a single top-level function: {filename}:{file_lineno}"")
    leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
    ctx = make_source_context(source, filename, file_lineno, leading_whitespace_len, True)
return ParsedDef(py_ast, ctx, source, filename, file_lineno)
"
442,"if not self.is_cuda:
raise AttributeError(
""Can't get __cuda_array_interface__ on non-CUDA tensor type: %s ""
                ""If CUDA data is required use tensor.cuda() to copy tensor to device memory.""
                % self.type()
)
if self.is_sparse:
raise AttributeError(
""Can't get __cuda_array_interface__ on sparse type: %s ""
                ""Use Tensor.to_dense() to convert to a dense tensor first.""
                % self.type()
)
# RuntimeError, matching tensor.__array__() behavior.
","if not self.is_cuda:
raise AttributeError(
""Can't get __cuda_array_interface__ on non-CUDA tensor type: %s ""
                ""If CUDA data is required use tensor.cuda() to copy tensor to device memory."" %
                self.type()
)
if self.is_sparse:
raise AttributeError(
""Can't get __cuda_array_interface__ on sparse type: %s ""
                ""Use Tensor.to_dense() to convert to a dense tensor first."" %
                self.type()
)
# RuntimeError, matching tensor.__array__() behavior.
"
443,"""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, ""align_to"")
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)], ellipsis_idx
        )
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
","""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, 'align_to')
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)],
            ellipsis_idx)
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
"
444,"tensor([[ 0,  1],
[ 2,  3]], dtype=torch.int8)
"""""".format(
        **new_common_args
    ),
)
add_docstr_all(
    ""new_full"",
    r""""""
new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.
","tensor([[ 0,  1],
[ 2,  3]], dtype=torch.int8)
"""""".format(**new_common_args))
add_docstr_all('new_full',
               r""""""
new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.
"
445,"tensor
See :func:`torch.add`
"""""",
)
add_docstr_all(
    ""add_"",
    r""""""
add_(other, *, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.add`
"""""",
)
add_docstr_all(
    ""addbmm"",
    r""""""
addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addbmm`
"""""",
)
add_docstr_all(
    ""addbmm_"",
    r""""""
addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addbmm`
"""""",
)
add_docstr_all(
    ""addcdiv"",
    r""""""
addcdiv(tensor1, tensor2, *, value=1) -> Tensor
See :func:`torch.addcdiv`
"""""",
)
add_docstr_all(
    ""addcdiv_"",
    r""""""
addcdiv_(tensor1, tensor2, *, value=1) -> Tensor
In-place version of :meth:`~Tensor.addcdiv`
"""""",
)
add_docstr_all(
    ""addcmul"",
    r""""""
addcmul(tensor1, tensor2, *, value=1) -> Tensor
See :func:`torch.addcmul`
"""""",
)
add_docstr_all(
    ""addcmul_"",
    r""""""
addcmul_(tensor1, tensor2, *, value=1) -> Tensor
In-place version of :meth:`~Tensor.addcmul`
"""""",
)
add_docstr_all(
    ""addmm"",
    r""""""
addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addmm`
"""""",
)
add_docstr_all(
    ""addmm_"",
    r""""""
addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addmm`
"""""",
)
add_docstr_all(
    ""addmv"",
    r""""""
addmv(mat, vec, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addmv`
"""""",
)
add_docstr_all(
    ""addmv_"",
    r""""""
addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addmv`
"""""",
)
add_docstr_all(
    ""sspaddmm"",
    r""""""
sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.sspaddmm`
"""""",
)
add_docstr_all(
    ""smm"",
    r""""""
smm(mat) -> Tensor
See :func:`torch.smm`
"""""",
)
add_docstr_all(
    ""addr"",
    r""""""
addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addr`
"""""",
)
add_docstr_all(
    ""addr_"",
    r""""""
addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addr`
"""""",
)
add_docstr_all(
    ""align_as"",
    r""""""
align_as(other) -> Tensor
Permutes the dimensions of the :attr:`self` tensor to match the dimension order
","tensor
See :func:`torch.add`
"""""")
add_docstr_all('add_',
               r""""""
add_(other, *, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.add`
"""""")
add_docstr_all('addbmm',
               r""""""
addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addbmm`
"""""")
add_docstr_all('addbmm_',
               r""""""
addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addbmm`
"""""")
add_docstr_all('addcdiv',
               r""""""
addcdiv(tensor1, tensor2, *, value=1) -> Tensor
See :func:`torch.addcdiv`
"""""")
add_docstr_all('addcdiv_',
               r""""""
addcdiv_(tensor1, tensor2, *, value=1) -> Tensor
In-place version of :meth:`~Tensor.addcdiv`
"""""")
add_docstr_all('addcmul',
               r""""""
addcmul(tensor1, tensor2, *, value=1) -> Tensor
See :func:`torch.addcmul`
"""""")
add_docstr_all('addcmul_',
               r""""""
addcmul_(tensor1, tensor2, *, value=1) -> Tensor
In-place version of :meth:`~Tensor.addcmul`
"""""")
add_docstr_all('addmm',
               r""""""
addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addmm`
"""""")
add_docstr_all('addmm_',
               r""""""
addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addmm`
"""""")
add_docstr_all('addmv',
               r""""""
addmv(mat, vec, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addmv`
"""""")
add_docstr_all('addmv_',
               r""""""
addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addmv`
"""""")
add_docstr_all('sspaddmm',
               r""""""
sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.sspaddmm`
"""""")
add_docstr_all('smm',
               r""""""
smm(mat) -> Tensor
See :func:`torch.smm`
"""""")
add_docstr_all('addr',
               r""""""
addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.addr`
"""""")
add_docstr_all('addr_',
               r""""""
addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.addr`
"""""")
add_docstr_all('align_as',
               r""""""
align_as(other) -> Tensor
Permutes the dimensions of the :attr:`self` tensor to match the dimension order
"
446,".. warning::
Throws an error if :attr:`self` is not a sparse COO tensor.
"""""",
)
add_docstr_all(
    ""contiguous"",
    r""""""
contiguous(memory_format=torch.contiguous_format) -> Tensor
Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If
",".. warning::
Throws an error if :attr:`self` is not a sparse COO tensor.
"""""")
add_docstr_all('contiguous',
               r""""""
contiguous(memory_format=torch.contiguous_format) -> Tensor
Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If
"
447,"the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""logcumsumexp"",
    r""""""
logcumsumexp(dim) -> Tensor
See :func:`torch.logcumsumexp`
"""""",
)
add_docstr_all(
    ""cummax"",
    r""""""
cummax(dim) -> (Tensor, Tensor)
See :func:`torch.cummax`
"""""",
)
add_docstr_all(
    ""cummin"",
    r""""""
cummin(dim) -> (Tensor, Tensor)
See :func:`torch.cummin`
"""""",
)
add_docstr_all(
    ""cumprod"",
    r""""""
cumprod(dim, dtype=None) -> Tensor
See :func:`torch.cumprod`
"""""",
)
add_docstr_all(
    ""cumprod_"",
    r""""""
cumprod_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumprod`
"""""",
)
add_docstr_all(
    ""cumsum"",
    r""""""
cumsum(dim, dtype=None) -> Tensor
See :func:`torch.cumsum`
"""""",
)
add_docstr_all(
    ""cumsum_"",
    r""""""
cumsum_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumsum`
"""""",
)
add_docstr_all(
    ""data_ptr"",
    r""""""
data_ptr() -> int
Returns the address of the first element of :attr:`self` tensor.
"""""",
)
add_docstr_all(
    ""dequantize"",
    r""""""
dequantize() -> Tensor
Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
"""""",
)
add_docstr_all(
    ""dense_dim"",
    r""""""
dense_dim() -> int
Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
","the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(**common_args))

add_docstr_all('logcumsumexp',
               r""""""
logcumsumexp(dim) -> Tensor
See :func:`torch.logcumsumexp`
"""""")
add_docstr_all('cummax',
               r""""""
cummax(dim) -> (Tensor, Tensor)
See :func:`torch.cummax`
"""""")
add_docstr_all('cummin',
               r""""""
cummin(dim) -> (Tensor, Tensor)
See :func:`torch.cummin`
"""""")
add_docstr_all('cumprod',
               r""""""
cumprod(dim, dtype=None) -> Tensor
See :func:`torch.cumprod`
"""""")
add_docstr_all('cumprod_',
               r""""""
cumprod_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumprod`
"""""")
add_docstr_all('cumsum',
               r""""""
cumsum(dim, dtype=None) -> Tensor
See :func:`torch.cumsum`
"""""")
add_docstr_all('cumsum_',
               r""""""
cumsum_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumsum`
"""""")
add_docstr_all('data_ptr',
               r""""""
data_ptr() -> int
Returns the address of the first element of :attr:`self` tensor.
"""""")
add_docstr_all('dequantize',
               r""""""
dequantize() -> Tensor
Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
"""""")
add_docstr_all('dense_dim',
               r""""""
dense_dim() -> int
Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
"
448,">>> x.get_device()
0
>>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor
"""""",
)
add_docstr_all(
    ""values"",
    r""""""
values() -> Tensor
Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.
",">>> x.get_device()
0
>>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor
"""""")
add_docstr_all('values',
               r""""""
values() -> Tensor
Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.
"
449,"tensor([[-1.,  2., -1.],
[-1.,  5., -1.],
[-1.,  8., -1.]])
"""""",
)
add_docstr_all(
    ""index_put_"",
    r""""""
index_put_(indices, values, accumulate=False) -> Tensor
Puts values from the tensor :attr:`values` into the tensor :attr:`self` using
","tensor([[-1.,  2., -1.],
[-1.,  5., -1.],
[-1.,  8., -1.]])
"""""")
add_docstr_all('index_put_',
               r""""""
index_put_(indices, values, accumulate=False) -> Tensor
Puts values from the tensor :attr:`values` into the tensor :attr:`self` using
"
450,"[[ 0.0793,  0.0036],
[-0.2569, -0.1055]]]),
size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)
"""""",
)
add_docstr_all(
    ""inverse"",
    r""""""
inverse() -> Tensor
See :func:`torch.inverse`
"""""",
)
add_docstr_all(
    ""isnan"",
    r""""""
isnan() -> Tensor
See :func:`torch.isnan`
"""""",
)
add_docstr_all(
    ""isinf"",
    r""""""
isinf() -> Tensor
See :func:`torch.isinf`
"""""",
)
add_docstr_all(
    ""isposinf"",
    r""""""
isposinf() -> Tensor
See :func:`torch.isposinf`
"""""",
)
add_docstr_all(
    ""isneginf"",
    r""""""
isneginf() -> Tensor
See :func:`torch.isneginf`
"""""",
)
add_docstr_all(
    ""isfinite"",
    r""""""
isfinite() -> Tensor
See :func:`torch.isfinite`
"""""",
)
add_docstr_all(
    ""isclose"",
    r""""""
isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor
See :func:`torch.isclose`
"""""",
)
add_docstr_all(
    ""isreal"",
    r""""""
isreal() -> Tensor
See :func:`torch.isreal`
"""""",
)
add_docstr_all(
    ""is_coalesced"",
    r""""""
is_coalesced() -> bool
Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor
","[[ 0.0793,  0.0036],
[-0.2569, -0.1055]]]),
size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)
"""""")
add_docstr_all('inverse',
               r""""""
inverse() -> Tensor
See :func:`torch.inverse`
"""""")
add_docstr_all('isnan',
               r""""""
isnan() -> Tensor
See :func:`torch.isnan`
"""""")
add_docstr_all('isinf',
               r""""""
isinf() -> Tensor
See :func:`torch.isinf`
"""""")
add_docstr_all('isposinf',
               r""""""
isposinf() -> Tensor
See :func:`torch.isposinf`
"""""")
add_docstr_all('isneginf',
               r""""""
isneginf() -> Tensor
See :func:`torch.isneginf`
"""""")
add_docstr_all('isfinite',
               r""""""
isfinite() -> Tensor
See :func:`torch.isfinite`
"""""")
add_docstr_all('isclose',
               r""""""
isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor
See :func:`torch.isclose`
"""""")
add_docstr_all('isreal',
               r""""""
isreal() -> Tensor
See :func:`torch.isreal`
"""""")
add_docstr_all('is_coalesced',
               r""""""
is_coalesced() -> bool
Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor
"
451,"Tensor. Default: ``torch.contiguous_format``. Note that memory format of
:attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.
"""""",
)
add_docstr_all(
    ""rot90"",
    r""""""
rot90(k, dims) -> Tensor
See :func:`torch.rot90`
"""""",
)
add_docstr_all(
    ""round"",
    r""""""
round() -> Tensor
See :func:`torch.round`
"""""",
)
add_docstr_all(
    ""round_"",
    r""""""
round_() -> Tensor
In-place version of :meth:`~Tensor.round`
"""""",
)
add_docstr_all(
    ""rsqrt"",
    r""""""
rsqrt() -> Tensor
See :func:`torch.rsqrt`
"""""",
)
add_docstr_all(
    ""rsqrt_"",
    r""""""
rsqrt_() -> Tensor
In-place version of :meth:`~Tensor.rsqrt`
"""""",
)
add_docstr_all(
    ""scatter_"",
    r""""""
scatter_(dim, index, src, reduce=None) -> Tensor
Writes all values from the tensor :attr:`src` into :attr:`self` at the indices
","Tensor. Default: ``torch.contiguous_format``. Note that memory format of
:attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.
"""""")
add_docstr_all('rot90',
               r""""""
rot90(k, dims) -> Tensor
See :func:`torch.rot90`
"""""")
add_docstr_all('round',
               r""""""
round() -> Tensor
See :func:`torch.round`
"""""")
add_docstr_all('round_',
               r""""""
round_() -> Tensor
In-place version of :meth:`~Tensor.round`
"""""")
add_docstr_all('rsqrt',
               r""""""
rsqrt() -> Tensor
See :func:`torch.rsqrt`
"""""")
add_docstr_all('rsqrt_',
               r""""""
rsqrt_() -> Tensor
In-place version of :meth:`~Tensor.rsqrt`
"""""")
add_docstr_all('scatter_',
               r""""""
scatter_(dim, index, src, reduce=None) -> Tensor
Writes all values from the tensor :attr:`src` into :attr:`self` at the indices
"
452,":meth:`select` is equivalent to slicing. For example,
``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and
``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.
"""""",
)
add_docstr_all(
    ""set_"",
    r""""""
set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor
Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,
",":meth:`select` is equivalent to slicing. For example,
``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and
``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.
"""""")
add_docstr_all('set_',
               r""""""
set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor
Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,
"
453,":noindex:
See :func:`torch.std`
"""""",
)
add_docstr_all(
    ""storage"",
    r""""""
storage() -> torch.Storage
Returns the underlying storage.
"""""",
)
add_docstr_all(
    ""storage_offset"",
    r""""""
storage_offset() -> int
Returns :attr:`self` tensor's offset in the underlying storage in terms of
",":noindex:
See :func:`torch.std`
"""""")
add_docstr_all('storage',
               r""""""
storage() -> torch.Storage
Returns the underlying storage.
"""""")
add_docstr_all('storage_offset',
               r""""""
storage_offset() -> int
Returns :attr:`self` tensor's offset in the underlying storage in terms of
"
454,"tensor([[ 0,  0,  0],
[ 9,  0, 10],
[ 0,  0,  0]])
"""""",
)
add_docstr_all(
    ""to_sparse"",
    r""""""
to_sparse(sparseDims) -> Tensor
Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
:ref:`coordinate format <sparse-coo-docs>`.
","tensor([[ 0,  0,  0],
[ 9,  0, 10],
[ 0,  0,  0]])
"""""")
add_docstr_all('to_sparse',
               r""""""
to_sparse(sparseDims) -> Tensor
Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
:ref:`coordinate format <sparse-coo-docs>`.
"
455,"The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`
attribute will be populated, see :attr:`is_leaf` for more details.
"""""",
)
add_docstr_all(
    ""is_leaf"",
    r""""""
All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.
For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were
","The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`
attribute will be populated, see :attr:`is_leaf` for more details.
"""""")
add_docstr_all('is_leaf',
               r""""""
All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.
For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were
"
456,"real_str = _scalar_str(self.real, formatter1)
imag_str = (_scalar_str(self.imag, formatter2) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
        if imag_str[0] == ""+"" or imag_str[0] == ""-"":
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
else:
return formatter1.format(self.item())

def _vector_str(self, indent, summarize, formatter1, formatter2=None):
# length includes spaces and comma between elements
element_length = formatter1.width() + 2
","real_str = _scalar_str(self.real, formatter1)
imag_str = (_scalar_str(self.imag, formatter2) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
        if imag_str[0] == '+' or imag_str[0] == '-':
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
else:
return formatter1.format(self.item())
def _vector_str(self, indent, summarize, formatter1, formatter2=None):
# length includes spaces and comma between elements
element_length = formatter1.width() + 2
"
457,"return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = (
            [_val_formatter(val) for val in self[: PRINT_OPTS.edgeitems].tolist()]
            + ["" ...""]
            + [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems :].tolist()]
        )
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [
        data[i : i + elements_per_line] for i in range(0, len(data), elements_per_line)
    ]
    lines = ["", "".join(line) for line in data_lines]
    return ""["" + ("","" + ""\n"" + "" "" * (indent + 1)).join(lines) + ""]""

# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
","return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = ([_val_formatter(val) for val in self[:PRINT_OPTS.edgeitems].tolist()] +
                [' ...'] +
                [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems:].tolist()])
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [data[i:i + elements_per_line] for i in range(0, len(data), elements_per_line)]
    lines = [', '.join(line) for line in data_lines]
    return '[' + (',' + '\n' + ' ' * (indent + 1)).join(lines) + ']'
# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
"
458,"tensor_str = _tensor_str(self, indent)
if self.layout != torch.strided:
        suffixes.append(""layout="" + str(self.layout))
# Use inp here to get the original grad_fn and not the one generated by the forward grad
# unpacking.
if inp.grad_fn is not None:
name = type(inp.grad_fn).__name__
        if name == ""CppFunction"":
            name = inp.grad_fn.name().rsplit(""::"", 1)[-1]
        suffixes.append(""grad_fn=<{}>"".format(name))
elif inp.requires_grad:
        suffixes.append(""requires_grad=True"")
if self.has_names():
        suffixes.append(""names={}"".format(self.names))
if tangent is not None:
        suffixes.append(""tangent={}"".format(tangent))

    return _add_suffixes(
        prefix + tensor_str, suffixes, indent, force_newline=self.is_sparse
    )
def _str(self):
with torch.no_grad():
","tensor_str = _tensor_str(self, indent)
if self.layout != torch.strided:
        suffixes.append('layout=' + str(self.layout))
# Use inp here to get the original grad_fn and not the one generated by the forward grad
# unpacking.
if inp.grad_fn is not None:
name = type(inp.grad_fn).__name__
        if name == 'CppFunction':
            name = inp.grad_fn.name().rsplit('::', 1)[-1]
        suffixes.append('grad_fn=<{}>'.format(name))
elif inp.requires_grad:
        suffixes.append('requires_grad=True')
if self.has_names():
        suffixes.append('names={}'.format(self.names))
if tangent is not None:
        suffixes.append('tangent={}'.format(tangent))
    return _add_suffixes(prefix + tensor_str, suffixes, indent, force_newline=self.is_sparse)
def _str(self):
with torch.no_grad():
"
459,"tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
>>> torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.arccos,
    r""""""
arccos(input, *, out=None) -> Tensor
Alias for :func:`torch.acos`.
"""""",
)
add_docstr(
    torch.acosh,
    r""""""
acosh(input, *, out=None) -> Tensor
Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`.
","tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
>>> torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
"""""".format(**common_args))

add_docstr(torch.arccos, r""""""
arccos(input, *, out=None) -> Tensor
Alias for :func:`torch.acos`.
"""""")
add_docstr(torch.acosh, r""""""
acosh(input, *, out=None) -> Tensor
Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`.
"
460,"beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`)
alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
{out}
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.smm,
    r""""""
smm(input, mat) -> Tensor
Performs a matrix multiplication of the sparse matrix :attr:`input`
","beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`)
alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
{out}
"""""".format(**common_args))
add_docstr(torch.smm,
           r""""""
smm(input, mat) -> Tensor
Performs a matrix multiplication of the sparse matrix :attr:`input`
"
461,"tensor([[ 1.,  2.],
[ 2.,  4.],
[ 3.,  6.]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.allclose,
    r""""""
allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool
This function checks if all :attr:`input` and :attr:`other` satisfy the condition:
.. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
""""""
    + r""""""
elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to
`numpy.allclose <https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html>`_
","tensor([[ 1.,  2.],
[ 2.,  4.],
[ 3.,  6.]])
"""""".format(**common_args))

add_docstr(torch.allclose,
           r""""""
allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool
This function checks if all :attr:`input` and :attr:`other` satisfy the condition:
.. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
"""""" + r""""""
elementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to
`numpy.allclose <https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html>`_
"
462,">>> torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-1, -7,  3], dtype=torch.int8)
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.broadcast_to,
    r""""""
broadcast_to(input, shape) -> Tensor
Broadcasts :attr:`input` to the shape :attr:`\shape`.
",">>> torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))
tensor([-1, -7,  3], dtype=torch.int8)
"""""".format(**common_args))

add_docstr(torch.broadcast_to,
           r""""""
broadcast_to(input, shape) -> Tensor
Broadcasts :attr:`input` to the shape :attr:`\shape`.
"
463,"tensor([6, 7, 8]),
tensor([ 9, 10, 11]),
tensor([12]))
"""""",
)
add_docstr(
    torch.unsafe_chunk,
    r""""""
unsafe_chunk(input, chunks, dim=0) -> List of Tensors
Works like :func:`torch.chunk` but without enforcing the autograd restrictions
","tensor([6, 7, 8]),
tensor([ 9, 10, 11]),
tensor([12]))
"""""")
add_docstr(torch.unsafe_chunk,
           r""""""
unsafe_chunk(input, chunks, dim=0) -> List of Tensors
Works like :func:`torch.chunk` but without enforcing the autograd restrictions
"
464,"True
>>> torch.can_cast(torch.float, torch.int)
False
"""""",
)
add_docstr(
    torch.corrcoef,
    r""""""
corrcoef(input) -> Tensor
Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the :attr:`input` matrix,
","True
>>> torch.can_cast(torch.float, torch.int)
False
"""""")
add_docstr(torch.corrcoef, r""""""
corrcoef(input) -> Tensor
Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the :attr:`input` matrix,
"
465,">>> x.real
tensor([ 0.3100, -0.5445, -1.6492, -0.0638])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.imag,
    r""""""
imag(input) -> Tensor
Returns a new tensor containing imaginary values of the :attr:`self` tensor.
",">>> x.real
tensor([ 0.3100, -0.5445, -1.6492, -0.0638])
"""""".format(**common_args))
add_docstr(torch.imag,
           r""""""
imag(input) -> Tensor
Returns a new tensor containing imaginary values of the :attr:`self` tensor.
"
466,"Keyword args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.clamp,
    r""""""
clamp(input, min=None, max=None, *, out=None) -> Tensor
Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
","Keyword args:
{memory_format}
"""""".format(**common_args))

add_docstr(torch.clamp, r""""""
clamp(input, min=None, max=None, *, out=None) -> Tensor
Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
"
467,"[3, 6, 7, 6, 7],
[4, 8, 9, 8, 9]])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.complex,
    r""""""
complex(real, imag, *, out=None) -> Tensor
Constructs a complex tensor with its real part equal to :attr:`real` and its
","[3, 6, 7, 6, 7],
[4, 8, 9, 8, 9]])
"""""".format(**common_args))
add_docstr(torch.complex,
           r""""""
complex(real, imag, *, out=None) -> Tensor
Constructs a complex tensor with its real part equal to :attr:`real` and its
"
468,".. math::
\text{out}_{i} = conj(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = conj(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
469,"When :attr:`input` is on the CPU, the implementation of torch.cosh may use
the Sleef library, which rounds very large results to infinity or negative
infinity. See `here <https://sleef.org/purec.xhtml>`_ for details.
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.cross,
    r""""""
cross(input, other, dim=None, *, out=None) -> Tensor
","When :attr:`input` is on the CPU, the implementation of torch.cosh may use
the Sleef library, which rounds very large results to infinity or negative
infinity. See `here <https://sleef.org/purec.xhtml>`_ for details.
"""""".format(**common_args))

add_docstr(torch.cross,
           r""""""
cross(input, other, dim=None, *, out=None) -> Tensor
"
470,"[0., 1., 0.],
[0., 0., 1.]], dtype=torch.float64)
"""""",
)
add_docstr(
    torch.eq,
    r""""""
eq(input, other, *, out=None) -> Tensor
Computes element-wise equality
","[0., 1., 0.],
[0., 0., 1.]], dtype=torch.float64)
"""""")
add_docstr(torch.eq, r""""""
eq(input, other, *, out=None) -> Tensor
Computes element-wise equality
"
471,">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.greater,
    r""""""
greater(input, other, *, out=None) -> Tensor
Alias for :func:`torch.gt`.
"""""",
)
add_docstr(
    torch.histc,
    r""""""
histc(input, bins=100, min=0, max=0, *, out=None) -> Tensor
Computes the histogram of a tensor.
",">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
"""""".format(**common_args))

add_docstr(torch.greater, r""""""
greater(input, other, *, out=None) -> Tensor
Alias for :func:`torch.gt`.
"""""")
add_docstr(torch.histc,
           r""""""
histc(input, bins=100, min=0, max=0, *, out=None) -> Tensor
Computes the histogram of a tensor.
"
472,">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.histogram,
    r""""""
histogram(input, bins, *, range=None, weight=None, density=False, out=None) -> (Tensor, Tensor)
Computes a histogram of the values in a tensor.
",">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)
tensor([ 0.,  2.,  1.,  0.])
"""""".format(**common_args))

add_docstr(torch.histogram,
           r""""""
histogram(input, bins, *, range=None, weight=None, density=False, out=None) -> (Tensor, Tensor)
Computes a histogram of the values in a tensor.
"
473,"tensor([[ 0.1427, -0.5414],
[-0.4664, -0.1228],
[-1.1734,  0.7230]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.inverse,
    r""""""
inverse(input, *, out=None) -> Tensor
Alias for :func:`torch.linalg.inv`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isin,
    r""""""
isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor
Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns
","tensor([[ 0.1427, -0.5414],
[-0.4664, -0.1228],
[-1.1734,  0.7230]])
"""""".format(**common_args))

add_docstr(torch.inverse, r""""""
inverse(input, *, out=None) -> Tensor
Alias for :func:`torch.linalg.inv`
"""""".format(**common_args))

add_docstr(torch.isin, r""""""
isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor
Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns
"
474,">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isposinf,
    r""""""
isposinf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is positive infinity or not.
",">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])
"""""".format(**common_args))

add_docstr(torch.isposinf,
           r""""""
isposinf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is positive infinity or not.
"
475,".. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
""""""
    + r""""""
where :attr:`input` and :attr:`other` are finite. Where :attr:`input`
and/or :attr:`other` are nonfinite they are close if and only if
",".. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
"""""" + r""""""
where :attr:`input` and :attr:`other` are finite. Where :attr:`input`
and/or :attr:`other` are nonfinite they are close if and only if
"
476,">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isnan,
    r""""""
isnan(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input`
",">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])
"""""".format(**common_args))

add_docstr(torch.isnan, r""""""
isnan(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input`
"
477,">>> torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([False, True, False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isreal,
    r""""""
isreal(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input` is real-valued or not.
",">>> torch.isnan(torch.tensor([1, float('nan'), 2]))
tensor([False, True, False])
"""""".format(**common_args))

add_docstr(torch.isreal, r""""""
isreal(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input` is real-valued or not.
"
478,">>> c = torch.tensor([3])
>>> torch.lcm(a, c)
tensor([15, 30, 15])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.ldexp,
    r""""""
ldexp(input, other, *, out=None) -> Tensor
Multiplies :attr:`input` by 2**:attr:`other`.
.. math::
\text{{out}}_i = \text{{input}}_i * 2^\text{{other}}_i
""""""
    + r""""""
Typically this function is used to construct floating point numbers by multiplying
mantissas in :attr:`input` with integral powers of two created from the exponents
",">>> c = torch.tensor([3])
>>> torch.lcm(a, c)
tensor([15, 30, 15])
"""""".format(**common_args))

add_docstr(torch.ldexp, r""""""
ldexp(input, other, *, out=None) -> Tensor
Multiplies :attr:`input` by 2**:attr:`other`.
.. math::
\text{{out}}_i = \text{{input}}_i * 2^\text{{other}}_i
"""""" + r""""""
Typically this function is used to construct floating point numbers by multiplying
mantissas in :attr:`input` with integral powers of two created from the exponents
"
479,">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.linspace,
    r""""""
linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
",">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(**common_args))

add_docstr(torch.linspace, r""""""
linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
"
480,"tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
>>> torch.log(a)
tensor([ nan,  nan,  nan,  nan,  nan])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.log10,
    r""""""
log10(input, *, out=None) -> Tensor
Returns a new tensor with the logarithm to the base 10 of the elements
","tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])
>>> torch.log(a)
tensor([ nan,  nan,  nan,  nan,  nan])
"""""".format(**common_args))

add_docstr(torch.log10,
           r""""""
log10(input, *, out=None) -> Tensor
Returns a new tensor with the logarithm to the base 10 of the elements
"
481,"tensor([1.2589])
>>> torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.logsumexp,
    r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Returns the log of summed exponentials of each row of the :attr:`input`
","tensor([1.2589])
>>> torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
"""""".format(**factory_common_args))

add_docstr(torch.logsumexp,
           r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Returns the log of summed exponentials of each row of the :attr:`input`
"
482,"[False, False, False, True]])
>>> torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.matrix_rank,
    r""""""
matrix_rank(input, tol=None, symmetric=False, *, out=None) -> Tensor
Returns the numerical rank of a 2-D tensor. The method to compute the
","[False, False, False, True]])
>>> torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
"""""".format(**common_args))

add_docstr(torch.matrix_rank, r""""""
matrix_rank(input, tol=None, symmetric=False, *, out=None) -> Tensor
Returns the numerical rank of a 2-D tensor. The method to compute the
"
483,"See :func:`torch.maximum`.
"""""".format(
        **single_dim_common
    ),
)
add_docstr(
    torch.maximum,
    r""""""
maximum(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
","See :func:`torch.maximum`.
"""""".format(**single_dim_common))
add_docstr(torch.maximum, r""""""
maximum(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
"
484,">>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])
>>> torch.fmin(a, b)
tensor([-9.3000, 0.1000, 2.1000,    nan])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.amin,
    r""""""
amin(input, dim, keepdim=False, *, out=None) -> Tensor
Returns the minimum value of each slice of the :attr:`input` tensor in the given
",">>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])
>>> torch.fmin(a, b)
tensor([-9.3000, 0.1000, 2.1000,    nan])
"""""".format(**common_args))

add_docstr(torch.amin,
           r""""""
amin(input, dim, keepdim=False, *, out=None) -> Tensor
Returns the minimum value of each slice of the :attr:`input` tensor in the given
"
485,">>> b = a + (torch.randn(50, 1) * 5).long()
>>> torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.mul,
    r""""""
mul(input, other, *, out=None) -> Tensor
Multiplies each element of the input :attr:`input` with the scalar
",">>> b = a + (torch.randn(50, 1) * 5).long()
>>> torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
"""""".format(**single_dim_common))

add_docstr(torch.mul, r""""""
mul(input, other, *, out=None) -> Tensor
Multiplies each element of the input :attr:`input` with the scalar
"
486,"True
>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
"""""",
)
add_docstr(
    torch.rad2deg,
    r""""""
rad2deg(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
","True
>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
"""""")
add_docstr(torch.rad2deg,
           r""""""
rad2deg(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
"
487,">>> torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
[ 0.3816,  0.7249,  0.0998]])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.rand_like,
    r""""""
rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor with the same size as :attr:`input` that is filled with
",">>> torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
[ 0.3816,  0.7249,  0.0998]])
"""""".format(**factory_common_args))

add_docstr(torch.rand_like,
           r""""""
rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor with the same size as :attr:`input` that is filled with
"
488,">>> torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
"""""".format(
        **factory_data_common_args
    ),
)

add_docstr(
    torch.range,
    r""""""
range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1`
",">>> torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
"""""".format(**factory_data_common_args))

add_docstr(torch.range,
           r""""""
range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1`
"
489,".. math::
\text{out}_{i+1} = \text{out}_i + \text{step}.
""""""
    + r""""""
.. warning::
This function is deprecated and will be removed in a future release because its behavior is inconsistent with
Python's range builtin. Instead, use :func:`torch.arange`, which produces values in [start, end).
",".. math::
\text{out}_{i+1} = \text{out}_i + \text{step}.
"""""" + r""""""
.. warning::
This function is deprecated and will be removed in a future release because its behavior is inconsistent with
Python's range builtin. Instead, use :func:`torch.arange`, which produces values in [start, end).
"
490,"tensor([-2.0755,  1.0226,  0.0831,  0.4806])
>>> torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.square,
    r""""""
square(input, *, out=None) -> Tensor
Returns a new tensor with the square of the elements of :attr:`input`.
","tensor([-2.0755,  1.0226,  0.0831,  0.4806])
>>> torch.sqrt(a)
tensor([    nan,  1.0112,  0.2883,  0.6933])
"""""".format(**common_args))

add_docstr(torch.square,
           r""""""
square(input, *, out=None) -> Tensor
Returns a new tensor with the square of the elements of :attr:`input`.
"
491,">>> e, v = a_big.symeig(eigenvectors=True)
>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True
"""""",
)
add_docstr(
    torch.t,
    r""""""
t(input) -> Tensor
Expects :attr:`input` to be <= 2-D tensor and transposes dimensions 0
",">>> e, v = a_big.symeig(eigenvectors=True)
>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)
True
"""""")
add_docstr(torch.t,
           r""""""
t(input) -> Tensor
Expects :attr:`input` to be <= 2-D tensor and transposes dimensions 0
"
492,"the main diagonal. The main diagonal are the set of indices
:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where
:math:`d_{1}, d_{2}` are the dimensions of the matrix.
""""""
    + r""""""
Args:
{input}
diagonal (int, optional): the diagonal to consider
","the main diagonal. The main diagonal are the set of indices
:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where
:math:`d_{1}, d_{2}` are the dimensions of the matrix.
"""""" + r""""""
Args:
{input}
diagonal (int, optional): the diagonal to consider
"
493,"tensor([ 0.0552,  0.9730,  0.3973, -1.0780])
>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)
tensor([0.1000, 1.0000, 0.4000, 0.0000])
"""""",
)
add_docstr(
    torch.fake_quantize_per_channel_affine,
    r""""""
fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max) -> Tensor
Returns a new tensor with the data in :attr:`input` fake quantized per channel using :attr:`scale`,
","tensor([ 0.0552,  0.9730,  0.3973, -1.0780])
>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)
tensor([0.1000, 1.0000, 0.4000, 0.0000])
"""""")
add_docstr(torch.fake_quantize_per_channel_affine,
           r""""""
fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max) -> Tensor
Returns a new tensor with the data in :attr:`input` fake quantized per channel using :attr:`scale`,
"
494,">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var_mean(a, unbiased=False)
(tensor(0.1754), tensor(-0.8509))
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.zeros,
    r""""""
zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `0`, with the shape defined
",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var_mean(a, unbiased=False)
(tensor(0.1754), tensor(-0.8509))
"""""".format(**multi_dim_common))

add_docstr(torch.zeros,
           r""""""
zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `0`, with the shape defined
"
495,">>> torch.empty_like(a)
tensor([[0, 0, 0],
[0, 0, 0]], device='cuda:0', dtype=torch.int32)
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.empty_like,
    r""""""
empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns an uninitialized tensor with the same size as :attr:`input`.
",">>> torch.empty_like(a)
tensor([[0, 0, 0],
[0, 0, 0]], device='cuda:0', dtype=torch.int32)
"""""".format(**factory_common_args))

add_docstr(torch.empty_like,
           r""""""
empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns an uninitialized tensor with the same size as :attr:`input`.
"
496,">>> torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
[ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
"""""".format(
        **factory_like_common_args
    ),
)

add_docstr(
    torch.empty_strided,
    r""""""
empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -> Tensor
Returns a tensor filled with uninitialized data. The shape and strides of the tensor is
",">>> torch.empty((2,3), dtype=torch.int64)
tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
[ 7.5751e+18,  7.1428e+18,  7.5955e+18]])
"""""".format(**factory_like_common_args))

add_docstr(torch.empty_strided,
           r""""""
empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) -> Tensor
Returns a tensor filled with uninitialized data. The shape and strides of the tensor is
"
497,">>>                            [4, 5, 6],
>>>                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
"""""",
)
add_docstr(
    torch.combinations,
    r""""""
combinations(input, r=2, with_replacement=False) -> seq
Compute combinations of length :math:`r` of the given tensor. The behavior is similar to
",">>>                            [4, 5, 6],
>>>                            [7, 8, 9]]))
(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
"""""")
add_docstr(torch.combinations,
           r""""""
combinations(input, r=2, with_replacement=False) -> seq
Compute combinations of length :math:`r` of the given tensor. The behavior is similar to
"
498,"def _import_dotted_name(name):
    components = name.split(""."")
obj = __import__(components[0])
for component in components[1:]:
obj = getattr(obj, component)
","def _import_dotted_name(name):
    components = name.split('.')
obj = __import__(components[0])
for component in components[1:]:
obj = getattr(obj, component)
"
499,"# and the frame (which holds reference to all the object in its temporary scope)
# holding reference the traceback.

class KeyErrorMessage(str):
r""""""str subclass that returns itself in repr""""""

def __repr__(self):
return self
class ExceptionWrapper(object):
r""""""Wraps an exception plus traceback to communicate across threads""""""

def __init__(self, exc_info=None, where=""in background""):
# It is important that we don't store exc_info, see
# NOTE [ Python Traceback Reference Cycle Problem ]
","# and the frame (which holds reference to all the object in its temporary scope)
# holding reference the traceback.
class KeyErrorMessage(str):
r""""""str subclass that returns itself in repr""""""
def __repr__(self):
return self
class ExceptionWrapper(object):
r""""""Wraps an exception plus traceback to communicate across threads""""""
def __init__(self, exc_info=None, where=""in background""):
# It is important that we don't store exc_info, see
# NOTE [ Python Traceback Reference Cycle Problem ]
"
500,">>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])
>>> torch.fft.ifft(t)
tensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])
"""""".format(
        **common_args
    ),
)

fft2 = _add_docstr(
    _fft.fft_fft2,
    r""""""
fft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor
Computes the 2 dimensional discrete Fourier transform of :attr:`input`.
",">>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])
>>> torch.fft.ifft(t)
tensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])
"""""".format(**common_args))

fft2 = _add_docstr(_fft.fft_fft2, r""""""
fft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor
Computes the 2 dimensional discrete Fourier transform of :attr:`input`.
"
501,"torch.Size([10, 9])
>>> torch.testing.assert_close(roundtrip, t, check_stride=False)
"""""".format(
        **common_args
    ),
)
rfftn = _add_docstr(
    _fft.fft_rfftn,
    r""""""
rfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor
Computes the N-dimensional discrete Fourier transform of real :attr:`input`.
","torch.Size([10, 9])
>>> torch.testing.assert_close(roundtrip, t, check_stride=False)
"""""".format(**common_args))
rfftn = _add_docstr(_fft.fft_rfftn, r""""""
rfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor
Computes the N-dimensional discrete Fourier transform of real :attr:`input`.
"
502,"pad = int(n_fft // 2)
input = F.pad(input.view(extended_shape), [pad, pad], pad_mode)
input = input.view(input.shape[-signal_dim:])
    return _VF.stft(
        input,
        n_fft,
        hop_length,
        win_length,
        window,  # type: ignore[attr-defined]
        normalized,
        onesided,
        return_complex,
    )


def istft(
    input: Tensor,
    n_fft: int,
    hop_length: Optional[int] = None,
    win_length: Optional[int] = None,
    window: Optional[Tensor] = None,
    center: bool = True,
    normalized: bool = False,
    onesided: Optional[bool] = None,
    length: Optional[int] = None,
    return_complex: bool = False,
) -> Tensor:
r""""""Inverse short time Fourier Transform. This is expected to be the inverse of :func:`~torch.stft`.
It has the same parameters (+ additional optional parameter of :attr:`length`) and it should return the
least squares estimation of the original signal. The algorithm will check using the NOLA condition (
","pad = int(n_fft // 2)
input = F.pad(input.view(extended_shape), [pad, pad], pad_mode)
input = input.view(input.shape[-signal_dim:])
    return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
                    normalized, onesided, return_complex)

def istft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,
          win_length: Optional[int] = None, window: Optional[Tensor] = None,
          center: bool = True, normalized: bool = False,
          onesided: Optional[bool] = None, length: Optional[int] = None,
          return_complex: bool = False) -> Tensor:
r""""""Inverse short time Fourier Transform. This is expected to be the inverse of :func:`~torch.stft`.
It has the same parameters (+ additional optional parameter of :attr:`length`) and it should return the
least squares estimation of the original signal. The algorithm will check using the NOLA condition (
"
503,"return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2.0, compute_mode=""use_mm_for_euclid_dist_if_necessary""):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
","return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
"
504,".. warning:: GPU support is a beta feature, subject to changes.
""""""
    def __init__(
        self, *, devices: Optional[List[Union[int, str, torch.device]]] = None
    ):
r""""""
Create an empty unset ``Future``. If the future is intended to hold
values containing CUDA tensors, (a superset of) their CUDA devices must
",".. warning:: GPU support is a beta feature, subject to changes.
""""""
    def __init__(self, *, devices: Optional[List[Union[int, str, torch.device]]] = None):
r""""""
Create an empty unset ``Future``. If the future is intended to hold
values containing CUDA tensors, (a superset of) their CUDA devices must
"
505,"import shutil
import sys
import tempfile
import warnings
import zipfile
from urllib.parse import urlparse  # noqa: F401
from urllib.request import urlopen, Request
import torch
try:
    from tqdm.auto import (
        tqdm,
    )  # automatically select proper tqdm submodule if available
except ImportError:
try:
from tqdm import tqdm
except ImportError:
# fake tqdm if it's not installed
class tqdm(object):  # type: ignore[no-redef]
            def __init__(
                self,
                total=None,
                disable=False,
                unit=None,
                unit_scale=None,
                unit_divisor=None,
            ):
self.total = total
self.disable = disable
self.n = 0
","import shutil
import sys
import tempfile
import torch
import warnings
import zipfile
from urllib.request import urlopen, Request
from urllib.parse import urlparse  # noqa: F401
try:
    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if available
except ImportError:
try:
from tqdm import tqdm
except ImportError:
# fake tqdm if it's not installed
class tqdm(object):  # type: ignore[no-redef]

            def __init__(self, total=None, disable=False,
                         unit=None, unit_scale=None, unit_divisor=None):
self.total = total
self.disable = disable
self.n = 0
"
506,"# this causes confusion with path on both Linux and Windows.
# Backslash is not allowed in Github branch name so no need to
# to worry about it.
    normalized_br = branch.replace(""/"", ""_"")
# Github renames folder repo-v1.x.x to repo-1.x.x
# We don't know the repo name before downloading the zip file
# and inspect name from it.
# To check if cached repo exists, we need to normalize folder names.
    repo_dir = os.path.join(hub_dir, ""_"".join([repo_owner, repo_name, normalized_br]))
use_cache = (not force_reload) and os.path.exists(repo_dir)
if use_cache:
if verbose:
            sys.stderr.write(""Using cache found in {}\n"".format(repo_dir))
else:
# Validate the tag/branch is from the original repo instead of a forked repo
if not skip_validation:
_validate_not_a_forked_repo(repo_owner, repo_name, branch)
        cached_file = os.path.join(hub_dir, normalized_br + "".zip"")
_remove_if_exists(cached_file)
url = _git_archive_link(repo_owner, repo_name, branch)
        sys.stderr.write('Downloading: ""{}"" to {}\n'.format(url, cached_file))
download_url_to_file(url, cached_file, progress=False)
with zipfile.ZipFile(cached_file) as cached_zipfile:
","# this causes confusion with path on both Linux and Windows.
# Backslash is not allowed in Github branch name so no need to
# to worry about it.
    normalized_br = branch.replace('/', '_')
# Github renames folder repo-v1.x.x to repo-1.x.x
# We don't know the repo name before downloading the zip file
# and inspect name from it.
# To check if cached repo exists, we need to normalize folder names.
    repo_dir = os.path.join(hub_dir, '_'.join([repo_owner, repo_name, normalized_br]))
use_cache = (not force_reload) and os.path.exists(repo_dir)
if use_cache:
if verbose:
            sys.stderr.write('Using cache found in {}\n'.format(repo_dir))
else:
# Validate the tag/branch is from the original repo instead of a forked repo
if not skip_validation:
_validate_not_a_forked_repo(repo_owner, repo_name, branch)
        cached_file = os.path.join(hub_dir, normalized_br + '.zip')
_remove_if_exists(cached_file)
url = _git_archive_link(repo_owner, repo_name, branch)
        sys.stderr.write('Downloading: \""{}\"" to {}\n'.format(url, cached_file))
download_url_to_file(url, cached_file, progress=False)
with zipfile.ZipFile(cached_file) as cached_zipfile:
"
507,"def _check_module_exists(name):
import importlib.util

return importlib.util.find_spec(name) is not None
","def _check_module_exists(name):
import importlib.util
return importlib.util.find_spec(name) is not None
"
508,"try:
if hash_prefix is not None:
sha256 = hashlib.sha256()
        with tqdm(
            total=file_size,
            disable=not progress,
            unit=""B"",
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
while True:
buffer = u.read(8192)
if len(buffer) == 0:
","try:
if hash_prefix is not None:
sha256 = hashlib.sha256()
        with tqdm(total=file_size, disable=not progress,
                  unit='B', unit_scale=True, unit_divisor=1024) as pbar:
while True:
buffer = u.read(8192)
if len(buffer) == 0:
"
509,"# Note: This not only adds doc strings for functions in the linalg namespace, but
# also connects the torch.linalg Python namespace to the torch._C._linalg builtins.
cholesky = _add_docstr(
    _linalg.linalg_cholesky,
    r""""""
linalg.cholesky(A, *, upper=False, out=None) -> Tensor
Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.
","# Note: This not only adds doc strings for functions in the linalg namespace, but
# also connects the torch.linalg Python namespace to the torch._C._linalg builtins.
cholesky = _add_docstr(_linalg.linalg_cholesky, r""""""
linalg.cholesky(A, *, upper=False, out=None) -> Tensor
Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.
"
510,">>> torch.dist(L, torch.linalg.eig(A).eigenvalues)
tensor(2.4576e-07)
"""""",
)
eigh = _add_docstr(
    _linalg.linalg_eigh,
    r""""""
linalg.eigh(A, UPLO='L', *, out=None) -> (Tensor, Tensor)
Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.
",">>> torch.dist(L, torch.linalg.eig(A).eigenvalues)
tensor(2.4576e-07)
"""""")
eigh = _add_docstr(_linalg.linalg_eigh, r""""""
linalg.eigh(A, UPLO='L', *, out=None) -> (Tensor, Tensor)
Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.
"
511,".. _Representation of Orthogonal or Unitary Matrices:
https://www.netlib.org/lapack/lug/node128.html
"""""",
)
lstsq = _add_docstr(
    _linalg.linalg_lstsq,
    r""""""
torch.linalg.lstsq(A, B, rcond=None, *, driver=None) -> (Tensor, Tensor, Tensor, Tensor)
Computes a solution to the least squares problem of a system of linear equations.
",".. _Representation of Orthogonal or Unitary Matrices:
https://www.netlib.org/lapack/lug/node128.html
"""""")
lstsq = _add_docstr(_linalg.linalg_lstsq, r""""""
torch.linalg.lstsq(A, B, rcond=None, *, driver=None) -> (Tensor, Tensor, Tensor, Tensor)
Computes a solution to the least squares problem of a system of linear equations.
"
512,"tensor(1.6099e-06)
>>> torch.dist(Q.transpose(-2, -1) @ Q, torch.eye(4))
tensor(6.2158e-07)
"""""",
)
","tensor(1.6099e-06)
>>> torch.dist(Q.transpose(-2, -1) @ Q, torch.eye(4))
tensor(6.2158e-07)
"""""")
"
513,"import torch
from torch._C import (
    _has_torch_function,
    _has_torch_function_unary,
    _has_torch_function_variadic,
    _add_docstr,
)
__all__ = [
""get_ignored_functions"",
","import torch
from torch._C import (
    _has_torch_function, _has_torch_function_unary,
    _has_torch_function_variadic, _add_docstr)
__all__ = [
""get_ignored_functions"",
"
514,"torch.nn.functional.softsign: lambda input: -1,
torch.nn.functional.tanhshrink: lambda input: -1,
torch.nn.functional.threshold: lambda input, threshold, value, inplace=False: -1,
        torch.nn.functional.triplet_margin_loss: (
            lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=""mean"": -1
        ),
        torch.nn.functional.triplet_margin_with_distance_loss: (
            lambda anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction=""mean"": -1
        ),
torch.nn.functional.unfold: lambda input, kernel_size, dilation=1, padding=0, stride=1: -1,
torch.nonzero: lambda input, as_tuple=False: -1,
        torch.norm: lambda input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None: -1,
torch.linalg.norm: lambda input, ord=None, dim=None, keepdim=False, out=None, dtype=None: -1,
torch.linalg.vector_norm: lambda input, ord=2, dim=None, keepdim=False, out=None, dtype=None: -1,
        torch.linalg.matrix_norm: lambda input, ord=""fro"", dim=(
            -2,
            -1,
        ), keepdim=False, out=None, dtype=None: -1,
torch.norm_except_dim: lambda v, pow=2, dim=0: -1,
        torch.nuclear_norm: lambda input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None: -1,
torch.numel: lambda input: -1,
torch.orgqr: lambda input, tau: -1,
torch.ormqr: lambda input, input2, input3, left=True, transpose=False: -1,
","torch.nn.functional.softsign: lambda input: -1,
torch.nn.functional.tanhshrink: lambda input: -1,
torch.nn.functional.threshold: lambda input, threshold, value, inplace=False: -1,
        torch.nn.functional.triplet_margin_loss: (lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06,
                                                  swap=False, size_average=None, reduce=None, reduction='mean': -1),
        torch.nn.functional.triplet_margin_with_distance_loss: (lambda anchor, positive, negative, *,
                                                                distance_function=None, margin=1.0,
                                                                swap=False, reduction='mean': -1),
torch.nn.functional.unfold: lambda input, kernel_size, dilation=1, padding=0, stride=1: -1,
torch.nonzero: lambda input, as_tuple=False: -1,
        torch.norm: lambda input, p='fro', dim=None, keepdim=False, out=None, dtype=None: -1,
torch.linalg.norm: lambda input, ord=None, dim=None, keepdim=False, out=None, dtype=None: -1,
torch.linalg.vector_norm: lambda input, ord=2, dim=None, keepdim=False, out=None, dtype=None: -1,
        torch.linalg.matrix_norm: lambda input, ord='fro', dim=(-2, -1), keepdim=False, out=None, dtype=None: -1,
torch.norm_except_dim: lambda v, pow=2, dim=0: -1,
        torch.nuclear_norm: lambda input, p='fro', dim=None, keepdim=False, out=None, dtype=None: -1,
torch.numel: lambda input: -1,
torch.orgqr: lambda input, tau: -1,
torch.ormqr: lambda input, input2, input3, left=True, transpose=False: -1,
"
515,"if k.__name__.startswith(""bitwise_""):
# bitwise_<op> have dunder methods of the form __<op>__
# And so on.
            subname = k.__name__[len(""bitwise_"") :]
            names.extend(
                [""__"" + subname + ""__"", ""__i"" + subname + ""__"", ""__r"" + subname + ""__""]
            )
for name in names:
func = getattr(Tensor, name, None)
","if k.__name__.startswith(""bitwise_""):
# bitwise_<op> have dunder methods of the form __<op>__
# And so on.
            subname = k.__name__[len(""bitwise_""):]
            names.extend([
                ""__"" + subname + ""__"",
                ""__i"" + subname + ""__"",
                ""__r"" + subname + ""__""
            ])
for name in names:
func = getattr(Tensor, name, None)
"
516,"# We only collect arguments if they have a unique type, which ensures
# reasonable performance even with a long list of possibly overloaded
# arguments.
        if arg_type not in overloaded_types and hasattr(arg_type, ""__torch_function__""):
# Create lists explicitly for the first type (usually the only one
# done) to avoid setting up the iterator for overloaded_args.
if overloaded_types:
","# We only collect arguments if they have a unique type, which ensures
# reasonable performance even with a long list of possibly overloaded
# arguments.
        if (arg_type not in overloaded_types and hasattr(arg_type, '__torch_function__')):
# Create lists explicitly for the first type (usually the only one
# done) to avoid setting up the iterator for overloaded_args.
if overloaded_types:
"
517,"________
torch.is_tensor_like
Checks if something is a Tensor-like, including an exact ``Tensor``.
    """""",
)
has_torch_function_unary = _add_docstr(
","________
torch.is_tensor_like
Checks if something is a Tensor-like, including an exact ``Tensor``.
    """"""
)
has_torch_function_unary = _add_docstr(
"
518,"self._start_trace()
if self.record_steps:
            self.step_rec_fn = prof.record_function(
                ""ProfilerStep#"" + str(self.step_num)
            )
self.step_rec_fn.__enter__()
def export_chrome_trace(self, path: str):
","self._start_trace()
if self.record_steps:
            self.step_rec_fn = prof.record_function(""ProfilerStep#"" + str(self.step_num))
self.step_rec_fn.__enter__()
def export_chrome_trace(self, path: str):
"
519,"def _get_distributed_info(self):
import torch.distributed as dist

if not dist.is_available() or not dist.is_initialized():
return None
return {
""backend"": dist.get_backend(),
""rank"": dist.get_rank(),
            ""world_size"": dist.get_world_size(),
}
def _enter_actions(self):
if self.current_action == ProfilerAction.WARMUP:
self._start_warmup()
        elif self.current_action in [
            ProfilerAction.RECORD,
            ProfilerAction.RECORD_AND_SAVE,
        ]:
self._start_warmup()
self._start_trace()
","def _get_distributed_info(self):
import torch.distributed as dist
if not dist.is_available() or not dist.is_initialized():
return None
return {
""backend"": dist.get_backend(),
""rank"": dist.get_rank(),
            ""world_size"": dist.get_world_size()
}
def _enter_actions(self):
if self.current_action == ProfilerAction.WARMUP:
self._start_warmup()
        elif self.current_action in \
                [ProfilerAction.RECORD, ProfilerAction.RECORD_AND_SAVE]:
self._start_warmup()
self._start_trace()
"
520,"def __init__(self, dimension, scramble=False, seed=None):
if dimension > self.MAXDIM or dimension < 1:
            raise ValueError(
                ""Supported range of dimensionality ""
                f""for SobolEngine is [1, {self.MAXDIM}]""
            )
self.seed = seed
self.scramble = scramble
","def __init__(self, dimension, scramble=False, seed=None):
if dimension > self.MAXDIM or dimension < 1:
            raise ValueError(""Supported range of dimensionality ""
                             f""for SobolEngine is [1, {self.MAXDIM}]"")
self.seed = seed
self.scramble = scramble
"
521,"_package_registry.sort()
def check_module_version_greater_or_equal(
    module, req_version_tuple, error_if_malformed=True
):
    """"""
Check if a module's version satisfies requirements
Usually, a module's version string will be like 'x.y.z', which would be represented
","_package_registry.sort()
def check_module_version_greater_or_equal(module, req_version_tuple, error_if_malformed=True):
    '''
Check if a module's version satisfies requirements
Usually, a module's version string will be like 'x.y.z', which would be represented
"
522,"result = fn(storage, location)
if result is not None:
return result
    raise RuntimeError(
        ""don't know how to restore data location of ""
        + torch.typename(storage)
        + "" (tagged with ""
        + location
        + "")""
    )
def normalize_storage_type(storage_type):
","result = fn(storage, location)
if result is not None:
return result
    raise RuntimeError(""don't know how to restore data location of ""
                       + torch.typename(storage) + "" (tagged with ""
                       + location + "")"")
def normalize_storage_type(storage_type):
"
523,"Tensor = torch.Tensor
entr = _add_docstr(
    _special.special_entr,
    r""""""
entr(input, *, out=None) -> Tensor
Computes the entropy on :attr:`input` (as defined below), elementwise.
","Tensor = torch.Tensor
entr = _add_docstr(_special.special_entr,
                   r""""""
entr(input, *, out=None) -> Tensor
Computes the entropy on :attr:`input` (as defined below), elementwise.
"
524,">>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
"""""".format(
        **common_args
    ),
)

erfcx = _add_docstr(
    _special.special_erfcx,
    r""""""
erfcx(input, *, out=None) -> Tensor
Computes the scaled complementary error function for each element of :attr:`input`.
",">>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
"""""".format(**common_args))

erfcx = _add_docstr(_special.special_erfcx,
                    r""""""
erfcx(input, *, out=None) -> Tensor
Computes the scaled complementary error function for each element of :attr:`input`.
"
525,">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])
"""""".format(
        **common_args
    ),
)

xlog1py = _add_docstr(
    _special.special_xlog1py,
    r""""""
xlog1py(input, other, *, out=None) -> Tensor
Computes ``input * log1p(other)`` with the following cases.
",">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))
tensor([ 0.,  1.])
"""""".format(**common_args))

xlog1py = _add_docstr(_special.special_xlog1py,
                      r""""""
xlog1py(input, other, *, out=None) -> Tensor
Computes ``input * log1p(other)`` with the following cases.
"
526,".. math::
\text{out}_{i} = \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!) * (k+1)!}
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \frac{(\text{input}_{i})}{2} * \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!) * (k+1)!}
"""""" + r""""""
Args:
{input}
"
527,"Example::
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])
"""""".format(
        **common_args
    ),
)

ndtr = _add_docstr(
    _special.special_ndtr,
    r""""""
ndtr(input, *, out=None) -> Tensor
Computes the area under the standard Gaussian probability density function,
integrated from minus infinity to :attr:`input`, elementwise.
","Example::
>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])
"""""".format(**common_args))

ndtr = _add_docstr(_special.special_ndtr,
                   r""""""
ndtr(input, *, out=None) -> Tensor
Computes the area under the standard Gaussian probability density function,
integrated from minus infinity to :attr:`input`, elementwise.
"
528,".. math::
\text{ndtr}(x) = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t^2} dt
""""""
    + r""""""
Args:
{input}
",".. math::
\text{ndtr}(x) = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{x} e^{-\frac{1}{2}t^2} dt
"""""" + r""""""
Args:
{input}
"
529,"#   * (1)         -> Version(""1"")
#   * (1, 20)     -> Version(""1.20"")
#   * (1, 20, 1)  -> Version(""1.20.1"")
            return Version(""."".join((str(item) for item in inp)))
else:
raise InvalidVersion(inp)
","#   * (1)         -> Version(""1"")
#   * (1, 20)     -> Version(""1.20"")
#   * (1, 20, 1)  -> Version(""1.20.1"")
            return Version('.'.join((str(item) for item in inp)))
else:
raise InvalidVersion(inp)
"
530,"# version like 'parrot'
return super().__gt__(cmp)
def __eq__(self, cmp):
try:
return Version(self).__eq__(self._convert_to_version(cmp))
","# version like 'parrot'
return super().__gt__(cmp)

def __eq__(self, cmp):
try:
return Version(self).__eq__(self._convert_to_version(cmp))
"
531,"def is_shared(self) -> bool:
...
    def share_memory_(self) -> ""Storage"":
...
def size(self) -> int:
","def is_shared(self) -> bool:
...
    def share_memory_(self) -> 'Storage':
...
def size(self) -> int:
"
532,")
        code.append(""""""\
int64_t end_offset = offsets[rangeIndex + 1];
      int64_t length = end_offset - offsets[rangeIndex];"""""")
code.append(
""      for (""
"," ""        return false;\n""
 ""      }""
)
        code.append(
            """"""\
int64_t end_offset = offsets[rangeIndex + 1];
      int64_t length = end_offset - offsets[rangeIndex];""""""
        )
code.append(
""      for (""
 ""int64_t""
"
533,"import torchvision  # noqa: F401
except ImportError:
import warnings
warnings.warn('unable to load ""torchvision"" package')
RELEASE = os.environ.get('RELEASE', False)
import pytorch_sphinx_theme
","import torchvision  # noqa: F401
except ImportError:
import warnings

warnings.warn('unable to load ""torchvision"" package')
RELEASE = os.environ.get(""RELEASE"", False)
import pytorch_sphinx_theme
"
534,"# and can be moved outside of this function (and the setup(app) function
# can be deleted).
html_css_files = [
        'https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css'
]
# In Sphinx 1.8 it was renamed to `add_css_file`, 1.7 and prior it is
# `add_stylesheet` (deprecated in 1.8).
    add_css = getattr(app, 'add_css_file', app.add_stylesheet)
for css_file in html_css_files:
add_css(css_file)
# From PyTorch 1.5, we now use autogenerated files to document classes and
# functions. This breaks older references since
# https://pytorch.org/docs/stable/torch.html#torch.flip
","# and can be moved outside of this function (and the setup(app) function
# can be deleted).
html_css_files = [
        ""https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css""
]
# In Sphinx 1.8 it was renamed to `add_css_file`, 1.7 and prior it is
# `add_stylesheet` (deprecated in 1.8).
    add_css = getattr(app, ""add_css_file"", app.add_stylesheet)
for css_file in html_css_files:
add_css(css_file)

# From PyTorch 1.5, we now use autogenerated files to document classes and
# functions. This breaks older references since
# https://pytorch.org/docs/stable/torch.html#torch.flip
"
535,"# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    'ELU',
    'Hardshrink',
    'Hardtanh',
    'LeakyReLU',  # Perhaps we should add text explaining slight slope?
    'LogSigmoid',
    'PReLU',
    'ReLU',
    'ReLU6',
    'RReLU',
    'SELU',
    'SiLU',
    'Mish',
    'CELU',
    'GELU',
    'Sigmoid',
    'Softplus',
    'Softshrink',
    'Softsign',
    'Tanh',
    'Tanhshrink'
# 'Threshold'  Omit, pending cleanup. See PR5457
]
","# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    ""ELU"",
    ""Hardshrink"",
    ""Hardtanh"",
    ""LeakyReLU"",  # Perhaps we should add text explaining slight slope?
    ""LogSigmoid"",
    ""PReLU"",
    ""ReLU"",
    ""ReLU6"",
    ""RReLU"",
    ""SELU"",
    ""SiLU"",
    ""Mish"",
    ""CELU"",
    ""GELU"",
    ""Sigmoid"",
    ""Softplus"",
    ""Softshrink"",
    ""Softsign"",
    ""Tanh"",
    ""Tanhshrink""
# 'Threshold'  Omit, pending cleanup. See PR5457
]
"
536,"def get_submodule_folders():
git_modules_path = os.path.join(cwd, "".gitmodules"")
    default_modules_path = [os.path.join(third_party_path, name) for name in [
                            ""gloo"", ""cpuinfo"", ""tbb"", ""onnx"",
                            ""foxi"", ""QNNPACK"", ""fbgemm""
                            ]]
if not os.path.exists(git_modules_path):
return default_modules_path
with open(git_modules_path) as f:
        return [os.path.join(cwd, line.split(""="", 1)[1].strip()) for line in
                f.readlines() if line.strip().startswith(""path"")]
def check_submodules():
","def get_submodule_folders():
git_modules_path = os.path.join(cwd, "".gitmodules"")
    default_modules_path = [
        os.path.join(third_party_path, name)
        for name in [""gloo"", ""cpuinfo"", ""tbb"", ""onnx"", ""foxi"", ""QNNPACK"", ""fbgemm""]
    ]
if not os.path.exists(git_modules_path):
return default_modules_path
with open(git_modules_path) as f:
        return [
            os.path.join(cwd, line.split(""="", 1)[1].strip())
            for line in f.readlines()
            if line.strip().startswith(""path"")
        ]
def check_submodules():
"
537,"if RUN_BUILD_DEPS:
build_deps()
    extensions, cmdclass, packages, entry_points, extra_install_requires = configure_extension_build()
install_requires += extra_install_requires
","if RUN_BUILD_DEPS:
build_deps()
    (
        extensions,
        cmdclass,
        packages,
        entry_points,
        extra_install_requires,
    ) = configure_extension_build()
install_requires += extra_install_requires
"
538,"return _C._GLIBCXX_USE_CXX11_ABI
# Import the ops ""namespace""
from torch._ops import ops
from torch._classes import classes

# Import the quasi random sampler
from torch import quasirandom as quasirandom
# If you are seeing this, it means that this call site was not checked if
# the memory format could be preserved, and it was switched to old default
","return _C._GLIBCXX_USE_CXX11_ABI
# Import the quasi random sampler
from torch import quasirandom as quasirandom
from torch._classes import classes

# Import the ops ""namespace""
from torch._ops import ops
# If you are seeing this, it means that this call site was not checked if
# the memory format could be preserved, and it was switched to old default
"
539,"if torch.is_storage(obj):
serialized_storages.append(obj)
serialized_dtypes.append(obj.dtype)
            return ('storage', len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
if _serialized_reduces.get(id(obj)) is None:
","if torch.is_storage(obj):
serialized_storages.append(obj)
serialized_dtypes.append(obj.dtype)
            return (""storage"", len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
if _serialized_reduces.get(id(obj)) is None:
"
540,"def parseNestedExpr(expr, module) -> Tuple[Any, int]:
i = 0
        while i < len(expr) and expr[i] not in (',', '[', ']'):
i += 1
# Special case logic for the empty Tuple as a subscript (used
# in the type annotation `Tuple[()]`)
        if expr[:i] == '()':
return (), i
base = lookupInModule(expr[:i].strip(), module)
assert base is not None, f""Unresolvable type {expr[:i]}""
        if i == len(expr) or expr[i] != '[':
return base, i
        assert expr[i] == '['
parts = []
        while expr[i] != ']':
part_len = 0
i += 1
part, part_len = parseNestedExpr(expr[i:], module)
","def parseNestedExpr(expr, module) -> Tuple[Any, int]:
i = 0
        while i < len(expr) and expr[i] not in ("","", ""["", ""]""):
i += 1
# Special case logic for the empty Tuple as a subscript (used
# in the type annotation `Tuple[()]`)
        if expr[:i] == ""()"":
return (), i
base = lookupInModule(expr[:i].strip(), module)
assert base is not None, f""Unresolvable type {expr[:i]}""
        if i == len(expr) or expr[i] != ""["":
return base, i
        assert expr[i] == ""[""
parts = []
        while expr[i] != ""]"":
part_len = 0
i += 1
part, part_len = parseNestedExpr(expr[i:], module)
"
541,"return False
names = cls.__dict__
    fns = [getattr(cls, name) for name in names if inspect.isroutine(getattr(cls, name, None))]
    has_code = [hasattr(fn, '__code__') for fn in fns]
return all(has_code)
","return False
names = cls.__dict__
    fns = [
        getattr(cls, name)
        for name in names
        if inspect.isroutine(getattr(cls, name, None))
    ]
    has_code = [hasattr(fn, ""__code__"") for fn in fns]
return all(has_code)
"
542,"return lookup_in_class
def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):
""""""
Dispatches to either of 2 script functions based on a boolean argument.
In TorchScript, the boolean argument must be constant so that the correct
function to use can be determined at compile time.
""""""
def fn(*args, **kwargs):
dispatch_flag = False
if arg_name in kwargs:
","return lookup_in_class
def boolean_dispatch(
    arg_name, arg_index, default, if_true, if_false, module_name, func_name
):
""""""
Dispatches to either of 2 script functions based on a boolean argument.
In TorchScript, the boolean argument must be constant so that the correct
function to use can be determined at compile time.
""""""

def fn(*args, **kwargs):
dispatch_flag = False
if arg_name in kwargs:
"
543,"Used to denote the behavior of a function in TorchScript. See export() and
ignore() for details.
""""""
UNUSED = ""unused (ignored and replaced with raising of an exception)""
IGNORE = ""ignore (leave as a call to Python, cannot be torch.jit.save'd)""
EXPORT = ""export (compile this function even if nothing calls it)""
DEFAULT = ""default (compile if called from a exported function / forward)""
    COPY_TO_SCRIPT_WRAPPER = \
""if this method is not scripted, copy the python method onto the scripted model""
def export(fn):
","Used to denote the behavior of a function in TorchScript. See export() and
ignore() for details.
""""""

UNUSED = ""unused (ignored and replaced with raising of an exception)""
IGNORE = ""ignore (leave as a call to Python, cannot be torch.jit.save'd)""
EXPORT = ""export (compile this function even if nothing calls it)""
DEFAULT = ""default (compile if called from a exported function / forward)""
    COPY_TO_SCRIPT_WRAPPER = (
""if this method is not scripted, copy the python method onto the scripted model""
    )
def export(fn):
"
544,"def is_rref_instance(obj) -> bool:
return isinstance(obj, PyRRef)
else:
def is_rref_instance(obj) -> bool:
# If the RPC module doesn't exist then RRefs don't exist either.
return False
def is_final(ann) -> bool:
    return ann.__module__ in {'typing', 'typing_extensions'} and \
        (getattr(ann, '__origin__', None) is Final or isinstance(ann, type(Final)))
# allows BroadcastingList instance to be subscriptable
class BroadcastingListCls(object):
def __getitem__(self, types):
return
# mypy doesn't support parameters on types, so we have to explicitly type each
# list size
BroadcastingList1 = BroadcastingListCls()
","def is_rref_instance(obj) -> bool:
return isinstance(obj, PyRRef)

else:

def is_rref_instance(obj) -> bool:
# If the RPC module doesn't exist then RRefs don't exist either.
return False

def is_final(ann) -> bool:
    return ann.__module__ in {""typing"", ""typing_extensions""} and (
        getattr(ann, ""__origin__"", None) is Final or isinstance(ann, type(Final))
    )

# allows BroadcastingList instance to be subscriptable
class BroadcastingListCls(object):
def __getitem__(self, types):
return

# mypy doesn't support parameters on types, so we have to explicitly type each
# list size
BroadcastingList1 = BroadcastingListCls()
"
545,"def _isinstance(obj, target_type) -> bool:
if isinstance(target_type, collections.abc.Container):
if not isinstance(target_type, tuple):
            raise RuntimeError(""The second argument to ""
                               ""`torch.jit.isinstance` must be a type ""
                               ""or a tuple of types"")
for t_type in target_type:
if _isinstance(obj, t_type):
return True
","def _isinstance(obj, target_type) -> bool:
if isinstance(target_type, collections.abc.Container):
if not isinstance(target_type, tuple):
            raise RuntimeError(
                ""The second argument to ""
                ""`torch.jit.isinstance` must be a type ""
                ""or a tuple of types""
            )
for t_type in target_type:
if _isinstance(obj, t_type):
return True
"
546,"return res
if zero_power is None:
        zero_power = torch.eye(x.size(-1), x.size(-1), dtype=x.dtype, device=x.device) \
            .view(*([1] * len(list(x.shape[:-2]))), x.size(-1), x.size(-1))
return _polynomial_value(poly, x, zero_power, transition)
def _vector_polynomial_value(poly, x, zero_power=None):
""""""
Evaluates `poly(x)` for the (batched) vector input `x`.
","return res
if zero_power is None:
        zero_power = torch.eye(
            x.size(-1), x.size(-1), dtype=x.dtype, device=x.device
        ).view(*([1] * len(list(x.shape[:-2]))), x.size(-1), x.size(-1))
return _polynomial_value(poly, x, zero_power, transition)

def _vector_polynomial_value(poly, x, zero_power=None):
""""""
Evaluates `poly(x)` for the (batched) vector input `x`.
"
547,"dtype = _utils.get_floating_dtype(A)
device = A.device
if tol is None:
        feps = {torch.float32: 1.2e-07,
                torch.float64: 2.23e-16}[dtype]
tol = feps ** 0.5
m = A.shape[-1]
k = (1 if X is None else X.shape[-1]) if k is None else k
n = (k if n is None else n) if X is None else X.shape[-1]
    if (m < 3 * n):
raise ValueError(
            'LPBPCG algorithm is not applicable when the number of A rows (={})'
            ' is smaller than 3 x the number of requested eigenpairs (={})'
            .format(m, n))
    method = 'ortho' if method is None else method
iparams = {
        'm': m,
        'n': n,
        'k': k,
        'niter': 1000 if niter is None else niter,
}
fparams = {
        'tol': tol,
}
    bparams = {
        'largest': True if largest is None else largest
    }
    if method == 'ortho':
if ortho_iparams is not None:
iparams.update(ortho_iparams)
if ortho_fparams is not None:
fparams.update(ortho_fparams)
if ortho_bparams is not None:
bparams.update(ortho_bparams)
        iparams['ortho_i_max'] = iparams.get('ortho_i_max', 3)
        iparams['ortho_j_max'] = iparams.get('ortho_j_max', 3)
        fparams['ortho_tol'] = fparams.get('ortho_tol', tol)
        fparams['ortho_tol_drop'] = fparams.get('ortho_tol_drop', tol)
        fparams['ortho_tol_replace'] = fparams.get('ortho_tol_replace', tol)
        bparams['ortho_use_drop'] = bparams.get('ortho_use_drop', False)
if not torch.jit.is_scripting():
LOBPCG.call_tracker = LOBPCG_call_tracker  # type: ignore[assignment]
","dtype = _utils.get_floating_dtype(A)
device = A.device
if tol is None:
        feps = {torch.float32: 1.2e-07, torch.float64: 2.23e-16}[dtype]
tol = feps ** 0.5
m = A.shape[-1]
k = (1 if X is None else X.shape[-1]) if k is None else k
n = (k if n is None else n) if X is None else X.shape[-1]
    if m < 3 * n:
raise ValueError(
            ""LPBPCG algorithm is not applicable when the number of A rows (={})""
            "" is smaller than 3 x the number of requested eigenpairs (={})"".format(m, n)
        )
    method = ""ortho"" if method is None else method
iparams = {
        ""m"": m,
        ""n"": n,
        ""k"": k,
        ""niter"": 1000 if niter is None else niter,
}
fparams = {
        ""tol"": tol,
}
    bparams = {""largest"": True if largest is None else largest}
    if method == ""ortho"":
if ortho_iparams is not None:
iparams.update(ortho_iparams)
if ortho_fparams is not None:
fparams.update(ortho_fparams)
if ortho_bparams is not None:
bparams.update(ortho_bparams)
        iparams[""ortho_i_max""] = iparams.get(""ortho_i_max"", 3)
        iparams[""ortho_j_max""] = iparams.get(""ortho_j_max"", 3)
        fparams[""ortho_tol""] = fparams.get(""ortho_tol"", tol)
        fparams[""ortho_tol_drop""] = fparams.get(""ortho_tol_drop"", tol)
        fparams[""ortho_tol_replace""] = fparams.get(""ortho_tol_replace"", tol)
        bparams[""ortho_use_drop""] = bparams.get(""ortho_use_drop"", False)
if not torch.jit.is_scripting():
LOBPCG.call_tracker = LOBPCG_call_tracker  # type: ignore[assignment]
"
548,"class LOBPCG(object):
    """"""Worker class of LOBPCG methods.
    """"""

    def __init__(self,
                 A,        # type: Optional[Tensor]
                 B,        # type: Optional[Tensor]
                 X,        # type: Tensor
                 iK,       # type: Optional[Tensor]
                 iparams,  # type: Dict[str, int]
                 fparams,  # type: Dict[str, float]
                 bparams,  # type: Dict[str, bool]
                 method,   # type: str
                 tracker   # type: None
                 ):
# type: (...) -> None
# constant parameters
","class LOBPCG(object):
    """"""Worker class of LOBPCG methods.""""""

    def __init__(
        self,
        A,  # type: Optional[Tensor]
        B,  # type: Optional[Tensor]
        X,  # type: Tensor
        iK,  # type: Optional[Tensor]
        iparams,  # type: Dict[str, int]
        fparams,  # type: Dict[str, float]
        bparams,  # type: Dict[str, bool]
        method,  # type: str
        tracker,  # type: None
    ):
# type: (...) -> None
# constant parameters
"
549,"d_row = SBS.diagonal(0, -2, -1) ** -0.5
d_col = d_row.reshape(d_row.shape[0], 1)
# TODO: Consider reordering the operations to work with lower-triangular matrices
        R = torch.linalg.cholesky(((SBS * d_row) * d_col).transpose(-2, -1).conj()).transpose(-2, -1).conj()
# TODO: could use LAPACK ?trtri as R is upper-triangular
Rinv = torch.inverse(R)
return Rinv * d_col
    def _get_svqb(self,
                  U,     # Tensor
                  drop,  # bool
                  tau    # float
                  ):
# type: (Tensor, bool, float) -> Tensor
""""""Return B-orthonormal U.
","d_row = SBS.diagonal(0, -2, -1) ** -0.5
d_col = d_row.reshape(d_row.shape[0], 1)
# TODO: Consider reordering the operations to work with lower-triangular matrices
        R = (
            torch.linalg.cholesky(((SBS * d_row) * d_col).transpose(-2, -1).conj())
            .transpose(-2, -1)
            .conj()
        )
# TODO: could use LAPACK ?trtri as R is upper-triangular
Rinv = torch.inverse(R)
return Rinv * d_col
    def _get_svqb(self, U, drop, tau):  # Tensor  # bool  # float
# type: (Tensor, bool, float) -> Tensor
""""""Return B-orthonormal U.
"
550,"if _utils.is_sparse(A):
if len(A.shape) != 2:
            raise ValueError('pca_lowrank input is expected to be 2-dimensional tensor')
c = torch.sparse.sum(A, dim=(-2,)) / m
# reshape c
column_indices = c.indices()[0]
        indices = torch.zeros(2, len(column_indices),
                              dtype=column_indices.dtype,
                              device=column_indices.device)
indices[0] = column_indices
C_t = torch.sparse_coo_tensor(
            indices, c.values(), (n, 1), dtype=dtype, device=A.device)
ones_m1_t = torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
M = _utils.transpose(torch.sparse.mm(C_t, ones_m1_t))
","if _utils.is_sparse(A):
if len(A.shape) != 2:
            raise ValueError(""pca_lowrank input is expected to be 2-dimensional tensor"")
c = torch.sparse.sum(A, dim=(-2,)) / m
# reshape c
column_indices = c.indices()[0]
        indices = torch.zeros(
            2,
            len(column_indices),
            dtype=column_indices.dtype,
            device=column_indices.device,
        )
indices[0] = column_indices
C_t = torch.sparse_coo_tensor(
            indices, c.values(), (n, 1), dtype=dtype, device=A.device
        )
ones_m1_t = torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
M = _utils.transpose(torch.sparse.mm(C_t, ones_m1_t))
"
551,"and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""
def __init__(self, name):
        super(_OpNamespace, self).__init__('torch.ops.' + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == '__file__':
            return 'torch.ops'
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = '{}::{}'.format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
","and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""

def __init__(self, name):
        super(_OpNamespace, self).__init__(""torch.ops."" + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == ""__file__"":
            return ""torch.ops""
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = ""{}::{}"".format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
"
552,"op.__module__ = self.__module__ + ""."" + self.name
return op
class _Ops(types.ModuleType):
    __file__ = '_ops.py'
def __init__(self):
        super(_Ops, self).__init__('torch.ops')
self.loaded_libraries = set()
def __getattr__(self, name):
","op.__module__ = self.__module__ + ""."" + self.name
return op

class _Ops(types.ModuleType):
    __file__ = ""_ops.py""
def __init__(self):
        super(_Ops, self).__init__(""torch.ops"")
self.loaded_libraries = set()
def __getattr__(self, name):
"
553,"def __prepare__(cls, name, this_bases):
return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})
","def __prepare__(cls, name, this_bases):
return meta.__prepare__(name, bases)
    return type.__new__(metaclass, ""temporary_class"", (), {})
"
554,"raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)

def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.rename_, (self,), self, *names, **rename_map)
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
","raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (
            isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))
        ):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)
def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.rename_, (self,), self, *names, **rename_map
            )
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
"
555,"tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.]], dtype=torch.float64)
"""""".format(**new_common_args))
add_docstr_all('abs',
               r""""""
abs() -> Tensor
See :func:`torch.abs`
"""""")
add_docstr_all('abs_',
               r""""""
abs_() -> Tensor
In-place version of :meth:`~Tensor.abs`
"""""")
add_docstr_all('absolute',
               r""""""
absolute() -> Tensor
Alias for :func:`abs`
"""""")
add_docstr_all('absolute_',
               r""""""
absolute_() -> Tensor
In-place version of :meth:`~Tensor.absolute`
Alias for :func:`abs_`
"""""")
add_docstr_all('acos',
               r""""""
acos() -> Tensor
See :func:`torch.acos`
"""""")
add_docstr_all('acos_',
               r""""""
acos_() -> Tensor
In-place version of :meth:`~Tensor.acos`
"""""")
add_docstr_all('arccos', r""""""
arccos() -> Tensor
See :func:`torch.arccos`
"""""")
add_docstr_all('arccos_', r""""""
arccos_() -> Tensor
In-place version of :meth:`~Tensor.arccos`
"""""")
add_docstr_all('acosh',
               r""""""
acosh() -> Tensor
See :func:`torch.acosh`
"""""")
add_docstr_all('acosh_',
               r""""""
acosh_() -> Tensor
In-place version of :meth:`~Tensor.acosh`
"""""")
add_docstr_all('arccosh', r""""""
acosh() -> Tensor
See :func:`torch.arccosh`
"""""")
add_docstr_all('arccosh_', r""""""
acosh_() -> Tensor
In-place version of :meth:`~Tensor.arccosh`
"""""")
add_docstr_all('add',
               r""""""
add(other, *, alpha=1) -> Tensor
Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`
","tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.]], dtype=torch.float64)
"""""".format(
        **new_common_args
    ),
)
add_docstr_all(
    ""abs"",
    r""""""
abs() -> Tensor
See :func:`torch.abs`
"""""",
)
add_docstr_all(
    ""abs_"",
    r""""""
abs_() -> Tensor
In-place version of :meth:`~Tensor.abs`
"""""",
)
add_docstr_all(
    ""absolute"",
    r""""""
absolute() -> Tensor
Alias for :func:`abs`
"""""",
)
add_docstr_all(
    ""absolute_"",
    r""""""
absolute_() -> Tensor
In-place version of :meth:`~Tensor.absolute`
Alias for :func:`abs_`
"""""",
)
add_docstr_all(
    ""acos"",
    r""""""
acos() -> Tensor
See :func:`torch.acos`
"""""",
)
add_docstr_all(
    ""acos_"",
    r""""""
acos_() -> Tensor
In-place version of :meth:`~Tensor.acos`
"""""",
)
add_docstr_all(
    ""arccos"",
    r""""""
arccos() -> Tensor
See :func:`torch.arccos`
"""""",
)
add_docstr_all(
    ""arccos_"",
    r""""""
arccos_() -> Tensor
In-place version of :meth:`~Tensor.arccos`
"""""",
)
add_docstr_all(
    ""acosh"",
    r""""""
acosh() -> Tensor
See :func:`torch.acosh`
"""""",
)
add_docstr_all(
    ""acosh_"",
    r""""""
acosh_() -> Tensor
In-place version of :meth:`~Tensor.acosh`
"""""",
)
add_docstr_all(
    ""arccosh"",
    r""""""
acosh() -> Tensor
See :func:`torch.arccosh`
"""""",
)
add_docstr_all(
    ""arccosh_"",
    r""""""
acosh_() -> Tensor
In-place version of :meth:`~Tensor.arccosh`
"""""",
)
add_docstr_all(
    ""add"",
    r""""""
add(other, *, alpha=1) -> Tensor
Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`
"
556,"Throws an error if :attr:`self` is not a sparse tensor.
See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.
"""""")
add_docstr_all('diag',
               r""""""
diag(diagonal=0) -> Tensor
See :func:`torch.diag`
"""""")
add_docstr_all('diag_embed',
               r""""""
diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor
See :func:`torch.diag_embed`
"""""")
add_docstr_all('diagflat',
               r""""""
diagflat(offset=0) -> Tensor
See :func:`torch.diagflat`
"""""")
add_docstr_all('diagonal',
               r""""""
diagonal(offset=0, dim1=0, dim2=1) -> Tensor
See :func:`torch.diagonal`
"""""")
add_docstr_all('fill_diagonal_',
               r""""""
fill_diagonal_(fill_value, wrap=False) -> Tensor
Fill the main diagonal of a tensor that has at least 2-dimensions.
","Throws an error if :attr:`self` is not a sparse tensor.
See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.
"""""",
)
add_docstr_all(
    ""diag"",
    r""""""
diag(diagonal=0) -> Tensor
See :func:`torch.diag`
"""""",
)
add_docstr_all(
    ""diag_embed"",
    r""""""
diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor
See :func:`torch.diag_embed`
"""""",
)
add_docstr_all(
    ""diagflat"",
    r""""""
diagflat(offset=0) -> Tensor
See :func:`torch.diagflat`
"""""",
)
add_docstr_all(
    ""diagonal"",
    r""""""
diagonal(offset=0, dim1=0, dim2=1) -> Tensor
See :func:`torch.diagonal`
"""""",
)
add_docstr_all(
    ""fill_diagonal_"",
    r""""""
fill_diagonal_(fill_value, wrap=False) -> Tensor
Fill the main diagonal of a tensor that has at least 2-dimensions.
"
557,"Throws an error if :attr:`self` is not a sparse COO tensor.
See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.
"""""")
add_docstr_all('is_contiguous',
               r""""""
is_contiguous(memory_format=torch.contiguous_format) -> bool
Returns True if :attr:`self` tensor is contiguous in memory in the order specified
","Throws an error if :attr:`self` is not a sparse COO tensor.
See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.
"""""",
)
add_docstr_all(
    ""is_contiguous"",
    r""""""
is_contiguous(memory_format=torch.contiguous_format) -> bool
Returns True if :attr:`self` tensor is contiguous in memory in the order specified
"
558,"types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every
value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`
will be uniform in ``[0, 2^53]``.
"""""")
add_docstr_all('rad2deg',
               r""""""
rad2deg() -> Tensor
See :func:`torch.rad2deg`
"""""")
add_docstr_all('rad2deg_',
               r""""""
rad2deg_() -> Tensor
In-place version of :meth:`~Tensor.rad2deg`
"""""")
add_docstr_all('deg2rad',
               r""""""
deg2rad() -> Tensor
See :func:`torch.deg2rad`
"""""")
add_docstr_all('deg2rad_',
               r""""""
deg2rad_() -> Tensor
In-place version of :meth:`~Tensor.deg2rad`
"""""")
add_docstr_all('ravel',
               r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""")
add_docstr_all('reciprocal',
               r""""""
reciprocal() -> Tensor
See :func:`torch.reciprocal`
"""""")
add_docstr_all('reciprocal_',
               r""""""
reciprocal_() -> Tensor
In-place version of :meth:`~Tensor.reciprocal`
"""""")
add_docstr_all('record_stream',
               r""""""
record_stream(stream)
Ensures that the tensor memory is not reused for another tensor until all
","types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every
value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`
will be uniform in ``[0, 2^53]``.
"""""",
)
add_docstr_all(
    ""rad2deg"",
    r""""""
rad2deg() -> Tensor
See :func:`torch.rad2deg`
"""""",
)
add_docstr_all(
    ""rad2deg_"",
    r""""""
rad2deg_() -> Tensor
In-place version of :meth:`~Tensor.rad2deg`
"""""",
)
add_docstr_all(
    ""deg2rad"",
    r""""""
deg2rad() -> Tensor
See :func:`torch.deg2rad`
"""""",
)
add_docstr_all(
    ""deg2rad_"",
    r""""""
deg2rad_() -> Tensor
In-place version of :meth:`~Tensor.deg2rad`
"""""",
)
add_docstr_all(
    ""ravel"",
    r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""",
)
add_docstr_all(
    ""reciprocal"",
    r""""""
reciprocal() -> Tensor
See :func:`torch.reciprocal`
"""""",
)
add_docstr_all(
    ""reciprocal_"",
    r""""""
reciprocal_() -> Tensor
In-place version of :meth:`~Tensor.reciprocal`
"""""",
)
add_docstr_all(
    ""record_stream"",
    r""""""
record_stream(stream)
Ensures that the tensor memory is not reused for another tensor until all
"
559,"Args:
shape (tuple of ints or int...): the desired shape
"""""")
add_docstr_all('reshape_as',
               r""""""
reshape_as(other) -> Tensor
Returns this tensor as the same shape as :attr:`other`.
","Args:
shape (tuple of ints or int...): the desired shape
"""""",
)
add_docstr_all(
    ""reshape_as"",
    r""""""
reshape_as(other) -> Tensor
Returns this tensor as the same shape as :attr:`other`.
"
560,">>> t.size(dim=1)
4
"""""")
add_docstr_all('solve',
               r""""""
solve(A) -> Tensor, Tensor
See :func:`torch.solve`
"""""")
add_docstr_all('sort',
               r""""""
sort(dim=-1, descending=False) -> (Tensor, LongTensor)
See :func:`torch.sort`
"""""")
add_docstr_all('msort',
               r""""""
msort() -> Tensor
See :func:`torch.msort`
"""""")
add_docstr_all('argsort',
               r""""""
argsort(dim=-1, descending=False) -> LongTensor
See :func:`torch.argsort`
"""""")
add_docstr_all('sparse_dim',
               r""""""
sparse_dim() -> int
Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
",">>> t.size(dim=1)
4
"""""",
)
add_docstr_all(
    ""solve"",
    r""""""
solve(A) -> Tensor, Tensor
See :func:`torch.solve`
"""""",
)
add_docstr_all(
    ""sort"",
    r""""""
sort(dim=-1, descending=False) -> (Tensor, LongTensor)
See :func:`torch.sort`
"""""",
)
add_docstr_all(
    ""msort"",
    r""""""
msort() -> Tensor
See :func:`torch.msort`
"""""",
)
add_docstr_all(
    ""argsort"",
    r""""""
argsort(dim=-1, descending=False) -> LongTensor
See :func:`torch.argsort`
"""""",
)
add_docstr_all(
    ""sparse_dim"",
    r""""""
sparse_dim() -> int
Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
"
561,"original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""")
add_docstr_all('sparse_resize_and_clear_',
               r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
","original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""",
)
add_docstr_all(
    ""sparse_resize_and_clear_"",
    r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
"
562,"size (torch.Size): the desired size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""")
add_docstr_all('sqrt',
               r""""""
sqrt() -> Tensor
See :func:`torch.sqrt`
"""""")
add_docstr_all('sqrt_',
               r""""""
sqrt_() -> Tensor
In-place version of :meth:`~Tensor.sqrt`
"""""")
add_docstr_all('square',
               r""""""
square() -> Tensor
See :func:`torch.square`
"""""")
add_docstr_all('square_',
               r""""""
square_() -> Tensor
In-place version of :meth:`~Tensor.square`
"""""")
add_docstr_all('squeeze',
               r""""""
squeeze(dim=None) -> Tensor
See :func:`torch.squeeze`
"""""")
add_docstr_all('squeeze_',
               r""""""
squeeze_(dim=None) -> Tensor
In-place version of :meth:`~Tensor.squeeze`
"""""")
add_docstr_all('std',
               r""""""
std(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.std`
","size (torch.Size): the desired size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""",
)
add_docstr_all(
    ""sqrt"",
    r""""""
sqrt() -> Tensor
See :func:`torch.sqrt`
"""""",
)
add_docstr_all(
    ""sqrt_"",
    r""""""
sqrt_() -> Tensor
In-place version of :meth:`~Tensor.sqrt`
"""""",
)
add_docstr_all(
    ""square"",
    r""""""
square() -> Tensor
See :func:`torch.square`
"""""",
)
add_docstr_all(
    ""square_"",
    r""""""
square_() -> Tensor
In-place version of :meth:`~Tensor.square`
"""""",
)
add_docstr_all(
    ""squeeze"",
    r""""""
squeeze(dim=None) -> Tensor
See :func:`torch.squeeze`
"""""",
)
add_docstr_all(
    ""squeeze_"",
    r""""""
squeeze_(dim=None) -> Tensor
In-place version of :meth:`~Tensor.squeeze`
"""""",
)
add_docstr_all(
    ""std"",
    r""""""
std(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.std`
"
563,"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""")
add_docstr_all('view_as',
               r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
","Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""",
)
add_docstr_all(
    ""view_as"",
    r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
"
564,"Args:
other (:class:`torch.Tensor`): The result tensor has the same size
as :attr:`other`.
"""""")
add_docstr_all('expand',
               r""""""
expand(*sizes) -> Tensor
Returns a new view of the :attr:`self` tensor with singleton dimensions expanded
","Args:
other (:class:`torch.Tensor`): The result tensor has the same size
as :attr:`other`.
"""""",
)
add_docstr_all(
    ""expand"",
    r""""""
expand(*sizes) -> Tensor
Returns a new view of the :attr:`self` tensor with singleton dimensions expanded
"
565,"return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = ([_val_formatter(val) for val in self[:PRINT_OPTS.edgeitems].tolist()] +
                [' ...'] +
                [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems:].tolist()])
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [data[i:i + elements_per_line] for i in range(0, len(data), elements_per_line)]
    lines = [', '.join(line) for line in data_lines]
    return '[' + (',' + '\n' + ' ' * (indent + 1)).join(lines) + ']'
# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
","return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = (
            [_val_formatter(val) for val in self[: PRINT_OPTS.edgeitems].tolist()]
            + ["" ...""]
            + [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems :].tolist()]
        )
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [
        data[i : i + elements_per_line] for i in range(0, len(data), elements_per_line)
    ]
    lines = ["", "".join(line) for line in data_lines]
    return ""["" + ("","" + ""\n"" + "" "" * (indent + 1)).join(lines) + ""]""

# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
"
566,"regx = re.compile(r""\n\s{4}(?!\s)"")
kwargs = [section.strip() for section in regx.split(desc)]
kwargs = [section for section in kwargs if len(section) > 0]
    return {desc.split(' ')[0]: desc for desc in kwargs}
def merge_dicts(*dicts):
return {x: d[x] for d in dicts for x in d}
common_args = parse_kwargs(""""""
input (Tensor): the input tensor.
generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
out (Tensor, optional): the output tensor.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned tensor. Default: ``torch.preserve_format``.
"""""")
reduceops_common_args = merge_dicts(common_args, parse_kwargs(""""""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
If specified, the input tensor is casted to :attr:`dtype` before the operation
is performed. This is useful for preventing data type overflows. Default: None.
keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
""""""))

multi_dim_common = merge_dicts(reduceops_common_args, parse_kwargs(""""""
dim (int or tuple of ints): the dimension or dimensions to reduce.
""""""), {'keepdim_details': """"""
If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
output tensor having 1 (or ``len(dim)``) fewer dimension(s).
""""""})

single_dim_common = merge_dicts(reduceops_common_args, parse_kwargs(""""""
dim (int): the dimension to reduce.
""""""), {'keepdim_details': """"""If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensor having 1 fewer dimension than :attr:`input`.""""""})

factory_common_args = merge_dicts(common_args, parse_kwargs(""""""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
","regx = re.compile(r""\n\s{4}(?!\s)"")
kwargs = [section.strip() for section in regx.split(desc)]
kwargs = [section for section in kwargs if len(section) > 0]
    return {desc.split("" "")[0]: desc for desc in kwargs}
def merge_dicts(*dicts):
return {x: d[x] for d in dicts for x in d}
common_args = parse_kwargs(
    """"""
input (Tensor): the input tensor.
generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
out (Tensor, optional): the output tensor.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned tensor. Default: ``torch.preserve_format``.
""""""
)
reduceops_common_args = merge_dicts(
    common_args,
    parse_kwargs(
        """"""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
If specified, the input tensor is casted to :attr:`dtype` before the operation
is performed. This is useful for preventing data type overflows. Default: None.
keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
""""""
    ),
)

multi_dim_common = merge_dicts(
    reduceops_common_args,
    parse_kwargs(
        """"""
dim (int or tuple of ints): the dimension or dimensions to reduce.
""""""
    ),
    {
        ""keepdim_details"": """"""
If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
output tensor having 1 (or ``len(dim)``) fewer dimension(s).
""""""
    },
)

single_dim_common = merge_dicts(
    reduceops_common_args,
    parse_kwargs(
        """"""
dim (int): the dimension to reduce.
""""""
    ),
    {
        ""keepdim_details"": """"""If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensor having 1 fewer dimension than :attr:`input`.""""""
    },
)

factory_common_args = merge_dicts(
    common_args,
    parse_kwargs(
        """"""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
"
567,".. math::
\text{out}_{i} = \cosh^{-1}(\text{input}_{i})
"""""" + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \cosh^{-1}(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
"
568,">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.sspaddmm,
           r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
",">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.sspaddmm,
    r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
"
569,"False
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True
"""""")
add_docstr(torch.all,
           r""""""
all(input) -> Tensor
Tests if all elements in :attr:`input` evaluate to `True`.
","False
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True
"""""",
)
add_docstr(
    torch.all,
    r""""""
all(input) -> Tensor
Tests if all elements in :attr:`input` evaluate to `True`.
"
570,"tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
>>> torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
"""""".format(**common_args))

add_docstr(torch.arctan, r""""""
arctan(input, *, out=None) -> Tensor
Alias for :func:`torch.atan`.
"""""")
add_docstr(torch.atan2,
           r""""""
atan2(input, other, *, out=None) -> Tensor
Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}`
","tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
>>> torch.atan(a)
tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.arctan,
    r""""""
arctan(input, *, out=None) -> Tensor
Alias for :func:`torch.atan`.
"""""",
)
add_docstr(
    torch.atan2,
    r""""""
atan2(input, other, *, out=None) -> Tensor
Element-wise arctangent of :math:`\text{{input}}_{{i}} / \text{{other}}_{{i}}`
"
571,"[3, 6, 7, 6, 7],
[4, 8, 9, 8, 9]])
"""""".format(**common_args))
add_docstr(torch.complex,
           r""""""
complex(real, imag, *, out=None) -> Tensor
Constructs a complex tensor with its real part equal to :attr:`real` and its
","[3, 6, 7, 6, 7],
[4, 8, 9, 8, 9]])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.complex,
    r""""""
complex(real, imag, *, out=None) -> Tensor
Constructs a complex tensor with its real part equal to :attr:`real` and its
"
572,">>> torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])
"""""".format(**common_args))

add_docstr(torch.conj,
           r""""""
conj(input) -> Tensor
Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype,
",">>> torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.conj,
    r""""""
conj(input) -> Tensor
Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype,
"
573,"tensor(inf)
>>> torch.dist(x, y, 1)
tensor(2.6537)
"""""".format(**common_args))

add_docstr(torch.div, r""""""
div(input, other, *, rounding_mode=None, out=None) -> Tensor
Divides each element of the input ``input`` by the corresponding element of
","tensor(inf)
>>> torch.dist(x, y, 1)
tensor(2.6537)
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.div,
    r""""""
div(input, other, *, rounding_mode=None, out=None) -> Tensor
Divides each element of the input ``input`` by the corresponding element of
"
574,"tensor([2.0, 1.0])
>>> torch.floor_divide(a, 1.4)
tensor([2.0, 2.0])
"""""".format(**common_args))

add_docstr(torch.fmod,
           r""""""
fmod(input, other, *, out=None) -> Tensor
Applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
","tensor([2.0, 1.0])
>>> torch.floor_divide(a, 1.4)
tensor([2.0, 2.0])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.fmod,
    r""""""
fmod(input, other, *, out=None) -> Tensor
Applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
"
575,"[  2.,   4.,   6.],
[  3.,   6.,   9.],
[  4.,   8.,  12.]])
"""""")
add_docstr(torch.ger,
           r""""""
ger(input, vec2, *, out=None) -> Tensor
Alias of :func:`torch.outer`.
","[  2.,   4.,   6.],
[  3.,   6.,   9.],
[  4.,   8.,  12.]])
"""""",
)
add_docstr(
    torch.ger,
    r""""""
ger(input, vec2, *, out=None) -> Tensor
Alias of :func:`torch.outer`.
"
576,">>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32
"""""")
add_docstr(torch.get_num_threads,
           r""""""
get_num_threads() -> int
Returns the number of threads used for parallelizing CPU operations
"""""")
add_docstr(torch.get_num_interop_threads,
           r""""""
get_num_interop_threads() -> int
Returns the number of threads used for inter-op parallelism on CPU
(e.g. in JIT interpreter)
"""""")
add_docstr(torch.gt, r""""""
gt(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} > \text{other}` element-wise.
"""""" + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
",">>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor
torch.float32
"""""",
)
add_docstr(
    torch.get_num_threads,
    r""""""
get_num_threads() -> int
Returns the number of threads used for parallelizing CPU operations
"""""",
)
add_docstr(
    torch.get_num_interop_threads,
    r""""""
get_num_interop_threads() -> int
Returns the number of threads used for inter-op parallelism on CPU
(e.g. in JIT interpreter)
"""""",
)
add_docstr(
    torch.gt,
    r""""""
gt(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} > \text{other}` element-wise.
""""""
    + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
"
577,"(tensor([ 0.,  5.,  2.,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.]))
>>> torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.]), density=True)
(tensor([ 0.,  0.9524,  0.3810,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.]))
"""""".format(**common_args))

add_docstr(torch.hypot,
           r""""""
hypot(input, other, *, out=None) -> Tensor
Given the legs of a right triangle, return its hypotenuse.
","(tensor([ 0.,  5.,  2.,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.]))
>>> torch.histogram(torch.tensor([1., 2, 1]), bins=4, range=(0., 3.), weight=torch.tensor([1., 2., 4.]), density=True)
(tensor([ 0.,  0.9524,  0.3810,  0.]), tensor([0., 0.75, 1.5, 2.25, 3.]))
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.hypot,
    r""""""
hypot(input, other, *, out=None) -> Tensor
Given the legs of a right triangle, return its hypotenuse.
"
578,"The shapes of ``input`` and ``other`` must be
:ref:`broadcastable <broadcasting-semantics>`.
"""""" + r""""""
Args:
input (Tensor): the first input tensor
other (Tensor): the second input tensor
","The shapes of ``input`` and ``other`` must be
:ref:`broadcastable <broadcasting-semantics>`.
""""""
    + r""""""
Args:
input (Tensor): the first input tensor
other (Tensor): the second input tensor
"
579,"Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with no values is ambiguous
"""""".format(**common_args))

add_docstr(torch.kron,
           r""""""
kron(input, other, *, out=None) -> Tensor
Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`.
","Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with no values is ambiguous
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.kron,
    r""""""
kron(input, other, *, out=None) -> Tensor
Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`.
"
580,"[  1.0000,   2.0000],
[ 10.9635,   4.8501],
[  8.9332,   5.2418]])
"""""")
add_docstr(torch.lt, r""""""
lt(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} < \text{other}` element-wise.
"""""" + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
","[  1.0000,   2.0000],
[ 10.9635,   4.8501],
[  8.9332,   5.2418]])
"""""",
)
add_docstr(
    torch.lt,
    r""""""
lt(input, other, *, out=None) -> Tensor
Computes :math:`\text{input} < \text{other}` element-wise.
""""""
    + r""""""
The second argument can be a number or a tensor whose shape is
:ref:`broadcastable <broadcasting-semantics>` with the first argument.
"
581,"[ 1.9700,  1.1106, -1.0318, -1.0816]])
>>> torch.amax(a, 1)
tensor([1.4878, 2.0992, 0.0164, 1.9700])
"""""".format(**multi_dim_common))

add_docstr(torch.argmax,
           r""""""
argmax(input) -> LongTensor
Returns the indices of the maximum value of all elements in the :attr:`input` tensor.
","[ 1.9700,  1.1106, -1.0318, -1.0816]])
>>> torch.amax(a, 1)
tensor([1.4878, 2.0992, 0.0164, 1.9700])
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.argmax,
    r""""""
argmax(input) -> LongTensor
Returns the indices of the maximum value of all elements in the :attr:`input` tensor.
"
582,"tensor([1., 2.])
>>> t.nanquantile(0.5, dim=1)
tensor([   nan, 1.5000])
"""""".format(**single_dim_common))

add_docstr(torch.min,
           r""""""
min(input) -> Tensor
Returns the minimum value of all elements in the :attr:`input` tensor.
","tensor([1., 2.])
>>> t.nanquantile(0.5, dim=1)
tensor([   nan, 1.5000])
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.min,
    r""""""
min(input) -> Tensor
Returns the minimum value of all elements in the :attr:`input` tensor.
"
583,"tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""")
add_docstr(torch.nan_to_num,
           r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
","tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""",
)
add_docstr(
    torch.nan_to_num,
    r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
"
584,">>> torch.numel(a)
16
"""""".format(**common_args))
add_docstr(torch.ones,
           r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `1`, with the shape defined
",">>> torch.numel(a)
16
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.ones,
    r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with the scalar value `1`, with the shape defined
"
585,"torch.float32
>>> torch.promote_types(torch.uint8, torch.long)
torch.long
"""""")
add_docstr(torch.qr,
           r""""""
qr(input, some=True, *, out=None) -> (Tensor, Tensor)
Computes the QR decomposition of a matrix or a batch of matrices :attr:`input`,
","torch.float32
>>> torch.promote_types(torch.uint8, torch.long)
torch.long
"""""",
)
add_docstr(
    torch.qr,
    r""""""
qr(input, some=True, *, out=None) -> (Tensor, Tensor)
Computes the QR decomposition of a matrix or a batch of matrices :attr:`input`,
"
586,"[6, 7]])
"""""".format(**factory_common_args))
add_docstr(torch.randint_like,
           """"""
randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
","[6, 7]])
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.randint_like,
    """"""
randint_like(input, low=0, high, \\*, dtype=None, layout=torch.strided, device=None, requires_grad=False, \
memory_format=torch.preserve_format) -> Tensor
"
587,".. math::
\text{out}_{{i+1}} = \text{out}_{i} + \text{step}
"""""" + r""""""
Args:
start (Number): the starting value for the set of points. Default: ``0``.
end (Number): the ending value for the set of points
",".. math::
\text{out}_{{i+1}} = \text{out}_{i} + \text{step}
""""""
    + r""""""
Args:
start (Number): the starting value for the set of points. Default: ``0``.
end (Number): the ending value for the set of points
"
588,">>> torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
[ 1.0000], dtype=torch.float64)
"""""")
add_docstr(torch.set_num_threads, r""""""
set_num_threads(int)
Sets the number of threads used for intraop parallelism on CPU.
",">>> torch.tensor([1e-323], dtype=torch.float64)
tensor(9.88131e-324 *
[ 1.0000], dtype=torch.float64)
"""""",
)
add_docstr(
    torch.set_num_threads,
    r""""""
set_num_threads(int)
Sets the number of threads used for intraop parallelism on CPU.
"
589,">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std_mean(a, unbiased=False)
(tensor(0.4188), tensor(-0.8509))
"""""".format(**multi_dim_common))

add_docstr(torch.sub, r""""""
sub(input, other, *, alpha=1, out=None) -> Tensor
Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.
.. math::
\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i
"""""" + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std_mean(a, unbiased=False)
(tensor(0.4188), tensor(-0.8509))
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.sub,
    r""""""
sub(input, other, *, alpha=1, out=None) -> Tensor
Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.
.. math::
\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i
""""""
    + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
"
590,"tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(**common_args))

add_docstr(torch.flip,
           r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
","tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.flip,
    r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
"
591,"tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
>>> torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])
"""""".format(**common_args))

add_docstr(torch.topk,
           r""""""
topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)
Returns the :attr:`k` largest elements of the given :attr:`input` tensor along
","tensor([ 0.8986, -0.7279,  1.1745,  0.2611])
>>> torch.tanh(a)
tensor([ 0.7156, -0.6218,  0.8257,  0.2553])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.topk,
    r""""""
topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)
Returns the :attr:`k` largest elements of the given :attr:`input` tensor along
"
592,".. note::
When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to
prevent overflow during calculation.
"""""" + r""""""
Args:
row (``int``): number of rows in the 2-D matrix.
col (``int``): number of columns in the 2-D matrix.
",".. note::
When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to
prevent overflow during calculation.
""""""
    + r""""""
Args:
row (``int``): number of rows in the 2-D matrix.
col (``int``): number of columns in the 2-D matrix.
"
593,">>> a
tensor([[0, 0, 1],
[1, 2, 2]])
"""""".format(**factory_common_args))

add_docstr(torch.true_divide, r""""""
true_divide(dividend, divisor, *, out) -> Tensor
Alias for :func:`torch.div` with ``rounding_mode=None``.
"""""".format(**common_args))

add_docstr(torch.trunc,
           r""""""
trunc(input, *, out=None) -> Tensor
Returns a new tensor with the truncated integer values of
",">>> a
tensor([[0, 0, 1],
[1, 2, 2]])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.true_divide,
    r""""""
true_divide(dividend, divisor, *, out) -> Tensor
Alias for :func:`torch.div` with ``rounding_mode=None``.
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.trunc,
    r""""""
trunc(input, *, out=None) -> Tensor
Returns a new tensor with the truncated integer values of
"
594,">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var(a, unbiased=False)
tensor(0.1754)
"""""".format(**multi_dim_common))

add_docstr(torch.var_mean,
           r""""""
var_mean(input, dim, unbiased, keepdim=False, *, out=None) -> (Tensor, Tensor)
If :attr:`unbiased` is ``True``, Bessel's correction will be used to calculate
",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var(a, unbiased=False)
tensor(0.1754)
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.var_mean,
    r""""""
var_mean(input, dim, unbiased, keepdim=False, *, out=None) -> (Tensor, Tensor)
If :attr:`unbiased` is ``True``, Bessel's correction will be used to calculate
"
595,">>> torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
"""""".format(**factory_common_args))

add_docstr(torch.zeros_like,
           r""""""
zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `0`, with the same size as
",">>> torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.zeros_like,
    r""""""
zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `0`, with the same size as
"
596,".. note::
See also :func:`torch.nonzero`.
"""""")
add_docstr(torch.logdet,
           r""""""
logdet(input) -> Tensor
Calculates log determinant of a square matrix or batches of square matrices.
",".. note::
See also :func:`torch.nonzero`.
"""""",
)
add_docstr(
    torch.logdet,
    r""""""
logdet(input) -> Tensor
Calculates log determinant of a square matrix or batches of square matrices.
"
597,">>> g_cpu = torch.Generator()
>>> g_cpu_other = torch.Generator()
>>> g_cpu.set_state(g_cpu_other.get_state())
"""""")
add_docstr(torch.Generator.get_state,
           r""""""
Generator.get_state() -> Tensor
Returns the Generator state as a ``torch.ByteTensor``.
",">>> g_cpu = torch.Generator()
>>> g_cpu_other = torch.Generator()
>>> g_cpu.set_state(g_cpu_other.get_state())
"""""",
)
add_docstr(
    torch.Generator.get_state,
    r""""""
Generator.get_state() -> Tensor
Returns the Generator state as a ``torch.ByteTensor``.
"
598,"# examples, don't have a __name__.
return repr(func)
# vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,
# sends those into func, and then unwraps the output BatchedTensors. Operations
# on BatchedTensors perform the batched operations that the user is asking for.
","# examples, don't have a __name__.
return repr(func)

# vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,
# sends those into func, and then unwraps the output BatchedTensors. Operations
# on BatchedTensors perform the batched operations that the user is asking for.
"
599,"add_node(node)
else:
ge = next(executors_it)
            visualize_graph_executor(ge, name + '/', pb_graph,
                                     partial(inline_graph, node=node))
def add_node(node):
        if node.kind() == 'prim::FusionGroup':
return add_fusion_group(node)
        elif node.kind() == 'prim::GraphExecutor':
return add_graph_executor(node)
op, name = name_for(node)
pb_node = pb_graph.node.add(op=op, name=name)
","add_node(node)
else:
ge = next(executors_it)
            visualize_graph_executor(
                ge, name + ""/"", pb_graph, partial(inline_graph, node=node)
            )
def add_node(node):
        if node.kind() == ""prim::FusionGroup"":
return add_fusion_group(node)
        elif node.kind() == ""prim::GraphExecutor"":
return add_graph_executor(node)
op, name = name_for(node)
pb_node = pb_graph.node.add(op=op, name=name)
"
600,">>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)
>>> torch.testing.assert_close(ifftn, two_iffts, check_stride=False)
"""""".format(**common_args))
rfft = _add_docstr(_fft.fft_rfft, r""""""
rfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the one dimensional Fourier transform of real-valued :attr:`input`.
",">>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)
>>> torch.testing.assert_close(ifftn, two_iffts, check_stride=False)
"""""".format(
        **common_args
    ),
)
rfft = _add_docstr(
    _fft.fft_rfft,
    r""""""
rfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the one dimensional Fourier transform of real-valued :attr:`input`.
"
601,">>> torch.fft.ifft(t)
tensor([ 2.0000-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,
0.5000+0.6882j])
"""""".format(**common_args))

fftfreq = _add_docstr(_fft.fft_fftfreq, r""""""
fftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Computes the discrete Fourier Transform sample frequencies for a signal of size :attr:`n`.
",">>> torch.fft.ifft(t)
tensor([ 2.0000-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,
"""""".format(
        **common_args
    ),
)

fftfreq = _add_docstr(
    _fft.fft_fftfreq,
    r""""""
fftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Computes the discrete Fourier Transform sample frequencies for a signal of size :attr:`n`.
"
602,"# The JIT doesn't understand Union, so only add type annotation for mypy
def meshgrid(*tensors: Union[Tensor, List[Tensor]]) -> Tuple[Tensor, ...]:
return _meshgrid(*tensors)
else:
def meshgrid(*tensors):
r""""""Take :math:`N` tensors, each of which can be either scalar or 1-dimensional
vector, and create :math:`N` N-dimensional grids, where the :math:`i` :sup:`th` grid is defined by
","# The JIT doesn't understand Union, so only add type annotation for mypy
def meshgrid(*tensors: Union[Tensor, List[Tensor]]) -> Tuple[Tensor, ...]:
return _meshgrid(*tensors)


else:

def meshgrid(*tensors):
r""""""Take :math:`N` tensors, each of which can be either scalar or 1-dimensional
vector, and create :math:`N` N-dimensional grids, where the :math:`i` :sup:`th` grid is defined by
"
603,"return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,
         win_length: Optional[int] = None, window: Optional[Tensor] = None,
         center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,
         onesided: Optional[bool] = None,
         return_complex: Optional[bool] = None) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
","return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(
    input: Tensor,
    n_fft: int,
    hop_length: Optional[int] = None,
    win_length: Optional[int] = None,
    window: Optional[Tensor] = None,
    center: bool = True,
    pad_mode: str = ""reflect"",
    normalized: bool = False,
    onesided: Optional[bool] = None,
    return_complex: Optional[bool] = None,
) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
"
604,"""""""
if has_torch_function_unary(input):
return handle_torch_function(
            unique, (input,), input, sorted=sorted, return_inverse=return_inverse,
            return_counts=return_counts, dim=dim)
if dim is not None:
output, inverse_indices, counts = _VF.unique_dim(
","""""""
if has_torch_function_unary(input):
return handle_torch_function(
            unique,
            (input,),
            input,
            sorted=sorted,
            return_inverse=return_inverse,
            return_counts=return_counts,
            dim=dim,
        )
if dim is not None:
output, inverse_indices, counts = _VF.unique_dim(
"
605,"return handle_torch_function(tensordot, (a, b), a, b, dims=dims)
if not isinstance(dims, (tuple, list, torch.Tensor, int)):
        raise RuntimeError(""tensordot expects dims to be int or ""
                           + ""Tuple[List[int], List[int]] or ""
                           + ""List[List[int]] containing two lists, but got ""
                           + f""dims={dims}"")
dims_a: List[int] = []
dims_b: List[int] = []
","return handle_torch_function(tensordot, (a, b), a, b, dims=dims)
if not isinstance(dims, (tuple, list, torch.Tensor, int)):
        raise RuntimeError(
            ""tensordot expects dims to be int or ""
            + ""Tuple[List[int], List[int]] or ""
            + ""List[List[int]] containing two lists, but got ""
            + f""dims={dims}""
        )
dims_a: List[int] = []
dims_b: List[int] = []
"
606,"else:
return result[0], result[1]  # A_LU, pivots
# The return type of lu depends on `get_infos`, so in order to resolve the output type
# of lu in TorchScript we need to statically know the value of `get_infos`
lu = boolean_dispatch(
    arg_name='get_infos',
arg_index=2,
default=False,
if_true=_lu_with_infos,
if_false=_lu_no_infos,
module_name=__name__,
    func_name='lu')
lu.__doc__ = _lu_impl.__doc__
def align_tensors(*tensors):
    raise RuntimeError('`align_tensors` not yet implemented.')
","else:
return result[0], result[1]  # A_LU, pivots

# The return type of lu depends on `get_infos`, so in order to resolve the output type
# of lu in TorchScript we need to statically know the value of `get_infos`
lu = boolean_dispatch(
    arg_name=""get_infos"",
arg_index=2,
default=False,
if_true=_lu_with_infos,
if_false=_lu_no_infos,
module_name=__name__,
    func_name=""lu"",
)
lu.__doc__ = _lu_impl.__doc__

def align_tensors(*tensors):
    raise RuntimeError(""`align_tensors` not yet implemented."")
"
607,"def _get_torch_home():
torch_home = os.path.expanduser(
        os.getenv(ENV_TORCH_HOME,
                  os.path.join(os.getenv(ENV_XDG_CACHE_HOME,
                                         DEFAULT_CACHE_DIR), 'torch')))
return torch_home
def _parse_repo_info(github):
branch = MASTER_BRANCH
    if ':' in github:
        repo_info, branch = github.split(':')
else:
repo_info = github
    repo_owner, repo_name = repo_info.split('/')
return repo_owner, repo_name, branch
def _read_url(url):
with urlopen(url) as r:
        return r.read().decode(r.headers.get_content_charset('utf-8'))
def _validate_not_a_forked_repo(repo_owner, repo_name, branch):
# Use urlopen to avoid depending on local git.
    headers = {'Accept': 'application/vnd.github.v3+json'}
token = os.environ.get(ENV_GITHUB_TOKEN)
if token is not None:
        headers['Authorization'] = f'token {token}'
for url_prefix in (
            f'https://api.github.com/repos/{repo_owner}/{repo_name}/branches',
            f'https://api.github.com/repos/{repo_owner}/{repo_name}/tags'):
page = 0
while True:
page += 1
            url = f'{url_prefix}?per_page=100&page={page}'
response = json.loads(_read_url(Request(url, headers=headers)))
# Empty response means no more data to process
if not response:
break
for br in response:
                if br['name'] == branch or br['commit']['sha'].startswith(branch):
return
    raise ValueError(f'Cannot find {branch} in https://github.com/{repo_owner}/{repo_name}. '
                     'If it\'s a commit from a forked repo, please call hub.load() with forked repo directly.')
def _get_cache_or_reload(github, force_reload, verbose=True, skip_validation=False):
","def _get_torch_home():
torch_home = os.path.expanduser(
        os.getenv(
            ENV_TORCH_HOME,
            os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), ""torch""),
        )
    )
return torch_home
def _parse_repo_info(github):
branch = MASTER_BRANCH
    if "":"" in github:
        repo_info, branch = github.split("":"")
else:
repo_info = github
    repo_owner, repo_name = repo_info.split(""/"")
return repo_owner, repo_name, branch
def _read_url(url):
with urlopen(url) as r:
        return r.read().decode(r.headers.get_content_charset(""utf-8""))
def _validate_not_a_forked_repo(repo_owner, repo_name, branch):
# Use urlopen to avoid depending on local git.
    headers = {""Accept"": ""application/vnd.github.v3+json""}
token = os.environ.get(ENV_GITHUB_TOKEN)
if token is not None:
        headers[""Authorization""] = f""token {token}""
for url_prefix in (
        f""https://api.github.com/repos/{repo_owner}/{repo_name}/branches"",
        f""https://api.github.com/repos/{repo_owner}/{repo_name}/tags"",
    ):
page = 0
while True:
page += 1
            url = f""{url_prefix}?per_page=100&page={page}""
response = json.loads(_read_url(Request(url, headers=headers)))
# Empty response means no more data to process
if not response:
break
for br in response:
                if br[""name""] == branch or br[""commit""][""sha""].startswith(branch):
return
    raise ValueError(
        f""Cannot find {branch} in https://github.com/{repo_owner}/{repo_name}. ""
        ""If it's a commit from a forked repo, please call hub.load() with forked repo directly.""
    )
def _get_cache_or_reload(github, force_reload, verbose=True, skip_validation=False):
"
608,"Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
"""""" + fr""""""
.. note:: {common_notes[""sync_note""]}
"""""" + r""""""
.. seealso::
","Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
""""""
    + fr""""""
.. note:: {common_notes[""sync_note""]}
""""""
    + r""""""
.. seealso::
"
609,">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""")
eigvalsh = _add_docstr(_linalg.linalg_eigvalsh, r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
",">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""",
)
eigvalsh = _add_docstr(
    _linalg.linalg_eigvalsh,
    r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
"
610,"This function computes `X = \ `:attr:`A`\ `.inverse() @ \ `:attr:`B` in a faster and
more numerically stable way than performing the computations separately.
"""""" + fr""""""
.. note:: {common_notes[""sync_note""]}
"""""" + r""""""
Args:
A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.
","This function computes `X = \ `:attr:`A`\ `.inverse() @ \ `:attr:`B` in a faster and
more numerically stable way than performing the computations separately.
""""""
    + fr""""""
.. note:: {common_notes[""sync_note""]}
""""""
    + r""""""
Args:
A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.
"
611,"torch.Size([6, 4])
>>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B, atol=1e-6)
True
"""""")
qr = _add_docstr(_linalg.linalg_qr, r""""""
qr(A, mode='reduced', *, out=None) -> (Tensor, Tensor)
Computes the QR decomposition of a matrix.
","torch.Size([6, 4])
>>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B, atol=1e-6)
True
"""""",
)
qr = _add_docstr(
    _linalg.linalg_qr,
    r""""""
qr(A, mode='reduced', *, out=None) -> (Tensor, Tensor)
Computes the QR decomposition of a matrix.
"
612,"tensor(1.6099e-06)
>>> torch.dist(Q.transpose(-2, -1) @ Q, torch.eye(4))
tensor(6.2158e-07)
"""""")
","tensor(1.6099e-06)
>>> torch.dist(Q.transpose(-2, -1) @ Q, torch.eye(4))
tensor(6.2158e-07)
"""""",
)
"
613,"ret.update(ret2)
return ret
def wrap_torch_function(dispatcher: Callable):
""""""Wraps a given function with ``__torch_function__`` -related functionality.
","ret.update(ret2)
return ret

def wrap_torch_function(dispatcher: Callable):
""""""Wraps a given function with ``__torch_function__`` -related functionality.
"
614,"# cannot be overriden by __torch_function__
if func in get_ignored_functions():
                msg = (""{}.{} is in the tuple returned by torch._overrides.get_ignored_functions ""
                       ""but still has an explicit override"")
                assert func not in get_testing_overrides(), msg.format(namespace, func.__name__)
continue
overridable_funcs[namespace].append(func)
return overridable_funcs
@functools.lru_cache(None)
def _get_tensor_methods() -> Set[Callable]:
    """""" Returns a set of the overridable methods on ``torch.Tensor`` """"""
overridable_funcs = get_overridable_functions()
methods = set(overridable_funcs[torch.Tensor])
return methods
def is_tensor_method_or_property(func: Callable) -> bool:
""""""
Returns True if the function passed in is a handler for a
","# cannot be overriden by __torch_function__
if func in get_ignored_functions():
                msg = (
                    ""{}.{} is in the tuple returned by torch._overrides.get_ignored_functions ""
                    ""but still has an explicit override""
                )
                assert func not in get_testing_overrides(), msg.format(
                    namespace, func.__name__
                )
continue
overridable_funcs[namespace].append(func)
return overridable_funcs

@functools.lru_cache(None)
def _get_tensor_methods() -> Set[Callable]:
    """"""Returns a set of the overridable methods on ``torch.Tensor``""""""
overridable_funcs = get_overridable_functions()
methods = set(overridable_funcs[torch.Tensor])
return methods

def is_tensor_method_or_property(func: Callable) -> bool:
""""""
Returns True if the function passed in is a handler for a
"
615,"r'''
PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.
Profiler's context manager API can be used to better understand what model operators are the most expensive,
examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
","r""""""
PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.
Profiler's context manager API can be used to better understand what model operators are the most expensive,
examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
"
616,"self._start_trace()
if self.record_steps:
            self.step_rec_fn = prof.record_function(""ProfilerStep#"" + str(self.step_num))
self.step_rec_fn.__enter__()
def export_chrome_trace(self, path: str):
","self._start_trace()
if self.record_steps:
            self.step_rec_fn = prof.record_function(
                ""ProfilerStep#"" + str(self.step_num)
            )
self.step_rec_fn.__enter__()
def export_chrome_trace(self, path: str):
"
617,"Adds a user defined metadata with a string key and a string value
into the trace file
""""""
        wrapped_value = ""\"""" + value.replace('""', '\\""') + ""\""""
torch.autograd._add_metadata_json(key, wrapped_value)
def add_metadata_json(self, key: str, value: str):
","Adds a user defined metadata with a string key and a string value
into the trace file
""""""
        wrapped_value = '""' + value.replace('""', '\\""') + '""'
torch.autograd._add_metadata_json(key, wrapped_value)
def add_metadata_json(self, key: str, value: str):
"
618,"n = 2 ** m
total_n = self.num_generated + n
if not (total_n & (total_n - 1) == 0):
            raise ValueError(""The balance properties of Sobol' points require ""
                             ""n to be a power of 2. {0} points have been ""
                             ""previously generated, then: n={0}+2**{1}={2}. ""
                             ""If you still want to do this, please use ""
                             ""'SobolEngine.draw()' instead.""
                             .format(self.num_generated, m, total_n))
return self.draw(n=n, out=out, dtype=dtype)
def reset(self):
","n = 2 ** m
total_n = self.num_generated + n
if not (total_n & (total_n - 1) == 0):
            raise ValueError(
                ""The balance properties of Sobol' points require ""
                ""n to be a power of 2. {0} points have been ""
                ""previously generated, then: n={0}+2**{1}={2}. ""
                ""If you still want to do this, please use ""
                ""'SobolEngine.draw()' instead."".format(self.num_generated, m, total_n)
            )
return self.draw(n=n, out=out, dtype=dtype)
def reset(self):
"
619,"def _is_torchscript_zip(zip_file):
    return 'constants.pkl' in zip_file.get_all_records()
","def _is_torchscript_zip(zip_file):
    return ""constants.pkl"" in zip_file.get_all_records()
"
620,"tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""")
psi = _add_docstr(_special.special_psi,
                  r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""")
digamma = _add_docstr(_special.special_digamma,
                      r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
"""""" + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
","tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""",
)
psi = _add_docstr(
    _special.special_psi,
    r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""",
)
digamma = _add_docstr(
    _special.special_digamma,
    r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
""""""
    + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
"
621,".. note::
This function is implemented only for nonnegative integers :math:`n \geq 0`.
"""""" + """"""
Args:
n (int): the order of the polygamma function
{input}
",".. note::
This function is implemented only for nonnegative integers :math:`n \geq 0`.
""""""
    + """"""
Args:
n (int): the order of the polygamma function
{input}
"
622,".. note::
Also known as quantile function for Normal Distribution.
"""""" + r""""""
Args:
{input}
",".. note::
Also known as quantile function for Normal Distribution.
""""""
    + r""""""
Args:
{input}
"
623,"return self.clone()
def __deepcopy__(self, memo):
        memo = memo.setdefault('torch', {})
if self._cdata in memo:
return memo[self._cdata]
new_storage = self.clone()
","return self.clone()
def __deepcopy__(self, memo):
        memo = memo.setdefault(""torch"", {})
if self._cdata in memo:
return memo[self._cdata]
new_storage = self.clone()
"
624,"qscheme=torch.per_tensor_affine,
reduce_range=True)
def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
","qscheme=torch.per_tensor_affine,
reduce_range=True)
default_fused_act_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                       quant_min=0,
                                                                       quant_max=255,
                                                                       dtype=torch.quint8,)


default_fused_wt_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                      quant_min=-128,
                                                                      quant_max=127,
                                                                      dtype=torch.qint8,
                                                                      qscheme=torch.per_tensor_symmetric)

default_fused_per_channel_wt_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,
                                                                                  quant_min=-128,
                                                                                  quant_max=127,
                                                                                  dtype=torch.qint8,
                                                                                  qscheme=torch.per_channel_symmetric)

def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
"
625,"Tensor mul_sparse(const Tensor& self, const Tensor& other) {
auto commonDtype = at::result_type(self, other);
  Tensor result = at::empty({0}, self.options().dtype(commonDtype));
return at::mul_out(result, self, other);  // redispatch!
}
","Tensor mul_sparse(const Tensor& self, const Tensor& other) {
auto commonDtype = at::result_type(self, other);
  // Arbitrary (dense, sparse) and (sparse, dense) multiplication is not
  // currently supported, but (0dim-dense, sparse) and (sparse, 0dim-dense) is.
  // Make sure we use the sparse exemplar for result.
  auto result_options = self.is_sparse() ?
    self.options().dtype(commonDtype) : other.options().dtype(commonDtype);
  Tensor result = at::empty({0}, result_options);
return at::mul_out(result, self, other);  // redispatch!
}
"
626,"}
}
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::pack_hook_ = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_ = nullptr;
void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
TORCH_CHECK(!pack_hook_ && !unpack_hook_,
""Setting default hooks but they have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
}
  std::mutex PyDefaultSavedVariableHooks::mutex_;
  PyObject* PyDefaultSavedVariableHooks::pack_hook_(nullptr);
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_(nullptr);
void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    std::lock_guard<std::mutex> lock(mutex_);
TORCH_CHECK(!pack_hook_ && !unpack_hook_,
""Setting default hooks but they have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
627,"}
void PyDefaultSavedVariableHooks::reset_hooks() {
if (Py_IsInitialized()) {
py::gil_scoped_acquire gil;
Py_XDECREF(pack_hook_);
","}
void PyDefaultSavedVariableHooks::reset_hooks() {
    std::lock_guard<std::mutex> lock(mutex_);
if (Py_IsInitialized()) {
py::gil_scoped_acquire gil;
Py_XDECREF(pack_hook_);
"
628,"from ..overrides import has_torch_function, handle_torch_function
from . import functional
from . import forward_ad
__all__ = ['Variable', 'Function', 'backward', 'grad_mode']
","from ..overrides import has_torch_function, handle_torch_function
from . import functional
from . import forward_ad
from . import saved_variable_default_hooks as graph
__all__ = ['Variable', 'Function', 'backward', 'grad_mode']
"
629,"Py_XDECREF(data_);
}
}
}}
","Py_XDECREF(data_);
}
}

  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::pack_hook_ = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_ = nullptr;

  void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    TORCH_CHECK(!pack_hook_ && !unpack_hook_,
        ""Setting default hooks but they have already been set. ""
        ""Hint: only one pair of hooks is allowed at a time."");
    pack_hook_ = pack_hook.release().ptr();
    unpack_hook_ = unpack_hook.release().ptr();
  }

  void PyDefaultSavedVariableHooks::reset_hooks() {
    if (Py_IsInitialized()) {
      py::gil_scoped_acquire gil;
      Py_XDECREF(pack_hook_);
      Py_XDECREF(unpack_hook_);
    }
    pack_hook_ = nullptr;
    unpack_hook_ = nullptr;
  }

  std::unique_ptr<SavedVariableHooks> PyDefaultSavedVariableHooks::get_hooks() {
    if (!pack_hook_ || !unpack_hook_) {
      return nullptr;
    }
    py::gil_scoped_acquire gil;
    py::function pack_hook = py::reinterpret_borrow<py::function>(pack_hook_);
    py::function unpack_hook = py::reinterpret_borrow<py::function>(unpack_hook_);
    return std::make_unique<PySavedVariableHooks>(pack_hook, unpack_hook);
  }

}}
"
630,"'argmax', 'argmin', 'argsort', 'searchsorted',
'bucketize',
# Functions that return booleans are not differentiable
    'isnan', 'isposinf', 'isneginf', 'isinf'
# Functions return none are not differentiable
'record_stream',
}
","'argmax', 'argmin', 'argsort', 'searchsorted',
'bucketize',
# Functions that return booleans are not differentiable
    'isnan', 'isposinf', 'isneginf', 'isinf', 'signbit', 'isin',
# Functions return none are not differentiable
'record_stream',
}
"
631,"m.def(""_clear_callbacks"", []() {
at::clearCallbacks();
});
  m.def(""_register_default_hooks"", [](py::function &pack_hook, py::function &unpack_hook) {
    torch::autograd::PyDefaultSavedVariableHooks::set_hooks(pack_hook, unpack_hook);
  });
  m.def(""_reset_default_hooks"", []() {
    torch::autograd::PyDefaultSavedVariableHooks::reset_hooks();
  });
py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
","m.def(""_clear_callbacks"", []() {
at::clearCallbacks();
});
py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
"
632,"#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/functions/basic_ops.h>
#include <torch/csrc/autograd/python_anomaly_mode.h>
#include <torch/csrc/autograd/python_saved_variable_hooks.h>
#include <torch/csrc/autograd/python_function.h>
#include <torch/csrc/utils/pycfunction_helpers.h>
#include <ATen/BatchedTensorImpl.h>
","#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/functions/basic_ops.h>
#include <torch/csrc/autograd/python_anomaly_mode.h>
#include <torch/csrc/autograd/python_function.h>
#include <torch/csrc/utils/pycfunction_helpers.h>
#include <ATen/BatchedTensorImpl.h>
"
633,"if (!is_output || is_leaf_) {
saved_original_ = true;
data_ = variable;
      register_hooks(Engine::get_default_engine().get_default_saved_variable_hooks());
return;
}
","if (!is_output || is_leaf_) {
saved_original_ = true;
data_ = variable;
return;
}
"
634,"static void polygamma_kernel(TensorIteratorBase& iter, int64_t n) {
if (n == 0) {
digamma_kernel(iter);
} else {
AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.dtype(), ""polygamma"", [&]() {
cpu_kernel(
","static void polygamma_kernel(TensorIteratorBase& iter, int64_t n) {
if (n == 0) {
digamma_kernel(iter);
  } else if (n == 1) {
    trigamma_kernel(iter);
} else {
AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.dtype(), ""polygamma"", [&]() {
cpu_kernel(
"
635,"REGISTER_DISPATCH(asinh_stub, &CPU_CAPABILITY::asinh_kernel);
REGISTER_DISPATCH(atanh_stub, &CPU_CAPABILITY::atanh_kernel);
REGISTER_DISPATCH(digamma_stub, &CPU_CAPABILITY::digamma_kernel);
REGISTER_DISPATCH(polygamma_stub, &CPU_CAPABILITY::polygamma_kernel);
REGISTER_DISPATCH(kaiser_window_stub, &CPU_CAPABILITY::kaiser_window_kernel);
REGISTER_DISPATCH(special_entr_stub, &CPU_CAPABILITY::entr_kernel);
","REGISTER_DISPATCH(asinh_stub, &CPU_CAPABILITY::asinh_kernel);
REGISTER_DISPATCH(atanh_stub, &CPU_CAPABILITY::atanh_kernel);
REGISTER_DISPATCH(digamma_stub, &CPU_CAPABILITY::digamma_kernel);
REGISTER_DISPATCH(trigamma_stub, &CPU_CAPABILITY::trigamma_kernel);
REGISTER_DISPATCH(polygamma_stub, &CPU_CAPABILITY::polygamma_kernel);
REGISTER_DISPATCH(kaiser_window_stub, &CPU_CAPABILITY::kaiser_window_kernel);
REGISTER_DISPATCH(special_entr_stub, &CPU_CAPABILITY::entr_kernel);
"
636,"REGISTER_DISPATCH(asinh_stub, &CPU_CAPABILITY::asinh_kernel);
REGISTER_DISPATCH(atanh_stub, &CPU_CAPABILITY::atanh_kernel);
REGISTER_DISPATCH(digamma_stub, &CPU_CAPABILITY::digamma_kernel);
REGISTER_DISPATCH(trigamma_stub, &CPU_CAPABILITY::trigamma_kernel);
REGISTER_DISPATCH(polygamma_stub, &CPU_CAPABILITY::polygamma_kernel);
REGISTER_DISPATCH(kaiser_window_stub, &CPU_CAPABILITY::kaiser_window_kernel);
REGISTER_DISPATCH(special_entr_stub, &CPU_CAPABILITY::entr_kernel);
","REGISTER_DISPATCH(asinh_stub, &CPU_CAPABILITY::asinh_kernel);
REGISTER_DISPATCH(atanh_stub, &CPU_CAPABILITY::atanh_kernel);
REGISTER_DISPATCH(digamma_stub, &CPU_CAPABILITY::digamma_kernel);
REGISTER_DISPATCH(polygamma_stub, &CPU_CAPABILITY::polygamma_kernel);
REGISTER_DISPATCH(kaiser_window_stub, &CPU_CAPABILITY::kaiser_window_kernel);
REGISTER_DISPATCH(special_entr_stub, &CPU_CAPABILITY::entr_kernel);
"
637,"Py_XDECREF(data_);
}
}
}}
","Py_XDECREF(data_);
}
}

  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::pack_hook_ = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_ = nullptr;

  void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    TORCH_CHECK(!pack_hook_ && !unpack_hook_,
        ""Setting default hooks but they have already been set. ""
        ""Hint: only one pair of hooks is allowed at a time."");
    pack_hook_ = pack_hook.release().ptr();
    unpack_hook_ = unpack_hook.release().ptr();
  }

  void PyDefaultSavedVariableHooks::reset_hooks() {
    if (Py_IsInitialized()) {
      py::gil_scoped_acquire gil;
      Py_XDECREF(pack_hook_);
      Py_XDECREF(unpack_hook_);
    }
    pack_hook_ = nullptr;
    unpack_hook_ = nullptr;
  }

  std::unique_ptr<SavedVariableHooks> PyDefaultSavedVariableHooks::get_hooks() {
    py::gil_scoped_acquire acquire;
    if (!pack_hook_ || !unpack_hook_) {
      return nullptr;
    }
    py::function pack_hook = py::reinterpret_borrow<py::function>(pack_hook_);
    py::function unpack_hook = py::reinterpret_borrow<py::function>(unpack_hook_);
    return std::make_unique<PySavedVariableHooks>(pack_hook, unpack_hook);
  }

}}
"
638,"if (!is_output || is_leaf_) {
saved_original_ = true;
data_ = variable;
return;
}
","if (!is_output || is_leaf_) {
saved_original_ = true;
data_ = variable;
      register_hooks(Engine::get_default_engine().get_default_saved_variable_hooks());
return;
}
"
639,"}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
  if (!hooks) {
    return;
  }
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
640,"// See discussion in forward_grad.h for why these are global variables and not
// thread local
std::mutex all_forward_levels_mutex_;
    uint64_t next_forward_idx_ = 0;
std::vector<std::shared_ptr<ForwardADLevel>> all_forward_levels_;
const static at::Tensor singleton_undefined_tensor;
","// See discussion in forward_grad.h for why these are global variables and not
// thread local
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
std::mutex all_forward_levels_mutex_;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
std::vector<std::shared_ptr<ForwardADLevel>> all_forward_levels_;
const static at::Tensor singleton_undefined_tensor;
"
641,"continue
if node.op == 'output':
rv = map_arg(node.args[0], lambda n: val_map[n])
                return rv
val_map[node] = self.node_copy(node, lambda n : val_map[n])
return None
","continue
if node.op == 'output':
rv = map_arg(node.args[0], lambda n: val_map[n])
                return rv if not return_output_node else (rv, node)
val_map[node] = self.node_copy(node, lambda n : val_map[n])
return None
"
642,"//
// TODO: if we extend TensorIterator to accept 3 inputs,
// we can probably make this a bit more performant.
Tensor do_trapz(const Tensor& y, const Tensor& dx, int64_t dim) {
Tensor left = y.slice(dim, 0, -1);
Tensor right = y.slice(dim, 1);

return ((left + right) * dx).sum(dim) / 2.;
}
// When dx is constant, the above formula simplifies
// to dx * [(\sum_{i=1}^n y_i) - (y_1 + y_n)/2]
Tensor do_trapz(const Tensor& y, double dx, int64_t dim) {
return (y.sum(dim) - (y.select(dim, 0) + y.select(dim, -1)) * (0.5)) * dx;
}
","//
// TODO: if we extend TensorIterator to accept 3 inputs,
// we can probably make this a bit more performant.
Tensor do_trapezoid(const Tensor& y, const Tensor& dx, int64_t dim) {
Tensor left = y.slice(dim, 0, -1);
Tensor right = y.slice(dim, 1);
    // If the dimensions of 'dx' and '(left + right)' do not match
    // broadcasting is attempted here.
return ((left + right) * dx).sum(dim) / 2.;
}
// When dx is constant, the above formula simplifies
// to dx * [(\sum_{i=1}^n y_i) - (y_1 + y_n)/2]
Tensor do_trapezoid(const Tensor& y, double dx, int64_t dim) {
return (y.sum(dim) - (y.select(dim, 0) + y.select(dim, -1)) * (0.5)) * dx;
}
"
643,"// respective
//   C stdlib functions
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <ctype.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <errno.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <math.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <string.h>
#include <locale>
#define D_PNAN ((double)+NAN)
","// respective
//   C stdlib functions
#include <cctype>
#include <cerrno>
#include <cmath>
#include <cstring>
#include <locale>
#define D_PNAN ((double)+NAN)
"
644,".. code:: python
        U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()
Args:
input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more
",".. code:: python
        U = torch.linalg.cholesky(A).transpose(-2, -1).conj()

    This transform will produce equivalent results for all valid (symmetric positive definite) inputs.
Args:
input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more
"
645,"See also the `full description of these drivers`_
:attr:`cond` is used to determine the effective rank of the matrices in :attr:`A`
when :attr:`driver` is one of (`'gelsy'`, `'gelsd'`, `'gelss'`).
In this case, if :math:`\sigma_i` are the singular values of `A` in decreasing order,
:math:`\sigma_i` will be rounded down to zero if :math:`\sigma_i \leq \text{cond} \cdot \sigma_1`.
If :attr:`cond`\ `= None` (default), :attr:`cond` is set to the machine precision of the dtype of :attr:`A`.
This function returns the solution to the problem and some extra information in a named tuple of
four tensors `(solution, residuals, rank, singular_values)`. For inputs :attr:`A`, :attr:`B`
","See also the `full description of these drivers`_
:attr:`rcond` is used to determine the effective rank of the matrices in :attr:`A`
when :attr:`driver` is one of (`'gelsy'`, `'gelsd'`, `'gelss'`).
In this case, if :math:`\sigma_i` are the singular values of `A` in decreasing order,
:math:`\sigma_i` will be rounded down to zero if :math:`\sigma_i \leq \text{rcond} \cdot \sigma_1`.
If :attr:`rcond`\ `= None` (default), :attr:`rcond` is set to the machine precision of the dtype of :attr:`A`.
This function returns the solution to the problem and some extra information in a named tuple of
four tensors `(solution, residuals, rank, singular_values)`. For inputs :attr:`A`, :attr:`B`
"
646,"Using the :attr:`dim` argument to compute matrix norms::
    >>> m = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)
    >>> LA.norm(m, dim=(1,2))
tensor([ 3.7417, 11.2250])
    >>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""")
","Using the :attr:`dim` argument to compute matrix norms::
    >>> A = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)
    >>> LA.norm(A, dim=(1,2))
tensor([ 3.7417, 11.2250])
    >>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""")
"
647,"Examples::
    >>> a = torch.randn(5, 3)
    >>> a
    tensor([[-0.3357, -0.2987, -1.1096],
            [ 1.4894,  1.0016, -0.4572],
            [-1.9401,  0.7437,  2.0968],
            [ 0.1515,  1.3812,  1.5491],
            [-1.8489, -0.5907, -2.5673]])
    >>>
    >>> # reconstruction in the full_matrices=False case
    >>> u, s, vh = torch.linalg.svd(a, full_matrices=False)
    >>> u.shape, s.shape, vh.shape
(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))
    >>> torch.dist(a, u @ torch.diag(s) @ vh)
tensor(1.0486e-06)
    >>>
    >>> # reconstruction in the full_matrices=True case
    >>> u, s, vh = torch.linalg.svd(a)
    >>> u.shape, s.shape, vh.shape
(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))
    >>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)
    >>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)
tensor(1.0486e-06)
    >>>
    >>> # extra dimensions
    >>> a_big = torch.randn(7, 5, 3)
    >>> u, s, vh = torch.linalg.svd(a_big, full_matrices=False)
    >>> torch.dist(a_big, u @ torch.diag_embed(s) @ vh)
tensor(3.0957e-06)
.. _the resulting vectors will span the same subspace:
","Examples::
    >>> A = torch.randn(5, 3)
    >>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    >>> U.shape, S.shape, Vh.shape
(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))
    >>> torch.dist(A, U @ torch.diag(S) @ Vh)
tensor(1.0486e-06)

    >>> U, S, Vh = torch.linalg.svd(A)
    >>> U.shape, S.shape, Vh.shape
(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))
    >>> torch.dist(A, U[:, :3] @ torch.diag(S) @ Vh)
tensor(1.0486e-06)

    >>> A = torch.randn(7, 5, 3)
    >>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    >>> torch.dist(A, U @ torch.diag_embed(S) @ Vh)
tensor(3.0957e-06)
.. _the resulting vectors will span the same subspace:
"
648,"return std::make_shared<SimpleValue>(v);
} break;
case prim::GetAttr: {
        checkApplyNumInputs(apply, 2);
auto obj = emitSugaredExpr(apply.inputs()[0], 1);
auto selector = apply.inputs()[1];
if (selector.kind() != TK_STRINGLITERAL) {
","return std::make_shared<SimpleValue>(v);
} break;
case prim::GetAttr: {
        checkApplyNumInputsRange(apply, 2, 3);
auto obj = emitSugaredExpr(apply.inputs()[0], 1);
auto selector = apply.inputs()[1];
if (selector.kind() != TK_STRINGLITERAL) {
"
649,"cudaError_t err = cudaEventQuery(event);
if (err == cudaErrorNotReady) {
break;
} else if (err != cudaSuccess) {
return err;
","cudaError_t err = cudaEventQuery(event);
if (err == cudaErrorNotReady) {
        // ignore and clear the error if not ready
        cudaGetLastError();
break;
} else if (err != cudaSuccess) {
return err;
"
650,"class _Joinable(ABC):
r""""""
This defines an abstract base class for joinable classes. A joinable class
    (inheriting from :class:`_Joinable`) should implement a private
    ``_join_hook()`` method that returns a :class:`_JoinHook` instance.
""""""
@abstractmethod
def __init__(self):
","class _Joinable(ABC):
r""""""
This defines an abstract base class for joinable classes. A joinable class
    (inheriting from :class:`_Joinable`) should implement :meth:`_join_hook()`,
    which returns a :class:`_JoinHook` instance, in addition to
    :meth:`_join_device()` and :meth:`_join_process_group()` that return device
    and process group information, respectively.
""""""
@abstractmethod
def __init__(self):
"
651,"def _join_device(self) -> torch.device:
r""""""
Returns the device from which to perform collective communications
        needed for the join context manager implementation itself.
""""""
...
","def _join_device(self) -> torch.device:
r""""""
Returns the device from which to perform collective communications
        needed by the join context manager implementation itself.
""""""
...
"
652,"update_submodules()
gen_compile_commands()
run_autogen()
","update_submodules()
gen_compile_commands()
run_autogen()


if __name__ == ""__main__"":
    generate_build_files()
"
653,"0,                                           /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","0,                                           /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
654,"sizeof(DisableTorchFunction),                /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","sizeof(DisableTorchFunction),                /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
655,"return cpu_tensors;
}
void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
auto& schema_args = op.schema().arguments();
","return cpu_tensors;
}
c10::optional<c10::Device> compute_target_device(std::vector<at::Tensor>& t_args, std::vector<c10::List<at::Tensor>> tlist_args) {
  // Decide what device to move the output tensor(s) to.
  // The current convention is that we use the first tensor arg to pick the device
  // Barring that, we take the first tensor from a TensorList arg.
  if (t_args.size() > 0) {
    return t_args[0].device();
  } else {
    // We need to loop through all of the (potentially multiple) TensorList arguments
    // In case, e.g. the first one is empty but the second is not.
    for (auto& tens_list : tlist_args) {
      for (const auto i : c10::irange(tens_list.size())) {
        return tens_list.get(i).device();
      }
    }
  }
  return c10::nullopt;
}

void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
auto& schema_args = op.schema().arguments();
"
656,"continue  # skip constraints that cannot be checked
if param not in self.__dict__ and isinstance(getattr(type(self), param), lazy_property):
continue  # skip checking lazily-constructed args
if not constraint.check(getattr(self, param)).all():
raise ValueError(""The parameter {} has invalid values"".format(param))
super(Distribution, self).__init__()
","continue  # skip constraints that cannot be checked
if param not in self.__dict__ and isinstance(getattr(type(self), param), lazy_property):
continue  # skip checking lazily-constructed args
                value = getattr(self, param)
                valid = constraint.check(value)
                if not valid.all():
                    raise ValueError(
                        f""Expected parameter {param} ""
                        f""({type(value).__name__} of shape {tuple(value.shape)}) ""
                        f""of distribution {repr(self)} ""
                        f""to satisfy the constraint {repr(constraint)}, ""
                        f""but found invalid values:\n{value}""
                    )
if not constraint.check(getattr(self, param)).all():
raise ValueError(""The parameter {} has invalid values"".format(param))
super(Distribution, self).__init__()
"
657,"raise ValueError(""Either `probs` or `logits` must be specified, but not both."")
if probs is not None:
self.probs, = broadcast_all(probs)
            if not self.probs.gt(0).all():
                raise ValueError('All elements of probs must be greater than 0')
else:
self.logits, = broadcast_all(logits)
probs_or_logits = probs if probs is not None else logits
","raise ValueError(""Either `probs` or `logits` must be specified, but not both."")
if probs is not None:
self.probs, = broadcast_all(probs)
else:
self.logits, = broadcast_all(logits)
probs_or_logits = probs if probs is not None else logits
"
658,"else:
batch_shape = probs_or_logits.size()
super(Geometric, self).__init__(batch_shape, validate_args=validate_args)
def expand(self, batch_shape, _instance=None):
new = self._get_checked_instance(Geometric, _instance)
","else:
batch_shape = probs_or_logits.size()
super(Geometric, self).__init__(batch_shape, validate_args=validate_args)
        if self._validate_args and probs is not None:
            # Add an extra check beyond unit_interval
            value = self.probs
            valid = value > 0
            if not valid.all():
                invalid_value = value.data[~valid]
                raise ValueError(
                    ""Expected parameter probs ""
                    f""({type(value).__name__} of shape {tuple(value.shape)}) ""
                    f""of distribution {repr(self)} ""
                    f""to be positive but found invalid values:\n{invalid_value}""
                )
def expand(self, batch_shape, _instance=None):
new = self._get_checked_instance(Geometric, _instance)
"
659,", listeners_(std::make_unique<detail::RegistrationListenerList>())
, mutex_() {}
// NOLINTNEXTLINE(modernize-use-equals-default)
Dispatcher::~Dispatcher() {}
C10_EXPORT Dispatcher& Dispatcher::realSingleton() {
static Dispatcher _singleton;
",", listeners_(std::make_unique<detail::RegistrationListenerList>())
, mutex_() {}
Dispatcher::~Dispatcher() = default;
C10_EXPORT Dispatcher& Dispatcher::realSingleton() {
static Dispatcher _singleton;
"
660,"thread_pool_shared_->work_.notify_one();
}
// Remembers current streams on all devices where a context has been created.
// Only called if Engine::execute detects at least one node runs on a cuda stream.
void GraphTask::stash_current_streams() {
const auto guard = c10::impl::VirtualGuardImpl{c10::DeviceType::CUDA};
auto num_gpus = guard.deviceCount();
caller_current_streams_.resize(num_gpus);
if (num_gpus > 0) {
for (c10::DeviceIndex idx = 0; idx < num_gpus;  idx++) {
#ifdef __HIP_PLATFORM_HCC__
","thread_pool_shared_->work_.notify_one();
}
// Remembers current and default streams on all devices where a context has been created.
// Only called if Engine::execute detects at least one node runs on a cuda stream.
void GraphTask::stash_current_streams() {
const auto guard = c10::impl::VirtualGuardImpl{c10::DeviceType::CUDA};
auto num_gpus = guard.deviceCount();
caller_current_streams_.resize(num_gpus);
  caller_default_streams_.resize(num_gpus);
if (num_gpus > 0) {
for (c10::DeviceIndex idx = 0; idx < num_gpus;  idx++) {
#ifdef __HIP_PLATFORM_HCC__
"
661,"return f;
},
py::return_value_policy::reference)
.def_static(
""distribute_loop"",
[](For* f) { return LoopNest::distributeLoop(f); },
","return f;
},
py::return_value_policy::reference)
      .def(
          ""tile"",
          [](LoopNest& self, For* x, For* y, int x_factor, int y_factor) {
            return self.tile(x, y, x_factor, y_factor);
          },
          py::return_value_policy::reference)
.def_static(
""distribute_loop"",
[](For* f) { return LoopNest::distributeLoop(f); },
"
662,"const auto inputs_w = params.linear_ih(inputs);
auto unstacked_output =
(*this)(inputs_w.unbind(0), input_hidden, params, true);
return {at::stack(unstacked_output.outputs, 0),
unstacked_output.final_hidden};
}
auto unstacked_output = (*this)(inputs.unbind(0), input_hidden, params);
return {at::stack(unstacked_output.outputs, 0),
unstacked_output.final_hidden};
}
","const auto inputs_w = params.linear_ih(inputs);
auto unstacked_output =
(*this)(inputs_w.unbind(0), input_hidden, params, true);
      TORCH_CHECK(unstacked_output.outputs.size()>0, ""Expected sequence length to be larger than 0 in RNN"");
return {at::stack(unstacked_output.outputs, 0),
unstacked_output.final_hidden};
}
auto unstacked_output = (*this)(inputs.unbind(0), input_hidden, params);
    TORCH_CHECK(unstacked_output.outputs.size()>0, ""Expected sequence length to be larger than 0 in RNN"");
return {at::stack(unstacked_output.outputs, 0),
unstacked_output.final_hidden};
}
"
663,"unzip_namedshape, single_ellipsis_index, is_ellipsis)
from torch.overrides import (
has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    handle_torch_function)
import torch.utils.hooks as hooks
","unzip_namedshape, single_ellipsis_index, is_ellipsis)
from torch.overrides import (
has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    handle_torch_function, get_default_nowrap_functions)
import torch.utils.hooks as hooks
"
664,"using PATH = std::unordered_map<std::string,
std::unordered_map<std::string, std::string>>;
// Referenced the logic in llvm-cxxfilt.cpp.
// Starting from LLVM 9 it provides a `demangle()` API. Here we keep our ad-hoc
// version for backward compatibility.
","using PATH = std::unordered_map<std::string,
std::unordered_map<std::string, std::string>>;
inline std::string _name(const Value* V) {
  return V->getName().str();
}

// Referenced the logic in llvm-cxxfilt.cpp.
// Starting from LLVM 9 it provides a `demangle()` API. Here we keep our ad-hoc
// version for backward compatibility.
"
665,"(options.layout() == c10::kStrided));
if (memory_format == MemoryFormat::Preserve) {
    if (self.is_non_overlapping_and_dense()) {
// Copy all strides
auto r = at::empty_strided(self.sizes(),
self.strides(),
","(options.layout() == c10::kStrided));
if (memory_format == MemoryFormat::Preserve) {
    if (self.is_non_overlapping_and_dense() && options.device().supports_as_strided()) {
// Copy all strides
auto r = at::empty_strided(self.sizes(),
self.strides(),
"
666,"Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]
>>> tensor_list
[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
","Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]
>>> tensor_list
[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
"
667,"torch::Tensor output = torch::empty({0}, input.options());
  // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores,clang-diagnostic-unused-variable)
  int64_t batch_size = input.size(0);
int64_t channels = input.size(1);
  // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores,clang-diagnostic-unused-variable)
  int64_t input_height = input.size(2);
  // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores,clang-diagnostic-unused-variable)
  int64_t input_width = input.size(3);
output.resize_as_(input);
ctx->saved_data[""scale""].toTensor().resize_as_(input);
","torch::Tensor output = torch::empty({0}, input.options());
int64_t channels = input.size(1);
output.resize_as_(input);
ctx->saved_data[""scale""].toTensor().resize_as_(input);
"
668,"unsigned previous = y - 1;
for (size_t x = 1; x <= n; ++x) {
      // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)
      int old_row = row[x];
row[x] = std::min(
previous + (word1[y - 1] == word2[x - 1] ? 0u : 1u),
std::min(row[x - 1], row[x]) + 1);
","unsigned previous = y - 1;
for (size_t x = 1; x <= n; ++x) {
      const auto old_row = row[x];
row[x] = std::min(
previous + (word1[y - 1] == word2[x - 1] ? 0u : 1u),
std::min(row[x - 1], row[x]) + 1);
"
669,"for (const IValue& v : stack) {
if (v.isTensor()) {
      // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
      at::Tensor t = v.toTensor();
if (t.defined() && t.requires_grad()) {
// requires grad tensors cannot be constants
return c10::nullopt;
","for (const IValue& v : stack) {
if (v.isTensor()) {
      const at::Tensor& t = v.toTensor();
if (t.defined() && t.requires_grad()) {
// requires grad tensors cannot be constants
return c10::nullopt;
"
670,"// it was created by the op.
checkInputPreconditions(stack);
  // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
  const auto schema = node->schema();
std::vector<AliasAndIValue> inputsToCheck;
for (const auto i : c10::irange(schema.arguments().size())) {
","// it was created by the op.
checkInputPreconditions(stack);
  const auto& schema = node->schema();
std::vector<AliasAndIValue> inputsToCheck;
for (const auto i : c10::irange(schema.arguments().size())) {
"
671,"const Expr* mod_rhs = IRSimplifier::simplify(mod->rhs());
bool merged = false;
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)
      for (int j = mod_rounds.size() - 1; j >= 0; j--) {
const Term* mr = mod_rounds[j];
auto a = isModRound(mr);
CHECK(a);
","const Expr* mod_rhs = IRSimplifier::simplify(mod->rhs());
bool merged = false;
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-narrowing-conversions)
      for (int64_t j = mod_rounds.size() - 1; j >= 0; j--) {
const Term* mr = mod_rounds[j];
auto a = isModRound(mr);
CHECK(a);
"
672,"const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout_ != kNoTimeout && elapsed > timeout_) {
        throw std::runtime_error(""Timeout waiting for key: "" + key);
}
std::this_thread::sleep_for(std::chrono::milliseconds(10));
continue;
","const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout_ != kNoTimeout && elapsed > timeout_) {
        TORCH_CHECK(false, ""Timeout waiting for key: "" + key);
}
std::this_thread::sleep_for(std::chrono::milliseconds(10));
continue;
"
673,"at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    throw std::runtime_error(""input tensor has to be dense"");
}
return tensor;
}
","at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false, ""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    TORCH_CHECK(false, ""input tensor has to be dense"");
}
return tensor;
}
"
674,"c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupMPI::allreduce_coalesced(
std::vector<at::Tensor>& tensors,
const AllreduceCoalescedOptions& opts) {
  throw std::runtime_error(
""allreduce_coalesced is currently not supported with MPI"");
}
","c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupMPI::allreduce_coalesced(
std::vector<at::Tensor>& tensors,
const AllreduceCoalescedOptions& opts) {
  TORCH_CHECK(false,
""allreduce_coalesced is currently not supported with MPI"");
}
"
675,"if (rank_ != opts.rootRank) {
if (outputTensors.size() > 0) {
      throw std::runtime_error(
""Gather: number of output tensors should be 0 ""
""for non-root"");
}
} else {
if (outputTensors.size() != 1) {
      throw std::runtime_error(""Gather: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
      throw std::runtime_error(
""Gather: number of output tensors should equal ""
""to the world size"");
}
","if (rank_ != opts.rootRank) {
if (outputTensors.size() > 0) {
      TORCH_CHECK(false,
""Gather: number of output tensors should be 0 ""
""for non-root"");
}
} else {
if (outputTensors.size() != 1) {
      TORCH_CHECK(false, ""Gather: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
      TORCH_CHECK(false,
""Gather: number of output tensors should equal ""
""to the world size"");
}
"
676,"// Only check device match for the first tensor in the list; the call to
// newLikeFlat() below will check the rest.
if (tensor_lists[i].front().get_device() != other[i].get_device()) {
      throw std::runtime_error(
""Corresponding input/output tensors to scatter/gather must all reside""
"" on the same device"");
}
for (const auto& t : tensor_lists[i]) {
if (t.numel() != other[i].numel()) {
        throw std::runtime_error(
""All tensor operands to scatter/gather must have the same number of elements"");
}
}
","// Only check device match for the first tensor in the list; the call to
// newLikeFlat() below will check the rest.
if (tensor_lists[i].front().get_device() != other[i].get_device()) {
      TORCH_CHECK(false,
""Corresponding input/output tensors to scatter/gather must all reside""
"" on the same device"");
}
for (const auto& t : tensor_lists[i]) {
if (t.numel() != other[i].numel()) {
        TORCH_CHECK(false,
""All tensor operands to scatter/gather must have the same number of elements"");
}
}
"
677,"std::vector<at::Tensor>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllToAllOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
","std::vector<at::Tensor>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllToAllOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
"
678,"cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    TORCH_CHECK(false,
""Number of workers for FileStore should be greater than zero"");
}
}
","cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    throw std::runtime_error(
""Number of workers for FileStore should be greater than zero"");
}
}
"
679,"void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false,
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
","void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
"
680,"for (const auto& tensor : tensors) {
if ((tensor.numel() != t_in.numel()) ||
(tensor.scalar_type() != t_in.scalar_type())) {
      TORCH_CHECK(false, ""Tensors are not equal in size or data type"");
}
checkSingleTensorHelper(tensor);
}
","for (const auto& tensor : tensors) {
if ((tensor.numel() != t_in.numel()) ||
(tensor.scalar_type() != t_in.scalar_type())) {
      throw std::runtime_error(""Tensors are not equal in size or data type"");
}
checkSingleTensorHelper(tensor);
}
"
681,"std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL does not support allgather_coalesced"");
}
","std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL does not support allgather_coalesced"");
}
"
682,"std::vector<at::Tensor>& /* unused */,
int /* unused */,
int /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports recv for NCCL lib version >= 2.7.0"");
}
#endif
","std::vector<at::Tensor>& /* unused */,
int /* unused */,
int /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports recv for NCCL lib version >= 2.7.0"");
}
#endif
"
683,"while (true) {
int res = tcputil::poll(events.get(), 1, timeout.count());
if (res == 0) {
      TORCH_CHECK(false,
""waiting for processes to ""
""connect has timed out"");
} else if (res == -1) {
","while (true) {
int res = tcputil::poll(events.get(), 1, timeout.count());
if (res == 0) {
      throw std::runtime_error(
""waiting for processes to ""
""connect has timed out"");
} else if (res == -1) {
"
684,"cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    throw std::runtime_error(
""Number of workers for FileStore should be greater than zero"");
}
}
","cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    TORCH_CHECK(false,
""Number of workers for FileStore should be greater than zero"");
}
}
"
685,"invalidArgument(""unsupported layout"");
}
} else {
    throw std::runtime_error(""Invalid backend"");
}
enqueue(work);
","invalidArgument(""unsupported layout"");
}
} else {
    TORCH_CHECK(false, ""Invalid backend"");
}
enqueue(work);
"
686,"work = c10::make_intrusive<AsyncGatherCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    throw std::runtime_error(""Invalid backend"");
}
enqueue(work);
return work;
","work = c10::make_intrusive<AsyncGatherCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    TORCH_CHECK(false, ""Invalid backend"");
}
enqueue(work);
return work;
"
687,"at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    throw std::runtime_error(""input tensor has to be dense"");
}
return tensor;
}
","at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false, ""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    TORCH_CHECK(false, ""input tensor has to be dense"");
}
return tensor;
}
"
688,"ProcessGroupMPI::ProcessGroupMPI(int rank, int size, MPI_Comm pgComm)
: ProcessGroup(rank, size), stop_(false), pgComm_(pgComm) {
if (pgComm_ == MPI_COMM_NULL) {
    throw std::runtime_error(""pgComm_ must not be MPI_COMM_NULL"");
}
// Start the worker thread accepting MPI calls
","ProcessGroupMPI::ProcessGroupMPI(int rank, int size, MPI_Comm pgComm)
: ProcessGroup(rank, size), stop_(false), pgComm_(pgComm) {
if (pgComm_ == MPI_COMM_NULL) {
    TORCH_CHECK(false, ""pgComm_ must not be MPI_COMM_NULL"");
}
// Start the worker thread accepting MPI calls
"
689,""" ran for "",
timeElapsed.count(),
"" milliseconds before timing out."");
        throw std::runtime_error(exceptionMsg);
}
// Check for errors and throw appropriate exception.
checkAndThrowException();
",""" ran for "",
timeElapsed.count(),
"" milliseconds before timing out."");
        TORCH_CHECK(false, exceptionMsg);
}
// Check for errors and throw appropriate exception.
checkAndThrowException();
"
690,"// across distinct GPUs.
void check_gpu_tensors(const std::vector<at::Tensor>& tensors) {
if (tensors.size() == 0) {
    throw std::runtime_error(""Tensor list must be nonempty"");
}
if (tensors.size() > static_cast<size_t>(at::cuda::getNumGPUs())) {
    throw std::runtime_error(
""Tensor list mustn't be larger than the number of available GPUs"");
}
","// across distinct GPUs.
void check_gpu_tensors(const std::vector<at::Tensor>& tensors) {
if (tensors.size() == 0) {
    TORCH_CHECK(false, ""Tensor list must be nonempty"");
}
if (tensors.size() > static_cast<size_t>(at::cuda::getNumGPUs())) {
    TORCH_CHECK(false,
""Tensor list mustn't be larger than the number of available GPUs"");
}
"
691,"TensorOptions linspace_logspace_infer_options(
const Scalar& start,
const Scalar& end,
    const TensorOptions& options) {
  auto result_options = options;
if (start.isComplex() || end.isComplex()) {
    // Since result_options.has_dtype() returns true (dtype is default type),
    // even if the user hasn't specified the dtype.
    // We just check to see if either `start` or `end` is complex,
    // and if the `result_dtype` is not complex (be it default float type or
    // user provided), we cast it to default complex dtype with a Warning!.
    auto result_dtype = c10::typeMetaToScalarType(options.dtype());
    if (!at::isComplexType(result_dtype)) {
      TORCH_WARN(
          ""As either `start` or `stop` is complex, return type will be the complex dtype corresponding to default dtype."",
          ""In future, this may throw an error when a non-complex dtype arg is passed as input along "",
          ""with complex valued start or end value."");
      result_options = result_options.dtype(c10::get_default_complex_dtype());
}
}
  return result_options;
}
} // anonymous namespace
","TensorOptions linspace_logspace_infer_options(
const Scalar& start,
const Scalar& end,
    const TensorOptions& options,
    const char* fn_name) {
if (start.isComplex() || end.isComplex()) {
    const auto default_complex_dtype = c10::get_default_complex_dtype();
    if (options.has_dtype()) {
      auto dtype = c10::typeMetaToScalarType(options.dtype());
      TORCH_CHECK(at::isComplexType(dtype),
          fn_name, "": inferred dtype "", default_complex_dtype, "" can't be safely cast to passed dtype "", dtype);
    } else {
      return options.dtype(default_complex_dtype);
}
}
  return options.has_dtype() ? options : options.dtype(c10::get_default_dtype());
}
} // anonymous namespace
"
692,"C10_CUDA_CHECK(cudaDeviceSynchronize());
}
const char* get_cuda_check_prefix() noexcept {
static char* device_blocking_flag = getenv(""CUDA_LAUNCH_BLOCKING"");
static bool blocking_enabled =
(device_blocking_flag && atoi(device_blocking_flag));
if (blocking_enabled) {
    return ""CUDA error: "";
} else {
    return ""CUDA kernel errors might be ""
           ""asynchronously reported at some other API call,so the ""
           ""stacktrace below might be incorrect. For debugging ""
           ""consider passing CUDA_LAUNCH_BLOCKING=1. CUDA error: "";
}
}
","C10_CUDA_CHECK(cudaDeviceSynchronize());
}
const char* get_cuda_check_suffix() noexcept {
static char* device_blocking_flag = getenv(""CUDA_LAUNCH_BLOCKING"");
static bool blocking_enabled =
(device_blocking_flag && atoi(device_blocking_flag));
if (blocking_enabled) {
    return """";
} else {
    return ""\nCUDA kernel errors might be asynchronously reported at some""
           "" other API call,so the stacktrace below might be incorrect.""
           ""\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."";
}
}
"
693,"\text{stride[1]} \times w + n)
\end{aligned}
    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
","\text{stride[1]} \times w + n)
\end{aligned}
    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides
for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
"
694,"module_states, self.broadcast_bucket_size, authoritative_rank
)
def _ddp_init_helper(self, parameters, expect_sparse_gradient, param_to_name_mapping):
""""""
Initialization helper function that does the following:
","module_states, self.broadcast_bucket_size, authoritative_rank
)
    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f""{str(err_type)}: {err_msg}"")
        raise err_type(err_msg)

def _ddp_init_helper(self, parameters, expect_sparse_gradient, param_to_name_mapping):
""""""
Initialization helper function that does the following:
"
695,"# Ensure we covered all parameters
if len(param_set) != len(param_index_to_param_fqn):
            raise ValueError(
(
""Expected param to name mapping to cover all parameters, but""
f"" got conflicting lengths: {len(param_set)} vs ""
","# Ensure we covered all parameters
if len(param_set) != len(param_index_to_param_fqn):
            self._log_and_throw(
                ValueError,
(
""Expected param to name mapping to cover all parameters, but""
f"" got conflicting lengths: {len(param_set)} vs ""
"
696,"CAFFE_ENFORCE(nnapi_.Compilation_setPreference);
int ret = nnapi_.Compilation_setPreference(compilation,preference);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Compilation_finish(ANeuralNetworksCompilation* compilation) {
CAFFE_ENFORCE(nnapi_.Compilation_finish);
int ret = nnapi_.Compilation_finish(compilation);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Execution_create(ANeuralNetworksCompilation* compilation, ANeuralNetworksExecution** execution) {
CAFFE_ENFORCE(nnapi_.Execution_create);
int ret = nnapi_.Execution_create(compilation,execution);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
void check_Execution_free(ANeuralNetworksExecution* execution) {
","CAFFE_ENFORCE(nnapi_.Compilation_setPreference);
int ret = nnapi_.Compilation_setPreference(compilation,preference);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Compilation_setPreference"", ""failed with error "", ret
  );
return ret;
}
int check_Compilation_finish(ANeuralNetworksCompilation* compilation) {
CAFFE_ENFORCE(nnapi_.Compilation_finish);
int ret = nnapi_.Compilation_finish(compilation);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Compilation_finish"", ""failed with error "", ret
  );
return ret;
}
int check_Execution_create(ANeuralNetworksCompilation* compilation, ANeuralNetworksExecution** execution) {
CAFFE_ENFORCE(nnapi_.Execution_create);
int ret = nnapi_.Execution_create(compilation,execution);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Execution_create"", ""failed with error "", ret
  );
return ret;
}
void check_Execution_free(ANeuralNetworksExecution* execution) {
"
697,"observed.qconfig = other.qconfig
# Set the linear weights
        observed.out_proj.weight = other.out_proj.weight
        observed.out_proj.bias = other.out_proj.bias
if other._qkv_same_embed_dim:
# Use separate params
bias = other.in_proj_bias
","observed.qconfig = other.qconfig
# Set the linear weights
        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969
        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]
        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]
if other._qkv_same_embed_dim:
# Use separate params
bias = other.in_proj_bias
"
698,"def __init__(
self,
remote_device: str,
        module_cls: Type[nn.Module],
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
","def __init__(
self,
remote_device: str,
        module_cls: nn.Module,
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
"
699,"def __init__(
self,
remote_device: str,
        module_cls: nn.Module,
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
","def __init__(
self,
remote_device: str,
        module_cls: Type[nn.Module],
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
"
700,"c10::optional<ModuleInstanceInfo> module_instance_info)
: fn_(fn),
source_range_(std::move(source_range)),
      module_instance_info_(std::move(module_instance_info)) {}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
","c10::optional<ModuleInstanceInfo> module_instance_info)
: fn_(fn),
source_range_(std::move(source_range)),
      module_instance_info_(std::move(module_instance_info)) {
  if (fn_) {
    set_function_name(fn_->name());
  }
}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
"
701,"def __eq__(self, other):
if isinstance(other, _DataPipeType):
            return self.issubtype(other) and other.issubtype(self)
return NotImplemented
def __hash__(self):
return hash(self.param)
def issubtype(self, other):
if isinstance(other, _DataPipeType):
return issubtype(self.param, other.param)
if isinstance(other, type):
","def __eq__(self, other):
if isinstance(other, _DataPipeType):
            return self.param == other.param
return NotImplemented
def __hash__(self):
return hash(self.param)
def issubtype(self, other):
        if isinstance(other.param, _GenericAlias):
            if getattr(other.param, '__origin__', None) is Generic:
                return True
if isinstance(other, _DataPipeType):
return issubtype(self.param, other.param)
if isinstance(other, type):
"
702,"#include <torch/csrc/jit/mobile/module.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/runtime/jit_exception.h>
","#include <torch/csrc/jit/mobile/module.h>
#include <torch/csrc/jit/backends/backend_exception.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/runtime/jit_exception.h>
"
703,"#include <c10/util/Exception.h>
#include <torch/csrc/jit/backends/backend_debug_handler.h>
#include <torch/csrc/jit/frontend/source_range.h>
#include <torch/csrc/jit/ir/attributes.h>
#include <torch/csrc/jit/ir/ir.h>
","#include <c10/util/Exception.h>
#include <torch/csrc/jit/backends/backend_debug_handler.h>
#include <torch/csrc/jit/backends/backend_debug_info.h>
#include <torch/csrc/jit/frontend/source_range.h>
#include <torch/csrc/jit/ir/attributes.h>
#include <torch/csrc/jit/ir/ir.h>
"
704,"elements,
qn_cache,
debug_info_elements,
      debug_handle_manager);
}
void SetExportModuleExtraFilesHook(ExportModuleExtraFilesHook hook) {
","elements,
qn_cache,
debug_info_elements,
      debug_info_recorder);
}
void SetExportModuleExtraFilesHook(ExportModuleExtraFilesHook hook) {
"
705,"storage = loaded_storages[key]
return storage
elif typename == ""reduce_package"":
reduce_id, func, args = data
if reduce_id not in loaded_reduces:
loaded_reduces[reduce_id] = func(self, *args)
","storage = loaded_storages[key]
return storage
elif typename == ""reduce_package"":
                # to fix BC breaking change, objects on this load path
                # will be loaded multiple times erroneously
                if len(data) == 2:
                    func, args = data
                    return func(self, *args)
reduce_id, func, args = data
if reduce_id not in loaded_reduces:
loaded_reduces[reduce_id] = func(self, *args)
"
706,"adjust_observers_for_cat(node, model, modules)
else:  # output
                    prev_node = node.args[0]
                    if isinstance(prev_node, Node):
                        if is_activation_post_process_node(prev_node, modules):
                            prev_node = prev_node.args[0]
                    elif isinstance(prev_node, dict):
                        # get first value
                        prev_node = list(prev_node.items())[0][1]
                        assert isinstance(prev_node, Node)
                        if is_activation_post_process_node(prev_node, modules):
                            prev_node = prev_node.args[0]

                    # we check for node again because some graphs can return
                    # None
                    if isinstance(prev_node, Node):
                        prev_node_qconfig = qconfig_map.get(prev_node.name, None)
                        # this modifies node inplace
                        maybe_insert_input_observers_for_node(
                            node, prev_node_qconfig, model, modules, graph,
                            node_name_to_target_dtype,
                            qhandler, prepare_custom_config_dict)
#
# After this point, the current node has input and output observers
","adjust_observers_for_cat(node, model, modules)
else:  # output
                    maybe_insert_observers_before_graph_output(
                        node, output_quantized_idxs,
                        node_name_to_target_dtype, qconfig_map,
                        model, modules, graph)
#
# After this point, the current node has input and output observers
"
707,"Returns:
Module: self
""""""
self.training = mode
for module in self.children():
module.train(mode)
","Returns:
Module: self
""""""
        if not isinstance(mode, bool):
            raise ValueError(""training mode is expected to be boolean"")
self.training = mode
for module in self.children():
module.train(mode)
"
708,"TORCH_SELECTIVE_SCHEMA(""aten::dequantize.any(Any tensors) -> Any""),
[](Stack* stack) { dequantize(*stack); },
aliasAnalysisFromSchema()),
DEFINE_UNARY_OP_WITH_COMPLEX(aten::log, std::log(a), float, float),
DEFINE_STRING_OP(aten::add, a + b, str),
DEFINE_COMPARISON_OP_WITH_COMPLEX(aten::eq, a == b),
","TORCH_SELECTIVE_SCHEMA(""aten::dequantize.any(Any tensors) -> Any""),
[](Stack* stack) { dequantize(*stack); },
aliasAnalysisFromSchema()),
     OperatorGenerator(
         TORCH_SELECTIVE_SCHEMA(""aten::sorted.str(str[](a) input) -> (str[])""),
         listCopyAndSort<std::string>,
         aliasAnalysisFromSchema()),
DEFINE_UNARY_OP_WITH_COMPLEX(aten::log, std::log(a), float, float),
DEFINE_STRING_OP(aten::add, a + b, str),
DEFINE_COMPARISON_OP_WITH_COMPLEX(aten::eq, a == b),
"
709,"def __exit__(self, exc_type, exc_val, exc_tb):
if not self.enabled:
return
        if torch.cuda.is_available():
torch.cuda.synchronize()
if self.kineto_activities:
self.kineto_results = torch.autograd._disable_profiler()
","def __exit__(self, exc_type, exc_val, exc_tb):
if not self.enabled:
return
        if self.use_cuda:
torch.cuda.synchronize()
if self.kineto_activities:
self.kineto_results = torch.autograd._disable_profiler()
"
710,"} else {
result = s_max / s_min;
}
    return result;
}
// ord == 1 ord == inf
","} else {
result = s_max / s_min;
}
    // squeeze the result for NumPy compatibility
    return result.squeeze(-1);
}
// ord == 1 ord == inf
"
711,"https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem
"""""")
det = _add_docstr(_linalg.linalg_det, r""""""
linalg.det(A, *, out=None) -> Tensor
","https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem
"""""")
inv_ex = _add_docstr(_linalg.linalg_inv_ex, r""""""
linalg.inv_ex(input, *, check_errors=False, out=None) -> (Tensor, Tensor)

Computes the inverse of a square matrix if it is invertible.

Returns a namedtuple ``(inverse,info)``. ``inverse`` contains the result of inverting the input matrix.
``info`` stores the LAPACK error codes.

If :attr:`input` is not an invertible matrix, or if it's a batch of matrices
and one or more of them is not an invertible matrix,
then ``info`` stores a positive integer for the corresponding matrix.
The positive integer indicates the diagonal element of the LU decomposition of the input matrix that is exactly zero.
``info`` filled with zeros indicates that the inversion was successful.
If ``check_errors=True`` and ``info`` contains positive integers, then a RuntimeError is thrown.

Supports inputs of float, double, cfloat and cdouble dtypes.
Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions.

.. note:: Given inputs on a CUDA device, this function may synchronize that device with the CPU.

.. warning:: This function is ""experimental"" and it may change in a future PyTorch release.

.. seealso::

        :func:`torch.linalg.inv` is a NumPy compatible variant that always checks for errors.

Args:
    input (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions
                    consisting of square matrices.
    check_errors (bool, optional): controls whether to check the content of ``info``. Default: `False`.

Keyword args:
    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.

Examples::

    >>> a = torch.randn(3, 3)
    >>> inverse, info = torch.linalg.inv_ex(a)
    >>> a
    tensor([[-0.0464,  0.2302, -1.3568],
            [-0.5437, -1.2301, -0.6918],
            [ 0.2328, -1.4910, -0.3003]])
    >>> l
    tensor([[ 0.4320, -1.3653,  1.1931],
            [ 0.2117, -0.2152, -0.4605],
            [-0.7159,  0.0102, -0.1190]])
    >>> info
    tensor(0, dtype=torch.int32)

"""""")

det = _add_docstr(_linalg.linalg_det, r""""""
linalg.det(A, *, out=None) -> Tensor
"
712,"}
}
string mapOnnxStatusToString(onnxStatus status) {
switch (status) {
case ONNXIFI_STATUS_SUCCESS:
","}
}
string mapOnnxStateToString(onnxEventState state) {
  switch (state) {
    case ONNXIFI_EVENT_STATE_NONSIGNALLED:
      return ""ONNXIFI_EVENT_STATE_NONSIGNALLED"";
    default:
      return ""ONNXIFI_EVENT_STATE_STRING_NOT_MAPPED"";
  }
}

string mapOnnxStatusToString(onnxStatus status) {
switch (status) {
case ONNXIFI_STATUS_SUCCESS:
"
713,"} else if (r.idx == 3) {
return new_with_tensor(options, scalar_type, r.tensor(0));
} else if (r.idx == 4) {
PyObject* arg = r.pyobject(0);
auto deviceOptional = r.deviceOptional(1);
check_legacy_ctor_device(dispatch_key, deviceOptional);
","} else if (r.idx == 3) {
return new_with_tensor(options, scalar_type, r.tensor(0));
} else if (r.idx == 4) {
    TORCH_CHECK(false, ""Legacy tensor constructor of the form torch.Tensor(tensor, device=device) "" \
                ""is not supported.  Use torch.tensor(...) or torch.as_tensor(...) instead."");
  } else if (r.idx == 5) {
PyObject* arg = r.pyobject(0);
auto deviceOptional = r.deviceOptional(1);
check_legacy_ctor_device(dispatch_key, deviceOptional);
"
714,"""new(Storage storage)"",
""new(*, int64_t cdata)|hidden"",
""new(Tensor other)"",  // this doesn't have a dtype/device because it creates an alias.
""new(IntArrayRef size, *, Device? device=None)"",
""new(PyObject* data, *, Device? device=None)"",
});
","""new(Storage storage)"",
""new(*, int64_t cdata)|hidden"",
""new(Tensor other)"",  // this doesn't have a dtype/device because it creates an alias.
    ""new(Tensor other, *, Device? device=None)|hidden"",  // prevent Tensor matching with IntArrayRef, PyObject*
""new(IntArrayRef size, *, Device? device=None)"",
""new(PyObject* data, *, Device? device=None)"",
});
"
715,"fqn = f""{module_name}.{param_name}""
# Bypass ignored parameters since those are not reduced by DDP
# to begin with.
                if fqn not in self.parameters_to_ignore:
if param not in param_set:
raise ValueError(
f""Param with name {fqn} found in module parameters, but not DDP parameters.""
)
param_index = param_to_param_index[param]
param_index_to_param_fqn[param_index] = fqn
","fqn = f""{module_name}.{param_name}""
# Bypass ignored parameters since those are not reduced by DDP
# to begin with.
                if fqn not in self.parameters_to_ignore and param.requires_grad:
if param not in param_set:
raise ValueError(
f""Param with name {fqn} found in module parameters, but not DDP parameters.""
                            "" This indicates a bug in DDP, please report an issue to PyTorch.""
)
param_index = param_to_param_index[param]
param_index_to_param_fqn[param_index] = fqn
"
716,"m.impl(""var.names_dim"", CppFunction::makeFallthrough());
m.impl(""var.names_out"", CppFunction::makeFallthrough());
m.impl(""var.out"", CppFunction::makeFallthrough());
m.impl(""var_mean"", CppFunction::makeFallthrough());
m.impl(""var_mean.dim"", CppFunction::makeFallthrough());
m.impl(""var_mean.names_dim"", CppFunction::makeFallthrough());
m.impl(""zero_"", CppFunction::makeFallthrough());
m.impl(""zeros_like"", CppFunction::makeFallthrough());
","m.impl(""var.names_dim"", CppFunction::makeFallthrough());
m.impl(""var.names_out"", CppFunction::makeFallthrough());
m.impl(""var.out"", CppFunction::makeFallthrough());
  m.impl(""var.correction"", CppFunction::makeFallthrough());
  m.impl(""var.correction_out"", CppFunction::makeFallthrough());
  m.impl(""var.correction_names"", CppFunction::makeFallthrough());
  m.impl(""var.correction_names_out"", CppFunction::makeFallthrough());
m.impl(""var_mean"", CppFunction::makeFallthrough());
m.impl(""var_mean.dim"", CppFunction::makeFallthrough());
m.impl(""var_mean.names_dim"", CppFunction::makeFallthrough());
  m.impl(""var_mean.correction"", CppFunction::makeFallthrough());
  m.impl(""var_mean.correction_names"", CppFunction::makeFallthrough());
m.impl(""zero_"", CppFunction::makeFallthrough());
m.impl(""zeros_like"", CppFunction::makeFallthrough());
"
717,"});
}
static void std_var_kernel_impl(TensorIterator &iter, bool unbiased, bool take_sqrt) {
AT_DISPATCH_FLOATING_TYPES_AND_HALF(iter.dtype(), ""std_cpu"", [&] {
binary_kernel_reduce(
      iter,
      WelfordOps<scalar_t, double, int64_t, double, std::tuple<scalar_t, scalar_t>> { unbiased, take_sqrt },
      WelfordData<double, int64_t, double>()
    );
});
}
","});
}
static void std_var_kernel_impl(TensorIterator& iter, int64_t correction, bool take_sqrt) {
AT_DISPATCH_FLOATING_TYPES_AND_HALF(iter.dtype(), ""std_cpu"", [&] {
binary_kernel_reduce(
        iter,
        WelfordOps<
            scalar_t,
            double,
            int64_t,
            double,
            std::tuple<scalar_t, scalar_t>>{correction, take_sqrt},
        WelfordData<double, int64_t, double>());
});
}
"
718,"}
}
Tensor var_backward(const Tensor & grad, const Tensor & self, bool unbiased) {
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (self.numel() - unbiased)) * grad * (self - self.mean());
}
Tensor var_backward(Tensor grad, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  if (self.dim() == 0) {
    return var_backward(grad, self, unbiased);
}
if (!keepdim && self.dim() > 1) {
grad = unsqueeze_multiple(grad, dim, self.sizes().size());
}
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (_safe_size(self.sizes(), dim) - unbiased)) * grad * (self - self.mean(dim, true));
}
Tensor std_backward(const Tensor & result, const Tensor & grad, const Tensor & self, bool unbiased) {
  return var_backward((grad / (result * 2)).masked_fill_(result == 0, 0), self, unbiased);
}

Tensor std_backward(const Tensor & result, Tensor grad, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  return var_backward((grad / (result * 2)).masked_fill_(result == 0, 0), self, dim, unbiased, keepdim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, IntArrayRef dim, bool keepdim) {
return sum_backward(grad, sizes, dim, keepdim) / _safe_size(sizes, dim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, int numel) {
return grad.expand(sizes) / numel;
}
Tensor var_std_mean_backward(const variable_list& grads, const Tensor & self, const Tensor & r1, const Tensor & r2, IntArrayRef dim, bool unbiased, bool keepdim, bool is_std) {
  Tensor grad;
  if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, dim, unbiased, keepdim) : var_backward(grads[0], self, dim, unbiased, keepdim);
  }
  if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), dim, keepdim);
    grad = grads[0].defined() ? grad + mean_grad : mean_grad;
}
  return grad;
}
Tensor var_std_mean_backward(const variable_list& grads, const Tensor & self, const Tensor & r1, const Tensor & r2, bool unbiased, bool is_std) {
Tensor grad;
if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, unbiased) : var_backward(grads[0], self, unbiased);
}
if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), self.numel());
    grad = grads[0].defined() ? grad + mean_grad : mean_grad;
}
return grad;
}
","}
}
static Tensor var_backward(const Tensor & grad, const Tensor & self, int64_t correction) {
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (self.numel() - correction)) * grad * (self - self.mean());
}
Tensor var_backward(Tensor grad, const Tensor& self, c10::optional<IntArrayRef> dim_opt,
    c10::optional<int64_t> correction_opt, bool keepdim) {
  auto correction = correction_opt.value_or(1);
  if (self.dim() == 0 || !dim_opt.has_value()) {
    return var_backward(grad, self, correction);
}
  auto dim = dim_opt.value();
if (!keepdim && self.dim() > 1) {
grad = unsqueeze_multiple(grad, dim, self.sizes().size());
}
  const int64_t dof = _safe_size(self.sizes(), dim) - correction;
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / dof) * grad * (self - self.mean(dim, /*keepdim=*/true));
}
Tensor std_backward(
    const Tensor& result, const Tensor& grad, const Tensor& self,
    c10::optional<IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  auto grad_var = (grad / (result * 2)).masked_fill_(result == 0, 0);
  return var_backward(grad_var, self, dim, correction, keepdim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, IntArrayRef dim, bool keepdim) {
return sum_backward(grad, sizes, dim, keepdim) / _safe_size(sizes, dim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, int64_t numel) {
return grad.expand(sizes) / numel;
}
static Tensor mean_backward(
    const Tensor& grad, const IntArrayRef sizes, int64_t numel,
    c10::optional<IntArrayRef> dim, bool keepdim) {
  if (dim.has_value()) {
    return mean_backward(grad, sizes, *dim, keepdim);
  } else {
    return mean_backward(grad, sizes, numel);
}
}
Tensor var_std_mean_backward(
    const variable_list& grads, const Tensor& self, const Tensor& r1,
    const Tensor& r2, c10::optional<IntArrayRef> dim,
    c10::optional<int64_t> correction, bool keepdim, bool is_std) {
Tensor grad;
if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, dim, correction, keepdim)
                  : var_backward(grads[0], self, dim, correction, keepdim);
}
if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), self.numel(), dim, keepdim);
    grad = grad.defined() ? grad + mean_grad : mean_grad;
}
return grad;
}
"
719,"unbiased: bool,
keepdim: bool):
def backward(grad_output):
                grad_self = AD_var_backward_1(grad_output, self, dim, unbiased, keepdim)
return grad_self, None, None, None
return torch.var(self, dim, unbiased, keepdim), backward
def tanh(self):
output = torch.tanh(self)
def backward(grad_output):
","unbiased: bool,
keepdim: bool):
def backward(grad_output):
                correction = AD_bool_to_int(unbiased)
                grad_self = AD_var_backward_1(grad_output, self, dim, correction, keepdim)
return grad_self, None, None, None
return torch.var(self, dim, unbiased, keepdim), backward
        def var_2(self,
                  dim: Optional[List[int]],
                  *,
                  correction: Optional[int],
                  keepdim: bool):
            def backward(grad_output):
                grad_self = AD_var_backward_2(grad_output, self, dim, correction, keepdim)
                return grad_self, None, None, None

            return torch.var(self, dim, correction=correction, keepdim=keepdim), backward

def tanh(self):
output = torch.tanh(self)
def backward(grad_output):
"
720,"module_name = getattr(obj, ""__module__"", None)
if module_name is not None:
return module_name
        else:
            return ""__main__""
class _SysImporter(Importer):
","module_name = getattr(obj, ""__module__"", None)
if module_name is not None:
return module_name

        # Protect the iteration by using a list copy of self.modules against dynamic
        # modules that trigger imports of other modules upon calls to getattr.
        for module_name, module in self.modules.copy().items():
            if (
                module_name == ""__main__""
                or module_name == ""__mp_main__""  # bpo-42406
                or module is None
            ):
                continue
            try:
                if _getattribute(module, name)[0] is obj:
                    return module_name
            except AttributeError:
                pass

        return ""__main__""
class _SysImporter(Importer):
"
721,"#include <torch/csrc/jit/mobile/import.h>
#include <ATen/core/ivalue.h>
#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/interpreter.h>
","#include <torch/csrc/jit/mobile/import.h>
#include <ATen/core/ivalue.h>
#include <c10/util/ScopeExit.h>
#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/interpreter.h>
"
722,"return callee_;
}
std::vector<InlinedCallStackEntry> InlinedCallStack::vec() {
std::vector<InlinedCallStackEntry> r;
c10::optional<InlinedCallStackPtr> current = intrusive_from_this();
","return callee_;
}
void InlinedCallStack::setCallee(c10::optional<InlinedCallStackPtr> callee) {
  callee_ = std::move(callee);
}

c10::optional<ModuleInstanceInfo> InlinedCallStack::module_instance() const {
  return module_instance_info_;
}

SourceRange InlinedCallStack::source_range() const {
  return source_range_;
}

std::vector<InlinedCallStackEntry> InlinedCallStack::vec() {
std::vector<InlinedCallStackEntry> r;
c10::optional<InlinedCallStackPtr> current = intrusive_from_this();
"
723,"""But the model version is "",
model_version);
  bool has_debug_info = debug_info_vals.has_value();
  if (has_debug_info) {
TORCH_CHECK(
        debug_info_vals->size() == vals.size(),
""The numbers of bytecode values and debug info values do not match."");
}
","""But the model version is "",
model_version);
  bool has_debug_handles = debug_handles.has_value();
  if (has_debug_handles) {
TORCH_CHECK(
        debug_handles->size() == vals.size(),
""The numbers of bytecode values and debug info values do not match."");
}
"
724,"return Tup(std::move(ivalue_entries));
}
std::string getModulePath(Node* node, const std::string& root_scope_string) {
  constexpr size_t kFunction = 0;
  constexpr size_t kModuleInstanceInfo = 2;

  if (!node->callstack()) {
    return root_scope_string + "".forward"";
  } else {
    std::string module_info = root_scope_string;
    auto callstack_ptr = *(node->callstack());
    const auto& vec = callstack_ptr->vec();

    for (const auto& element : vec) {
      const auto& opt_module_instance_info =
          std::get<kModuleInstanceInfo>(element);
      if (opt_module_instance_info.has_value()) {
        const auto& module_instance_info = opt_module_instance_info.value();
        if (module_instance_info.class_type()) {
          const auto& class_type = module_instance_info.class_type();
          const auto& instance_name = module_instance_info.instance_name();
          auto type_name = class_type->name()->qualifiedName();
          type_name = type_name.substr(type_name.find_last_of('.') + 1);
          module_info.append(""."")
              .append(instance_name)
              .append(""("")
              .append(type_name)
              .append("")"")
              .append(""."")
              .append(std::get<kFunction>(element)->name());
        } else {
          module_info += "".(UNKNOWN_INSTANCE(UNKNOWN_TYPE)"";
        }
      } else {
        module_info += "".(UNKNOWN_INSTANCE(UNKNOWN_TYPE)"";
      }
    }

    return module_info;
  }
}

std::string getModuleTypeName(const Module& module, const std::string& prefix) {
  std::string moduleType = module.type()->str();
  size_t lastDotIndex = moduleType.rfind('.');
  if (lastDotIndex != std::string::npos) {
    moduleType = moduleType.substr(lastDotIndex + 1);
  }
  return prefix + ""("" + moduleType + "")"";
}

std::pair<IValue, c10::optional<IValue>> getFunctionTuple(
const Module& module,
const Function& func,
    const bool save_mobile_debug_info,
    const SourceRangeTagMap& source_range_tag_map) {
auto graph = func.graph()->copy();
Inline(*graph);
","return Tup(std::move(ivalue_entries));
}
std::pair<IValue, IValue> getFunctionTuple(
const Module& module,
const Function& func,
    BackendDebugHandleManager& debug_handle_manager) {
auto graph = func.graph()->copy();
Inline(*graph);
"
725,"void writeByteCode(const Module& module, const bool save_mobile_debug_info) {
std::vector<c10::IValue> elements;
elements.emplace_back(
static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
    c10::optional<std::vector<c10::IValue>> debug_info_elements;
    if (save_mobile_debug_info) {
      debug_info_elements = std::vector<c10::IValue>();
      debug_info_elements->emplace_back(
          static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
    }
moduleMethodsTuple(
        module,
        elements,
        debug_info_elements,
        save_mobile_debug_info,
        source_range_tags_);
auto telements = Tup(std::move(elements));
writeArchive(""bytecode"", telements);
if (save_mobile_debug_info) {
      auto debug_info_telements = Tup(std::move(debug_info_elements.value()));
      writeArchive(""mobile_debug"", debug_info_telements);
}
}
","void writeByteCode(const Module& module, const bool save_mobile_debug_info) {
std::vector<c10::IValue> elements;
    BackendDebugHandleManager debug_handle_manager;
elements.emplace_back(
static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
    std::vector<c10::IValue> debug_info_elements;
    // Always save debug handles
    debug_info_elements.emplace_back(
        static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
moduleMethodsTuple(
        module, elements, debug_info_elements, debug_handle_manager);
auto telements = Tup(std::move(elements));
writeArchive(""bytecode"", telements);
    auto debug_info_telements = Tup(std::move(debug_info_elements));

    // At the moment keeping this feature experimental
    // since we have not evaluated how this affect model size
    // and we have not build any utility to strip off debug info
    // when desired
    // TODO: Build utility to strip off debug map. It should also do the
    // same for debug_pkl files
if (save_mobile_debug_info) {
      // Note that stripping off debug map will not strip off
      // debug handles.
      // The reason we save debug handles conditionally is so that
      // we dont end up with a model that has debug handles but has not
      // debug map to correlate debug handels with.
      // Once we have a model with both handles and debug map, we can
      // strip off debug map and have a lean model served to production.
      // If exception ocurrs we have a model with debug map that can be
      // used to symbolicate debug handles
      writeArchive(""mobile_debug_handles"", debug_info_telements);
      // Now get the debug-handles-to-inlined-cs-ptr-map
      // And serialize that in a separate archive
      auto debug_handle_cs_ptr_map = debug_handle_manager.getCallStackPtrMap();
      CallStackDebugInfoPickler cs_debug_info_pickler;
      auto cs_data = cs_debug_info_pickler.pickle(
          debug_handle_cs_ptr_map, source_range_tags_);
      // Write out map: [debug-handle, {source range, InlinedCallStack}]
      std::string filename = ""callstack_debug_map.pkl"";
      static constexpr size_t kMinToCompress = 200;
      writer_.writeRecord(
          filename,
          cs_data.data(),
          cs_data.size(),
          cs_data.size() > kMinToCompress /*compress*/);
}
}
"
726,""""""".format(**common_args))
add_docstr(torch.add, r""""""
add(input, other, *, out=None)
Adds the scalar :attr:`other` to each element of the input :attr:`input`
and returns a new resulting tensor.
",""""""".format(**common_args))
add_docstr(torch.add, r""""""
add(input, other, *, out=None) -> Tensor
Adds the scalar :attr:`other` to each element of the input :attr:`input`
and returns a new resulting tensor.
"
727,""""""".format(**single_dim_common))
add_docstr(torch.mul, r""""""
mul(input, other, *, out=None)
Multiplies each element of the input :attr:`input` with the scalar
:attr:`other` and returns a new resulting tensor.
",""""""".format(**single_dim_common))
add_docstr(torch.mul, r""""""
mul(input, other, *, out=None) -> Tensor
Multiplies each element of the input :attr:`input` with the scalar
:attr:`other` and returns a new resulting tensor.
"
728,"#include <ATen/Config.h>
#include <ATen/cuda/CUDAConfig.h>
#include <ATen/cuda/CUDAEvent.h>
#include <ATen/cuda/Exceptions.h>
#include <ATen/InitialTensorOptions.h>
#include <ATen/MatrixRef.h>
","#include <ATen/Config.h>
#include <ATen/cuda/CUDAConfig.h>
#include <ATen/cuda/CUDAEvent.h>
#include <ATen/cuda/CUDAGraphsUtils.cuh>
#include <ATen/cuda/Exceptions.h>
#include <ATen/InitialTensorOptions.h>
#include <ATen/MatrixRef.h>
"
729,"True
"""""")
inv = _add_docstr(_linalg.linalg_inv, r""""""
linalg.inv(A, *, out=None) -> Tensor
","True
"""""")
cholesky_ex = _add_docstr(_linalg.linalg_cholesky_ex, r""""""
linalg.cholesky_ex(input, *, check_errors=False, out=None) -> (Tensor, Tensor)

Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.

Supports inputs of float, double, cfloat and cdouble dtypes.
Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions.

Returns a namedtuple ``(L,info)``. ``L`` contains the result of the Cholesky decomposition.
``info`` stores the LAPACK error codes.

If :attr:`input` is not a Hermitian positive-definite matrix, or if it's a batch of matrices
and one or more of them is not a Hermitian positive-definite matrix,
then ``info`` stores a positive integer for the corresponding matrix.
The positive integer indicates the order of the leading minor that is not positive-definite,
and the decomposition could not be completed.
``info`` filled with zeros indicates that the decomposition was successful.
If ``check_errors=True`` and ``info`` contains positive integers, then a RuntimeError is thrown.

.. note:: Given inputs on a CUDA device, this function may synchronize that device with the CPU.

.. warning:: This function is ""experimental"" and it may change in a future PyTorch release.

.. seealso::
        :func:`torch.linalg.cholesky` is a NumPy compatible variant that always checks for errors.

Args:
    input (Tensor): the Hermitian `n \times n` matrix or the batch of such matrices of size
                    `(*, n, n)` where `*` is one or more batch dimensions.
    check_errors (bool, optional): controls whether to check the content of ``infos``. Default: `False`.

Keyword args:
    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.

Examples::

    >>> a = torch.randn(2, 2, dtype=torch.complex128)
    >>> a = a @ a.t().conj()  # creates a Hermitian positive-definite matrix
    >>> l, info = torch.linalg.cholesky_ex(a)
    >>> a
    tensor([[ 2.3792+0.0000j, -0.9023+0.9831j],
            [-0.9023-0.9831j,  0.8757+0.0000j]], dtype=torch.complex128)
    >>> l
    tensor([[ 1.5425+0.0000j,  0.0000+0.0000j],
            [-0.5850-0.6374j,  0.3567+0.0000j]], dtype=torch.complex128)
    >>> info
    tensor(0, dtype=torch.int32)

"""""")

inv = _add_docstr(_linalg.linalg_inv, r""""""
linalg.inv(A, *, out=None) -> Tensor
"
730,"return self;
}
Tensor detach(const Tensor & self) {
RECORD_FUNCTION(""detach"", std::vector<c10::IValue>({self}));
  std::function<at::Tensor(const at::Tensor&)> func=nullptr;
  auto result = as_view(/* base */ self, /* output */ self, /* is_bw_differentiable */ false,
                        /* is_fw_differentiable */ true, /* view_func */ func, /* creation_meta */ CreationMeta::DEFAULT,
                        /*allow_tensor_metadata_change=*/false);
namedinference::propagate_names(result, self);
// detach only backward gradients for both primal and tangent
","return self;
}
Tensor detach(c10::DispatchKeySet ks, const Tensor & self) {
  auto& self_ = unpack(self, ""self"", 0);
RECORD_FUNCTION(""detach"", std::vector<c10::IValue>({self}));
  auto result = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::detach(ks & c10::after_autograd_keyset, self_);
  })();
namedinference::propagate_names(result, self);
// detach only backward gradients for both primal and tangent
"
731,"if line:
code.append(""  %s"" % (line.strip()))
print(""\n"".join(code))
os.kill(os.getpid(), signal.SIGINT)
","if line:
code.append(""  %s"" % (line.strip()))
            # Log also with logger, as it is comment practice to suppress print().
print(""\n"".join(code))
            log.info(""\n"".join(code))
os.kill(os.getpid(), signal.SIGINT)
"
732,"for (auto& t :
utils::unflatten_sparse_tensors(inds, vals, chunk.tensors)) {
// See NOTE [ Version Counter in comm.*_coalesced ]
Variable var = t;
device_outputs.push_back(make_variable(var.tensor_data(), false));
}
","for (auto& t :
utils::unflatten_sparse_tensors(inds, vals, chunk.tensors)) {
// See NOTE [ Version Counter in comm.*_coalesced ]
          // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
Variable var = t;
device_outputs.push_back(make_variable(var.tensor_data(), false));
}
"
733,"for (auto& t :
utils::unflatten_dense_tensors(results[i], chunk.tensors)) {
// See NOTE [ Version Counter in comm.*_coalesced ]
Variable var = t;
device_outputs.push_back(make_variable(var.tensor_data(), false));
}
","for (auto& t :
utils::unflatten_dense_tensors(results[i], chunk.tensors)) {
// See NOTE [ Version Counter in comm.*_coalesced ]
          // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
Variable var = t;
device_outputs.push_back(make_variable(var.tensor_data(), false));
}
"
734,"};
struct Benchmark {
Benchmark(
torch::deploy::InterpreterManager& manager,
size_t n_threads,
std::string strategy,
std::string file_to_run,
size_t n_seconds = 5)
: manager_(manager),
n_threads_(n_threads),
","};
struct Benchmark {
  // NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)
Benchmark(
torch::deploy::InterpreterManager& manager,
size_t n_threads,
std::string strategy,
      // NOLINTNEXTLINE(modernize-pass-by-value)
std::string file_to_run,
      // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
size_t n_seconds = 5)
: manager_(manager),
n_threads_(n_threads),
"
735,"#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <iostream>
#include <assert.h>
#include <pybind11/embed.h>
#include <stdio.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>
","#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <iostream>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <pybind11/embed.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <stdio.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>
"
736,"const WorkerInfo& ProcessGroupAgent::getWorkerInfo(worker_id_t id) const {
TORCH_CHECK(
id >= 0 && id < allWorkerInfo_.size(), ""Invalid destination: "", id);
return allWorkerInfo_[id];
}
","const WorkerInfo& ProcessGroupAgent::getWorkerInfo(worker_id_t id) const {
TORCH_CHECK(
      // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
id >= 0 && id < allWorkerInfo_.size(), ""Invalid destination: "", id);
return allWorkerInfo_[id];
}
"
737,"}
index_map_[GpuLower::lowerValue(*(input_ids.end() - 1))
>as<kir::IterDomain>()] = out_ind;
return;
}
","}
index_map_[GpuLower::lowerValue(*(input_ids.end() - 1))
                   // NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)
return;
}
"
738,"Val* IterDomain::extent() const {
TORCH_CHECK(isLoweredVal(extent_));
if (isThread()) {
if (extent_->getValType() == ValType::KirScalar) {
if (extent_->as<kir::Int>()->isConst()) {
return extent_;
","Val* IterDomain::extent() const {
TORCH_CHECK(isLoweredVal(extent_));
if (isThread()) {
    // NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
if (extent_->getValType() == ValType::KirScalar) {
if (extent_->as<kir::Int>()->isConst()) {
return extent_;
"
739,"// based on the NVRTC version
major = prop->major;
minor = prop->minor;
if (nvrtc_major <= 7 && prop->major > 5) { // 7 supports 2-5.x
major = 5;
minor = 0;
} else if (nvrtc_major <= 8 && prop->major > 6) { // 8 supports 2-6.x
major = 6;
minor = 0;
} else if (nvrtc_major <= 9 && prop->major >= 7) { // 9 supports 3-7.2
major = 7;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
minor = (prop->major == 7 && prop->minor <= 2) ? prop->minor : 0;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_major <= 10 && prop->major >= 7) { // 10 supports 3-7.5
major = 7;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
minor = (prop->major == 7 && prop->minor <= 5) ? prop->minor : 0;
","// based on the NVRTC version
major = prop->major;
minor = prop->minor;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
if (nvrtc_major <= 7 && prop->major > 5) { // 7 supports 2-5.x
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
major = 5;
minor = 0;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_major <= 8 && prop->major > 6) { // 8 supports 2-6.x
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
major = 6;
minor = 0;
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_major <= 9 && prop->major >= 7) { // 9 supports 3-7.2
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
major = 7;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
minor = (prop->major == 7 && prop->minor <= 2) ? prop->minor : 0;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
} else if (nvrtc_major <= 10 && prop->major >= 7) { // 10 supports 3-7.5
    // NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
major = 7;
// NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)
minor = (prop->major == 7 && prop->minor <= 5) ? prop->minor : 0;
"
740,"throw std::runtime_error(""support only 3D gpu_block_index"");
}
const Expr* prev = nullptr;
if (gpu_block_extents_.size() <= gpu_block_index) {
gpu_block_extents_.resize(gpu_block_index + 1);
} else {
","throw std::runtime_error(""support only 3D gpu_block_index"");
}
const Expr* prev = nullptr;
    // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
if (gpu_block_extents_.size() <= gpu_block_index) {
gpu_block_extents_.resize(gpu_block_index + 1);
} else {
"
741,"scalar = opterm->scalar();
variables = opterm->variables();
}
if (expr->isConstant()) {
scalar = combine_scalars(scalar, expr);
} else {
","scalar = opterm->scalar();
variables = opterm->variables();
}
    // NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
if (expr->isConstant()) {
scalar = combine_scalars(scalar, expr);
} else {
"
742,"return std::make_tuple(hx, cx);
}
/**
 * Note [DropoutState and CUDA graph capture]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * (1) Telling a capturing stream to wait on an event recorded in a non-capturing stream is an error.
 * (2) Telling a non-capturing stream to wait on an event recorded during capture is also an error.
 *
 * So DropoutState's usage syncs could error if an RNN with dropout is called in an uncaptured region
 * then called in a captured region (triggering 1), or called in a captured region then called
 # in an uncaptured region (triggering 2).
 *
 * To prevent 1 and 2, lock() only syncs on the last usage event if it was recorded in the same
 * capture state as the current state (which also means the same graph, if capture is in progress).
 *
 * The solution should be safe as long as capture obeys the following restrictions:
 *  - Only one capture may be underway at a time in a given process.
 *  - While a capture is underway, no calls to eager ops on noncapturing streams (on any thread)
 *    may interleave with the captured ops.
 *
 * TODO: As people experiment with capture, keep an eye out for use cases that might need to
 * relax those restrictions.
 *
 * See https://github.com/pytorch/pytorch/pull/56433 for more discussion.
 */

struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
","return std::make_tuple(hx, cx);
}
struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
"
743,"# will write to the parent's error file. In this case just log the
# original error file contents and overwrite the error file.
self._rm(my_error_file)
            self._write_error_file(my_error_file , json.dumps(rootcause_error))
log.info(f""dumped error file to parent's {my_error_file}"")
else:
log.error(
","# will write to the parent's error file. In this case just log the
# original error file contents and overwrite the error file.
self._rm(my_error_file)
            self._write_error_file(my_error_file, json.dumps(rootcause_error))
log.info(f""dumped error file to parent's {my_error_file}"")
else:
log.error(
"
744,"import torch
import torch.fx
from torch.fx.node import Node, map_aggregate
from torch.fx.operator_schemas import create_type_hint
from typing import Any, Tuple, NamedTuple, Optional
class TensorMetadata(NamedTuple):
","import torch
import torch.fx
from torch.fx.node import Node, map_aggregate
from typing import Any, Tuple, NamedTuple, Optional
class TensorMetadata(NamedTuple):
"
745,"for node_a_arg in node_a.args[num_non_param_args:]:
if isinstance(node_a_arg, Node):
arg_a = return_first_non_observer_node(node_a_arg, gm_a)
            if arg_a.op == 'get_attr':
                arg_a_copy_name = \
                    get_new_attr_name_with_prefix(arg_a.name + '_shadow_copy_')(gm_b)
                arg_a_obj = getattr_from_fqn(gm_a, arg_a.target)  # type: ignore[arg-type]
                setattr(gm_b, arg_a_copy_name, arg_a_obj.detach())
                node_a_arg_copy = graph_c.create_node(
                    'get_attr', arg_a_copy_name, (), {}, arg_a_copy_name)
                new_args.append(node_a_arg_copy)
            else:
                raise AssertionError(
                    f""handling of node with op {arg_a.op} is not implemented"")
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
","for node_a_arg in node_a.args[num_non_param_args:]:
if isinstance(node_a_arg, Node):
arg_a = return_first_non_observer_node(node_a_arg, gm_a)
            node_a_arg_copy = _copy_node_from_a_to_c(arg_a, gm_a, gm_b, graph_c)
            new_args.append(node_a_arg_copy)
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
"
746,"for node_a_k, node_a_kwarg in node_a.kwargs.items():
if isinstance(node_a_kwarg, Node):
kwarg_a = return_first_non_observer_node(node_a_kwarg, gm_a)
            kwarg_a_copy_name = \
                get_new_attr_name_with_prefix(kwarg_a.name + '_shadow_copy_')(gm_b)
            kwarg_a_obj = getattr_from_fqn(gm_a, kwarg_a.target)  # type: ignore[arg-type]
            setattr(gm_b, kwarg_a_copy_name, kwarg_a_obj.detach())
            node_a_kwarg_copy = graph_c.create_node(
                'get_attr', kwarg_a_copy_name, (), {}, kwarg_a_copy_name)
new_kwargs[node_a_k] = node_a_kwarg_copy
else:
new_kwargs[node_a_k] = node_a_kwarg
","for node_a_k, node_a_kwarg in node_a.kwargs.items():
if isinstance(node_a_kwarg, Node):
kwarg_a = return_first_non_observer_node(node_a_kwarg, gm_a)
            node_a_kwarg_copy = _copy_node_from_a_to_c(kwarg_a, gm_a, gm_b, graph_c)
new_kwargs[node_a_k] = node_a_kwarg_copy
else:
new_kwargs[node_a_k] = node_a_kwarg
"
747,"import argparse
import yaml
import asyncio
from typing import List, Dict, Any, Optional
","import argparse
import yaml
import asyncio
import shutil
import re
from typing import List, Dict, Any, Optional
"
748,"return [x.strip() for x in all_files if x.strip() != """"]
async def run_step(step: Dict[str, Any], job_name: str, files: Optional[List[str]]) -> bool:
env = os.environ.copy()
env[""GITHUB_WORKSPACE""] = ""/tmp""
if files is None:
","return [x.strip() for x in all_files if x.strip() != """"]
async def run_step(step: Dict[str, Any], job_name: str, files: Optional[List[str]], quiet: bool) -> bool:
env = os.environ.copy()
env[""GITHUB_WORKSPACE""] = ""/tmp""
if files is None:
"
749,"return ret
def gaussian_nll_loss(input, target, var, *, full=False, eps=1e-6, reduction='mean'):
r""""""Gaussian negative log likelihood loss.
See :class:`~torch.nn.GaussianNLLLoss` for details.
","return ret
def gaussian_nll_loss(
    input: Tensor,
    target: Tensor,
    var: Tensor,
    full: bool = False,
    eps: float = 1e-6,
    reduction: str = ""mean"",
) -> Tensor:
r""""""Gaussian negative log likelihood loss.
See :class:`~torch.nn.GaussianNLLLoss` for details.
"
750,"with torch.no_grad():
var.clamp_(min=eps)
    # Calculate loss (without constant)
    loss = 0.5 * (torch.log(var) + (input - target)**2 / var).view(input.size(0), -1).sum(dim=1)

    # Add constant to loss term if required
if full:
        D = input.size(1)
        loss = loss + 0.5 * D * math.log(2 * math.pi)
    # Apply reduction
if reduction == 'mean':
return loss.mean()
elif reduction == 'sum':
","with torch.no_grad():
var.clamp_(min=eps)
    # Calculate the loss
    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)
if full:
        loss += 0.5 * math.log(2 * math.pi)
if reduction == 'mean':
return loss.mean()
elif reduction == 'sum':
"
751,"init.kaiming_uniform_(self.weight, a=math.sqrt(5))
if self.bias is not None:
fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
init.uniform_(self.bias, -bound, bound)
def forward(self, input: Tensor) -> Tensor:
","init.kaiming_uniform_(self.weight, a=math.sqrt(5))
if self.bias is not None:
fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
init.uniform_(self.bias, -bound, bound)
def forward(self, input: Tensor) -> Tensor:
"
752,"py::call_guard<py::gil_scoped_release>());
module.attr(""_DEFAULT_FIRST_BUCKET_BYTES"") = ::c10d::kDefaultFirstBucketBytes;
module.attr(""_DEFAULT_NO_TIMEOUT"") = py::cast(kNoTimeout);
Py_RETURN_TRUE;
","py::call_guard<py::gil_scoped_release>());
module.attr(""_DEFAULT_FIRST_BUCKET_BYTES"") = ::c10d::kDefaultFirstBucketBytes;
  module.attr(""_DEFAULT_PG_TIMEOUT"") = py::cast(kProcessGroupDefaultTimeout);
module.attr(""_DEFAULT_NO_TIMEOUT"") = py::cast(kNoTimeout);
Py_RETURN_TRUE;
"
753,"optional_memory_format.value());
return at::native::resize_as_sparse_(self, the_template);
}
  Tensor& result = self.resize_(the_template.sizes());
if (optional_memory_format.has_value()) {
auto memory_format = optional_memory_format.value();
if (memory_format == MemoryFormat::Preserve) {
","optional_memory_format.value());
return at::native::resize_as_sparse_(self, the_template);
}
  const Tensor& result = self.resize_(the_template.sizes());
if (optional_memory_format.has_value()) {
auto memory_format = optional_memory_format.value();
if (memory_format == MemoryFormat::Preserve) {
"
754,"'wrapper_registrations': wrapper_registrations,
})
def emit_body(fn: NativeFunctionWithDifferentiabilityInfo) -> List[str]:
assert dispatch_strategy(fn) == 'use_derived'
f = fn.func
","'wrapper_registrations': wrapper_registrations,
})
@with_native_function_with_differentiability_info
def emit_body(fn: NativeFunctionWithDifferentiabilityInfo) -> List[str]:
assert dispatch_strategy(fn) == 'use_derived'
f = fn.func
"
755,"raise AssertionError(f""base type should have been value type {t}"")
elif isinstance(t, OptionalType):
if str(t.elem) == 'Tensor':
            if mutable:
return NamedCType(binds, MutRefCType(BaseCType(tensorT)))  # TODO: fix this discrepancy
else:
return NamedCType(binds, ConstRefCType(OptionalCType(BaseCType(tensorT))))
","raise AssertionError(f""base type should have been value type {t}"")
elif isinstance(t, OptionalType):
if str(t.elem) == 'Tensor':
            if mutable and not local.use_const_ref_for_mutable_tensors():
return NamedCType(binds, MutRefCType(BaseCType(tensorT)))  # TODO: fix this discrepancy
else:
return NamedCType(binds, ConstRefCType(OptionalCType(BaseCType(tensorT))))
"
756,"return config, cmd
@record
def run(args):
if args.standalone:
etcd_server = EtcdServer()
","return config, cmd
def run(args):
if args.standalone:
etcd_server = EtcdServer()
"
757,"if not inputs:
raise Exception(""Please provide inputs for at least 1 function"")
get_bundled_inputs_functions_and_info_template = """"
for function, input_list in inputs.items():
","if not inputs:
raise Exception(""Please provide inputs for at least 1 function"")
    if hasattr(model, ""get_all_bundled_inputs"") or hasattr(model, ""get_bundled_inputs_functions_and_info""):
        raise Exception(
            ""Models can only be augmented with bundled inputs once. ""
            ""This Model seems to have already been augmented with ""
            ""bundled inputs. Please start afresh with one that ""
            ""doesn't have bundled inputs."",
        )

get_bundled_inputs_functions_and_info_template = """"
for function, input_list in inputs.items():
"
758,"switch (t) {
case DispatchKey::CPU:
return DispatchKey::AutogradCPU;
case DispatchKey::CUDA:
return DispatchKey::AutogradCUDA;
case DispatchKey::XLA:
","switch (t) {
case DispatchKey::CPU:
return DispatchKey::AutogradCPU;
    case DispatchKey::XPU:
      return DispatchKey::AutogradXPU;
case DispatchKey::CUDA:
return DispatchKey::AutogradCUDA;
case DispatchKey::XLA:
"
759,"from .dynamic_rendezvous import create_handler
def _create_etcd_handler(params: RendezvousParameters) -> RendezvousHandler:
from . import static_tcp_rendezvous
return static_tcp_rendezvous.create_rdzv_handler(params)
def _create_static_handler(params: RendezvousParameters) -> RendezvousHandler:
from . import etcd_rendezvous
return etcd_rendezvous.create_rdzv_handler(params)
","from .dynamic_rendezvous import create_handler
def _create_static_handler(params: RendezvousParameters) -> RendezvousHandler:
from . import static_tcp_rendezvous
return static_tcp_rendezvous.create_rdzv_handler(params)
def _create_etcd_handler(params: RendezvousParameters) -> RendezvousHandler:
from . import etcd_rendezvous
return etcd_rendezvous.create_rdzv_handler(params)
"
760,"rdzv_endpoint: str = """"
rdzv_backend: str = ""etcd""
rdzv_configs: Dict[str, Any] = field(default_factory=dict)
    rdzv_timeout: int = 300
max_restarts: int = 3
monitor_interval: float = 30
start_method: str = ""spawn""
","rdzv_endpoint: str = """"
rdzv_backend: str = ""etcd""
rdzv_configs: Dict[str, Any] = field(default_factory=dict)
    rdzv_timeout: int = 900
max_restarts: int = 3
monitor_interval: float = 30
start_method: str = ""spawn""
"
761,"return properties
def get_jit_class_def(cls, self_name):
# Get defs for each method within the current class independently
# TODO: proper overriding analysis when implementing class inheritance
","return properties
def get_class_assigns(ctx, cls_ast):
    assigns = []

    def maybe_build_assign(builder, entry):
        nonlocal assigns
        try:
            assigns.append(builder(ctx, entry))
        except NotSupportedError:
            pass
    for entry in cls_ast.body:
        if isinstance(entry, ast.Assign):
            maybe_build_assign(StmtBuilder.build_Assign, entry)
        elif isinstance(entry, ast.AnnAssign):
            maybe_build_assign(StmtBuilder.build_AnnAssign, entry)
    return assigns


def get_jit_class_def(cls, self_name):
# Get defs for each method within the current class independently
# TODO: proper overriding analysis when implementing class inheritance
"
762,"Tensor arccos(const Tensor& self) { return self.acos(); }
Tensor& arccos_(Tensor& self) { return self.acos_(); }
static Tensor wrapped_scalar_tensor(const Scalar& scalar) {
  auto tensor = scalar_to_tensor(scalar);
  tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
  return tensor;
}

Tensor& rad2deg_out(const Tensor& self, Tensor& result) {
TORCH_CHECK(!self.is_complex(), ""rad2deg is not supported for complex tensors."");
constexpr double M_180_PI = 57.295779513082320876798154814105170332405472466564;
","Tensor arccos(const Tensor& self) { return self.acos(); }
Tensor& arccos_(Tensor& self) { return self.acos_(); }
Tensor& rad2deg_out(const Tensor& self, Tensor& result) {
TORCH_CHECK(!self.is_complex(), ""rad2deg is not supported for complex tensors."");
constexpr double M_180_PI = 57.295779513082320876798154814105170332405472466564;
"
763,"def save_module(self, module_name: str, dependencies=True):
""""""Save the code for `module_name` into the package. Code for the module is resolved using the `importers` path to find the
module object, and then using its `__file__` attribute to find the source code.
Args:
module_name (str): e.g. `my_package.my_subpackage`, code will be saved to provide code for this package.
            dependencies (bool, optional): If True, we scan the source for dependencies (see :ref:`Dependencies`).
""""""
module = self._import_module(module_name)
source = self._get_source_of_module(module)
","def save_module(self, module_name: str, dependencies=True):
""""""Save the code for `module_name` into the package. Code for the module is resolved using the `importers` path to find the
module object, and then using its `__file__` attribute to find the source code.

Args:
module_name (str): e.g. `my_package.my_subpackage`, code will be saved to provide code for this package.
            dependencies (bool, optional): If True, we scan the source for dependencies.
""""""
module = self._import_module(module_name)
source = self._get_source_of_module(module)
"
764,"allow_empty: bool = True,
):
""""""Include `module` in the list of external modules the package can import.
        This will prevent dependency discover from saving
it in the package. The importer will load an external module directly from the standard import system.
Code for extern modules must also exist in the process loading the package.
","allow_empty: bool = True,
):
""""""Include `module` in the list of external modules the package can import.
        This will prevent dependency discovery from saving
it in the package. The importer will load an external module directly from the standard import system.
Code for extern modules must also exist in the process loading the package.
"
765,"""""""Add `module_name` to the list of external modules, regardless of whether it is
required by other modules.
        Prefer using `extern` to only mark modules extern if they are actually required by the packaged code.
""""""
if module_name not in self.extern_modules:
self.extern_modules.append(module_name)
","""""""Add `module_name` to the list of external modules, regardless of whether it is
required by other modules.
        Prefer using :meth:`extern` to only mark modules extern if they are actually required by the packaged code.
""""""
if module_name not in self.extern_modules:
self.extern_modules.append(module_name)
"
766,"examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
.. note::
    An earlier version of the API in ``torch.autograd`` module is considered legacy and will be deprecated.
'''
","examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
.. note::
    An earlier version of the API in :mod:`torch.autograd` module is considered legacy and will be deprecated.
'''
"
767,"with torch.profiler.profile(
activities=[
torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA],
# In this example with wait=1, warmup=1, active=2,
# profiler will skip the first step/iteration,
","with torch.profiler.profile(
activities=[
torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
# In this example with wait=1, warmup=1, active=2,
# profiler will skip the first step/iteration,
"
768,"}
}
std::shared_ptr<Graph> graph_;
std::unique_ptr<AliasDb> aliasDb_ = nullptr;
// Minimal size of a fusion group
size_t min_group_size_;
// If true, shapes are ignored
","}
}
  // This function parses the option provided by the environment variable
  // ""PYTORCH_TENSOREXPR_DONT_FUSE"".
  // This variable allows users to disable fusion on a list of specified
  // operators that are separated by ':'. e.g.,
  // 'PYTORCH_TENSOREXPR_DONT_FUSE=""clamp:mul:add""' disables fusion on
  // aten::clamp, aten::mul and aten::add.
  void parseTENotFuseOption() {
    const char* option = std::getenv(""PYTORCH_TENSOREXPR_DONT_FUSE"");
    std::stringstream in_ss;
    if (option) {
      in_ss << option;
    }

    std::string line;
    while (std::getline(in_ss, line, ':')) {
      if (line.size() == 0) {
        continue;
      }
      operators_not_to_fuse.insert(c10::Symbol::aten(line));
    }
  }

std::shared_ptr<Graph> graph_;
std::unique_ptr<AliasDb> aliasDb_ = nullptr;
  std::set<NodeKind> operators_not_to_fuse;
// Minimal size of a fusion group
size_t min_group_size_;
// If true, shapes are ignored
"
769,"std::vector<Tensor> unsafe_split(const Tensor& self, int64_t split_size, int64_t dim) {
auto result = at::native::split(self, split_size, dim);
for (auto& t : result) {
    t.unsafeGetTensorImpl()->set_version_counter(c10::VariableVersion());
}
return result;
}
","std::vector<Tensor> unsafe_split(const Tensor& self, int64_t split_size, int64_t dim) {
auto result = at::native::split(self, split_size, dim);
for (auto& t : result) {
    t.unsafeGetTensorImpl()->set_version_counter(c10::VariableVersion(/*version=*/0));
}
return result;
}
"
770,"import json
import logging
import os
import shutil
import time
import traceback
import warnings
","import json
import logging
import os
import time
import traceback
import warnings
"
771,"the smallest integer greater than or equal to each element.
.. math::
    \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1
"""""" + r""""""
Args:
{input}
","the smallest integer greater than or equal to each element.
.. math::
    \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil
"""""" + r""""""
Args:
{input}
"
772,"operand_id = self.jitval_operand_map[jitval]
return (operand_id, self.operands[operand_id])
def get_tensor_operand_or_constant(self, jitval):
operand_id = self.jitval_operand_map.get(jitval)
if operand_id is None:
","operand_id = self.jitval_operand_map[jitval]
return (operand_id, self.operands[operand_id])
    def get_tensor_operand_by_jitval_fixed_size(self, jitval):
        op_id, oper = self.get_tensor_operand_by_jitval(jitval)
        for s in oper.shape:
            if s <= 0:
                # TODO: Improve this error message, possibly after converting
                # many callsites to support flexible size.
                raise Exception(""Flexible size is not supported for this operand."")
        return op_id, oper

def get_tensor_operand_or_constant(self, jitval):
operand_id = self.jitval_operand_map.get(jitval)
if operand_id is None:
"
773,"assert node.inputsSize() == 4
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
dim_ctype, dim = self.get_constant_value(node.inputsAt(1))
assert dim_ctype.kind() == ""ListType""
assert dim_ctype.getElementType().kind() == ""IntType""
","assert node.inputsSize() == 4
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
dim_ctype, dim = self.get_constant_value(node.inputsAt(1))
assert dim_ctype.kind() == ""ListType""
assert dim_ctype.getElementType().kind() == ""IntType""
"
774,"assert node.inputsSize() == 1
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
out_oper = in_oper._replace(
op_type=NNAPI_OperandCode.TENSOR_FLOAT32,
scale=0.0,
","assert node.inputsSize() == 1
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
out_oper = in_oper._replace(
op_type=NNAPI_OperandCode.TENSOR_FLOAT32,
scale=0.0,
"
775,"assert node.inputsSize() == 1
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
inputs = [None] * 1
inputs[0] = in_id
","assert node.inputsSize() == 1
assert node.outputsSize() == 1
        in_id, in_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
inputs = [None] * 1
inputs[0] = in_id
"
776,"self.add_addmm_or_linear(node, False, jit_input, jit_weight, jit_bias)
def add_addmm_or_linear(self, node, transpose_weight, jit_input, jit_weight, jit_bias):
        input_id, input_oper = self.get_tensor_operand_by_jitval(jit_input)
bias_id, bias_oper = self.get_tensor_operand_for_weight(jit_bias)
assert len(input_oper.shape) == 2
","self.add_addmm_or_linear(node, False, jit_input, jit_weight, jit_bias)
def add_addmm_or_linear(self, node, transpose_weight, jit_input, jit_weight, jit_bias):
        input_id, input_oper = self.get_tensor_operand_by_jitval_fixed_size(jit_input)
bias_id, bias_oper = self.get_tensor_operand_for_weight(jit_bias)
assert len(input_oper.shape) == 2
"
777,"def clip_grad_norm_(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0,
        error_if_nonfinite: bool = True) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
The norm is computed over all gradients together, as if they were
","def clip_grad_norm_(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0,
        error_if_nonfinite: bool = False) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
The norm is computed over all gradients together, as if they were
"
778,"}
}
}} // namespace c10::impl
","}
}
bool tls_is_dispatch_keyset_excluded(DispatchKeySet ks) {
  return raw_local_dispatch_key_set.excluded().isSupersetOf(ks);
}

bool tls_is_dispatch_keyset_included(DispatchKeySet ks) {
  return raw_local_dispatch_key_set.included().isSupersetOf(ks);
}
}} // namespace c10::impl
"
779,"Tensor & detach_(Tensor & self) {
RECORD_FUNCTION(""detach_"", std::vector<c10::IValue>({self}));
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<torch::autograd::DifferentiableViewMeta*>(torch::autograd::impl::get_autograd_meta(self));
","Tensor & detach_(Tensor & self) {
RECORD_FUNCTION(""detach_"", std::vector<c10::IValue>({self}));
  assert_no_inference_tensor(self);
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<torch::autograd::DifferentiableViewMeta*>(torch::autograd::impl::get_autograd_meta(self));
"
780,"""Only a single TORCH_LIBRARY can be used to register the namespace "", ns,
""; please put all of your definitions in a single TORCH_LIBRARY block.  ""
""If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL ""
    ""(which can be duplicated).  Previous registration of TORCH_LIBRARY was "",
found->second, ""; latest registration was "", debug
);
libraries_.emplace(ns, std::move(debug));
","""Only a single TORCH_LIBRARY can be used to register the namespace "", ns,
""; please put all of your definitions in a single TORCH_LIBRARY block.  ""
""If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL ""
    ""(which can be duplicated).  If you really intended to define operators for a ""
    ""single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to ""
    ""explicitly indicate this.  ""
    ""Previous registration of TORCH_LIBRARY was "",
found->second, ""; latest registration was "", debug
);
libraries_.emplace(ns, std::move(debug));
"
781,"else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('InferenceMode::is_enabled() ? '
                             'CreationMeta::INFERENCE_MODE : '
                             '(at::GradMode::is_enabled() ? CreationMeta::DEFAULT : CreationMeta::NO_GRAD_MODE)')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
","else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('at::GradMode::is_enabled() ? '
                             'CreationMeta::DEFAULT : '
                             'CreationMeta::NO_GRAD_MODE')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
"
782,"optimization_blocklist: A set with type of MobileOptimizerType. When set is not passed,
optimization method will run all the optimizer pass; otherwise, optimizer
method will run the optimization pass that is not included inside optimization_blocklist.
        perserved_methods: A list of methods that needed to be preserved when freeze_module pass is invoked
backend: Device type to use for running the result model ('CPU'(default), 'Vulkan' or 'Metal').
methods_to_optimize: List of functions to optimize, CPU only, forward is optimized if it exists
Returns:
","optimization_blocklist: A set with type of MobileOptimizerType. When set is not passed,
optimization method will run all the optimizer pass; otherwise, optimizer
method will run the optimization pass that is not included inside optimization_blocklist.
        preserved_methods: A list of methods that needed to be preserved when freeze_module pass is invoked
backend: Device type to use for running the result model ('CPU'(default), 'Vulkan' or 'Metal').
methods_to_optimize: List of functions to optimize, CPU only, forward is optimized if it exists
Returns:
"
783,"#include <c10/core/WrapDimMinimal.h>
#include <c10/core/impl/LocalDispatchKeySet.h>
#include <c10/util/Optional.h>
C10_DEFINE_bool(
caffe2_keep_on_shrink,
","#include <c10/core/WrapDimMinimal.h>
#include <c10/core/impl/LocalDispatchKeySet.h>
#include <c10/util/Optional.h>
#include <c10/core/InferenceMode.h>
C10_DEFINE_bool(
caffe2_keep_on_shrink,
"
784,"api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
","api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
    inplace_view_body.append(THROW_IF_VARIABLETYPE_ON)
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
"
785,"(void)_any_requires_grad;
"""""")
SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
","(void)_any_requires_grad;
"""""")
SETUP_ASSERT_NO_INFERENCE_TENSOR = CodeTemplate(""""""\
assert_no_inference_tensor( ${tensor_args} );
"""""")

SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
"
786,"case DispatchKey::CompositeImplicitAutograd:
return ""CompositeImplicitAutograd"";
    case DispatchKey::DefaultBackend:
      return ""DefaultBackend"";
case DispatchKey::TESTING_ONLY_GenericWrapper:
return ""TESTING_ONLY_GenericWrapper"";
","case DispatchKey::CompositeImplicitAutograd:
return ""CompositeImplicitAutograd"";
    case DispatchKey::CompositeExplicitAutograd:
      return ""CompositeExplicitAutograd"";
case DispatchKey::TESTING_ONLY_GenericWrapper:
return ""TESTING_ONLY_GenericWrapper"";
"
787,"return autograd_dispatch_keyset;
case DispatchKey::CompositeImplicitAutograd:
return math_dispatch_keyset;
    case DispatchKey::DefaultBackend:
return backend_dispatch_keyset;
default:
return DispatchKeySet(t);
","return autograd_dispatch_keyset;
case DispatchKey::CompositeImplicitAutograd:
return math_dispatch_keyset;
    case DispatchKey::CompositeExplicitAutograd:
return backend_dispatch_keyset;
default:
return DispatchKeySet(t);
"
788,"DispatchKey.CPU,
DispatchKey.CUDA,
DispatchKey.CompositeImplicitAutograd,
        DispatchKey.DefaultBackend,
}
if options.backend_whitelist:
dispatch_keys = [k for k in dispatch_keys if is_generic_dispatch_key(k) or str(k) in options.backend_whitelist]
","DispatchKey.CPU,
DispatchKey.CUDA,
DispatchKey.CompositeImplicitAutograd,
        DispatchKey.CompositeExplicitAutograd,
}
if options.backend_whitelist:
dispatch_keys = [k for k in dispatch_keys if is_generic_dispatch_key(k) or str(k) in options.backend_whitelist]
"
789,"// Ops in the following registration list are registered as
//   (1) CompositeImplicitAutograd kernels
//   (2) Autograd kernels
//   (3) DefaultBackend kernels and additionally Autograd kernels
// The reason for (3) is that ops that also use dispatch (e.g. register CPU/CUDA/QuantizedCPU
// kernels) will skip picking up CompositeImplicitAutograd kernels for Autograd, so we register them to both
// DefaultBackend and Autograd instead. See
// https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword
// for more details.
// Invariant:
// - Ops registered to CompositeImplicitAutograd or DefaultBackend below must match `MANUAL_BACKEND` set in tools/autograd/gen_variable_type.py.
//   and they have manual_kernel_registration=True in native_functions.yaml.
// - Ops registered to DispatchKey::Autograd below must be included in `MANUAL_AUTOGRAD` in tools/autograd/gen_variable_type.py
","// Ops in the following registration list are registered as
//   (1) CompositeImplicitAutograd kernels
//   (2) Autograd kernels
//   (3) CompositeExplicitAutograd kernels and additionally Autograd kernels
// The reason for (3) is that ops that also use dispatch (e.g. register CPU/CUDA/QuantizedCPU
// kernels) will skip picking up CompositeImplicitAutograd kernels for Autograd, so we register them to both
// CompositeExplicitAutograd and Autograd instead. See
// https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword
// for more details.
// Invariant:
// - Ops registered to CompositeImplicitAutograd or CompositeExplicitAutograd below must match `MANUAL_BACKEND` set in tools/autograd/gen_variable_type.py.
//   and they have manual_kernel_registration=True in native_functions.yaml.
// - Ops registered to DispatchKey::Autograd below must be included in `MANUAL_AUTOGRAD` in tools/autograd/gen_variable_type.py
"
790,"auto& pythonRpcHandler = PythonRpcHandler::getInstance();
// Need GIL to destruct the py::object returned by deserialize()
py::gil_scoped_acquire acquire;
return jit::toIValue(
          pythonRpcHandler.deserialize(resp.serializedPyObj()),
PyObjectType::get());
}
default: {
","auto& pythonRpcHandler = PythonRpcHandler::getInstance();
// Need GIL to destruct the py::object returned by deserialize()
py::gil_scoped_acquire acquire;
      py::object value = pythonRpcHandler.deserialize(resp.serializedPyObj());
      pythonRpcHandler.handleException(value);
return jit::toIValue(
          value,
PyObjectType::get());
}
default: {
"
791,"for (const auto& tensor : message.tensors()) {
dataPtrs.emplace_back(tensor.storage().data_ptr());
}
child->markCompletedWithDataPtrs(
                toPyIValue(message), std::move(dataPtrs));
}
}));
return child;
","for (const auto& tensor : message.tensors()) {
dataPtrs.emplace_back(tensor.storage().data_ptr());
}

            // toPyIValue might throw and we need to record the appropriate exception.
            IValue ivalue;
            try {
              ivalue = toPyIValue(message);
            } catch (std::exception& e) {
              child->setErrorIfNeeded(std::current_exception());
              return;
            }

child->markCompletedWithDataPtrs(
                ivalue, std::move(dataPtrs));
}
}));
return child;
"
792,"uint8_t max_q = (1 << bitwidth) - 1;
uint64_t bit_start = 0;
if (random) {
    for (int start = 0; start < input_size; start += segment_size) {
uint64_t stride = start + segment_size <= input_size ? segment_size
: input_size - start;
      int i = 0;
constexpr int VLEN = 8;
for (; i < stride / VLEN * VLEN; i += VLEN) {
__m256 r_v = _mm256_loadu_ps(&random_buffer[start + i]);
","uint8_t max_q = (1 << bitwidth) - 1;
uint64_t bit_start = 0;
if (random) {
    for (uint64_t start = 0; start < input_size; start += segment_size) {
uint64_t stride = start + segment_size <= input_size ? segment_size
: input_size - start;
      uint64_t i = 0;
constexpr int VLEN = 8;
for (; i < stride / VLEN * VLEN; i += VLEN) {
__m256 r_v = _mm256_loadu_ps(&random_buffer[start + i]);
"
793,"bit_start += bitwidth;
}
} else {
    for (int start = 0; start < input_size; start += segment_size) {
uint64_t stride = start + segment_size <= input_size ? segment_size
: input_size - start;
      int i = 0;
for (; i < stride; ++i) {
float fval = input_data[start + i];
float thetimes = (fval - minimum_element) * gap_inverse;
","bit_start += bitwidth;
}
} else {
    for (uint64_t start = 0; start < input_size; start += segment_size) {
uint64_t stride = start + segment_size <= input_size ? segment_size
: input_size - start;
      uint64_t i = 0;
for (; i < stride; ++i) {
float fval = input_data[start + i];
float thetimes = (fval - minimum_element) * gap_inverse;
"
794,", offset(range.begin) {
int64_t linear_offset = range.begin;
int64_t ndim = values.size();
  for (int dim = 0; dim < ndim; dim++) {
int64_t size = shape[dim];
if (size > 0) {
values[dim] = linear_offset % size;
",", offset(range.begin) {
int64_t linear_offset = range.begin;
int64_t ndim = values.size();
  for (const auto dim : c10::irange(ndim)) {
int64_t size = shape[dim];
if (size > 0) {
values[dim] = linear_offset % size;
"
795,"#include <ATen/core/function_schema.h>
#include <ATen/core/jit_type.h>
#include <c10/macros/Macros.h>
#include <ATen/core/grad_mode.h>
#include <ATen/core/function.h>
#include <iostream>
","#include <ATen/core/function_schema.h>
#include <ATen/core/jit_type.h>
#include <c10/macros/Macros.h>
#include <c10/util/irange.h>
#include <ATen/core/grad_mode.h>
#include <ATen/core/function.h>
#include <iostream>
"
796,"hook_err_msg
);
    for (int i = 1; i < forward_args.size(); ++i) {
if (*forward_args[i].type() != *input_tuple_types[i - 1]) {
TORCH_CHECK(
false,
","hook_err_msg
);
    for (const auto i : c10::irange(1, forward_args.size())) {
if (*forward_args[i].type() != *input_tuple_types[i - 1]) {
TORCH_CHECK(
false,
"
797,"pre_hook_err_msg
);
// check that contained types match forward types
  for (int i = 1; i < forward_args.size(); ++i) {
if (*forward_args[i].type() != *return_tuple_types[i - 1]) {
TORCH_CHECK(
false,
","pre_hook_err_msg
);
// check that contained types match forward types
  for (const auto i : c10::irange(1, forward_args.size())) {
if (*forward_args[i].type() != *return_tuple_types[i - 1]) {
TORCH_CHECK(
false,
"
798,"#include <ATen/NativeFunctions.h>
#include <ATen/Parallel.h>
#include <tuple>
#include <vector>
","#include <ATen/NativeFunctions.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>

#include <tuple>
#include <vector>
"
799,"#include <ATen/native/CPUBlas.h>
#include <ATen/native/im2col.h>
namespace at {
namespace native {
","#include <ATen/native/CPUBlas.h>
#include <ATen/native/im2col.h>
#include <c10/util/irange.h>

namespace at {
namespace native {
"
800,"#include <ATen/NamedTensorUtils.h>
#include <bitset>
namespace at { namespace native {
","#include <ATen/NamedTensorUtils.h>
#include <c10/util/irange.h>

#include <bitset>
namespace at { namespace native {
"
801,"static int64_t countUnset(std::bitset<kMaxNamedTensorDim> set, int64_t up_to_idx) {
int64_t result = 0;
  for (auto i = 0; i < up_to_idx; ++i) {
if (!set.test(i)) result++;
}
return result;
","static int64_t countUnset(std::bitset<kMaxNamedTensorDim> set, int64_t up_to_idx) {
int64_t result = 0;
  for (const auto i : c10::irange(up_to_idx)) {
if (!set.test(i)) result++;
}
return result;
"
802,"std::vector<int64_t> new_sizes(names.size(), 1);
std::vector<int64_t> new_strides(names.size(), 0);
  for (auto idx = 0U; idx < tensor_names.size(); ++idx) {
const auto& dim = tensor_names[idx];
TORCH_CHECK(dim.isBasic(),
""align_to: All input dims must be named. Found unnamed dim at index "",
","std::vector<int64_t> new_sizes(names.size(), 1);
std::vector<int64_t> new_strides(names.size(), 0);
  for (const auto idx : c10::irange(tensor_names.size())) {
const auto& dim = tensor_names[idx];
TORCH_CHECK(dim.isBasic(),
""align_to: All input dims must be named. Found unnamed dim at index "",
"
803,"Tensor output = at::empty_like(values);
auto inp = values.accessor<float,1>();
auto out = output.accessor<float,1>();
  for(int i = 0; i < values.size(0); ++i) {
out[i] = inp[i] + addends->at(i);
}
return output;
","Tensor output = at::empty_like(values);
auto inp = values.accessor<float,1>();
auto out = output.accessor<float,1>();
  for (const auto i : c10::irange(values.size(0))) {
out[i] = inp[i] + addends->at(i);
}
return output;
"
804,"const Tensor& zero_points,
ScalarType dtype) {
std::vector<Tensor> quantized_tensors;
  for (auto i = 0; i < tensors.size(); ++i) {
quantized_tensors.push_back(at::quantize_per_tensor(
tensors[i],
scales[i].item<double>(),
","const Tensor& zero_points,
ScalarType dtype) {
std::vector<Tensor> quantized_tensors;
  for (const auto i : c10::irange(tensors.size())) {
quantized_tensors.push_back(at::quantize_per_tensor(
tensors[i],
scales[i].item<double>(),
"
805,"#include <ostream>
#include <sstream>
namespace caffe2 {
ProfDAGCounters::ProfDAGCounters(const std::shared_ptr<const NetDef>& net_def) {
","#include <ostream>
#include <sstream>
#include <c10/util/irange.h>

namespace caffe2 {
ProfDAGCounters::ProfDAGCounters(const std::shared_ptr<const NetDef>& net_def) {
"
806,"bool normalize_by_lengths,
OutType* out) {
int64_t current = 0;
  for (int m = 0; m < output_size; ++m) {
memset(out, 0, sizeof(OutType) * block_size);
if (current + lengths[m] > index_size) {
return false;
","bool normalize_by_lengths,
OutType* out) {
int64_t current = 0;
  for (const auto m : c10::irange(output_size)) {
memset(out, 0, sizeof(OutType) * block_size);
if (current + lengths[m] > index_size) {
return false;
"
807,"const float scale = weight * scale_bias[0];
const float bias = weight * scale_bias[1];
      for (int j = 0; j < block_size; ++j) {
out[j] += scale * input[fused_block_size * indices[current] + j] + bias;
}
","const float scale = weight * scale_bias[0];
const float bias = weight * scale_bias[1];
      for (const auto j : c10::irange(block_size)) {
out[j] += scale * input[fused_block_size * indices[current] + j] + bias;
}
"
808,"#include ""caffe2/core/timer.h""
#include ""caffe2/core/workspace.h""
namespace caffe2 {
// Constants for user tracepoints
","#include ""caffe2/core/timer.h""
#include ""caffe2/core/workspace.h""
#include <c10/util/irange.h>

namespace caffe2 {
// Constants for user tracepoints
"
809,"#include ""caffe2/core/init.h""
#include ""observers/observer_config.h""
namespace caffe2 {
const std::string NetObserverReporterPrint::IDENTIFIER = ""Caffe2Observer "";
","#include ""caffe2/core/init.h""
#include ""observers/observer_config.h""
#include <c10/util/irange.h>

namespace caffe2 {
const std::string NetObserverReporterPrint::IDENTIFIER = ""Caffe2Observer "";
"
810,"return kernelAddress_;
}
void** LLVMCodeGenImpl::getArgvAddress() const {
  return argv_.get();
}

LLVMCodeGenImpl::LLVMCodeGenImpl(
Stmt* stmt,
const std::vector<CodeGen::BufferArg>& args,
","return kernelAddress_;
}
LLVMCodeGenImpl::LLVMCodeGenImpl(
Stmt* stmt,
const std::vector<CodeGen::BufferArg>& args,
"
811,"}
Tensor rsub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
  return native::sub(other, self, alpha);
}
Tensor& atan2_out(Tensor& result, const Tensor& self, const Tensor& other) {
","}
Tensor rsub(const Tensor& self, const Tensor& other, const Scalar& alpha) {
  return at::sub(other, self, alpha); // redispatch!
}
Tensor& atan2_out(Tensor& result, const Tensor& self, const Tensor& other) {
"
812,"}
Tensor sparse_coo_tensor_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
static PythonArgParser parser({
""sparse_coo_tensor(PyObject* indices, PyObject* values, *, ScalarType dtype=None, Device? device=None, bool requires_grad=False)"",
""sparse_coo_tensor(PyObject* indices, PyObject* values, IntArrayRef size, *, ScalarType dtype=None, Device? device=None, bool requires_grad=False)"",
","}
Tensor sparse_coo_tensor_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
  TORCH_INTERNAL_ASSERT(!isSparse(dispatchKeyToBackend(dispatch_key)));
static PythonArgParser parser({
""sparse_coo_tensor(PyObject* indices, PyObject* values, *, ScalarType dtype=None, Device? device=None, bool requires_grad=False)"",
""sparse_coo_tensor(PyObject* indices, PyObject* values, IntArrayRef size, *, ScalarType dtype=None, Device? device=None, bool requires_grad=False)"",
"
813,"}
Tensor _sparse_coo_tensor_unsafe_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
enum {
ARG_INDICES = 0,
ARG_VALUES,
","}
Tensor _sparse_coo_tensor_unsafe_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
  TORCH_INTERNAL_ASSERT(!isSparse(dispatchKeyToBackend(dispatch_key)));
enum {
ARG_INDICES = 0,
ARG_VALUES,
"
814,"// Next, shuffle by permuting the new upscale_factor dims alongside the height and width dims.
std::vector<int64_t> permutation(self.sizes().begin(), self_sizes_batch_end);
// std::iota is used to maintain the batch dims within the permutation.
  // Since 2 dims were added, the correct batch dim offsets are now:
  // -added_dims_shape.size(), ..., -7, -6.
  std::iota(permutation.begin(), permutation.end(), -added_dims_shape.size());
permutation.insert(permutation.end(), {-5 /* oc */, -2 /* h */, -4 /* 1st upscale_factor */, -1 /* w */,
3 /* 2nd upscale_factor */});
const auto input_permuted = input_reshaped.permute(permutation);
","// Next, shuffle by permuting the new upscale_factor dims alongside the height and width dims.
std::vector<int64_t> permutation(self.sizes().begin(), self_sizes_batch_end);
// std::iota is used to maintain the batch dims within the permutation.
  std::iota(permutation.begin(), permutation.end(), 0);
permutation.insert(permutation.end(), {-5 /* oc */, -2 /* h */, -4 /* 1st upscale_factor */, -1 /* w */,
const auto input_permuted = input_reshaped.permute(permutation);
"
815,"ideep::tensor& y = itensor_from_mkldnn(other);
ideep::tensor& z = itensor_from_mkldnn(result);
  const std::vector<float> scales{1.0, alpha.to<float>()};
  ideep::sum::compute(scales, {x, y}, z);
return result;
}
","ideep::tensor& y = itensor_from_mkldnn(other);
ideep::tensor& z = itensor_from_mkldnn(result);
  if (result.is_same(other)) {
    const std::vector<float> scales{alpha.to<float>(), 1.0};
    ideep::sum::compute(scales, {y, x}, z);
  } else {
    const std::vector<float> scales{1.0, alpha.to<float>()};
    ideep::sum::compute(scales, {x, y}, z);
  }
return result;
}
"
816,"if node.name not in match_map and node.name not in all_matched:
for pattern, value in patterns.items():
if is_match(modules, node, pattern):
if value is BinaryOp:
use_copy_node = all_node_args_have_no_tensors(node)
if use_copy_node:
value = CopyNode  # type: ignore
                        matched: List[Any] = []
                        record_match(pattern, node, matched)
                        for n in matched:
                            match_map[n.name] = (
                                node, matched, pattern, value(self, node),  # type: ignore
                                self.qconfig_map[n.name])
                            all_matched.add(n.name)
                        # break after finding the first match
                        break
# add custom module instances to the match result
assert self.modules is not None
","if node.name not in match_map and node.name not in all_matched:
for pattern, value in patterns.items():
if is_match(modules, node, pattern):
                        skip_this_match = False
if value is BinaryOp:
use_copy_node = all_node_args_have_no_tensors(node)
if use_copy_node:
                                # TODO(future PR): update the pattern to quantize
                                # handler logic to take this into account.
value = CopyNode  # type: ignore

                            this_node_qconfig = self.qconfig_map[node.name]
                            if this_node_qconfig:
                                dtypes = get_qconfig_dtypes(this_node_qconfig)
                                # TODO(future PR): update the pattern to quantize
                                # handler logic to take this into account.
                                skip_this_match = (
                                    (node.target in binary_op_supported_dtypes) and
                                    (dtypes not in binary_op_supported_dtypes[node.target])
                                )

                        if not skip_this_match:
                            matched: List[Any] = []
                            record_match(pattern, node, matched)
                            for n in matched:
                                match_map[n.name] = (
                                    node, matched, pattern, value(self, node),  # type: ignore
                                    self.qconfig_map[n.name])
                                all_matched.add(n.name)
                            # break after finding the first match
                            break
# add custom module instances to the match result
assert self.modules is not None
"
817,">>> out.backward()
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
              File ""/your/pytorch/install/torch/tensor.py"", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
allow_unreachable=True)  # allow_unreachable flag
",">>> out.backward()
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
              File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
allow_unreachable=True)  # allow_unreachable flag
"
818,"""nccl_socket_ifname"", &c10::DDPLoggingData::nccl_socket_ifname)
.def_readwrite(
""nccl_blocking_wait"", &c10::DDPLoggingData::nccl_blocking_wait)
.def_readwrite(""nccl_debug"", &c10::DDPLoggingData::nccl_debug)
.def_readwrite(""nccl_nthreads"", &c10::DDPLoggingData::nccl_nthreads)
.def_readwrite(""nccl_ib_timeout"", &c10::DDPLoggingData::nccl_ib_timeout)
","""nccl_socket_ifname"", &c10::DDPLoggingData::nccl_socket_ifname)
.def_readwrite(
""nccl_blocking_wait"", &c10::DDPLoggingData::nccl_blocking_wait)
      .def_readwrite(""nccl_async_error_handling"", &c10::DDPLoggingData::nccl_async_error_handling)
.def_readwrite(""nccl_debug"", &c10::DDPLoggingData::nccl_debug)
.def_readwrite(""nccl_nthreads"", &c10::DDPLoggingData::nccl_nthreads)
.def_readwrite(""nccl_ib_timeout"", &c10::DDPLoggingData::nccl_ib_timeout)
"
819,"#include <torch/csrc/python_headers.h>
#include <c10/util/intrusive_ptr.h>
#include <c10d/FileStore.hpp>
#include <c10d/TCPStore.hpp>
#ifndef _WIN32
","#include <torch/csrc/python_headers.h>
#include <c10/util/intrusive_ptr.h>
#include <c10d/Utils.hpp>
#include <c10d/FileStore.hpp>
#include <c10d/TCPStore.hpp>
#ifndef _WIN32
"
820,"_verify_model_across_ranks,
_verify_replicas_within_process,
_test_python_store,
)
if sys.platform != 'win32':
from torch._C._distributed_c10d import (
","_verify_model_across_ranks,
_verify_replicas_within_process,
_test_python_store,
        _DistributedDebugLevel,
        _get_debug_mode
)
if sys.platform != 'win32':
from torch._C._distributed_c10d import (
"
821,"list(produces_sparse_gradient(module) for module, _ in replica)
for replica in modules_and_parameters]
        # The bucket size limit is specified in the constructor.
        # Additionally, we allow for a single small bucket for parameters
        # that are defined first, such that their gradients don't spill into
        # a much larger bucket, adding unnecessary latency after gradient
        # computation finishes. Experiments showed 1MB is a reasonable value.
        bucket_indices = dist._compute_bucket_assignment_by_size(
            parameters[0],
            [dist._DEFAULT_FIRST_BUCKET_BYTES, self.bucket_bytes_cap],
            expect_sparse_gradient[0])

        # Note: reverse list of buckets because we want to approximate the
        # order in which their gradients are produced, and assume they
        # are used in the forward pass in the order they are defined.
        self.reducer = dist.Reducer(
            parameters,
            list(reversed(bucket_indices)),
            self.process_group,
            expect_sparse_gradient,
            self.bucket_bytes_cap,
            self.find_unused_parameters,
            self.gradient_as_bucket_view)

        self.logger = dist.Logger(self.reducer)

        # Set logging data that can be got during construction time.
        self.logger.set_construction_data_and_log(
            self.module.__class__.__name__,
            [] if self.device_ids is None else self.device_ids,
            -1 if self.output_device is None else self.output_device,
            self.broadcast_buffers)

        # passing a handle to torch.nn.SyncBatchNorm layer
        self._passing_sync_batchnorm_handle(self._module_copies)
    def __getstate__(self):
        self._check_default_group()
        attrs = copy.copy(self.__dict__)
        del attrs['process_group']
        del attrs['reducer']
        del attrs['logger']
        return attrs
    def __setstate__(self, state):
        # If serializable, then the process group should be the default one
        self.process_group = _get_default_group()
        super(DistributedDataParallel, self).__setstate__(state)
        self.__dict__.setdefault('require_forward_param_sync', True)
        self.__dict__.setdefault('require_backward_grad_sync', True)
        self._ddp_init_helper()
def _check_default_group(self):
pickle_not_supported = False
","list(produces_sparse_gradient(module) for module, _ in replica)
for replica in modules_and_parameters]
        # The following modules_params and modules_buffers are used for
        # param/buffer sync in _sync_params.
        self.modules_params = [list(self._get_parameters(m)) for m in self._module_copies]
        # Collect buffers for modules, filtering out buffers that should be ignored.
        named_module_buffers = [
            [(buffer, buffer_name) for buffer_name, buffer in m.named_buffers()]
            for m in self._module_copies
        ]
        self.modules_buffers = [
            [
                buffer
                for (buffer, buffer_name) in module_buffers
                if buffer_name not in self.parameters_to_ignore
            ]
            for module_buffers in named_module_buffers
        ]
        return parameters, expect_sparse_gradient
    def _get_parameters(self, m, recurse=True):
        """"""
        Returns a generator of module parameters
        """"""
        def model_parameters(m):
            ps = m._former_parameters.values() \
                if hasattr(m, ""_former_parameters"") \
                else m.parameters(recurse=False)
            for p in ps:
                yield p

        for m in m.modules() if recurse else [m]:
            for p in model_parameters(m):
                yield p
def _check_default_group(self):
pickle_not_supported = False
"
822,"const TensorOptions& options) {
const auto steps_ = steps.value_or(100);
TORCH_CHECK(steps_ >= 0, ""number of steps must be non-negative"");
  Tensor result = at::empty({steps_}, options);
return at::linspace_out(result, start, end, steps);
}
","const TensorOptions& options) {
const auto steps_ = steps.value_or(100);
TORCH_CHECK(steps_ >= 0, ""number of steps must be non-negative"");
  auto result_options = linspace_logspace_infer_options(start, end, options);
  Tensor result = at::empty({steps_}, result_options);
return at::linspace_out(result, start, end, steps);
}
"
823,"Keyword arguments:
{out}
    {dtype}
{layout}
{device}
{requires_grad}
","Keyword arguments:
{out}
    dtype (torch.dtype, optional): the data type to perform the computation in.
        Default: if None, uses the global default dtype (see torch.get_default_dtype())
        when both :attr:`start` and :attr:`end` are real,
        and corresponding complex dtype when either is complex.
{layout}
{device}
{requires_grad}
"
824,"``callback`` and will be marked as completed when the given
``callback`` finishes.
Example::
>>> import torch
>>>
","``callback`` and will be marked as completed when the given
``callback`` finishes.
        .. note:: Note that if the callback function throws, either
            through the original future being completed with an exception and
            calling ``fut.wait()``, or through other code in the callback, the
            future returned by ``then`` will be marked appropriately with the
            encountered error. However, if this callback later completes
            additional futures, those futures are not marked as completed with
            an error and the user is responsible for handling completion/waiting
            on those futures independently.

Example::
>>> import torch
>>>
"
825,"return stream;
}
} // namespace ops
} // namespace vulkan
} // namespace native
","return stream;
}
#endif /* VULKAN_TENSOR_DEBUG */

} // namespace ops
} // namespace vulkan
} // namespace native
"
826,"""""""
return StreamContext(stream)
def Stream(device: int = -1, priority: int = 0) -> 'torch.classes.cuda.Stream':
r""""""Wrapper around a CUDA stream.
A CUDA stream is a linear sequence of execution that belongs to a specific
device, independent from other streams.  See :ref:`cuda-semantics` for
details.
Arguments:
        device(int, optional): a device on which to allocate
            the stream. If :attr:`device` is ``None`` (default) or a negative
            integer, this will use the current device.
priority(int, optional): priority of the stream. Can be either
1 (high priority) or 0 (low priority). By default, streams have
priority 0.
","""""""
return StreamContext(stream)
def Stream(device: Optional[torch.device] = None, priority: int = 0) -> 'torch.classes.cuda.Stream':
r""""""Wrapper around a CUDA stream.
A CUDA stream is a linear sequence of execution that belongs to a specific
device, independent from other streams.  See :ref:`cuda-semantics` for
details.
Arguments:
        device(torch.device, optional): a device on which to allocate
            the stream. If :attr:`device` is ``None`` (default), stream will be
            created on the current device.
priority(int, optional): priority of the stream. Can be either
priority 0.
"
827,"int,
int,
bool,
              std::chrono::milliseconds>(),
py::arg(""host_name""),
py::arg(""port""),
py::arg(""world_size"") = -1,
","int,
int,
bool,
              std::chrono::milliseconds,
              bool>(),
py::arg(""host_name""),
py::arg(""port""),
py::arg(""world_size"") = -1,
"
828,"// prevents accidental implicit conversion to bool
py::arg(""is_master"").noconvert() = false,
py::arg(""timeout"") =
              std::chrono::milliseconds(::c10d::Store::kDefaultTimeout))

.def_property_readonly(
""host"",
&::c10d::TCPStore::getHost,
","// prevents accidental implicit conversion to bool
py::arg(""is_master"").noconvert() = false,
py::arg(""timeout"") =
              std::chrono::milliseconds(::c10d::Store::kDefaultTimeout),
          py::arg(""wait_for_workers"") = true)
.def_property_readonly(
""host"",
&::c10d::TCPStore::getHost,
"
829,"if (isServer_) {
// Store daemon should end because of closed connection.
// daemon destructor should join the thread
    tcpStoreDaemon_.reset(nullptr);
tcputil::closeSocket(masterListenSocket_);
}
}
","if (isServer_) {
// Store daemon should end because of closed connection.
// daemon destructor should join the thread
    tcpStoreDaemon_ = nullptr;
tcputil::closeSocket(masterListenSocket_);
}
}
"
830,"import pickletools
from .find_file_dependencies import find_files_source_depends_on
from ._custom_import_pickler import create_custom_import_pickler
from ._importlib import _normalize_path
from ._mangling import is_mangled
from ._stdlib import is_stdlib_module
from .importer import Importer, OrderedImporter, sys_importer
import types
from typing import List, Any, Callable, Dict, Sequence, Tuple, Union, Iterable, BinaryIO, Optional
from pathlib import Path
import linecache
from urllib.parse import quote
import re
class PackageExporter:
","import pickletools
from .find_file_dependencies import find_files_source_depends_on
from ._custom_import_pickler import create_custom_import_pickler
from ._file_structure_representation import _create_folder_from_file_list, Folder
from ._glob_group import GlobPattern, _GlobGroup
from ._importlib import _normalize_path
from ._mangling import is_mangled
from ._stdlib import is_stdlib_module
from .importer import Importer, OrderedImporter, sys_importer
import types
from typing import List, Any, Callable, Dict, Sequence, Tuple, Union, BinaryIO, Optional
from pathlib import Path
import linecache
from urllib.parse import quote
class PackageExporter:
"
831,"# of quantizable objects (e.g. modules and functionals)
class DefaultQuantizeHandler(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                debug: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
assert self.all_node_args
root_module = quantizer.modules['']
","# of quantizable objects (e.g. modules and functionals)
class DefaultQuantizeHandler(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
assert self.all_node_args
root_module = quantizer.modules['']
"
832,"quantized = 0 in out_quant_idxs
result = obj.convert(
                        self, node, load_arg, debug=debug,
convert_custom_config_dict=convert_custom_config_dict)
if not is_observed_standalone_module_node:
quantized = is_output_quantized(node, obj)
","quantized = 0 in out_quant_idxs
result = obj.convert(
                        self, node, load_arg, is_reference=is_reference,
convert_custom_config_dict=convert_custom_config_dict)
if not is_observed_standalone_module_node:
quantized = is_output_quantized(node, obj)
"
833,"raise ValueError('All dicts must have the same number of keys')
return type(out)(((k, gather_map([d[k] for d in outputs]))
for k in out))
return type(out)(map(gather_map, zip(*outputs)))
# Recursive function calls like this create reference cycles.
","raise ValueError('All dicts must have the same number of keys')
return type(out)(((k, gather_map([d[k] for d in outputs]))
for k in out))
        if is_namedtuple(out):
            return type(out)._make(map(gather_map, zip(*outputs)))
return type(out)(map(gather_map, zip(*outputs)))
# Recursive function calls like this create reference cycles.
"
834,"return pred_meta
class PredictorExportMeta(collections.namedtuple(
'PredictorExportMeta',
        'predict_net, parameters, inputs, outputs, shapes, name, \
        extra_init_net, global_init_net, net_type, num_workers, trainer_prefix')):
""""""
Metadata to be used for serializaing a net.
","return pred_meta
# pyre-fixme[13]: Pyre can't detect the attribute initialization via cls.super() here
class PredictorExportMeta(collections.namedtuple(
'PredictorExportMeta',
        'predict_net, parameters, inputs, outputs, shapes, name, '
        'extra_init_net, global_init_net, net_type, num_workers, trainer_prefix')):
""""""
Metadata to be used for serializaing a net.
"
835,"the parent domain.
""""""
    __slots__ = (""lengths"", ""_items"")
def __init__(self, values, lengths_blob=None):
if isinstance(lengths_blob, Field):
","the parent domain.
""""""
    __slots__: Sequence[str] = (""lengths"", ""_items"")
def __init__(self, values, lengths_blob=None):
if isinstance(lengths_blob, Field):
"
836,"LRU Hashing.
""""""
    __slots__ = (""_evicted_values"",)
def __init__(self, values, lengths_blob=None, evicted_values=None):
if isinstance(evicted_values, Field):
","LRU Hashing.
""""""
    __slots__: Sequence[str] = (""_evicted_values"",)
def __init__(self, values, lengths_blob=None, evicted_values=None):
if isinstance(evicted_values, Field):
"
837,"# setting cuda as the default GpuDeviceType as some tests
# like core, scope tests use GpuDeviceType even without gpu support
GpuDeviceType = caffe2_pb2.CUDA
NumGpuDevices = lambda: 0 # noqa
GetDeviceProperties = lambda x: None # noqa
GetGpuPeerAccessPattern = lambda: np.array([]) # noqa
GetGPUMemoryInfo = lambda: None # noqa
IsNUMAEnabled = C.is_numa_enabled
","# setting cuda as the default GpuDeviceType as some tests
# like core, scope tests use GpuDeviceType even without gpu support
GpuDeviceType = caffe2_pb2.CUDA
    # pyre-fixme[9]: incompatible type assignment
NumGpuDevices = lambda: 0 # noqa
GetDeviceProperties = lambda x: None # noqa
GetGpuPeerAccessPattern = lambda: np.array([]) # noqa
    # pyre-fixme[9]: incompatible type assignment
GetGPUMemoryInfo = lambda: None # noqa
IsNUMAEnabled = C.is_numa_enabled
"
838,"<< (*totalClientActiveCalls[0].data_ptr<int64_t>())
<< "" active client calls across all workers"";
if (*totalClientActiveCalls[0].data_ptr<int64_t>() == 0) {
break;
}
}
","<< (*totalClientActiveCalls[0].data_ptr<int64_t>())
<< "" active client calls across all workers"";
if (*totalClientActiveCalls[0].data_ptr<int64_t>() == 0) {
        if (shutdown) {
          shuttingDown_ = true;
          processGroup_->barrier()->wait();
        }
break;
}
}
"
839,"def div_0_3(self: Tensor, other: Tensor) -> Tensor:
if (self.is_floating_point() or other.is_floating_point()):
return self.true_divide(other)
  return self.floor_divide(other)
)SCRIPT"";
// Tensor x Scalar
","def div_0_3(self: Tensor, other: Tensor) -> Tensor:
if (self.is_floating_point() or other.is_floating_point()):
return self.true_divide(other)
  return self.divide(other, rounding_mode='trunc')
)SCRIPT"";
// Tensor x Scalar
"
840,"from torch.onnx.symbolic_helper import _block_list_in_opset
import torch.onnx.symbolic_opset9 as sym_opset9
","from torch.onnx.symbolic_helper import _block_list_in_opset, parse_args
import torch.onnx.symbolic_helper as sym_help
import torch.onnx.symbolic_opset9 as sym_opset9
"
841,"return out
def floordiv(g, self, other):
return floor_divide(g, self, other)
","return out
def _floor_divide(g, self, other):
    if sym_help._is_fp(self) or sym_help._is_fp(other):
        out = true_divide(g, self, other)
        return g.op('Floor', out)
    else:
        # Integer division does trunction rounding
        div = g.op('Div', self, other)
        # Division is negative if: self < 0 != other < 0
        zero = g.op('Constant', value_t=torch.tensor(0, dtype=torch.int64))
        negative = g.op('Xor',
                        g.op('Less', self, zero),
                        g.op('Less', other, zero))

        # For negative numbers with self % other != 0, subtract 1 to round down instead of up
        mod = g.op('Sub', self, g.op('Mul', div, other))
        fixup_mask = g.op('And', negative,
                          g.op('Not', g.op('Equal', mod, zero)))

        one = g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64))
        fixup = g.op('Sub', div, one)
        return g.op('Where', fixup_mask, fixup, div)


def floor_divide(g, self, other):
    # Deprecated behavior, floor_divide actually truncates
    return _trunc_divide(g, self, other)


def floordiv(g, self, other):
return floor_divide(g, self, other)
"
842,"});
}
template<typename T>
T copysign(T a, T b) {
  return std::copysign(a, b);
}

// Implement copysign for half precision floats using bit ops
// Sign is the most significant bit for both half and bfloat16 types
template<>
c10::Half copysign(c10::Half a, c10::Half b) {
  return c10::Half((a.x&0x7fff) | (b.x&0x8000), c10::Half::from_bits());
}

template<>
c10::BFloat16 copysign(c10::BFloat16 a, c10::BFloat16 b) {
   return c10::BFloat16((a.x&0x7fff) | (b.x&0x8000), c10::BFloat16::from_bits());
}

void copysign_kernel(TensorIterator& iter) {
AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""copysign_cpu"", [&]() {
cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {
","});
}
void copysign_kernel(TensorIterator& iter) {
AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""copysign_cpu"", [&]() {
cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {
"
843,"int64_t istrideT,
int64_t istrideH,
int64_t istrideW) {
  int64_t d = 0;
  at::internal::lazy_init_num_threads();
#pragma omp parallel for private(d)
  for (d = 0; d < sizeD; d++) {
    /* loop over output */
    int64_t ot, oh, ow;
    for (ot = 0; ot < osizeT; ot++) {
      int istartT = start_index(ot, osizeT, isizeT);
      int iendT = end_index(ot, osizeT, isizeT);
      int kT = iendT - istartT;

      for (oh = 0; oh < osizeH; oh++) {
        int istartH = start_index(oh, osizeH, isizeH);
        int iendH = end_index(oh, osizeH, isizeH);
        int kH = iendH - istartH;

        for (ow = 0; ow < osizeW; ow++) {
          int istartW = start_index(ow, osizeW, isizeW);
          int iendW = end_index(ow, osizeW, isizeW);
          int kW = iendW - istartW;

          /* local pointers */
          scalar_t* ip = input_p + d * istrideD + istartT * istrideT +
              istartH * istrideH + istartW * istrideW;
          scalar_t* op = output_p + d * osizeT * osizeH * osizeW +
              ot * osizeH * osizeW + oh * osizeW + ow;

          /* compute local average: */
          scalar_t sum = 0;
          int it, ih, iw;
          for (it = 0; it < kT; it++) {
            for (ih = 0; ih < kH; ih++) {
              for (iw = 0; iw < kW; iw++) {
                scalar_t val =
                    *(ip + it * istrideT + ih * istrideH + iw * istrideW);
                sum += val;
}
}
          }
          /* set output to local average */
          *op = sum / kT / kH / kW;
}
}
}
  }
}
void adaptive_avg_pool3d_out_cpu_template(
","int64_t istrideT,
int64_t istrideH,
int64_t istrideW) {
  at::parallel_for(0, sizeD, 1, [&](int64_t start, int64_t end) {
    for (int64_t d = start; d < end; d++) {
      /* loop over output */
      for (int64_t ot = 0; ot < osizeT; ot++) {
        int istartT = start_index(ot, osizeT, isizeT);
        int iendT = end_index(ot, osizeT, isizeT);
        int kT = iendT - istartT;

        for (int64_t oh = 0; oh < osizeH; oh++) {
          int istartH = start_index(oh, osizeH, isizeH);
          int iendH = end_index(oh, osizeH, isizeH);
          int kH = iendH - istartH;

          for (int64_t ow = 0; ow < osizeW; ow++) {
            int istartW = start_index(ow, osizeW, isizeW);
            int iendW = end_index(ow, osizeW, isizeW);
            int kW = iendW - istartW;

            /* local pointers */
            scalar_t* ip = input_p + d * istrideD + istartT * istrideT +
                istartH * istrideH + istartW * istrideW;
            scalar_t* op = output_p + d * osizeT * osizeH * osizeW +
                ot * osizeH * osizeW + oh * osizeW + ow;

            /* compute local average: */
            scalar_t sum = 0;
            for (int it = 0; it < kT; it++) {
              for (int ih = 0; ih < kH; ih++) {
                for (int iw = 0; iw < kW; iw++) {
                  scalar_t val =
                      *(ip + it * istrideT + ih * istrideH + iw * istrideW);
                  sum += val;
                }
}
}
            /* set output to local average */
            *op = sum / kT / kH / kW;
          }
}
}
}
  });
}
void adaptive_avg_pool3d_out_cpu_template(
"
844,"def __repr__(self) -> str:
return self.name
def replace_all_uses_with(self, replace_with : 'Node') -> List['Node']:
""""""
Replace all uses of ``self`` in the Graph with the Node ``replace_with``.
","def __repr__(self) -> str:
return self.name
    def _pretty_print_target(self, target):
        """"""
        Make target printouts more user-friendly.
        1) builtins will be printed as `builtins.xyz`
        2) operators will be printed as `operator.xyz`
        3) other callables will be printed with qualfied name, e.g. torch.add
        """"""
        if isinstance(target, str):
            return target
        if hasattr(target, '__module__'):
            if not hasattr(target, '__name__'):
                # Just to be defensive, if we don't have `__name__`, get the
                # qualname. Not sure if this happens for any members of `operator`
                # or `builtins`. This fallback path is not as good, since e.g.
                # things in `operator` have `_operator` as their __module__.
                return _get_qualified_name(target)
            if target.__module__ == 'builtins':
                return f'builtins.{target.__name__}'
            elif target.__module__ == '_operator':
                return f'operator.{target.__name__}'
        return _get_qualified_name(target)

    def format_node(self,
                    placeholder_names: List[str] = None,
                    maybe_return_typename: List[str] = None) -> Optional[str]:
        """"""
        Return a descriptive string representation of ``self``.

        This method can be used with no arguments as a debugging
        utility.

        This function is also used internally in the ``__str__`` method
        of ``Graph``. Together, the strings in ``placeholder_names``
        and ``maybe_return_typename`` make up the signature of the
        autogenerated ``forward`` function in this Graph's surrounding
        GraphModule. ``placeholder_names`` and ``maybe_return_typename``
        should not be used otherwise.

        Args:
            placeholder_names: A list that will store formatted strings
                representing the placeholders in the generated
                ``forward`` function. Internal use only.
            maybe_return_typename: A single-element list that will store
                a formatted string representing the output of the
                generated ``forward`` function. Internal use only.

        Returns:
            str: If 1) we're using ``format_node`` as an internal helper
                in the ``__str__`` method of ``Graph``, and 2) ``self``
                is a placeholder Node, return ``None``. Otherwise,
                return a  descriptive string representation of the
                current Node.
        """"""
        if self.op == 'placeholder':
            assert isinstance(self.target, str)
            arg_str = self.target
            arg_str += arg_str + f': {_type_repr(self.type)}' if self.type else ''
            if placeholder_names:
                placeholder_names.append(arg_str)
                return None
            maybe_typename = f'{_type_repr(self.type)} ' if self.type else ''
            default_val = '(default=' + str(self.args[0]) + ')' if self.args else ''
            return f'%{self.name} : {maybe_typename}[#users={len(self.users)}] = {self.op}[target={self.target}]{default_val}'
        elif self.op == 'get_attr':
            maybe_typename = f'{_type_repr(self.type)} ' if self.type is not None else ''
            return f'%{self.name} : {maybe_typename}[#users={len(self.users)}] = ' \
                   f'{self.op}[target={self._pretty_print_target(self.target)}]'
        elif self.op == 'output':
            if self.type and maybe_return_typename:
                maybe_return_typename[0] = f' -> {_type_repr(self.type)}'
            return f'return {self.args[0]}'
        else:
            maybe_typename = f'{_type_repr(self.type)} ' if self.type is not None else ''
            return f'%{self.name} : {maybe_typename}[#users={len(self.users)}] = ' \
                   f'{self.op}[target={self._pretty_print_target(self.target)}](' \
                   f'args = {_format_arg(self.args)}, kwargs = {_format_arg(self.kwargs)})'

def replace_all_uses_with(self, replace_with : 'Node') -> List['Node']:
""""""
Replace all uses of ``self`` in the Graph with the Node ``replace_with``.
"
845,"Args:
 ``activities`` - list of activity groups (CPU, CUDA) to use in profiling, supported values:
      ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``
 ``schedule`` - callable that takes step (int) as a single parameter and returns
``ProfilerAction`` value that specifies the profiler action to perform at each step;
 ``on_trace_ready`` - callable that is called at each step when ``schedule`` returns ``ProfilerAction.RECORD_AND_SAVE``
","Args:
      ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``;
      default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA;
``ProfilerAction`` value that specifies the profiler action to perform at each step;
"
846,"return self;
}
std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<std::vector<int64_t>> &sizes, int64_t dim) {
std::vector<Tensor> grad_inputs(sizes.size());
if (!grad.defined()) {
return grad_inputs;
}
dim = at::legacy_cat_wrap_dim(dim, sizes);
int64_t accumulate = 0;
for (size_t i = 0; i < sizes.size(); ++i) {
auto& shape = sizes[i];
// If input was empty tensor, gradInput should be empty tensor.
if (shape == std::vector<int64_t>({0})) {
      grad_inputs[i] = at::zeros({0}, grad.options());
continue;
}
auto size = shape[dim];
accumulate += size;
    grad_inputs[i] = grad.narrow(dim, accumulate - size, size);
}
return grad_inputs;
}
","return self;
}
std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<std::vector<int64_t>> &sizes, const std::vector<ScalarType> &dtypes, int64_t dim) {
std::vector<Tensor> grad_inputs(sizes.size());
if (!grad.defined()) {
return grad_inputs;
}
dim = at::legacy_cat_wrap_dim(dim, sizes);
int64_t accumulate = 0;

  Tensor grad_;
  bool grad_is_complex = grad.is_complex();
  if (grad_is_complex) {
    grad_ = at::real(grad);
  }
for (size_t i = 0; i < sizes.size(); ++i) {
    Tensor grad_val;
    if (!at::isComplexType(dtypes[i]) && grad_is_complex) {
      // R -> C
      grad_val = grad_;
    } else {
      grad_val = grad;
    }
auto& shape = sizes[i];
// If input was empty tensor, gradInput should be empty tensor.
if (shape == std::vector<int64_t>({0})) {
      grad_inputs[i] = at::zeros({0}, grad_val.options());
continue;
}
auto size = shape[dim];
accumulate += size;
    grad_inputs[i] = grad_val.narrow(dim, accumulate - size, size);
}
return grad_inputs;
}
"
847,"def atleast_3d(*tensors):
r""""""
    Returns a 3-dimensional view of each each input tensor with zero dimensions.
Input tensors with three or more dimensions are returned as-is.
Args:
input (Tensor or list of Tensors)
","def atleast_3d(*tensors):
r""""""
    Returns a 3-dimensional view of each input tensor with zero dimensions.
Input tensors with three or more dimensions are returned as-is.
Args:
input (Tensor or list of Tensors)
"
848,"from torch.fx import Graph, GraphModule, Node, symbolic_trace
import copy
from typing import Callable, Dict, List, NamedTuple, Set
","from .graph_module import GraphModule
from .graph import Graph
from .node import Node
from .symbolic_trace import symbolic_trace
import copy
from typing import Callable, Dict, List, NamedTuple, Set
"
849,"flush();
}
catch (const std::exception& e) {
    LOG(WARNING)
        << ""Vulkan: Context destructor raised an exception!  Error: ""
        << e.what();
}
catch (...) {
    LOG(WARNING) << ""Vulkan: Context destructor raised an unknown exception!"";
}
}
","flush();
}
catch (const std::exception& e) {
    TORCH_WARN(
        ""Vulkan: Context destructor raised an exception! Error: "",
        e.what());
}
catch (...) {
    TORCH_WARN(
        ""Vulkan: Context destructor raised an exception! ""
        ""Error: Unknown"");
}
}
"
850,"}
}
catch (const std::exception& e) {
    LOG(WARNING)
        << ""Vulkan: Descriptor pool destructor raised an exception!  Error: ""
        << e.what();
}
catch (...) {
    LOG(WARNING)
        << ""Vulkan: Descriptor pool destructor raised an unknown exception!"";
}
}
","}
}
catch (const std::exception& e) {
    TORCH_WARN(
        ""Vulkan: Descriptor pool destructor raised an exception! Error: "",
        e.what());
}
catch (...) {
    TORCH_WARN(
        ""Vulkan: Descriptor pool destructor raised an exception! ""
        ""Error: Unknown"");
}
}
"
851,"try {
return new Runtime(Configuration::kRuntime);
}
catch (...) {
      return nullptr;
}
  }());
  return runtime.get();
}
Runtime* runtime() {
  Runtime* const runtime = initialize();
  TORCH_CHECK(
runtime,
      ""Vulkan: Backend not available on this platform!""
      ""Calls to api::runtime() must have been guarded by api::available()."");
  return runtime;
}
} // namespace api
","try {
return new Runtime(Configuration::kRuntime);
}
    catch (const std::exception& e) {
      TORCH_WARN(
          ""Vulkan: Failed to initialize runtime! Error: "",
          e.what());
    }
catch (...) {
      TORCH_WARN(
          ""Vulkan: Failed to initialize runtime! ""
          ""Error: Unknown"");
}
    return nullptr;
  }());
  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(
runtime,
      ""Invalid Vulkan runtime!"");
  return runtime.get();
}
} // namespace api
"
852,"default:
TORCH_CHECK(
        false,
        ""Vulkan tensor format not supported!"");
}
return VK_FORMAT_UNDEFINED;
","default:
TORCH_CHECK(
          false,
          ""Vulkan tensor format not supported!"");
}
return VK_FORMAT_UNDEFINED;
"
853,"const auto& element = vals[i];
const auto& m_tuple = element.toTuple()->elements();
const std::string& function_name = m_tuple[0].toStringRef();
    IValue table = m_tuple[1];
auto function = std::unique_ptr<mobile::Function>(
new mobile::Function(c10::QualifiedName(function_name)));
const auto& ins_list =
        expect_field(table, ""instructions"", BYTECODE_INDEX_INSTRUCTION)
.toTuple()
>elements();
const auto& ops_list =
        expect_field(table, ""operators"", BYTECODE_INDEX_OPERATOR)
.toTuple()
>elements();
const auto& consts_list =
        expect_field(table, ""constants"", BYTECODE_INDEX_CONSTANT)
.toTuple()
>elements();
const auto& types_list =
        expect_field(table, ""types"", BYTECODE_INDEX_TYPE).toTuple()->elements();
const auto& register_size =
        expect_field(table, ""register_size"", BYTECODE_INDEX_REGISTER_SIZE)
.toInt();
std::vector<IValue> module_debug_info_list;
","const auto& element = vals[i];
const auto& m_tuple = element.toTuple()->elements();
const std::string& function_name = m_tuple[0].toStringRef();
    IValue codeTable = m_tuple[1];
    auto schemaTable = // older files do not store function schema
        (model_version > 0x4L || (model_version == 0x4L && m_tuple.size() >= 3))
        ? at::optional<IValue>{m_tuple[2]}
        : at::nullopt;
auto function = std::unique_ptr<mobile::Function>(
new mobile::Function(c10::QualifiedName(function_name)));
const auto& ins_list =
        expect_field(codeTable, ""instructions"", BYTECODE_INDEX_INSTRUCTION)
.toTuple()
const auto& ops_list =
        expect_field(codeTable, ""operators"", BYTECODE_INDEX_OPERATOR)
.toTuple()
const auto& consts_list =
        expect_field(codeTable, ""constants"", BYTECODE_INDEX_CONSTANT)
.toTuple()
const auto& types_list =
        expect_field(codeTable, ""types"", BYTECODE_INDEX_TYPE)
            .toTuple()
            ->elements();
const auto& register_size =
        expect_field(codeTable, ""register_size"", BYTECODE_INDEX_REGISTER_SIZE)
.toInt();
std::vector<IValue> module_debug_info_list;
"
854,"Example::
>>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
    return allreduce_fut(process_group, bucket.get_tensors()[0])
def fp16_compress_hook(
","Example::
>>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
    group_to_use = process_group if process_group is not None else dist.group.WORLD
    world_size = group_to_use.size()

    tensor = bucket.get_tensors()[0]
    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()

    def then_callback(fut):
        return [fut.value()[0].div_(world_size)]

    return fut.then(then_callback)
def fp16_compress_hook(
"
855,"3.6) Allreduces Qs as a batch;
3.7) Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.
TODO(wayi@): The above procedure does two matmul+allreduce steps per iteration --
one left multiplication and one right multiplication.
For warm-start, can take one such step at a time, and alternate between them.
","3.6) Allreduces Qs as a batch;
3.7) Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.
    Note that this communication hook enforces vanilla allreduce for the first `state.start_powerSGD_iter` iterations.
    This can not only allow the user to have a finer tuning over the tradeoff between speedup and accuracy,
    but also help abstract away some complexity of the internal optimization of DDP for future communication hook developers.

TODO(wayi@): The above procedure does two matmul+allreduce steps per iteration --
one left multiplication and one right multiplication.
For warm-start, can take one such step at a time, and alternate between them.
"
856,"q_idx += m * matrix_approximation_rank
# If warm-start is enabled, reuse Qs from the previous iteration if possible and skip filling random values.
    # The exceptions are the first time and when the buckets are rebuilt.
if not need_randomize_qs:
for q in qs:
_orthogonalize(q)
","q_idx += m * matrix_approximation_rank
# If warm-start is enabled, reuse Qs from the previous iteration if possible and skip filling random values.
    # The exception is the first iteration when PowerSGD is applied.
if not need_randomize_qs:
for q in qs:
_orthogonalize(q)
"
857,"using stream_set = std::unordered_set<cuda::CUDAStream>;
constexpr size_t kMinBlockSize = 512;       // all sizes are rounded to at least 512 bytes
constexpr size_t kSmallSize = 1048576;      // largest ""small"" allocation is 1 MiB
constexpr size_t kSmallBuffer = 2097152;    // ""small"" allocations are packed in 2 MiB blocks
constexpr size_t kLargeBuffer = 20971520;   // ""large"" allocations may be packed in 20 MiB blocks
constexpr size_t kMinLargeAlloc = 10485760; // allocations between 1 and 10 MiB may use kLargeBuffer
constexpr size_t kRoundLarge = 2097152;     // round up large allocations to 2 MiB
typedef std::bitset<static_cast<size_t>(StatType::NUM_TYPES)> StatTypes;
","using stream_set = std::unordered_set<cuda::CUDAStream>;
constexpr size_t kMinBlockSize  =      512; // all sizes are rounded to at least 512 bytes
constexpr size_t kSmallSize     =  1048576; // largest ""small"" allocation is 1 MiB
constexpr size_t kSmallBuffer   =  2097152; // ""small"" allocations are packed in 2 MiB blocks
constexpr size_t kLargeBuffer   = 20971520; // ""large"" allocations may be packed in 20 MiB blocks
constexpr size_t kMinLargeAlloc = 10485760; // allocations between 1 and 10 MiB may use kLargeBuffer
constexpr size_t kRoundLarge    =  2097152; // round up large allocations to 2 MiB
typedef std::bitset<static_cast<size_t>(StatType::NUM_TYPES)> StatTypes;
"
858,"def _no_mutation(self, *args, **kwargs):
    raise NotImplementedError(f""'{type(self).__name__}' object does not support mutation"")
def _create_immutable_container(base, mutable_functions):
container = type('immutable_' + base.__name__, (base,), {})
","
_help_mutation = """"""\
If you are attempting to modify the kwargs or args of a torch.fx.Node object,
instead create a new copy of it and assign the copy to the node:
    new_args = ... # copy and mutate args
    node.args = new_args
""""""

def _no_mutation(self, *args, **kwargs):
    raise NotImplementedError(f""'{type(self).__name__}' object does not support mutation. {_help_mutation}"")
def _create_immutable_container(base, mutable_functions):
container = type('immutable_' + base.__name__, (base,), {})
"
859,"@staticmethod
def build_For(ctx, stmt):
r = ctx.make_range(stmt.lineno, stmt.col_offset, stmt.col_offset + len(""for""))
return For(
r, [build_expr(ctx, stmt.target)],
[build_expr(ctx, stmt.iter)], build_stmts(ctx, stmt.body))
","@staticmethod
def build_For(ctx, stmt):
r = ctx.make_range(stmt.lineno, stmt.col_offset, stmt.col_offset + len(""for""))
        if stmt.orelse:
            raise NotSupportedError(r, ""else branches of for loops aren't supported"")

return For(
r, [build_expr(ctx, stmt.target)],
[build_expr(ctx, stmt.iter)], build_stmts(ctx, stmt.body))
"
860,"with torch.random.fork_rng(devices=[]):
# Fork this RNG to avoid changing the seed globally and affecting the random sampling anywhere else in the training.
# The seed makes sure that the initial random values are the same across all the DDP replicas.
            # Such seed should differ at every step.
# Since it is very slow to fork RNG state across all the CUDA devices,
# only fork on CPU and then move the generated tensor to the CUDA device (by overwriting q).
torch.manual_seed(state.rng.randint(1_000_000_000))
","with torch.random.fork_rng(devices=[]):
# Fork this RNG to avoid changing the seed globally and affecting the random sampling anywhere else in the training.
# The seed makes sure that the initial random values are the same across all the DDP replicas.
            # This seed should differ at every step.
# Since it is very slow to fork RNG state across all the CUDA devices,
# only fork on CPU and then move the generated tensor to the CUDA device (by overwriting q).
torch.manual_seed(state.rng.randint(1_000_000_000))
"
861,"else:
return ConstRefCType(tensor_type)
elif str(t) == 'Tensor?[]':
        return BaseCType('const c10::List<c10::optional<Tensor>> &', binds)
return cpp.argumenttype_type(t, mutable=mutable, binds=binds)
def returns_type(rs: Sequence[Return]) -> str:
","else:
return ConstRefCType(tensor_type)
elif str(t) == 'Tensor?[]':
        return ConstRefCType(BaseCType(""c10::List<c10::optional<Tensor>>"", binds))
return cpp.argumenttype_type(t, mutable=mutable, binds=binds)
def returns_type(rs: Sequence[Return]) -> str:
"
862,"} // namespace detail
} // namespace cuda
/**
* CUDAGeneratorImpl class implementation
","} // namespace detail
} // namespace cuda
/**
 * Note [Why enforce RNG offset % 4 == 0?]
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * Curand philox does allow offsets that aren't a multiple of 4.
 * But jit kernels don't use curand, they use a custom ""Philox"" class (see
 * torch/csrc/jit/tensorexpr/cuda_random.h or
 * torch/csrc/jit/codegen/cuda/runtime/random_numbers.cu).
 * The ""Philox"" constructor computes offset/4 (a uint64_t division) to locate its
 * internal start in its virtual bitstream viewed as 128-bit chunks, then, when called
 * in a thread, returns one 32-bit chunk at a time from that start in the bitstream.
 * In other words, if the incoming offset is not a multiple of 4, each thread
 * might repeat some previously-generated 32-bit values in the bitstream. See
 * https://github.com/pytorch/pytorch/pull/50169.
 */
/**
* CUDAGeneratorImpl class implementation
"
863,"#include <torch/csrc/jit/codegen/cuda/kernel_cache.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
#include <torch/csrc/jit/codegen/cuda/parser.h>
","#include <torch/csrc/jit/codegen/cuda/kernel_cache.h>

#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
#include <torch/csrc/jit/codegen/cuda/parser.h>
"
864,"#include <torch/csrc/jit/frontend/builtin_functions.h>
#include <torch/csrc/api/include/torch/jit.h>
#include <torch/csrc/jit/frontend/code_template.h>
#include <torch/csrc/jit/frontend/resolver.h>
","#include <torch/csrc/jit/frontend/builtin_functions.h>

#include <torch/csrc/api/include/torch/jit.h>
#include <torch/csrc/jit/frontend/code_template.h>
#include <torch/csrc/jit/frontend/resolver.h>
"
865,"#include <torch/csrc/jit/mobile/module.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/runtime/jit_exception.h>
","#include <torch/csrc/jit/mobile/module.h>

#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/runtime/jit_exception.h>
"
866,"#include <torch/csrc/jit/passes/constant_pooling.h>
#include <ATen/core/interned_strings.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir.h>
","#include <torch/csrc/jit/passes/constant_pooling.h>

#include <ATen/core/interned_strings.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir.h>
"
867,"#include <torch/csrc/jit/passes/inline_forked_closures.h>
#include <torch/csrc/jit/frontend/ir_emitter.h>
namespace torch {
","#include <torch/csrc/jit/passes/inline_forked_closures.h>

#include <torch/csrc/jit/frontend/ir_emitter.h>
namespace torch {
"
868,"#include <torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h>
#include <aten/src/ATen/InitialTensorOptions.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
","#include <torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h>

#include <aten/src/ATen/InitialTensorOptions.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
"
869,"#include <torch/csrc/jit/passes/specialize_autogradzero.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/ir/ir.h>
#include <torch/csrc/jit/jit_log.h>
","#include <torch/csrc/jit/passes/specialize_autogradzero.h>

#include <c10/util/Exception.h>
#include <torch/csrc/jit/ir/ir.h>
#include <torch/csrc/jit/jit_log.h>
"
870,"#include <torch/csrc/jit/python/python_custom_class.h>
#include <torch/csrc/jit/frontend/sugared_value.h>
#include <fmt/format.h>
","#include <torch/csrc/jit/python/python_custom_class.h>

#include <torch/csrc/jit/frontend/sugared_value.h>
#include <fmt/format.h>
"
871,"// register size
auto register_size = static_cast<int>(code.register_size());
  auto table = Table({{""instructions"", Tup(instructions)},
                      {""operators"", Tup(operators)},
                      {""constants"", Tup(deduplicated_constants)},
                      {""types"", Tup(types)},
                      {""register_size"", register_size}});
auto bytecode_vals = Tup({func.qualname().qualifiedName(), table});
c10::optional<IValue> debug_info_vals;
","// register size
auto register_size = static_cast<int>(code.register_size());
  auto table = Table(
      {{""instructions"", Tup(instructions)},
       {""operators"", Tup(operators)},
       {""constants"", Tup(deduplicated_constants)},
       {""types"", Tup(types)},
       {""register_size"", register_size}});
auto bytecode_vals = Tup({func.qualname().qualifiedName(), table});
c10::optional<IValue> debug_info_vals;
"
872,"#include <torch/csrc/jit/serialization/pickle.h>
#include <ATen/core/ivalue.h>
#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/WindowsTorchApiMacro.h>
","#include <torch/csrc/jit/serialization/pickle.h>

#include <ATen/core/ivalue.h>
#include <caffe2/serialize/inline_container.h>
#include <torch/csrc/WindowsTorchApiMacro.h>
"
873,"def _site_packages(dirname, platform):
if platform.startswith(""win""):
        os.path.join(pytdir.name, ""Lib"", ""site-packages"")
else:
template = os.path.join(dirname, ""lib"", ""python*.*"", ""site-packages"")
        spdir = glob.glob(template)[0]
return spdir
","def _site_packages(dirname, platform):
if platform.startswith(""win""):
        template = os.path.join(dirname, ""Lib"", ""site-packages"")
else:
template = os.path.join(dirname, ""lib"", ""python*.*"", ""site-packages"")
    spdir = glob.glob(template)[0]
return spdir
"
874,"Example (fork a free function):
    .. testcode::
import torch
from torch import Tensor
def foo(a : Tensor, b : int) -> Tensor:
","Example (fork a free function):
    .. code-block:: python

import torch
from torch import Tensor
def foo(a : Tensor, b : int) -> Tensor:
"
875,"A constraint object represents a region over which a variable is valid,
e.g. within which a variable can be optimized.
""""""
def check(self, value):
""""""
Returns a byte tensor of `sample_shape + batch_shape` indicating
","A constraint object represents a region over which a variable is valid,
e.g. within which a variable can be optimized.
""""""
    is_discrete = False

def check(self, value):
""""""
Returns a byte tensor of `sample_shape + batch_shape` indicating
"
876,"""""""
Constrain to an integer interval `(-inf, upper_bound]`.
""""""
def __init__(self, upper_bound):
self.upper_bound = upper_bound
","""""""
Constrain to an integer interval `(-inf, upper_bound]`.
""""""
    is_discrete = True

def __init__(self, upper_bound):
self.upper_bound = upper_bound
"
877,"dependent = _Dependent()
dependent_property = _DependentProperty
boolean = _Boolean()
nonnegative_integer = _IntegerGreaterThan(0)
positive_integer = _IntegerGreaterThan(1)
integer_interval = _IntegerInterval
","dependent = _Dependent()
dependent_property = _DependentProperty
boolean = _Boolean()
one_hot = _OneHot()
nonnegative_integer = _IntegerGreaterThan(0)
positive_integer = _IntegerGreaterThan(1)
integer_interval = _IntegerInterval
"
878,"return 1 - torch.exp(-self.rate * value)
def icdf(self, value):
        if self._validate_args:
            self._validate_sample(value)
return -torch.log(1 - value) / self.rate
def entropy(self):
","return 1 - torch.exp(-self.rate * value)
def icdf(self, value):
return -torch.log(1 - value) / self.rate
def entropy(self):
"
879,"return 0.5 - 0.5 * (value - self.loc).sign() * torch.expm1(-(value - self.loc).abs() / self.scale)
def icdf(self, value):
        if self._validate_args:
            self._validate_sample(value)
term = value - 0.5
return self.loc - self.scale * (term).sign() * torch.log1p(-2 * term.abs())
","return 0.5 - 0.5 * (value - self.loc).sign() * torch.expm1(-(value - self.loc).abs() / self.scale)
def icdf(self, value):
term = value - 0.5
return self.loc - self.scale * (term).sign() * torch.log1p(-2 * term.abs())
"
880,"tensor([ 0.,  1.])
"""""".format(**common_args))
# TODO: see https://github.com/pytorch/pytorch/issues/43667
add_docstr(torch.eye,
r""""""
eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
","tensor([ 0.,  1.])
"""""".format(**common_args))
add_docstr(torch.eye,
r""""""
eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
"
881,"[ 1.0100, -1.1975, -0.0102, -0.4732],
[-0.9240,  0.1207, -0.7506, -1.0213],
[ 1.7809, -1.2960,  0.9384,  0.1438]])
>>> torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])
"""""".format(**single_dim_common))
add_docstr(torch.mm,
","[ 1.0100, -1.1975, -0.0102, -0.4732],
[-0.9240,  0.1207, -0.7506, -1.0213],
[ 1.7809, -1.2960,  0.9384,  0.1438]])
    >>> torch.argmin(a)
    tensor(13)
>>> torch.argmin(a, dim=1)
tensor([ 2,  1,  3,  1])
    >>> torch.argmin(a, dim=1, keepdim=True)
    tensor([[2],
            [1],
            [3],
            [1]])
"""""".format(**single_dim_common))
add_docstr(torch.mm,
"
882,""""""".format(**factory_common_args))
# TODO: see https://github.com/pytorch/pytorch/issues/43667
add_docstr(torch.ones_like,
r""""""
ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
",""""""".format(**factory_common_args))
add_docstr(torch.ones_like,
r""""""
ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
"
883,"{""__idiv__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_div_>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__ifloordiv__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_floor_divide_>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__mod__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_remainder>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__eq__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_eq>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__ne__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_ne>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__lt__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_lt>), METH_VARARGS | METH_KEYWORDS, NULL},
","{""__idiv__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_div_>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__ifloordiv__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_floor_divide_>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__mod__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_remainder>), METH_VARARGS | METH_KEYWORDS, NULL},
  {""__imod__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_remainder_>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__eq__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_eq>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__ne__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_ne>), METH_VARARGS | METH_KEYWORDS, NULL},
{""__lt__"", castPyCFunctionWithKeywords(TypeError_to_NotImplemented_<THPVariable_lt>), METH_VARARGS | METH_KEYWORDS, NULL},
"
884,"# find _inputs_ to matched nodes that are not quantized, these
# have to be quantized, which requires measuring stats,
# initialize an DefaultQuantizeHandler object for each
        quants = self._find_quants(model.graph, matches)
self.activation_post_process_map = dict()
env: Dict[Any, Any] = {}
","# find _inputs_ to matched nodes that are not quantized, these
# have to be quantized, which requires measuring stats,
# initialize an DefaultQuantizeHandler object for each
        quants: Dict[str, Tuple[DefaultQuantizeHandler, Callable]] = \
            self._find_quants(model.graph, matches)
self.activation_post_process_map = dict()
env: Dict[Any, Any] = {}
"
885,"return env[n.name]
def load_quantized(n):
            if n.name not in quant_env:
                assert n.name in env, \
                    'trying to load quantized node but did not find node:' + \
                    n.name + ' in float environment:' + str(env)
                assert n.name in quants, \
                    'did not find quant object for node:' + n.name
                quant = quants[n.name][0]
                quant_env[n.name] = quant.convert(self, env[n.name])
return quant_env[n.name]
def load_x(n):
","return env[n.name]
def load_quantized(n):
            assert n.name in quant_env, \
                'trying to load quantized node but did not find node:' + \
                n.name + ' in quant environment:' + str(quant_env)
return quant_env[n.name]
def load_x(n):
"
886,"input_tensor = bucket.get_tensors()[0]
device = input_tensor.device
dtype = input_tensor.dtype
# Unflatten the input tensor into per-parameter tensors, for layer-wise compression.
tensors = [
input_tensor[offset : offset + length].view(sizes)
","input_tensor = bucket.get_tensors()[0]
device = input_tensor.device
dtype = input_tensor.dtype

    # Incorporate the error from the previous state into the gradients.
    bucket_index = bucket.get_index()
    input_tensor_cp = None
    total_length = input_tensor.shape[0]
    if state.use_error_feedback:
        # The buckets can be rebuilt during training.
        # In this case, the error tensor shape will not be aligned with the input tensor,
        # and the error will be re-initialized as zeros.
        if (
            bucket_index in state.error_dict
            and state.error_dict[bucket_index].shape[0] == total_length
        ):
            input_tensor.add_(state.error_dict[bucket_index])
        else:
            logging.info(
                ""A zero tensor of length {} that represents local error is created."".format(
                    total_length
                )
            )
            state.error_dict[bucket_index] = torch.zeros(total_length, device=device)

        # Keep a copy of the input tensor,
        # so that we can compute the local error caused by compression later,
        # by comparing this copy and the input tensor updated after decompression.
        input_tensor_cp = torch.clone(input_tensor).detach()

# Unflatten the input tensor into per-parameter tensors, for layer-wise compression.
tensors = [
input_tensor[offset : offset + length].view(sizes)
"
887,"}
# Whitelist for propagating the qconfig
_EXCLUDE_QCONFIG_PROPAGATE_LIST : Set[Callable] = {
    DeQuantStub,
}
_INCLUDE_QCONFIG_PROPAGATE_LIST : Set[Callable] = {
nn.Sequential,
}
","}
# Whitelist for propagating the qconfig
_INCLUDE_QCONFIG_PROPAGATE_LIST : Set[Callable] = {
nn.Sequential,
}
"
888,"return Module(owner_);
}
void Method::run(Stack& stack) {
  stack.insert(stack.begin(), owner()._ivalue()); // self
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
function_->run(stack);
}
IValue Method::operator()(std::vector<IValue> stack, const Kwargs& kwargs) {
  stack.insert(stack.begin(), owner()._ivalue()); // self
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
return (*function_)(std::move(stack), kwargs);
}
","return Module(owner_);
}
void Method::run(Stack& stack) {
  stack.insert(stack.begin(), owner()._ivalue());
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
function_->run(stack);
}
IValue Method::operator()(std::vector<IValue> stack, const Kwargs& kwargs) {
  stack.insert(stack.begin(), owner()._ivalue());
RECORD_TORCHSCRIPT_FUNCTION(name(), stack);
return (*function_)(std::move(stack), kwargs);
}
"
889,"return len;
};
  static const c10::QualifiedName torchPrefix = ""__torch__"";
  auto type_resolver = [&](const c10::QualifiedName& qn) {
    TypePtr type;
    // HACK: first we check whether the name starts with `__torch__` to tell if
    // it's ""supposed"" to be a class type. This is a reliable check today, but
    // there is no guarantee that this is the case. The real solution is to
    // merge type parsers so we can share class resolution logic.
    if (torchPrefix.isPrefixOf(qn)) {
      if (compilation_unit_->get_class(qn) == nullptr) {
        auto typeptr = ClassType::create(qn, compilation_unit_, true);
        compilation_unit_->register_type(typeptr);
      }
      type = compilation_unit_->get_class(qn);
    } else {
      type = c10::parseType(qn.qualifiedName());
    }
    return c10::StrongTypePtr(compilation_unit_, type);
};
auto obj_loader = [&](at::StrongTypePtr type, IValue input) {
","return len;
};
  auto type_resolver = [this](const c10::QualifiedName& qn) {
    return c10::StrongTypePtr(compilation_unit_, resolveTypeName(qn));
};
auto obj_loader = [&](at::StrongTypePtr type, IValue input) {
"
890,"Method::Method(const Module* owner, Function* function)
: owner_(owner), function_(function) {}
void Method::run(Stack& stack) {
auto observer = torch::observerConfig().getModuleObserver();
auto instance_key = std::rand();
/* if the metadata dict doesn't contain ""model_name"", copy the metadata and
","Method::Method(const Module* owner, Function* function)
: owner_(owner), function_(function) {}
void Method::run(Stack& stack) const {
auto observer = torch::observerConfig().getModuleObserver();
auto instance_key = std::rand();
/* if the metadata dict doesn't contain ""model_name"", copy the metadata and
"
891,"if state.use_error_feedback:
# Memorize the local errors.
state.error_dict[bucket_index] = input_tensor_cp - input_tensor
ret = input_tensor.resize_(total_length)
return [ret]
","if state.use_error_feedback:
# Memorize the local errors.
state.error_dict[bucket_index] = input_tensor_cp - input_tensor
            assert not torch.any(torch.isnan(state.error_dict[bucket_index]))
ret = input_tensor.resize_(total_length)
return [ret]
"
892,"if (it != shape_info_.end()) {
it->second.setDimType(std::vector<TensorBoundShape::DimType>(
it->second.shape.dims_size(), TensorBoundShape_DimType_CONSTANT));
}
}
","if (it != shape_info_.end()) {
it->second.setDimType(std::vector<TensorBoundShape::DimType>(
it->second.shape.dims_size(), TensorBoundShape_DimType_CONSTANT));
    if (op.type() == ""ConstantFill"" && op.input_size() >= 1) {
        auto it_input = shape_info_.find(op.input(0));
        if (it_input != shape_info_.end()) {
          it->second.setDimType(it_input->second.getDimType());
        }
    }
}
}
"
893,"import itertools
from dataclasses import dataclass
from typing import Optional, Union, Sequence, Set, List, Tuple, Dict
","from dataclasses import dataclass
from typing import Optional, Union, Sequence, Set, List, Tuple, Dict
"
894,"# NB: doesn't contain out arguments
@property
    def kwarg_only(self) -> Sequence[Argument]:
ret: List[Argument] = []
ret.extend(self.pre_tensor_options_kwarg_only)
if self.tensor_options is not None:
","# NB: doesn't contain out arguments
@property
    def flat_kwarg_only(self) -> Sequence[Argument]:
ret: List[Argument] = []
ret.extend(self.pre_tensor_options_kwarg_only)
if self.tensor_options is not None:
"
895,"#include <ATen/miopen/Handle.h>

#include <ATen/miopen/Exceptions.h>

#include <unordered_map>
#include <mutex>
namespace at { namespace native {

namespace {
struct Handle {
  miopenHandle_t handle;
  Handle() : handle(NULL) {
    MIOPEN_CHECK(miopenCreate(&handle));
  }
  ~Handle() {
    if (handle) {
      miopenDestroy(handle);
    }
  }
};
std::mutex mutex;
std::unordered_map<int, Handle> handles;
}  // namespace
miopenHandle_t getMiopenHandle()
{
int device;
HIP_CHECK(hipGetDevice(&device));
  std::lock_guard<std::mutex> guard(mutex);
  return handles[device].handle;
}
}} // namespace at::native
","#include <ATen/miopen/Exceptions.h>
#include <ATen/miopen/Handle.h>
#include <ATen/hip/detail/DeviceThreadHandles.h>
#include <c10/hip/HIPStream.h>
namespace at { namespace native {
namespace {
void createMIOpenHandle(miopenHandle_t *handle) {
  MIOPEN_CHECK(miopenCreate(handle));
}
void destroyMIOpenHandle(miopenHandle_t handle) {
// this is because of something dumb in the ordering of
// destruction. Sometimes atexit, the cuda context (or something)
// would already be destroyed by the time this gets destroyed. It
// happens in fbcode setting. @colesbury and I decided to not destroy
// the handle as a workaround.
//   - @soumith
//
// Further note: this is now disabled globally, because we are seeing
// the same issue as mentioned above in CUDA 11 CI.
//   - @zasdfgbnm
//
// #ifdef NO_MIOPEN_DESTROY_HANDLE
// #else
//   miopenDestroy(handle);
// #endif
}
using MIOpenPoolType = at::cuda::DeviceThreadHandlePool<miopenHandle_t, createMIOpenHandle, destroyMIOpenHandle>;
} // namespace
miopenHandle_t getMiopenHandle() {
int device;
HIP_CHECK(hipGetDevice(&device));
  // Thread local PoolWindows are lazily-initialized
  // to avoid initialization issues that caused hangs on Windows.
  // See: https://github.com/pytorch/pytorch/pull/22405
  // This thread local unique_ptrs will be destroyed when the thread terminates,
  // releasing its reserved handles back to the pool.
  static auto pool = std::make_shared<MIOpenPoolType>();
  thread_local std::unique_ptr<MIOpenPoolType::PoolWindow> myPoolWindow(
      pool->newPoolWindow());

  auto handle = myPoolWindow->reserve(device);
  MIOPEN_CHECK(miopenSetStream(handle, at::hip::getCurrentHIPStream()));
  return handle;
}
}} // namespace at::native
"
896,"{
TensorArg grad_output{ grad_output_t, ""grad_output"", 1 },
input{ input_t, ""input"", 2 };
  setMIOpenStreamToCurrent();
return miopen_convolution_backward_weight(
""miopen_convolution_backward_weight"",
weight_size, input, grad_output,
","{
TensorArg grad_output{ grad_output_t, ""grad_output"", 1 },
input{ input_t, ""input"", 2 };
return miopen_convolution_backward_weight(
""miopen_convolution_backward_weight"",
weight_size, input, grad_output,
"
897,"p1_index,
partitions
) -> float:
            """"""Given two partitions and a list of partitions, try to combine these two partitions
and see what is the cost of the modified partition list
""""""
p0 = partitions[p0_index]
","p1_index,
partitions
) -> float:
            """"""Given two partitions and a list of partitions, combine these two partitions
and see what is the cost of the modified partition list
""""""
p0 = partitions[p0_index]
"
898,"find two partitions to combine so the cost of the partitions can
be reduced.
The algorithm is :
               1. Going through all the partition pairs and see
               if the pair of partitions can be combined.
               2. If they are combined, the cost is calculated.
               3. Select the minimum cost and combine its cooresponding partition pair
""""""
partition_to_latency_mapping = get_partition_to_latency_mapping(self.partitions, node_to_latency_mapping)
cost = get_latency_of_partitioned_graph(self.partitions, partition_to_latency_mapping, transfer_rate_bytes_per_sec)
","find two partitions to combine so the cost of the partitions can
be reduced.
The algorithm is :
               1. Go through all the partition pairs and see
               if any pair of partitions can be combined.
               2. Calculate the cost after the combination.
               3. Select the minimum cost and combine its cooresponding partition pair.
""""""
partition_to_latency_mapping = get_partition_to_latency_mapping(self.partitions, node_to_latency_mapping)
cost = get_latency_of_partitioned_graph(self.partitions, partition_to_latency_mapping, transfer_rate_bytes_per_sec)
"
899,"return result;
}
// There are roughly three parts to compute einsum:
// 1. Parse equation to extract the labels for each input operand and output
// 2. Unsqueeze missing dimensions from input operands and permute to align them
// 3. Compute result by multiplying input operands and summing contraction
//    dimensions We do the last part by reducing to bmm.
Tensor einsum(std::string equation, TensorList operands) {
  TORCH_CHECK(!operands.empty(), ""einsum() must provide at least one operand"");
  checkDeviceType(""einsum()"", operands, operands[0].device().type());

  // Code for encoding ellipsis (""..."") with labels
  constexpr int ELLIPSIS = '.';

  // Find arrow (->) to split equation into lhs and rhs
  const auto arrow_pos = equation.find(""->"");
  const auto lhs = equation.substr(0, arrow_pos);

  // Convert labels for input operands into an index in [0, 25] and store
  // them in op_labels for each operand along with ELLIPSIS.
  std::vector<std::vector<int>> op_labels(operands.size());
  bool found_ell = false;
  std::string::size_type curr_op = 0;
  for (auto i = decltype(lhs.length()){0}; i < lhs.length(); ++i) {
    switch (lhs[i]) {
      case ' ':
        // Ignore spaces
        break;

      case '.':
        TORCH_CHECK(
            // Only one ellipsis per operand can be given
            !found_ell,
            ""einsum() found \'.\' for operand "",
            curr_op,
            "" for which an ellipsis was already found"");
        TORCH_CHECK(
            // Ensure it's a valid ellipsis
            i + 2 < lhs.length() && lhs[++i] == '.' && lhs[++i] == '.',
            ""einsum() found \'.\' for operand "",
            curr_op,
            "" that is not part of any ellipsis"");
        op_labels[curr_op].push_back(ELLIPSIS);
        found_ell = true;
        break;

      case ',':
        // Move onto next operand
        ++curr_op;
        TORCH_CHECK(
            curr_op < operands.size(),
            ""einsum() fewer operands were provided than specified in the equation"");
        found_ell = false;
        break;

      default:
        // Parse label
        TORCH_CHECK(
            lhs[i] >= 'a' && lhs[i] <= 'z',
            ""einsum() operand subscript must be in range [a, z] but found "",
            lhs[i],
            "" for operand "",
            curr_op);
        // Convert label to index in [0, 25] and store
        op_labels[curr_op].push_back(lhs[i] - 'a');
    }
}

  TORCH_CHECK(
      curr_op == operands.size() - 1,
      ""einsum() more operands were provided than specified in the equation"");

  // Labels must be within [a, z].
  constexpr int TOTAL_LABELS = 'z' - 'a' + 1;
  std::vector<int> label_count(TOTAL_LABELS, 0);

  // The maximum number of dimensions covered by any ellipsis, needed when
  // unsqueezing missing dimensions from operands to permute and broadcast
  int64_t ell_num_dim = 0;

  // Compute label frequency and number of dimensions covered by ellipsis
  // We do this after parsing labels to make it more readable and simpler
  // to compute the number of dimensions covered by ellipsis.
  for (std::size_t i = 0; i < operands.size(); ++i) {
    const Tensor operand = operands[i];
    std::vector<int> labels = op_labels[i];
    int64_t nlabels = labels.size();
    int64_t ndims = operand.dim();
    bool has_ellipsis = false;

    for (int label : labels) {
      if (label == ELLIPSIS) {
        --nlabels;
        has_ellipsis = true;
        ell_num_dim = std::max(ell_num_dim, ndims - nlabels);
      } else {
        ++label_count[label];
}
}

    TORCH_CHECK(
        has_ellipsis ? nlabels <= ndims : nlabels == ndims,
        ""einsum() the number of subscripts in the equation ("",
        nlabels,
        has_ellipsis ? "") is more than the number of dimensions (""
                     : "") does not match the number of dimensions ("",
        ndims,
        "") for operand "",
        i,
        has_ellipsis ? """" : "" and no ellipsis was given"");
}

  // Mapping of label to index in the permuted tensors (out_dims + sum_dims)
  // This will be used for aligning the dimensions of all input operands
  std::vector<int> label_perm_index(TOTAL_LABELS, -1);

  // Current index in the permuted shape
  int perm_index = 0;

  // Start index of ellipsis dimensions in the permuted shape
  int64_t ell_index = 0;
  found_ell = false;

  if (arrow_pos == std::string::npos) {
    // Implicit output is ellipsis (...) + labels seen only once
    perm_index = ell_num_dim;
    found_ell = true;
    for (int label = 0; label < TOTAL_LABELS; ++label) {
      if (label_count[label] == 1) {
        label_perm_index[label] = perm_index++;
}
}
  } else {
    // Parse explicit output
    const std::string rhs = equation.substr(arrow_pos + 2);
    for (std::size_t i = 0; i < rhs.length(); ++i) {
      switch (rhs[i]) {
        case ' ':
          // Ignore spaces
          break;

        case '.':
          TORCH_CHECK(
              // There can only be one ellipsis in the output
              !found_ell,
              ""einsum() found \'.\' for output but an ellipsis (...) was already found"");
          TORCH_CHECK(
              // Ensure ellipsis is correct
              i + 2 < rhs.length() && rhs[++i] == '.' && rhs[++i] == '.',
              ""einsum() found \'.\' for output that is not part of any ellipsis (...)"");
          ell_index = perm_index;
          perm_index += ell_num_dim;
          found_ell = true;
          break;

        default:
          TORCH_CHECK(
              rhs[i] >= 'a' && rhs[i] <= 'z',
              ""einsum() subscripts must be in range [a, z] but found "",
              rhs[i],
              "" for the output"");
          TORCH_CHECK(
              // Ensure label appeared at least once for some input operand and at
              // most once for the output
              label_count[rhs[i] - 'a'] > 0,
              ""einsum() output subscript "",
              rhs[i],
              label_count[rhs[i] - 'a'] == -1
                  ? "" appears more than once in the output""
                  : "" does not appear in the equation for any input operand"");
          label_perm_index[rhs[i] - 'a'] = perm_index++;

          // Set to -1 to mark that this label already appeared in the output
          label_count[rhs[i] - 'a'] = -1;
}
}
}

  // Save output size before adding sum dims
  const int out_size = perm_index;

  // If ellipsis is not part of the output, add to contraction dimensions
  if (ell_num_dim > 0 && !found_ell) {
    ell_index = perm_index;
    perm_index += ell_num_dim;
  }

  // Add contraction labels (labels not present in output)
  for (int label = 0; label < TOTAL_LABELS; ++label) {
    if (label_count[label] > 0 && label_perm_index[label] == -1) {
      label_perm_index[label] = perm_index++;
}
}
  // Here we unsqueeze missing dimensions to make all operands have the same
  // number of dimensions. We take diagonals for repeated labels within the
  // same operand. Finally we permute the operands to align dimensions as
  // per the perm_out_index we computed above.
  std::vector<Tensor> permuted_operands;
  for (std::size_t i = 0; i < operands.size(); ++i) {
    std::vector<int64_t> perm_shape(perm_index, -1);
    std::vector<int64_t> label_dim(TOTAL_LABELS, -1);
    const std::vector<int> labels = op_labels[i];
    Tensor operand = operands[i];
    const auto sizes = operand.sizes();
    std::size_t j = 0;

    for (int label : labels) {
      if (label == ELLIPSIS) {
        // Add missing dimensions under ellipsis
        int64_t num_dim_diff =
            ell_num_dim - (operand.dim() - labels.size() + 1);
        for (int64_t k = 0; k < num_dim_diff; ++k) {
          operand = operand.unsqueeze(j);
}
        for (int64_t k = 0; k < ell_num_dim; ++k) {
          perm_shape[ell_index + k] = j++;
}
      } else if (label_dim[label] != -1) {
        // Repeated label, take diagonal
        int64_t dim = label_dim[label];
        TORCH_CHECK(
            sizes[j] == sizes[dim],
            ""einsum() subscript "",
            char(label + 'a'),
            "" is repeated for operand "",
            i,
            "" but the sizes don't match, "",
            sizes[j],
            "" != "",
            sizes[dim]);
        operand = operand.diagonal(0, j, dim).movedim(-1, dim);
      } else {
        // Lookup output index for label
        label_dim[label] = j;
        perm_shape[label_perm_index[label]] = j++;
}
}

    // Add dimensions for missing labels
    for (int64_t& index : perm_shape) {
      if (index == -1) {
        operand = operand.unsqueeze(-1);
        index = j++;
}
}

    permuted_operands.push_back(operand.permute(perm_shape));
  }

  // Check if operands broadcast and keep track of last operand with
  // dimension size != 1 for optimizing reductions
  std::vector<std::size_t> dim_last_op(perm_index, 0);
  bool has_zero_size_dim = false;
  for (int dim = 0; dim < perm_index; ++dim) {
    int64_t broadcast_size = permuted_operands[0].size(dim);
    for (std::size_t i = 1; i < permuted_operands.size(); ++i) {
      int64_t dim_size = permuted_operands[i].size(dim);
      if (broadcast_size != dim_size && broadcast_size != 1 && dim_size != 1) {
        std::ostringstream msg;
        msg << ""einsum() operands do not broadcast with remapped shapes [original->remapped]:"";
        for (std::size_t j = 0; j < operands.size(); ++j) {
          msg << "" "" << operands[j].sizes() << ""->""
              << permuted_operands[j].sizes();
        }
        TORCH_CHECK(false, msg.str());
      }
      if (dim_size != 1) {
        broadcast_size = dim_size;
        dim_last_op[dim] = i;
}
}
    has_zero_size_dim |= broadcast_size == 0;
  }

  // Compute result
  Tensor result = permuted_operands[0];
  // Fast path for when an operand has zero sized dim
  if (has_zero_size_dim) {
    std::vector<int64_t> out_shape(out_size);
    for (int i = 0; i < out_size; ++i) {
      out_shape[i] = permuted_operands[dim_last_op[i]].size(i);
    }
    return at::zeros(out_shape, result.options());
}
  // Sum out or squeeze dimensions that are size 1 for all later operands
  int dim = out_size;
  for (int i = dim; i < perm_index; ++i, ++dim) {
    if (dim_last_op[i] == 0) {
      if (result.size(dim) == 1) {
        result = result.squeeze(dim--);
      } else {
        result = result.sum(dim--);
      }
}
}
  for (std::size_t i = 1; i < permuted_operands.size(); ++i) {
    Tensor operand = permuted_operands[i];
std::vector<int64_t> sum_dims;

    // Sum out or squeeze dimensions that are size 1 for all later operands
    dim = out_size;
    for (int j = dim; j < perm_index; ++j, ++dim) {
      if (dim_last_op[j] < i) {
        operand = operand.squeeze(dim);
        --dim;
      } else if (dim_last_op[j] == i) {
        if (result.size(dim) == 1) {
          operand = operand.sum(dim);
          result = result.squeeze(dim);
          --dim;
        } else {
          sum_dims.push_back(dim);
        }
}
}

    // Multiply tensors and sum out dimensions in sum_dims
    if (sum_dims.empty()) {
      result = result.mul(operand);
    } else if (sum_dims.size() == result.sizes().size()) {
      result = result.flatten().dot(operand.flatten());
    } else {
      result = sumproduct_pair(result, operand, sum_dims, false);
    }
}
return result;
}
","return result;
}
Tensor einsum(std::string eqn, TensorList tensors) {
  constexpr size_t number_of_letters = 26;
  std::string in_eqn;
  size_t pos;
  // The equation is given in terms of single lowercase letters ('a'..'z') and potentially an ellipsis.
  // Internally, we represent it using indices from 0 to num_total_dimensions, with each letter
  // mapped to an index and the ellipsis ('...') being mapped to a number of consequtive indices.
  // The mapping of letters to internal indices is given in letter_mapping. A value of -1 means that
  // the letter has not been assigned an index yet (because it has not been seen).
  // The ellipsis is defined by first_ell_idx (the first index) and num_ell_idxes (the number of indices).
  // A value of -1 for num_ell_idxes specifies that we have not seen an ellipsis yet.
  // Note: The internal indices are NOT the dimensions used internally. There is a mapping to them below.

  std::array<std::int64_t, number_of_letters> letter_mapping; // map letter to internal (numerical) label
  letter_mapping.fill(-1);
  int64_t num_ell_idxes = -1;
  int64_t first_ell_idx = 0;

  // The internal representation of the left hand side fo the equation (with ellipsis expanded) is stored in input_op_idxes.
  // For each operand, we have a vector mapping each dimension to an internal index.
  // We also keep track of the number of occurrences for each letter (to infer a right hand side if not given) and
  // of the last occurrence of each index.
  std::vector<std::vector<int64_t>> input_op_idxes;                   // the parsed operand indices
  std::array<std::int64_t, number_of_letters> num_letter_occurrences; // number of occurrence in the equation of this letter
  num_letter_occurrences.fill(0);
  std::vector<std::int64_t> last_idx_occurrence;                      // the last operator (left to right) using this index

  if ((pos = eqn.find(""->"")) != std::string::npos) { // check whether we have a right hand side. in_eq is the left hand side
    in_eqn = eqn.substr(0, pos);
  } else {
    in_eqn = eqn;
}
  // remove spaces for einsum compatibility (#9929)
  in_eqn.erase(std::remove_if(in_eqn.begin(), in_eqn.end(), isspace), in_eqn.end());

  // next we parse in_eq (the left hand side) by iterating. It is a string of comma separated terms per index
  int64_t operand = 0;
  std::stringstream eqn_stream(in_eqn);
  std::string term;
  int64_t num_total_idxes = 0;
  while (! eqn_stream.eof()) {
    std::getline(eqn_stream, term, ',');  // term = string with indices of current term
    TORCH_CHECK((int64_t) tensors.size()>operand, ""more operands in equation than tensors""); // we cannot have a longer equation than operands. We need to check here before we use the dimension

    int64_t ell_char_count = 0;            // handling of ellipsis '...' is a bit tedious, we count the '.'
    // if there is an ellipsis, the number of dimensions it represents must be total dim - letter dimensions
    int64_t candidate_num_ell_idxes = tensors[operand].dim() - term.size() + 3;
    int64_t dims_in_term = 0;              // dimensions we have seen
    std::vector<int64_t> current_op_idxes; // mapping of operand dimensions to indices for current term
    for (auto &c : term) {                 // c = character with a single letter or '.'
      if (c == '.') {
        ell_char_count++;
        TORCH_CHECK(ell_char_count <= 3, ""can only have '.' in one ellispis '...' in term "", operand, "" of the equation"");
        if (ell_char_count == 3) {        // this completes the ellipsis
          if (num_ell_idxes == -1) {      // if we have not seen an ellipsis before, keep track of indices and size
            first_ell_idx = num_total_idxes;
            num_ell_idxes = candidate_num_ell_idxes;
            num_total_idxes += num_ell_idxes;
          }
          else {                          // we have seen an ellipsis before, so we check compatibility
            TORCH_CHECK(candidate_num_ell_idxes == num_ell_idxes,
                     ""ellipsis must represent "", num_ell_idxes, "" dimensions in all terms"");
          }
          for (int64_t i = 0; i < num_ell_idxes; ++i) { // map ellipsis dimensions in operand to indices
            current_op_idxes.push_back(first_ell_idx + i);
            last_idx_occurrence.push_back(operand);
          }
          dims_in_term += num_ell_idxes;                // keep track of dimensions
        }
      } else {                                          // a letter (hopefully)
        TORCH_CHECK((ell_char_count == 0) || (ell_char_count == 3), ""'.' must only occur in ellipsis, operand "", operand);
        TORCH_CHECK(('a' <= c) && (c <= 'z'), ""only lowercase letters a-z allowed as indices"");
        int64_t letter_num = c-'a';                     // letter_num  = position in letter_mapping
        if (letter_mapping[letter_num] == -1) {         // new letter, add internal index and mapping
          letter_mapping[letter_num] = num_total_idxes;
          num_total_idxes++;
          last_idx_occurrence.push_back(operand);
        } else {                                        // letter we have already seen
          last_idx_occurrence[letter_mapping[letter_num]] = operand;
        }
        num_letter_occurrences[letter_num]++;
        current_op_idxes.push_back(letter_mapping[letter_num]);
        dims_in_term++;
}
}
    TORCH_CHECK(dims_in_term == tensors[operand].dim(), ""dimension mismatch for operand "", operand, "": equation "", dims_in_term, "" tensor "", tensors[operand].dim());
    input_op_idxes.push_back(std::move(current_op_idxes));
    operand++;
}
  // in the check below, we need ==, but > is captured above, so the error message can be specific that it is <.
  TORCH_CHECK((int64_t) tensors.size()==operand, ""more tensors than operands in equation"");

  // the following parses or infers output (right hand side)
  // it also assigns the idxes_to_preprocessed_dims (index -> dimension in preprocessed / output tensors)
  // for the output indices. -1 means that the index has not been assigned a dimension yet
  std::vector<int64_t> idxes_to_preprocessed_dims(num_total_idxes, -1);     // the position of the index in the tensor dimensions
  int64_t num_output_dims = 0;
  if (pos != std::string::npos) {            // parse the user provided right hand side
    int64_t ell_char_count = 0;
    for (auto &c : eqn.substr(pos+2)) {
      if (c == '.') {                        // '.' as part of ellipsis
        ell_char_count++;
        TORCH_CHECK(ell_char_count <= 3, ""can only have '.' in one ellispis '...' in right hand side of the equation"");
        if (ell_char_count == 3) {           // ellipsis complete
          TORCH_CHECK(num_ell_idxes >= 0, ""ellipsis '...' may only appear in right hand side if it does in left hand side"");
          for (int64_t i = 0; i < num_ell_idxes; ++i) {
            idxes_to_preprocessed_dims[first_ell_idx + i] = num_output_dims;
            num_output_dims++;
          }
        }
      } else if (! isspace(c)) {                              // letter (hopefully)
        TORCH_CHECK((ell_char_count == 0) || (ell_char_count == 3), ""'.' must only occur in ellipsis in the right hand side"");
        TORCH_CHECK(('a' <= c) && (c <= 'z'), ""only lowercase letters a-z allowed as indices"");
        int64_t letter_num = c-'a';
        TORCH_CHECK(idxes_to_preprocessed_dims[letter_mapping[letter_num]] == -1, ""index "", c, "" occurs twice in output"");
        idxes_to_preprocessed_dims[letter_mapping[letter_num]] = num_output_dims;
        num_output_dims++;
}
}
  } else { // create an inferred right hand side
    // the ellipsis (if in the lhs) comes first
    if (num_ell_idxes >= 0) {
      for (int64_t i = 0; i < num_ell_idxes; ++i) {
        idxes_to_preprocessed_dims[first_ell_idx + i] = num_output_dims;
        num_output_dims++;
      }
    }
    // then the indices that occur exactly once in alphabetic order
    for (size_t idx = 0; idx < number_of_letters; idx++) {
      if (num_letter_occurrences[idx] == 1) {
        idxes_to_preprocessed_dims[letter_mapping[idx]] = num_output_dims;
        num_output_dims++;
}
}
}
  // now we assign the idxes_to_preprocessed_dims (index -> dimension in preprocessed / output tensors)
  // for the non-output indices - those that are eventually summed over
  int64_t position = num_output_dims;
  for (int64_t i = 0; i < num_total_idxes; i++) {
    if (idxes_to_preprocessed_dims[i]==-1) {
      idxes_to_preprocessed_dims[i] = position;
      position++;
}
}
  // we now ""homogenize the dimensions"", i.e.
  // - take diagonals for duplicated indices
  // - permute the dimensions to match the order given by idxes_to_preprocessed_dims
  // - unsqueeze to create all dimensions for each index in each tensor where they are missing
  // we also check that sizes match
  // after this, all operands will have compatible shapes (i.e. all dimensions are aligned are broadcastable)
  std::vector<Tensor> preprocessed_operands;
  std::vector<std::int64_t> size_of_dims(num_total_idxes, -1); // keep track of sizes for each index, -1 means we have not seen a size yet
  for (int64_t op = 0; op < (int64_t) tensors.size(); op++) {
    auto preprocessed_op = tensors[op];
    std::vector<int64_t> idx_to_dim(num_total_idxes, -1); // the dimension which the index refers to in the original tensor, -1 means it does not appear
    std::vector<int64_t>& current_op_input_idxes = input_op_idxes[op];
    int64_t dim = 0; // there are two dimension indices: dim is after taking diagonals, i is in input
    for (size_t i = 0; i < current_op_input_idxes.size(); i++) {
      auto idx = current_op_input_idxes[i];
      auto dim_out = idxes_to_preprocessed_dims[idx];
      if (idx_to_dim[dim_out] == -1) { // first appearance
        idx_to_dim[dim_out] = dim;
        if (size_of_dims[idx] == -1) { // keep track of sizes
          size_of_dims[idx] = preprocessed_op.size(dim);
        }
        else {
          TORCH_CHECK(size_of_dims[idx] == preprocessed_op.size(dim), ""size of dimension does not match previous size, operand "", op, "", dim "", i);
}
        dim++;
      } else { // duplicate dimension in tensor --> take diagonal of idx_to_dim[dim_out] and dim and put the diagonal dimension to idx_to_dim[dim_out]
        TORCH_CHECK(size_of_dims[idx] == preprocessed_op.size(dim), ""size of dimension does not match previous size, operand "", op, "", dim "", i);
        preprocessed_op = preprocessed_op.diagonal(0, idx_to_dim[dim_out], dim);
        // diagonal moves the diagonal dimension to the back
        // now we permute the last dim back to idx_to_dim[dim_out]
        std::vector<int64_t> perm(preprocessed_op.dim(), 0);
        for (int64_t d = 0; d < preprocessed_op.dim(); d++) {
          if (d == idx_to_dim[dim_out]) {
            perm[d] = preprocessed_op.dim() - 1;
          } else {
            perm[d] = d - (d > idx_to_dim[dim_out]);
          }
}
        preprocessed_op = preprocessed_op.permute(perm);
}
}
    // now we permute the dimensions in the right order
    std::vector<int64_t> permutation; // permutation for this tensor
    for (auto &d : idx_to_dim) {
      if (d > -1) {
        permutation.push_back(d);
}
}
    preprocessed_op = preprocessed_op.permute(permutation);
    // finally, we insert dimensions for idxes not in the operand
    for (size_t dim = 0; dim < idx_to_dim.size(); dim++) {
      if (idx_to_dim[dim] == -1) {
        preprocessed_op = preprocessed_op.unsqueeze(dim);
}
}
    preprocessed_operands.push_back(std::move(preprocessed_op));
}
  // now we reduce the indices from left to right
  // numpy allows to optimize the path using various
  // algorithms (see eigen_path in numpy docs)
  // we start with the leftmost operator and reduce indices that
  // appear only there
  Tensor result = std::move(preprocessed_operands[0]);
  for (int64_t idx = 0; idx < num_total_idxes; idx++) {
    if ((last_idx_occurrence[idx] == 0)
        && (idxes_to_preprocessed_dims[idx]>=num_output_dims)) {
      result = result.sum(idxes_to_preprocessed_dims[idx], true);
}
}
  // now we process each tensor using sumproduct_pair
  for (int64_t i = 1; i < (int64_t) preprocessed_operands.size(); i++) {
std::vector<int64_t> sum_dims;
    for (int64_t idx = 0; idx < num_total_idxes; idx++) {
      if ((last_idx_occurrence[idx] == i)
          && (idxes_to_preprocessed_dims[idx]>=num_output_dims)) {
        sum_dims.push_back(idxes_to_preprocessed_dims[idx]);
}
}
    result = at::native::sumproduct_pair(result, std::move(preprocessed_operands[i]), sum_dims, true);
  }
  // finally, we squeeze out all non-result dimensions
  auto sizes = result.sizes().vec();
  for (int64_t dim = num_total_idxes-1; dim >= num_output_dims; dim--) {
    sizes.erase(sizes.begin() + dim);
}
  result = result.view(sizes);
return result;
}
"
900,"# there will be differences between the gradients that are never synchronized.
self.rng = np.random.RandomState(random_seed)
# Since there is only a single state instance for all the input buckets,
        # need to maintain a dictionary that maps each bucket to the local error.
        # TODO(wayi): Currently the key is the (hashcode of) input tensor, which may change across steps,
        # since the bucket can be rebuilt in the forward pass (to save peak memory usage).
        # Need to add an index field to the input bucket of comm hook.
self.error_dict = {}
def powerSGD_hook(
state: PowerSGDState,
    bucket: dist._GradBucket,
) -> torch.futures.Future:
""""""
This DDP communication hook implements a simplified PowerSGD gradient compression
","# there will be differences between the gradients that are never synchronized.
self.rng = np.random.RandomState(random_seed)
# Since there is only a single state instance for all the input buckets,
        # need to maintain a dictionary that maps each bucket index to the local error.
self.error_dict = {}
def powerSGD_hook(
state: PowerSGDState,
    bucket,
) -> torch.futures.Future:
""""""
This DDP communication hook implements a simplified PowerSGD gradient compression
"
901,"self.freeze_bn = freeze_bn if self.training else True
self.bn = _BN_CLASS_MAP[dim](out_channels, eps, momentum, True, True)
self.weight_fake_quant = self.qconfig.weight()
        self.zero_bias = torch.zeros(out_channels)
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
else:
","self.freeze_bn = freeze_bn if self.training else True
self.bn = _BN_CLASS_MAP[dim](out_channels, eps, momentum, True, True)
self.weight_fake_quant = self.qconfig.weight()
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
else:
"
902,"del self._parameters[key]
def __setattr__(self, key: Any, value: Any) -> None:
        if getattr(self, ""_initialized"", False) and not isinstance(value, torch.nn.Parameter):
            warnings.warn(""Setting attributes on ParameterDict is not supported."")
super(ParameterDict, self).__setattr__(key, value)
def __len__(self) -> int:
","del self._parameters[key]
def __setattr__(self, key: Any, value: Any) -> None:
        if getattr(self, ""_initialized"", False):
            if not hasattr(self, key) and not isinstance(value, torch.nn.Parameter):
                warnings.warn(""Setting attributes on ParameterDict is not supported."")
super(ParameterDict, self).__setattr__(key, value)
def __len__(self) -> int:
"
903,"switch (inst.op) {
case OP: {
if (at::hasGlobalCallbacks()) {
          if (auto debug_info = c10::ThreadLocalDebugInfo::get(
                  c10::DebugInfoKind::MOBILE_RUNTIME_INFO)) {
            if (auto* mobile_debug_info =
                    dynamic_cast<MobileDebugInfo*>(debug_info.get())) {
              mobile_debug_info->setOpIdx(pc);
            }
}
}
","switch (inst.op) {
case OP: {
if (at::hasGlobalCallbacks()) {
          if (auto* mobile_debug_info =
                  static_cast<MobileDebugInfo*>(c10::ThreadLocalDebugInfo::get(
                      c10::DebugInfoKind::MOBILE_RUNTIME_INFO))) {
            mobile_debug_info->setOpIdx(pc);
}
}
"
904,"if check_dependency(partitions[-1]):
return float('inf')
# Check if the modified partition list can be mapped to devices after combination
found_deivce = get_device_to_partitions_mapping(partitions, self.devices)
if not found_deivce:
return float('inf')
","if check_dependency(partitions[-1]):
return float('inf')
# Check if the modified partition list can be mapped to devices after combination
                reset_partition_device(partitions)
found_deivce = get_device_to_partitions_mapping(partitions, self.devices)
if not found_deivce:
return float('inf')
"
905,"if new_cost <= cost:
partition_pair = [i, j]
cost = new_cost
# If a partition pair is found, combine them
if len(partition_pair) != 0:
p0 = self.partitions[partition_pair[0]]
p1 = self.partitions[partition_pair[1]]
combine_two_partitions(p0, p1, self.partitions)
                get_bfs_level_partition(self.partitions)
                get_device_to_partitions_mapping(self.partitions, self.devices)
                return True
            return False
for node in self.graph_module.graph.nodes:
if node.op not in {'placeholder', 'get_attr', 'output'}:
","if new_cost <= cost:
partition_pair = [i, j]
cost = new_cost
                    reorganize_partitions(self.partitions)
# If a partition pair is found, combine them
if len(partition_pair) != 0:
p0 = self.partitions[partition_pair[0]]
p1 = self.partitions[partition_pair[1]]
combine_two_partitions(p0, p1, self.partitions)
            get_bfs_level_partition(self.partitions)
            reset_partition_device(self.partitions)
            get_device_to_partitions_mapping(self.partitions, self.devices)
            return len(partition_pair) != 0
for node in self.graph_module.graph.nodes:
if node.op not in {'placeholder', 'get_attr', 'output'}:
"
906,"def allreduce_hook(
    process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
This DDP communication hook just calls ``allreduce`` using ``GradBucket``
","def allreduce_hook(
    process_group: dist.ProcessGroup, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
This DDP communication hook just calls ``allreduce`` using ``GradBucket``
"
907,"return _error(""environment variable %s expected, but not set"" % var)
result = urlparse(url)
    query = dict(pair.split(""="") for pair in filter(None, result.query.split(""&"")))
if ""rank"" in query:
rank = int(query[""rank""])
","return _error(""environment variable %s expected, but not set"" % var)
result = urlparse(url)
    query: Dict[str, Union[int, str]]
    # mypy doesn't allow dict() to accept List of values (#257)
    query = dict(pair.split(""="") for pair in filter(None, result.query.split(""&"")))  # type: ignore[misc, arg-type]

    rank: Optional[Union[str, int]]
    world_size: Optional[Union[str, int]]
    master_port: Optional[Union[str, int]]
if ""rank"" in query:
rank = int(query[""rank""])
"
908,"def _write(out_path, text):
try:
with open(out_path, ""r"") as f:
old_text = f.read()
","def _write(out_path, text):
    old_text: Optional[str]
try:
with open(out_path, ""r"") as f:
old_text = f.read()
"
909,"is_default_group = (len(group_ranks) == 0)
backend = Backend(backend)
if backend == Backend.MPI:
if not is_mpi_available():
raise RuntimeError(
","is_default_group = (len(group_ranks) == 0)
backend = Backend(backend)
    pg: Union[ProcessGroupGloo, ProcessGroupMPI, ProcessGroupNCCL]
if backend == Backend.MPI:
if not is_mpi_available():
raise RuntimeError(
"
910,"return
if group == GroupMember.WORLD:
        _check_default_pg()
        return _default_pg.recv([tensor], src, tag)
else:
group_src_rank = _get_group_rank(group, src)
return group.recv([tensor], group_src_rank, tag)
","return
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        return default_pg.recv([tensor], src, tag)
else:
group_src_rank = _get_group_rank(group, src)
return group.recv([tensor], group_src_rank, tag)
"
911,"opts.rootTensor = dst_tensor
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.reduce(tensor_list, opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
","opts.rootTensor = dst_tensor
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.reduce(tensor_list, opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
"
912,"def _object_to_tensor(obj):
buffer = pickle.dumps(obj)
    byte_storage = torch.ByteStorage.from_buffer(buffer)
byte_tensor = torch.ByteTensor(byte_storage)
local_size = torch.LongTensor([byte_tensor.numel()])
return byte_tensor, local_size
","def _object_to_tensor(obj):
buffer = pickle.dumps(obj)
    byte_storage = torch.ByteStorage.from_buffer(buffer)  # type: ignore[attr-defined]
byte_tensor = torch.ByteTensor(byte_storage)
local_size = torch.LongTensor([byte_tensor.numel()])
return byte_tensor, local_size
"
913,"OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  if (op.operatorIterator_->def_count == 0) {
    // NB: registerSchema is not idempotent! Only do it once!
op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
listeners_->callOnOperatorRegistered(op);
  } else {
    checkSchemaCompatibility(op, schema, debug);
  }
// NB: do not increment the counts until AFTER error checking
","OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  TORCH_CHECK(op.operatorIterator_->def_count == 0, ""Tried to register an operator ("", schema, "") with the same name and overload name multiple times."",
                                                    "" Each overload's schema should only be registered with a single call to def()."",
                                                    "" Duplicate registration: "", debug, "". Original registration: "", op.operatorIterator_->op.debug());
op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
listeners_->callOnOperatorRegistered(op);
// NB: do not increment the counts until AFTER error checking
+op.operatorIterator_->def_count;
"
914,"/* depth */ not_reentrant_backward_call ? 0 : total_depth + 1,
/* cpu_ready_queue */ local_ready_queue);
// Now compute the dependencies for all executable functions and queue the root
  auto graph_root = std::make_shared<GraphRoot>(roots, inputs);
compute_dependencies(graph_root.get(), *graph_task);
if (!outputs.empty()) {
graph_task->init_to_execute(*graph_root, outputs, accumulate_grad);
}
  execute_with_graph_task(graph_task, graph_root);
// Avoid a refcount bump for the Future, since we check for refcount in
// DistEngine (see TORCH_INTERNAL_ASSERT(futureGrads.use_count() == 1)
// in dist_engine.cpp).
","/* depth */ not_reentrant_backward_call ? 0 : total_depth + 1,
/* cpu_ready_queue */ local_ready_queue);
  // If we receive a single root, skip creating extra root node
  bool skip_dummy_node = roots.size() == 1;
  auto graph_root = skip_dummy_node ?
    roots.at(0).function :
    std::make_shared<GraphRoot>(roots, inputs);

// Now compute the dependencies for all executable functions and queue the root
compute_dependencies(graph_root.get(), *graph_task);
if (!outputs.empty()) {
graph_task->init_to_execute(*graph_root, outputs, accumulate_grad);
}
  if (skip_dummy_node) {
    InputBuffer input_buffer(roots.at(0).function->num_inputs());
    auto input = inputs.at(0);

    const auto input_stream = InputMetadata(input).stream();
    const auto opt_next_stream = roots.at(0).function->stream(c10::DeviceType::CUDA);
    input_buffer.add(roots.at(0).input_nr,
                      std::move(input),
                      input_stream,
                      opt_next_stream);

    execute_with_graph_task(graph_task, graph_root, std::move(input_buffer));
  } else {
    execute_with_graph_task(graph_task, graph_root, InputBuffer(variable_list()));
  }
// Avoid a refcount bump for the Future, since we check for refcount in
// DistEngine (see TORCH_INTERNAL_ASSERT(futureGrads.use_count() == 1)
// in dist_engine.cpp).
"
915,"std::shared_ptr<at::ivalue::Future> PythonEngine::execute_with_graph_task(
const std::shared_ptr<GraphTask>& graph_task,
    std::shared_ptr<Node> graph_root) {
try {
    return Engine::execute_with_graph_task(graph_task, graph_root);
} catch (python_error& e) {
pybind11::gil_scoped_acquire gil;
if (!PyErr_Occurred()) {
","std::shared_ptr<at::ivalue::Future> PythonEngine::execute_with_graph_task(
const std::shared_ptr<GraphTask>& graph_task,
    std::shared_ptr<Node> graph_root,
    InputBuffer&& input_buffer) {
try {
    return Engine::execute_with_graph_task(graph_task, graph_root, std::move(input_buffer));
} catch (python_error& e) {
pybind11::gil_scoped_acquire gil;
if (!PyErr_Occurred()) {
"
916,"// which is by default considered as a tensor.
typesFromScalars.emplace_back(c10::kLong);
} else if (nkind == onnx::Constant) {
          typesFromScalars.emplace_back(
              input->node()->t(attr::value).scalar_type());
} else if (
auto scalar_type =
input->type()->cast<TensorType>()->scalarType()) {
","// which is by default considered as a tensor.
typesFromScalars.emplace_back(c10::kLong);
} else if (nkind == onnx::Constant) {
          auto tensor = input->node()->t(attr::value);
          auto rank = tensor.dim();
          auto scalar_type = tensor.scalar_type();
          // Mimic PyTorch scalar type promotion logic
          // from https://github.com/pytorch/pytorch/issues/9515
          // Quoting:
          //    A Tensor is a considered a ""wrapped number"" if it is
          //    auto-wrapped from a C++ or Python number type. Integer types are
          //    wrapped as 0-dim int64 tensors and floating-point types are
          //    wrapped as 0-dim double tensors.
          if (rank == 0) {
            auto default_scalar_type =
                at::typeMetaToScalarType(at::get_default_dtype());
            switch (scalar_type) {
              case at::kDouble:
                // floating-point numbers wrapped as double tensors are
                // considered to have default type, instead of double.
                typesFromScalars.emplace_back(default_scalar_type);
                break;
              case at::kLong:
              case at::kBool:
                // bool and integer numbers remain the same type.
                typesFromScalars.emplace_back(scalar_type);
                break;
              default:
                // other types are not from wrapped numbers,
                // track them as types from tensors.
                typesFromTensors.emplace_back(scalar_type);
                break;
            }
          } else {
            typesFromTensors.emplace_back(scalar_type);
          }
} else if (
auto scalar_type =
input->type()->cast<TensorType>()->scalarType()) {
"
917,"parser.add_argument('--profiling_tensor_size', default=1, type=int)
parser.add_argument('--workload', default='loop', type=str)
parser.add_argument('--internal_iter', default=256, type=int)
    parser.add_argument('--n', default=100, type=int)
    parser.add_argument('--use_timer', action='store_true')
parser.add_argument('--timer_min_run_time', default=100, type=int)
args = parser.parse_args()
","parser.add_argument('--profiling_tensor_size', default=1, type=int)
parser.add_argument('--workload', default='loop', type=str)
parser.add_argument('--internal_iter', default=256, type=int)
parser.add_argument('--timer_min_run_time', default=100, type=int)
args = parser.parse_args()
"
918,"safe_downcast<int32_t>(padding[Layout::Parameter::height]),
output_min,
output_max,
};
context->dispatch(
","safe_downcast<int32_t>(padding[Layout::Parameter::height]),
output_min,
output_max,
      v_weight.sizes()[1],
};
context->dispatch(
"
919,"}
OverlapKind overlaps(const IndexBounds& a, const IndexBounds& b) {
// All accesses to a buf must have the same dimensionality.
if (a.size() != b.size()) {
","}
OverlapKind overlaps(const IndexBounds& a, const IndexBounds& b) {
  if (a.empty() && b.empty()) {
    return ContainedOrEqual;
  }

// All accesses to a buf must have the same dimensionality.
if (a.size() != b.size()) {
"
920,"``allreduce`` protocol. It works only with flattened grads.
Example::
            >>> ddp_model._register_comm_hook(process_group, quantization_pertensor_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
","``allreduce`` protocol. It works only with flattened grads.
Example::
            >>> ddp_model.register_comm_hook(process_group, quantization_pertensor_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
"
921,"op.tensor.options().dtype(common_dtype_),
LEGACY_CONTIGUOUS_MEMORY_FORMAT);
op.current_dtype = common_dtype_;
        op.target_dtype = common_dtype_;
}
// Promotes inputs by creating temporaries of the correct dtype
","op.tensor.options().dtype(common_dtype_),
LEGACY_CONTIGUOUS_MEMORY_FORMAT);
op.current_dtype = common_dtype_;
}
// Promotes inputs by creating temporaries of the correct dtype
"
922,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.le(b);
});
    });
}
}
void gt_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""gt_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a > b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""gt_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.le(b);
});
      });
}
}
void gt_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""gt_cpu"", [&]() {
cpu_kernel(iter,
       [=](scalar_t a, scalar_t b) -> bool {
         return a > b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), ""gt_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
923,"for node in self.nodes:
self.used_mem_bytes += get_extra_size_of(node, self.nodes)
class PartitionResult(NamedTuple):
""""""NameTuple used for returning DAG and a new graph module
""""""
","for node in self.nodes:
self.used_mem_bytes += get_extra_size_of(node, self.nodes)
    def add_node(self, node):
        input_nodes: Dict[Node, None] = {}
        map_arg(node.args, lambda n: input_nodes.setdefault(n))
        map_arg(node.kwargs, lambda n: input_nodes.setdefault(n))
        # Add current node's input nodes if they are placeholder or constants
        for n in input_nodes:
            if n.op in {'placeholder', 'get_attr'}:
                self.nodes.add(n)
        self.nodes.add(node)


class PartitionResult(NamedTuple):
""""""NameTuple used for returning DAG and a new graph module
""""""
"
924,"size *= extents.width * extents.height * (4u * extents.depth);
}
else {
    size = std::accumulate(
        sizes.cbegin(),
        sizes.cend(),
        size,
        std::multiplies<int64_t>());
}
return size;
","size *= extents.width * extents.height * (4u * extents.depth);
}
else {
    size *= prod_intlist(sizes);
}
return size;
"
925,"args_str = ', '.join(map(str, args))
dispatch_to_all_backends = dispatch is not None and dispatch in KEYWORD_ALL_BACKENDS
        if target is Target.DECLARATION:
            assert dispatch is not None
            return f""{returns_type} {name}({args_str});""
        elif target is Target.DEFINITION:
assert dispatch is not None
impl_name = f""at::native::{f.dispatch[dispatch]}""
","args_str = ', '.join(map(str, args))
dispatch_to_all_backends = dispatch is not None and dispatch in KEYWORD_ALL_BACKENDS
        if target is Target.DEFINITION:
assert dispatch is not None
impl_name = f""at::native::{f.dispatch[dispatch]}""
"
926,"target: Target,
# Selector object to determine which operators to generate
# registration code for.
    selector: SelectiveBuilder,
    # Only valid for generating registrations.  If True, only generate
    # def() invocations (for schema registration); do not generate
    # any impl() invocations for, e.g., catch-all kernels
    def_only: bool = False
) -> Callable[[NativeFunction], Optional[str]]:
    if def_only:
        assert target is Target.REGISTRATION and dispatch is None
@with_native_function
def func(f: NativeFunction) -> Optional[str]:
if dispatch is not None:
if dispatch not in f.dispatch:
return None
        else:
            if target is not Target.REGISTRATION:
                return None
op_name = f""aten::{f.func.name}""
if target is Target.REGISTRATION and not selector.is_operator_selected(op_name):
","target: Target,
# Selector object to determine which operators to generate
# registration code for.
    selector: SelectiveBuilder
) -> Callable[[NativeFunction], Optional[str]]:
    if dispatch is None:
        assert target == Target.REGISTRATION
@with_native_function
def func(f: NativeFunction) -> Optional[str]:
if dispatch is not None:
if dispatch not in f.dispatch:
return None
op_name = f""aten::{f.func.name}""
if target is Target.REGISTRATION and not selector.is_operator_selected(op_name):
"
927,"# propagate observed property from input
if is_observed(node.args[0]):
observed_node_names_set.add(node.name)
                elif (isinstance(obj, Add) or isinstance(obj, Mul)) and not obj.all_nodes:
                    if node.args[0].name in observed_node_names_set:
observed_node_names_set.add(node.name)
elif isinstance(obj, StandaloneModuleQuantizeHandler):
assert node.op == 'call_module'
output_is_observed = self.modules[node.target]._output_is_observed
if output_is_observed:
observed_node_names_set.add(node.name)
                elif qconfig is not None and obj.all_nodes:
# observer for outputs
new_observer = qconfig.activation()
# respect device affinity when adding observers
","# propagate observed property from input
if is_observed(node.args[0]):
observed_node_names_set.add(node.name)
                elif (isinstance(obj, Add) or isinstance(obj, Mul)) and obj.num_node_args == 1:
                    input_node = matched_nodes[-1]  # first node in the sequence

                    def input_is_observed(arg):
                        return isinstance(arg, Node) and arg.name in observed_node_names_set
                    # This is checking if one of the argument of add/mul
                    # is an observed node
                    # If both of the inputs are number,
                    # we will not consider the output to be observed
                    if input_is_observed(input_node.args[0]) or input_is_observed(input_node.args[1]):
observed_node_names_set.add(node.name)
elif isinstance(obj, StandaloneModuleQuantizeHandler):
assert node.op == 'call_module'
output_is_observed = self.modules[node.target]._output_is_observed
if output_is_observed:
observed_node_names_set.add(node.name)
                elif qconfig is not None and obj.all_node_args:
# observer for outputs
new_observer = qconfig.activation()
# respect device affinity when adding observers
"
928,"TORCH_CHECK(
false,
""Following ops cannot be found. "",
      ""May need to add them explicitly to the selective build operator whitelist, "",
      ""or re-run the export_opnames to update the whitelist:"",
error_message);
}
","TORCH_CHECK(
false,
""Following ops cannot be found. "",
      ""Check fburl.com/missing_ops for the fix."",
error_message);
}
"
929,"_pg_map[pg] = (backend, store)
_pg_names[pg] = group_name
return pg
","_pg_map[pg] = (backend, store)
_pg_names[pg] = group_name
    if not group_name_:
        _group_count += 1

return pg
"
930,"'--expt-relaxed-constexpr'
]
COMMON_HIPCC_FLAGS = [
'-fPIC',
'-D__HIP_PLATFORM_HCC__=1',
'-DCUDA_HAS_FP16=1',
'-D__HIP_NO_HALF_OPERATORS__=1',
'-D__HIP_NO_HALF_CONVERSIONS__=1',
","'--expt-relaxed-constexpr'
]
COMMON_HIP_FLAGS = [
'-fPIC',
'-D__HIP_PLATFORM_HCC__=1',
]

COMMON_HIPCC_FLAGS = [
'-DCUDA_HAS_FP16=1',
'-D__HIP_NO_HALF_OPERATORS__=1',
'-D__HIP_NO_HALF_CONVERSIONS__=1',
"
931,"else:
post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
                post_cflags += COMMON_HIPCC_FLAGS
append_std14_if_no_std_present(post_cflags)
cuda_post_cflags = None
","else:
post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
                post_cflags = COMMON_HIP_FLAGS + post_cflags
append_std14_if_no_std_present(post_cflags)
cuda_post_cflags = None
"
932,"qconfig_spec = dict(zip(qconfig_spec, itertools.repeat(default_qconfig)))
if mapping is None:
        mapping = get_dynamic_quant_module_mappings()
if not inplace:
model = copy.deepcopy(model)
","qconfig_spec = dict(zip(qconfig_spec, itertools.repeat(default_qconfig)))
if mapping is None:
        mapping = get_default_dynamic_quant_module_mappings()
if not inplace:
model = copy.deepcopy(model)
"
933,"""""""
float_dict = get_logger_dict(float_module)
quantized_dict = get_logger_dict(q_module)
    act_dict = {}
for key in quantized_dict:
match_key = _find_match(sorted(float_dict, reverse=True), key, ""stats"")
if match_key is not None:
","""""""
float_dict = get_logger_dict(float_module)
quantized_dict = get_logger_dict(q_module)
    act_dict: Dict[str, Dict] = {}
for key in quantized_dict:
match_key = _find_match(sorted(float_dict, reverse=True), key, ""stats"")
if match_key is not None:
"
934,"//      when there's no direct registration to its corresponding backend key or DefaultBackend.
//      For AutogradOther, we return ambiguousAutogradOtherKernel_ if there's registration
//      to any of its backends.
  if (isIncludedInAlias(dispatch_key, DispatchKey::Math)) {
if (auto math_registration = getKernelForDispatchKey(DispatchKey::Math)) {
if (dispatch_key == DispatchKey::AutogradOther
&& hasKernelForAnyDispatchKey(c10::autogradother_backends)) {
","//      when there's no direct registration to its corresponding backend key or DefaultBackend.
//      For AutogradOther, we return ambiguousAutogradOtherKernel_ if there's registration
//      to any of its backends.
  //      See Note [Undefined in dispatchTable_] for the special handling for Undefined.
  if (dispatch_key == DispatchKey::Undefined || isIncludedInAlias(dispatch_key, DispatchKey::Math)) {
if (auto math_registration = getKernelForDispatchKey(DispatchKey::Math)) {
if (dispatch_key == DispatchKey::AutogradOther
&& hasKernelForAnyDispatchKey(c10::autogradother_backends)) {
"
935,"//
void OperatorEntry::updateDispatchTableFull_(const c10::Dispatcher& dispatcher) {
// Note [Undefined in dispatchTable_]
// (1) it gives people place to specify functionality that should run when there are no dispatch keys,
  //     e.g., an empty TensorList argument
// (2) it would let us remove the explicit error checking code in the dispatch hotpath, and so when
//     no dispatch keys are available we just slide into the undefined handler which would then raise
  //     the error message./
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
updateDispatchTable_(dispatcher, static_cast<DispatchKey>(iter));
}
","//
void OperatorEntry::updateDispatchTableFull_(const c10::Dispatcher& dispatcher) {
// Note [Undefined in dispatchTable_]
  // DispatchKey Undefined is used in runtime:
// (1) it gives people place to specify functionality that should run when there are no dispatch keys,
  //     e.g., an op without Tensor inputs or empty TensorList arguments
// (2) it would let us remove the explicit error checking code in the dispatch hotpath, and so when
//     no dispatch keys are available we just slide into the undefined handler which would then raise
  //     the error message.
  // In the old world of catchAll, the only way to ""register"" a kernel to Undefined is by registering it to
  // catchAll. After catchAllKernel_ is removed, Undefined now can get a kernel from either DefaultBackend
  // or Math alias key so that we don't break the support. Ideally isIncludedInAlias(Undefined, Math)
  // should return true, it returns false because Undefined cannot be represented in a DispatchKeySet.
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
updateDispatchTable_(dispatcher, static_cast<DispatchKey>(iter));
}
"
936,"const T* mean_data = mean.data_ptr<T>();
const T* rstd_data = rstd.data_ptr<T>();
const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
  T* dX_data = dX->defined() ? dX->data_ptr<T>() : nullptr;
  T* dgamma_data = dgamma->defined() ? dgamma->data_ptr<T>() : nullptr;
  T* dbeta_data = dbeta->defined() ? dbeta->data_ptr<T>() : nullptr;
Tensor ds = at::empty({N, C}, X.options());
Tensor db = at::empty({N, C}, X.options());
T* ds_data = ds.data_ptr<T>();
","const T* mean_data = mean.data_ptr<T>();
const T* rstd_data = rstd.data_ptr<T>();
const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
  T* dX_data = dX.defined() ? dX.data_ptr<T>() : nullptr;
  T* dgamma_data = dgamma.defined() ? dgamma.data_ptr<T>() : nullptr;
  T* dbeta_data = dbeta.defined() ? dbeta.data_ptr<T>() : nullptr;
Tensor ds = at::empty({N, C}, X.options());
Tensor db = at::empty({N, C}, X.options());
T* ds_data = ds.data_ptr<T>();
"
937,"int64_t C,
int64_t HxW,
int64_t group,
    Tensor* dX,
    Tensor* dgamma,
    Tensor* dbeta) {
AT_DISPATCH_FLOATING_TYPES(
X.scalar_type(), ""GroupNormBackwardKernelImpl"", [&]() {
GroupNormBackwardKernelImplInternal<scalar_t>(
","int64_t C,
int64_t HxW,
int64_t group,
    Tensor& dX,
    Tensor& dgamma,
    Tensor& dbeta) {
AT_DISPATCH_FLOATING_TYPES(
X.scalar_type(), ""GroupNormBackwardKernelImpl"", [&]() {
GroupNormBackwardKernelImplInternal<scalar_t>(
"
938,"DEFINE_DISPATCH(GroupNormBackwardKernel);
std::tuple<at::Tensor, at::Tensor, at::Tensor> math_group_norm(
    const at::Tensor& input, const at::Tensor& weight,
    const at::Tensor& bias, int64_t N, int64_t C, int64_t HxW,
    int64_t group, double eps) {
auto input_shape = input.sizes();
at::Tensor input_reshaped = input.view({1, N * group, N ? -1 : 1});
auto outputs = at::native_batch_norm(
      input_reshaped, /*weight=*/{}, /*bias=*/{}, /*running_mean=*/{},
      /*running_var=*/{}, /*training=*/true, /*momentum=*/0, eps);
at::Tensor out = std::get<0>(outputs);
out = out.view(input_shape);
std::vector<int64_t> affine_param_shape(input.dim(), 1);
affine_param_shape[1] = C;
if (weight.defined() && bias.defined()) {
    out = bias.view(affine_param_shape).addcmul(out, weight.view(affine_param_shape), 1);
} else if (weight.defined()) {
out = out.mul(weight.view(affine_param_shape));
} else if (bias.defined()) {
","DEFINE_DISPATCH(GroupNormBackwardKernel);
std::tuple<at::Tensor, at::Tensor, at::Tensor> math_group_norm(
    const at::Tensor& input,
    const at::Tensor& weight,
    const at::Tensor& bias,
    int64_t N,
    int64_t C,
    int64_t HxW,
    int64_t group,
    double eps) {
auto input_shape = input.sizes();
at::Tensor input_reshaped = input.view({1, N * group, N ? -1 : 1});
auto outputs = at::native_batch_norm(
      input_reshaped,
      /*weight=*/{},
      /*bias=*/{},
      /*running_mean=*/{},
      /*running_var=*/{},
      /*training=*/true,
      /*momentum=*/0,
      eps);
at::Tensor out = std::get<0>(outputs);
out = out.view(input_shape);
std::vector<int64_t> affine_param_shape(input.dim(), 1);
affine_param_shape[1] = C;
if (weight.defined() && bias.defined()) {
    out = bias.view(affine_param_shape)
              .addcmul(out, weight.view(affine_param_shape), 1);
} else if (weight.defined()) {
out = out.mul(weight.view(affine_param_shape));
} else if (bias.defined()) {
"
939,"free_start_offset_to_size_iter.erase(alloc_offset);
free_end_offset_to_size_iter.erase(alloc_offset + it->first);
if (new_size > 0) {
          auto ref_it = free_size_to_offset.emplace(new_offset, new_size).first;
free_start_offset_to_size_iter.emplace(new_offset, ref_it);
free_end_offset_to_size_iter.emplace(new_offset + new_size, ref_it);
}
        allocated_offset_to_size.emplace(alloc_offset, mem_event.size);
}
allocation_offsets[mem_event.allocation_id] = alloc_offset;
} else {
","free_start_offset_to_size_iter.erase(alloc_offset);
free_end_offset_to_size_iter.erase(alloc_offset + it->first);
if (new_size > 0) {
          auto ref_it = free_size_to_offset.emplace(new_size, new_offset).first;
free_start_offset_to_size_iter.emplace(new_offset, ref_it);
free_end_offset_to_size_iter.emplace(new_offset + new_size, ref_it);
}
}
allocation_offsets[mem_event.allocation_id] = alloc_offset;
} else {
"
940,"return false;
}
  allocation_ptr_to_id_.emplace(ptr, allocation_id_);
allocation_id_++;
return true;
}
","return false;
}
  allocation_ptr_to_id_[ptr] =  allocation_id_;
allocation_id_++;
return true;
}
"
941,"// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
// TODO: we can remove (2.4) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.

// 1. Operator registration
if (auto direct_registration = getKernelForDispatchKey(dispatch_key)) {
return {*direct_registration.value(), ""kernel""};
","// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
// TODO: we can remove (2.4) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.

// 1. Operator registration
if (auto direct_registration = getKernelForDispatchKey(dispatch_key)) {
return {*direct_registration.value(), ""kernel""};
"
942,"}
// synchronizes the dispatch table entries for a given dispatch key *and its
// associated keys* with the current state of kernel registrations in the
// dispatcher.
// After a kernel has been registered to a dispatch key, a call to this
// function will synchronize the dispatcher state. See e.g. registerKernel()
void OperatorEntry::updateDispatchTable_(const c10::Dispatcher& dispatcher, DispatchKey dispatch_key) {
// Handle Undefined separately since it isn't a runtime key but we have an entry in dispatchTable_.
","}
// synchronizes the dispatch table entries for a given dispatch key *and its
// associated keys* with the current state of kernel registrations in the
// dispatcher.
// After a kernel has been registered to a dispatch key, a call to this
// function will synchronize the dispatcher state. See e.g. registerKernel()
void OperatorEntry::updateDispatchTable_(const c10::Dispatcher& dispatcher, DispatchKey dispatch_key) {
// Handle Undefined separately since it isn't a runtime key but we have an entry in dispatchTable_.
"
943,"}
// Note [Refresh Runtime Autograd entries in dispatchTable_]
// Registering to backend key might affect computed entry at its Autograd backend key due to (2.1) & (2.3).
  DispatchKey autograd_key = getAutogradKeyFromBackend(dispatch_key);
  updateDispatchTableEntry_(dispatcher, autograd_key);
}
// does a complete update of the dispatch table, synchronizing all
// runtime dispatch keys with the current state of kernel registrations
// in the dispatcher.
// Note that we use updateDispatchTable_() to perform our per-key updating,
// even though that function is equipped to handle out-of-order updates and
// alias key updates, neither of which we send it. This is deliberate - the
// current design is more tractable with all updates funneled through a single
// per-key update mechanism, than with multiple variations that assume different
// invariants.
","}
// Note [Refresh Runtime Autograd entries in dispatchTable_]
// Registering to backend key might affect computed entry at its Autograd backend key due to (2.1) & (2.3).
  if (c10::isBackendDispatchKey(dispatch_key)) {
    DispatchKey autograd_key = getAutogradKeyFromBackend(dispatch_key);
    updateDispatchTableEntry_(dispatcher, autograd_key);
  }
}
// does a complete update of the dispatch table, synchronizing all
// runtime dispatch keys with the current state of kernel registrations
// in the dispatcher.
// Note that we use updateDispatchTable_() to perform our per-key updating,
// even though that function is equipped to handle out-of-order updates and
// alias key updates, neither of which we send it. This is deliberate - the
// current design is more tractable with all updates funneled through a single
// per-key update mechanism, than with multiple variations that assume different
// invariants.
"
944,"return str << toString(rhs);
}
DispatchKey getAutogradKeyFromBackend(DispatchKey t) {
switch (t) {
case DispatchKey::CPU:
","return str << toString(rhs);
}
// for a given backend key, return the associated autograd key.
// for non-backend keys, return AutogradOther as a default.
// Note: it's convenient and fast to return a default here rather than (say)
// returning an optional<DispatchKey>, or throwing. But it makes callers
// responsible for either a) enforcing the invariant that only backend keys
// be passed as arguments, or b) interpreting our return value carefully.
//
DispatchKey getAutogradKeyFromBackend(DispatchKey t) {
switch (t) {
case DispatchKey::CPU:
"
945,"DispatchKey::PrivateUse3,
});
// math_dispatch_keyset contains all keys in backend_dispatch_keyset and autograd_dispatch_keyset
// Alias key DispatchKey::Math maps to math_dispatch_keyset.
constexpr DispatchKeySet math_dispatch_keyset = backend_dispatch_keyset | autograd_dispatch_keyset;
","DispatchKey::PrivateUse3,
});
bool isBackendDispatchKey(DispatchKey t) {
  return t != DispatchKey::Undefined && backend_dispatch_keyset.has(t);
}

// math_dispatch_keyset contains all keys in backend_dispatch_keyset and autograd_dispatch_keyset
// Alias key DispatchKey::Math maps to math_dispatch_keyset.
constexpr DispatchKeySet math_dispatch_keyset = backend_dispatch_keyset | autograd_dispatch_keyset;
"
946,"assert returns_type == dispatcher.returns_type(f.func.returns)
dispatcher_args = dispatcher.arguments(f.func)
dispatcher_args_types_str = ', '.join(map(lambda a: a.type, dispatcher_args))
            if dispatch is None or dispatch == 'Math' or dispatch == 'DefaultBackend':
type_name = f'TypeDefault::{name}'
else:
type_name = f'{dispatch}Type::{name}'
","assert returns_type == dispatcher.returns_type(f.func.returns)
dispatcher_args = dispatcher.arguments(f.func)
dispatcher_args_types_str = ', '.join(map(lambda a: a.type, dispatcher_args))
            if dispatch is None or dispatch in KEYWORD_ALL_BACKENDS:
type_name = f'TypeDefault::{name}'
else:
type_name = f'{dispatch}Type::{name}'
"
947,"return self._event_shape
@property
    def arg_constraints(self):
""""""
Returns a dictionary from argument names to
:class:`~torch.distributions.constraints.Constraint` objects that
","return self._event_shape
@property
    def arg_constraints(self) -> Dict[str, constraints.Constraint]:
""""""
Returns a dictionary from argument names to
:class:`~torch.distributions.constraints.Constraint` objects that
"
948,"if i != 1 and j != 1 and i != j:
raise ValueError('Value is not broadcastable with batch_shape+event_shape: {} vs {}.'.
format(actual_shape, expected_shape))

if not self.support.check(value).all():
raise ValueError('The value argument must be within the support')
","if i != 1 and j != 1 and i != j:
raise ValueError('Value is not broadcastable with batch_shape+event_shape: {} vs {}.'.
format(actual_shape, expected_shape))
        assert self.support is not None
if not self.support.check(value).all():
raise ValueError('The value argument must be within the support')
"
949,"from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import _sum_rightmost

class Independent(Distribution):
r""""""
","from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import _sum_rightmost
from typing import Dict
class Independent(Distribution):
r""""""
"
950,":class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli` and
:class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`
""""""
    arg_constraints = {}
def __init__(self, base_distribution, transforms, validate_args=None):
self.base_dist = base_distribution
",":class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli` and
:class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`
""""""
    arg_constraints: Dict[str, constraints.Constraint] = {}
def __init__(self, base_distribution, transforms, validate_args=None):
self.base_dist = base_distribution
"
951,"lazy_property)
from torch.nn.functional import pad
from torch.nn.functional import softplus
__all__ = [
'AbsTransform',
","lazy_property)
from torch.nn.functional import pad
from torch.nn.functional import softplus
from typing import List
__all__ = [
'AbsTransform',
"
952,"@property
def sign(self):
        if isinstance(self.scale, numbers.Number):
            return 1 if self.scale > 0 else -1 if self.scale < 0 else 0
return self.scale.sign()
def _call(self, x):
","@property
def sign(self):
        if isinstance(self.scale, numbers.Real):
            return 1 if float(self.scale) > 0 else -1 if float(self.scale) < 0 else 0
return self.scale.sign()
def _call(self, x):
"
953,"p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
The following norms can be calculated:
            =====  ============================  ==========================
            ord    matrix norm                   vector norm
            =====  ============================  ==========================
            None   Frobenius norm                2-norm
            'fro'  Frobenius norm                --
            'nuc'  nuclear norm                  --
            Other  as vec norm when dim is None  sum(abs(x)**ord)**(1./ord)
            =====  ============================  ==========================

        dim (int, 2-tuple of ints, 2-list of ints, optional): If it is an int,
            vector norm will be calculated, if it is 2-tuple of ints, matrix norm
            will be calculated. If the value is None, matrix norm will be calculated
            when the input tensor only has two dimensions, vector norm will be
            calculated when the input tensor only has one dimension. If the input
            tensor has more than two dimensions, the vector norm will be applied to
            last dimension.
keepdim (bool, optional): whether the output tensors have :attr:`dim`
retained or not. Ignored if :attr:`dim` = ``None`` and
:attr:`out` = ``None``. Default: ``False``
","p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
The following norms can be calculated:
            ======  ==============  ==========================
            ord     matrix norm     vector norm
            ======  ==============  ==========================
            'fro'   Frobenius norm  --
            'nuc'   nuclear norm    --
            Number  --              sum(abs(x)**ord)**(1./ord)
            ======  ==============  ==========================

            The vector norm can be calculated across any number of dimensions.
            The corresponding dimensions of :attr:`input` are flattened into
            one dimension, and the norm is calculated on the flattened
            dimension.

            Frobenius norm produces the same result as ``p=2`` in all cases
            except when :attr:`dim` is a list of three or more dims, in which
            case Frobenius norm throws an error.

            Nuclear norm can only be calculated across exactly two dimensions.

        dim (int, tuple of ints, list of ints, optional):
            Specifies which dimension or dimensions of :attr:`input` to
            calculate the norm across. If :attr:`dim` is ``None``, the norm will
            be calculated across all dimensions of :attr:`input`. If the norm
            type indicated by :attr:`p` does not support the specified number of
            dimensions, an error will occur.
keepdim (bool, optional): whether the output tensors have :attr:`dim`
retained or not. Ignored if :attr:`dim` = ``None`` and
:attr:`out` = ``None``. Default: ``False``
"
954,"""failed to create QNNPACK sigmoid operator"");
qy = at::_empty_affine_quantized(
input_contig.sizes(),
    input.options(),
output_scale,
    output_zero_point);
const pytorch_qnnp_status setupStatus = pytorch_qnnp_setup_sigmoid_nc_q8(
sigmoid_op,
","""failed to create QNNPACK sigmoid operator"");
qy = at::_empty_affine_quantized(
input_contig.sizes(),
    at::device(kCPU).dtype(input_contig.dtype()),
output_scale,
    output_zero_point,
    input_contig.suggest_memory_format());
const pytorch_qnnp_status setupStatus = pytorch_qnnp_setup_sigmoid_nc_q8(
sigmoid_op,
"
955,"{""QuantizedCPU"", c10::DispatchKey::QuantizedCPU},
{""Math"", c10::DispatchKey::Math},
{""Autograd"", c10::DispatchKey::Autograd},
{""AutogradCPU"", c10::DispatchKey::AutogradCPU},
{"""", c10::DispatchKey::Undefined},
};
","{""QuantizedCPU"", c10::DispatchKey::QuantizedCPU},
{""Math"", c10::DispatchKey::Math},
{""Autograd"", c10::DispatchKey::Autograd},
    {""DefaultBackend"", c10::DispatchKey::DefaultBackend},
{""AutogradCPU"", c10::DispatchKey::AutogradCPU},
{"""", c10::DispatchKey::Undefined},
};
"
956,"// a communicator the application receives an exception and its
// their responsibility to destroy the process group and recreate
// it to recover from errors.
              abortedCommIds.emplace(buildNcclUniqueIdStr(ncclComm->getNcclId()));
}
}
}
","// a communicator the application receives an exception and its
// their responsibility to destroy the process group and recreate
// it to recover from errors.
              abortedCommIds.emplace(
                  buildNcclUniqueIdStr(ncclComm->getNcclId()));
}
}
}
"
957,"at::cuda::CUDAStream& ncclStream = ncclStreams_[key][i];
(*work->cudaEvents_)[i].record(ncclStream);
work->ncclComms_[i] = ncclComms[i];
    work->blockingWait_ = blockingWait_;
    work->opTimeout_ = opTimeout_;
    work->store_ = store_;
}
if (asyncErrorHandling_) {
workEnqueue(work);
}
","at::cuda::CUDAStream& ncclStream = ncclStreams_[key][i];
(*work->cudaEvents_)[i].record(ncclStream);
work->ncclComms_[i] = ncclComms[i];
}
  // Set appropriate work parameters.
  work->blockingWait_ = blockingWait_;
  work->opTimeout_ = opTimeout_;
  work->store_ = store_;

if (asyncErrorHandling_) {
workEnqueue(work);
}
"
958,"std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
std::vector<at::Tensor>& inputs,
std::vector<at::Tensor>& outputs,
    Fn fn) {
return collective(
inputs,
outputs,
fn,
[](std::vector<at::cuda::CUDAStream>&) {},
      [](std::vector<at::cuda::CUDAStream>&) {});
}
template <typename Fn>
","std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
std::vector<at::Tensor>& inputs,
std::vector<at::Tensor>& outputs,
    Fn fn,
    OpType opType) {
return collective(
inputs,
outputs,
fn,
[](std::vector<at::cuda::CUDAStream>&) {},
      [](std::vector<at::cuda::CUDAStream>&) {},
      opType);
}
template <typename Fn>
"
959,"root,
comm,
stream.stream());
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
","root,
comm,
stream.stream());
      },
      OpType::BROADCAST);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
"
960,"def build_extensions(self) -> None:
self._check_abi()
for extension in self.extensions:
self._add_compile_flag(extension, '-DTORCH_API_INCLUDE_EXTENSION_H')
self._define_torch_extension_name(extension)
self._add_gnu_cpp_abi_flag(extension)
","def build_extensions(self) -> None:
self._check_abi()
for extension in self.extensions:
            # Ensure at least an empty list of flags for 'cxx' and 'nvcc' when
            # extra_compile_args is a dict. Otherwise, default torch flags do
            # not get passed. Necessary when only one of 'cxx' and 'nvcc' is
            # passed to extra_compile_args in CUDAExtension, i.e.
            #   CUDAExtension(..., extra_compile_args={'cxx': [...]})
            # or
            #   CUDAExtension(..., extra_compile_args={'nvcc': [...]})
            if isinstance(extension.extra_compile_args, dict):
                for ext in ['cxx', 'nvcc']:
                    if ext not in extension.extra_compile_args:
                        extension.extra_compile_args[ext] = []

self._add_compile_flag(extension, '-DTORCH_API_INCLUDE_EXTENSION_H')
self._define_torch_extension_name(extension)
self._add_gnu_cpp_abi_flag(extension)
"
961,"namespace c10d {
std::string opTypeToString(OpType opType) {
  switch (opType) {
    case OpType::BROADCAST:
      return ""BROADCAST"";
    case OpType::ALLREDUCE:
      return ""ALLREDUCE"";
    case OpType::ALLREDUCE_COALESCED:
      return ""ALLREDUCE_COALESCED"";
    case OpType::REDUCE:
      return ""REDUCE"";
    case OpType::ALLGATHER:
      return ""ALLGATHER"";
    case OpType::ALLGATHER_BASE:
      return ""ALLGATHER_BASE"";
    case OpType::ALLGATHER_COALESCED:
      return ""ALLGATHER_COALESCED"";
    case OpType::GATHER:
      return ""GATHER"";
    case OpType::SCATTER:
      return ""SCATTER"";
    case OpType::REDUCE_SCATTER:
      return ""REDUCE_SCATTER"";
    case OpType::ALLTOALL_BASE:
      return ""ALLTOALL_BASE"";
    case OpType::ALLTOALL:
      return ""ALLTOALL"";
    case OpType::SEND:
      return ""SEND"";
    case OpType::RECV:
      return ""RECV"";
    case OpType::RECVANYSOURCE:
      return ""RECVANYSOURCE"";
    case OpType::BARRIER:
      return ""BARRIER"";
    case OpType::UNKNOWN:
      return ""UNKNOWN"";
    default:
      TORCH_INTERNAL_ASSERT(""Unknown op type!"");
  }
  return ""UNKNOWN"";
}

bool isP2POp(OpType opType) {
  return opType == OpType::SEND || opType == OpType::RECV ||
      opType == OpType::RECVANYSOURCE;
}

ProcessGroup::Work::Work() : rank_(-1), opType_(OpType::UNKNOWN) {}

ProcessGroup::Work::Work(int rank, OpType opType)
    : rank_(rank), opType_(opType) {}

OpType ProcessGroup::Work::retrieveOpType() {
  return opType_;
}

ProcessGroup::Work::~Work() {}
bool ProcessGroup::Work::isCompleted() {
","namespace c10d {
ProcessGroup::Work::~Work() {}
bool ProcessGroup::Work::isCompleted() {
"
962,"auto val = std::stoi(errorHandle);
if (val == 1) {
asyncErrorHandling_ = true;
} else if (val != 0) {
throw std::runtime_error(
""Invalid value for environment variable: "" +
","auto val = std::stoi(errorHandle);
if (val == 1) {
asyncErrorHandling_ = true;
      LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL Async Error Handling enabled."";
} else if (val != 0) {
throw std::runtime_error(
""Invalid value for environment variable: "" +
"
963,"}
if (checkForNCCLErrors(ncclComms)) {
          LOG(INFO) << ""[Rank "" << rank_
                    << ""] Received NCCL errors for communicators in the cache"";
if (blockingWait_ || asyncErrorHandling_) {
            LOG(INFO) << ""[Rank "" << rank_
                      << ""] Aborting communicators that received errors"";
// We abort NCCL communicators that have received errors from this
// thread, and exceptions are set on the corresponding work objects.
// The workCleanupThread will then loop through the unfinished
","}
if (checkForNCCLErrors(ncclComms)) {
          LOG(INFO) << ""Received NCCL errors for communicators in the cache"";
if (blockingWait_ || asyncErrorHandling_) {
            LOG(INFO) << ""Aborting communicators that received errors"";
// We abort NCCL communicators that have received errors from this
// thread, and exceptions are set on the corresponding work objects.
// The workCleanupThread will then loop through the unfinished
"
964,"// Check for Timeouts in the WorkNCCL Operations, and abort all
// communicators accordingly.
if (work.timedOut()) {
          LOG(INFO)
              << ""[Rank "" << rank_
              << ""] Watchdog caught collective operation timeout for work: ""
              << work;
std::exception_ptr exception_ptr = std::make_exception_ptr(
std::runtime_error(""NCCL Operation Timed Out""));
work.setException(exception_ptr);
","// Check for Timeouts in the WorkNCCL Operations, and abort all
// communicators accordingly.
if (work.timedOut()) {
          LOG(INFO) << ""["" << rank_ << ""] caught collective operation timeout"";
std::exception_ptr exception_ptr = std::make_exception_ptr(
std::runtime_error(""NCCL Operation Timed Out""));
work.setException(exception_ptr);
"
965,"std::lock_guard<std::mutex> lock(mutex_);
if (futureNCCLCallbackStreams_[deviceIndex] == nullptr) {
futureNCCLCallbackStreams_[deviceIndex] =
          std::make_shared<at::cuda::CUDAStream>(
              at::cuda::getStreamFromPool(isHighPriorityStream_));
}
}
","std::lock_guard<std::mutex> lock(mutex_);
if (futureNCCLCallbackStreams_[deviceIndex] == nullptr) {
futureNCCLCallbackStreams_[deviceIndex] =
          std::make_shared<at::cuda::CUDAStream>(at::cuda::getStreamFromPool(isHighPriorityStream_));
}
}
"
966,"}
}
ProcessGroupNCCL::Options::Options()
    : opTimeout(kProcessGroupNCCLOpTimeoutMillis),
      isHighPriorityStream(false) {}
template <typename Fn, typename PreProcess, typename PostProcess>
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
","}
}
ProcessGroupNCCL::Options::Options()
    : opTimeout(kProcessGroupNCCLOpTimeoutMillis), isHighPriorityStream(false) {}
template <typename Fn, typename PreProcess, typename PostProcess>
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
"
967,"outputTensors[i][j].copy_(outputFlattened[i][j], true);
}
}
      },
      OpType::ALLGATHER);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::allgather_coalesced(
","outputTensors[i][j].copy_(outputFlattened[i][j], true);
}
}
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::allgather_coalesced(
"
968,"void ProcessGroupNCCL::ncclCommWatchdog() {
try {
ncclCommWatchdogInternal();
    LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
    LOG(INFO) << ""NCCL watchdog thread terminated with exception: "" << e.what();
} catch (...) {
    LOG(INFO) << ""NCCL watchdog thread terminated with unknown exception"";
}
}
","void ProcessGroupNCCL::ncclCommWatchdog() {
try {
    LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL watchdog thread started!"";
ncclCommWatchdogInternal();
    LOG(INFO) << ""[Rank "" << rank_
              << ""] NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
    LOG(INFO) << ""[Rank "" << rank_
              << ""] NCCL watchdog thread terminated with exception: ""
              << e.what();
} catch (...) {
    LOG(INFO) << ""[Rank "" << rank_
              << ""] NCCL watchdog thread terminated with unknown exception"";
}
}
"
969,"std::vector<at::Tensor>& tensors,
Fn fn,
int peer,
    NCCLCommType commType,
PreProcess pre,
PostProcess post) {
const auto devices = getDeviceList(tensors);
const auto key = getKeySendRecv(rank_, peer);
int p2pRank = rank_ < peer ? 0 : 1;
  auto& ncclComms = getNCCLComm(key, devices, commType, p2pRank);
// First let NCCL streams wait for input tensors allocation streams
syncStreams(devices, ncclEvents_[key], ncclStreams_[key]);
// Work itself will create the CUDA events on all GPUs of tensors
  auto work = initWork(devices);
  if (commType == NCCLCommType::RECV) {
// Store references to outputs and futureNCCLCallbackStream to be used by
// WorkNCCL::getFuture.
work->outputs_ = std::make_shared<std::vector<at::Tensor>>(tensors);
","std::vector<at::Tensor>& tensors,
Fn fn,
int peer,
    OpType opType,
PreProcess pre,
PostProcess post) {
const auto devices = getDeviceList(tensors);
const auto key = getKeySendRecv(rank_, peer);
int p2pRank = rank_ < peer ? 0 : 1;
  auto& ncclComms = getNCCLComm(key, devices, opType, p2pRank);
// First let NCCL streams wait for input tensors allocation streams
syncStreams(devices, ncclEvents_[key], ncclStreams_[key]);
// Work itself will create the CUDA events on all GPUs of tensors
  auto work = initWork(devices, rank_, opType);
  if (opType == OpType::RECV) {
// Store references to outputs and futureNCCLCallbackStream to be used by
// WorkNCCL::getFuture.
work->outputs_ = std::make_shared<std::vector<at::Tensor>>(tensors);
"
970,"std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
std::vector<at::Tensor>& inputs,
std::vector<at::Tensor>& outputs,
    Fn fn) {
return collective(
inputs,
outputs,
fn,
[](std::vector<at::cuda::CUDAStream>&) {},
      [](std::vector<at::cuda::CUDAStream>&) {});
}
template <typename Fn>
","std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::collective(
std::vector<at::Tensor>& inputs,
std::vector<at::Tensor>& outputs,
    Fn fn,
    OpType opType) {
return collective(
inputs,
outputs,
fn,
[](std::vector<at::cuda::CUDAStream>&) {},
      [](std::vector<at::cuda::CUDAStream>&) {},
      opType);
}
template <typename Fn>
"
971,"getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        });
} else {
c10d::checkSplitSizes(inputSplitSizes, inputTensor, size_);
c10d::checkSplitSizes(outputSplitSizes, outputTensor, size_);
","getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        },
        OpType::ALLTOALL_BASE);
} else {
c10d::checkSplitSizes(inputSplitSizes, inputTensor, size_);
c10d::checkSplitSizes(outputSplitSizes, outputTensor, size_);
"
972,"getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        });
}
}
","getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        },
        OpType::ALLTOALL_BASE);
}
}
"
973,"add_docstr(torch.matrix_exp,
r""""""
matrix_power(input) -> Tensor

Returns the matrix exponential. Supports batched input.
For a matrix ``A``, the matrix exponential is defined as
.. math::
    \exp^A = \sum_{k=0}^\infty A^k / k!.
"""""" + r""""""
The implementation is based on:
Bader, P.; Blanes, S.; Casas, F.
Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.
Mathematics 2019, 7, 1174.
","add_docstr(torch.matrix_exp,
r""""""
Returns the matrix exponential. Supports batched input.
For a matrix ``A``, the matrix exponential is defined as
.. math::
    \mathrm{e}^A = \sum_{k=0}^\infty A^k / k!
"""""" + r""""""
The implementation is based on:

Bader, P.; Blanes, S.; Casas, F.
Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.
Mathematics 2019, 7, 1174.
"
974,"dispatch = []
for i, dictionary in enumerate(grouped):
signature = dictionary['signature']
        signatures.append('""{}"",'.format(signature))
overload_index = i if not is_singleton else None
dispatch.append(emit_dispatch_case(overload_index, dictionary, is_python_method))
","dispatch = []
for i, dictionary in enumerate(grouped):
signature = dictionary['signature']
        signatures.append(f'{cpp_string(str(signature))},')
overload_index = i if not is_singleton else None
dispatch.append(emit_dispatch_case(overload_index, dictionary, is_python_method))
"
975,"return args;
}
void FunctionParameter::set_default_str(const std::string& str) {
if (str == ""None"") {
allow_none = true;
","return args;
}
// Parse a string literal to remove quotes and escape sequences
static std::string parse_string_literal(c10::string_view str) {
  TORCH_CHECK(str.length() >= 2, ""String defaults must be quoted"");

  if (str.front() == '""') {
    TORCH_CHECK(str.back() == '""',
                ""Mismatched quotes in string default: "", str);
  } else {
    TORCH_CHECK(str.front() == '\'' && str.back() == '\'',
                ""Invalid quotes in string default: "", str)
  }

  std::string parsed;
  parsed.reserve(str.size());
  for (size_t i = 1; i < str.size() - 1;) {
    if (str[i] != '\\') {
      parsed.push_back(str[i]);
      ++i;
      continue;
    }

    // Handle escape sequences
    TORCH_CHECK(i < str.size() - 2, ""String ends with escaped final quote: "", str)
    char c = str[i + 1];
    switch (c) {
      case '\\':
      case '\'':
      case '\""':
        break;
      case 'a':
        c = '\a';
        break;
      case 'b':
        c = '\b';
        break;
      case 'f':
        c = '\f';
        break;
      case 'n':
        c = '\n';
        break;
      case 'v':
        c = '\v';
        break;
      case 't':
        c = '\t';
        break;
      default:
        TORCH_CHECK(false, ""Unsupported escape sequence in string default: \\"", str[i + 1]);
    }
    parsed.push_back(c);
    i += 2;
  }
  return parsed;
}

void FunctionParameter::set_default_str(const std::string& str) {
if (str == ""None"") {
allow_none = true;
"
976," Start and end indices are either passed as two 1D input tensors or using the `starts` and `ends` arguments.
- If a negative value is passed for any of the start or end indices, it represents the number of elements before the end of that dimension. End indices are non-inclusive unless negative (end index -1 means up to and including the last element).
Github Links:
 https://github.com/pytorch/pytorch/blob/master/caffe2/operators/slice_op.cc
","- If a negative value is passed for any of the start or end indices, it represents |value| - 1 elements before the end of that dimension. End indices are non-inclusive unless negative (end index -1 means up to and including the last element).
Github Links:
"
977,"import time
import json
import torch
import torch.utils.cpp_extension as cpp_extension # noqa
""""""PyTorch performance microbenchmarks.
","import time
import json
import torch
import cpp_extension # noqa
""""""PyTorch performance microbenchmarks.
"
978,"template <typename T>
using shared_ptr_class_ = py::class_<T, std::shared_ptr<T>>;
PyObject* rpc_init(PyObject* /* unused */) {
auto rpc_module =
THPObjectPtr(PyImport_ImportModule(""torch.distributed.rpc""));
if (!rpc_module) {
","template <typename T>
using shared_ptr_class_ = py::class_<T, std::shared_ptr<T>>;
PyObject* rpc_init(PyObject* _unused, PyObject* noargs) {
auto rpc_module =
THPObjectPtr(PyImport_ImportModule(""torch.distributed.rpc""));
if (!rpc_module) {
"
979,"}
Tensor smooth_l1_loss(const Tensor& input, const Tensor& target, const int64_t reduction, double beta) {
  if (beta <= 0)
return at::native::l1_loss(input, target, reduction);
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
smooth_l1_stub(iter.device_type(), iter, beta);
","}
Tensor smooth_l1_loss(const Tensor& input, const Tensor& target, const int64_t reduction, double beta) {
  TORCH_CHECK(beta >= 0, ""smooth_l1_loss does not support negative values for beta."")
  if (beta == 0) {
return at::native::l1_loss(input, target, reduction);
  }
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
smooth_l1_stub(iter.device_type(), iter, beta);
"
980,"beta is an optional parameter that defaults to 1.
The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.
Args:
","beta is an optional parameter that defaults to 1.
    Note: When beta is set to 0, this is equivalent to we call out directly to :class:`L1Loss`.
    Passing a negative value in for beta will result in an exception.

The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.
Args:
"
981,"}
}
}
}
void ProcessGroupNCCL::ncclCommWatchdog() {
try {
ncclCommWatchdogInternal();
    LOG(INFO) << ""NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
LOG(INFO) << ""NCCL watchdog thread terminated with exception: "" << e.what();
} catch (...) {
","}
}
}

  if (asyncErrorHandling_) {
    workMetaListCV_.notify_one();
    workCleanupThread_.join();
  }
}
void ProcessGroupNCCL::ncclCommWatchdog() {
try {
ncclCommWatchdogInternal();
    LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
LOG(INFO) << ""NCCL watchdog thread terminated with exception: "" << e.what();
} catch (...) {
"
982,"std::unordered_map<std::string, std::string> copied_metadata =
owner_->metadata();
if (owner_->metadata().find(""model_name"") == owner_->metadata().end()) {
    copied_metadata[""model_name""] = name();
}
if (observer) {
observer->onEnterRunMethod(copied_metadata, function_->name());
","std::unordered_map<std::string, std::string> copied_metadata =
owner_->metadata();
if (owner_->metadata().find(""model_name"") == owner_->metadata().end()) {
    copied_metadata[""model_name""] = owner_->name();
}
if (observer) {
observer->onEnterRunMethod(copied_metadata, function_->name());
"
983,"return self.set_requires_grad(_requires_grad);
}
void retain_grad(const Tensor & self) {
TORCH_CHECK(self.requires_grad(), ""can't retain_grad on Tensor that has requires_grad=False"");
if (self.is_leaf()) {  // no-op for leaves
return;
","return self.set_requires_grad(_requires_grad);
}
void retain_grad(Tensor & self) {
TORCH_CHECK(self.requires_grad(), ""can't retain_grad on Tensor that has requires_grad=False"");
if (self.is_leaf()) {  // no-op for leaves
return;
"
984,"for (const auto& timedOutFuture : timedOutFutures) {
auto errStr =
          fmt::format(kRPCTimeoutErrorStr, timedOutFuture.timeout_.count());
auto err = makeRPCError(errStr, RPCErrorType::TIMEOUT);
if (!timedOutFuture.future_->hasError()) {
","for (const auto& timedOutFuture : timedOutFutures) {
auto errStr =
          fmt::format(kRpcTimeoutErrorStr, timedOutFuture.timeout_.count());
auto err = makeRPCError(errStr, RPCErrorType::TIMEOUT);
if (!timedOutFuture.future_->hasError()) {
"
985,"WorkerInfo(std::move(selfName), selfId),
std::move(cb),
std::chrono::milliseconds(
              (long)(opts.rpcTimeoutSeconds * kToMilliseconds))),
opts_(std::move(opts)),
threadPool_(opts_.numWorkerThreads),
context_(std::make_shared<tensorpipe::Context>(
","WorkerInfo(std::move(selfName), selfId),
std::move(cb),
std::chrono::milliseconds(
              (long)(opts.rpcTimeoutSeconds * kSecToMsConversion))),
opts_(std::move(opts)),
threadPool_(opts_.numWorkerThreads),
context_(std::make_shared<tensorpipe::Context>(
"
986,"if errors:
raise RuntimeError(
f""Followers {[e[0] for e in errors]} timed out in _all_gather ""
                f""after {timeout} seconds. The first exception is {errors[0][1]}""
)
return states.gathered_objects
","if errors:
raise RuntimeError(
f""Followers {[e[0] for e in errors]} timed out in _all_gather ""
                f""after {timeout:.2f} seconds. The first exception is {errors[0][1]}""
)
return states.gathered_objects
"
987,"# For any RpcAgent.
DEFAULT_RPC_TIMEOUT_SEC = _DEFAULT_RPC_TIMEOUT_SEC
DEFAULT_INIT_METHOD = _DEFAULT_INIT_METHOD
# For ProcessGroupAgent.
DEFAULT_NUM_SEND_RECV_THREADS = _DEFAULT_NUM_SEND_RECV_THREADS
","# For any RpcAgent.
DEFAULT_RPC_TIMEOUT_SEC = _DEFAULT_RPC_TIMEOUT_SEC
DEFAULT_INIT_METHOD = _DEFAULT_INIT_METHOD
DEFAULT_SHUTDOWN_TIMEOUT = 5.0
# For ProcessGroupAgent.
DEFAULT_NUM_SEND_RECV_THREADS = _DEFAULT_NUM_SEND_RECV_THREADS
"
988,"}
std::vector<Stmt*> stmts;
  int rsqi = 0;
for (auto& segment : innerSegments) {
// We never mask loops, they'll mask their contents.
if (!segment.mask()) {
TORCH_INTERNAL_ASSERT(segment.stmts().size() == 1);
","}
std::vector<Stmt*> stmts;
for (auto& segment : innerSegments) {
    bool need_sync = false;
// We never mask loops, they'll mask their contents.
if (!segment.mask()) {
TORCH_INTERNAL_ASSERT(segment.stmts().size() == 1);
"
989,"}
}
}
};
    init_handles(rec_fn.sorted_active_tls_handles_, sorted_tls_callbacks_);
    init_handles(rec_fn.sorted_active_global_handles_, sorted_global_callbacks_);
rec_fn.active = found_active_cb;
rec_fn.needs_inputs = found_needs_inputs;
if (found_needs_ids && found_active_cb) {
","}
}
}
      // Pre-allocate observer context list with nullptr.
      ctx_list.resize(num_callbacks);
};
    init_handles(rec_fn.sorted_active_tls_handles_, sorted_tls_callbacks_, rec_fn.tls_ctx_);
    init_handles(rec_fn.sorted_active_global_handles_, sorted_global_callbacks_, rec_fn.global_ctx_);
rec_fn.active = found_active_cb;
rec_fn.needs_inputs = found_needs_inputs;
if (found_needs_ids && found_active_cb) {
"
990,"import torch
from torch.quantization import (
    propagate_qconfig_,
    convert,
)

from ..quantization_mappings import (
    get_qat_module_mappings,
)

from torch.fx import (
GraphModule,
Proxy,
","import torch
from torch.fx import (
GraphModule,
Proxy,
"
991,"Tensor& cholesky_solve_out(Tensor& result, const Tensor& self, const Tensor& A, bool upper) {
Tensor result_tmp;
  result_tmp = at::_cholesky_solve_helper(self, A, upper);
result.resize_as_(result_tmp).copy_(result_tmp);
return result;
}
","Tensor& cholesky_solve_out(Tensor& result, const Tensor& self, const Tensor& A, bool upper) {
Tensor result_tmp;
  result_tmp = at::cholesky_solve(self, A, upper);
result.resize_as_(result_tmp).copy_(result_tmp);
return result;
}
"
992,"void CudaPrinter::visit(const Load* v) {
// TODO: find a better metric in using ldg or not. Support different dtypes.
  if (v->dtype().scalar_type() == ScalarType::Half) {
    if (v->indices().empty()) {
      os() << ""__half2float("" << *v->base_handle() << "")"";
    } else {
      os() << ""__half2float("" << *v->base_handle() << ""["" << *v->flat_index()
           << ""])"";
    }
    return;
  }
// Detects whether the load target is also a store target.
// TODO: this is currently too wide. It detects whether a store-target
// exists within the program. In fact, this check is only necessary within a
","void CudaPrinter::visit(const Load* v) {
// TODO: find a better metric in using ldg or not. Support different dtypes.
// Detects whether the load target is also a store target.
// TODO: this is currently too wide. It detects whether a store-target
// exists within the program. In fact, this check is only necessary within a
"
993,"} else {
os() << *v->base_handle() << ""["" << *v->flat_index() << ""] = "";
}
  if (v->value()->dtype().scalar_type() == ScalarType::Half) {
    os() << ""__float2half("" << *v->value() << "");"";
  } else {
    os() << *v->value() << "";"";
  }
  os() << std::endl;
}
void CudaPrinter::visit(const AtomicAdd* v) {
","} else {
os() << *v->base_handle() << ""["" << *v->flat_index() << ""] = "";
}
  os() << *v->value() << "";"";
}
void CudaPrinter::visit(const AtomicAdd* v) {
"
994,"void CudaPrinter::visit(const Let* v) {
emitIndent();
  if (v->dtype().scalar_type() == ScalarType::Half) {
    // we do math in floats so use that.
    os() << ""float"";
  } else {
    os() << cudaDtypeCppString(v->dtype());
  }
os() << "" "" << *v->var() << "" = "";
v->value()->accept(this);
os() << "";"" << std::endl;
","void CudaPrinter::visit(const Let* v) {
emitIndent();
  os() << cudaDtypeCppString(v->dtype());
os() << "" "" << *v->var() << "" = "";
v->value()->accept(this);
os() << "";"" << std::endl;
"
995,"const Tensor& offsets,
bool include_last_offset) {
int64_t ddim = src.size(1);
  auto* src_data = src.data_ptr<float>();
auto* select_indices_data = select_indices.data_ptr<int64_t>();
auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
","const Tensor& offsets,
bool include_last_offset) {
int64_t ddim = src.size(1);
auto* select_indices_data = select_indices.data_ptr<int64_t>();
auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
    auto* src_data = src.contiguous().data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
"
996,"return None, 2
def parse_json(json_file: str) -> List[CoverageRecord]:
print(""start parse:"", json_file)
json_obj, read_status = get_json_obj(json_file)
if read_status == 0:
","return None, 2
def parse_json(json_file: str, platform: TestPlatform) -> List[CoverageRecord]:
print(""start parse:"", json_file)
json_obj, read_status = get_json_obj(json_file)
if read_status == 0:
"
997,"std::tuple<Tensor &,Tensor &> _th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto res1_ = checked_dense_tensor_unwrap(res1, ""res1"", 0, ""_th_gels_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto res1_ = checked_dense_tensor_unwrap(res1, ""res1"", 0, ""_th_gels_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
998,"Tensor & _thnn_multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_multi_margin_loss_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _thnn_multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_multi_margin_loss_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
999,"Tensor & _thnn_nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_nll_loss_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _thnn_nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_nll_loss_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1000,"Tensor & _thnn_rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<at::Generator> generator) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_rrelu_with_noise_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _thnn_rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<at::Generator> generator) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_rrelu_with_noise_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1001,"std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & columns, const Tensor & ones) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_conv2d_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & columns, const Tensor & ones) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_conv2d_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
1002,"TORCH_CHECK(at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type()),
""var only supports floating-point dtypes"");
auto trivial_return = _allreduce_return_trivial(self, std::numeric_limits<double>::quiet_NaN());
  return trivial_return.has_value() ? trivial_return.value() : at::_var(self, unbiased);
}
Tensor var(const Tensor& self, IntArrayRef dim, bool unbiased, bool keepdim) {
","TORCH_CHECK(at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type()),
""var only supports floating-point dtypes"");
auto trivial_return = _allreduce_return_trivial(self, std::numeric_limits<double>::quiet_NaN());
  if (trivial_return.has_value()) {
    return trivial_return.value();
  }

  // NOTE: CPU performance significantly regressed when attempting to port to ATen,
  //   so this dispatches differently based on device type.
  //   See https://github.com/pytorch/pytorch/pull/43858.
  if (self.device().type() == kCPU) {
    return at::_var(self, unbiased);
  }

  Tensor result = at::empty({0}, self.options());
  return std_var_out(result, self, std::vector<int64_t>{}, unbiased, false, false);
}
Tensor var(const Tensor& self, IntArrayRef dim, bool unbiased, bool keepdim) {
"
1003,"module: input module
qconfig_dict: dictionary that maps from name of submodule to quantization
configuration
        white_list: list of quantizable modules
qconfig_parent: quantization config of parent module, we will fallback to
this config when there is no specified config for current
module
","module: input module
qconfig_dict: dictionary that maps from name of submodule to quantization
configuration
        allow_list: list of quantizable modules
qconfig_parent: quantization config of parent module, we will fallback to
this config when there is no specified config for current
module
"
1004,"None, module is modified inplace with added observer modules and forward_hooks
""""""
if qconfig_propagation_list is None:
        qconfig_propagation_list = DEFAULT_QCONFIG_PROPAGATE_WHITE_LIST
# respect device affinity when adding observers
if device is None:
devices = get_unique_devices_(module)
","None, module is modified inplace with added observer modules and forward_hooks
""""""
if qconfig_propagation_list is None:
        qconfig_propagation_list = DEFAULT_QCONFIG_PROPAGATE_ALLOWED_LIST
# respect device affinity when adding observers
if device is None:
devices = get_unique_devices_(module)
"
1005,"Args:
model: input model to be modified in-place
inplace: carry out model transformations in-place, the original module is mutated
        white_list: list of quantizable modules
observer_non_leaf_module_list: list of non-leaf modules we want to add observer
prehook: observer we want to add to forward_pre_hook
""""""
if not inplace:
model = copy.deepcopy(model)
    propagate_qconfig_list = white_list
if propagate_qconfig_list is None:
        propagate_qconfig_list = DEFAULT_QCONFIG_PROPAGATE_WHITE_LIST
propagate_qconfig_(model, qconfig_dict=None)
# sanity check common API misusage
","Args:
model: input model to be modified in-place
inplace: carry out model transformations in-place, the original module is mutated
        allow_list: list of quantizable modules
observer_non_leaf_module_list: list of non-leaf modules we want to add observer
prehook: observer we want to add to forward_pre_hook
""""""
if not inplace:
model = copy.deepcopy(model)
    propagate_qconfig_list = allow_list
if propagate_qconfig_list is None:
        propagate_qconfig_list = DEFAULT_QCONFIG_PROPAGATE_ALLOWED_LIST
propagate_qconfig_(model, qconfig_dict=None)
# sanity check common API misusage
"
1006,"return DictType(key, value)
if is_optional(ann):
if issubclass(ann.__args__[1], type(None)):
            valid_type = try_ann_to_type(ann.__args__[0], loc)
else:
            valid_type = try_ann_to_type(ann.__args__[1], loc)
        assert valid_type, ""Unsupported annotation {} could not be resolved."".format(repr(ann))
return OptionalType(valid_type)
if torch.distributed.rpc.is_available() and is_rref(ann):
return RRefType(try_ann_to_type(ann.__args__[0], loc))
","return DictType(key, value)
if is_optional(ann):
if issubclass(ann.__args__[1], type(None)):
            contained = ann.__args__[0]
else:
            contained = ann.__args__[1]
        valid_type = try_ann_to_type(contained, loc)
        msg = ""Unsupported annotation {} could not be resolved because {} could not be resolved.""
        assert valid_type, msg.format(repr(ann), repr(contained))
return OptionalType(valid_type)
if torch.distributed.rpc.is_available() and is_rref(ann):
return RRefType(try_ann_to_type(ann.__args__[0], loc))
"
1007,"observer->onExitLoadModel(result.metadata());
}
return result;
  } catch (const std::exception& ex) {
if (observer) {
      observer->onFailLoadModel(
          ""Error occured during loading model: "" + (std::string)ex.what());
}
    TORCH_CHECK(false, ex.what());
} catch (...) {
    if (observer) {
      observer->onFailLoadModel(""unknown exception"");
}
    TORCH_CHECK(false, ""unknown exception"");
}
}
","observer->onExitLoadModel(result.metadata());
}
return result;
  } catch (c10::Error& error) {
if (observer) {
      observer->onFailLoadModel(error.what());
}
    TORCH_RETHROW(error);
} catch (...) {
    auto currentException = std::current_exception();
    try {
      if (!currentException) {
        TORCH_CHECK(false, ""Unknown exception"");
      } else {
        try {
          std::rethrow_exception(currentException);
        } catch (const std::exception& e) {
          TORCH_CHECK(false, e.what());
        }
      }
    } catch (c10::Error& error) {
      if (observer) {
        observer->onFailLoadModel(error.what());
      }
      TORCH_RETHROW(error);
}
}
}
"
1008,"from .fx import Fuser  # noqa: F401
from .fx import Quantizer  # noqa: F401
from torch.fx import GraphModule
def _check_is_graph_module(model):
if not isinstance(model, GraphModule):
","from .fx import Fuser  # noqa: F401
from .fx import Quantizer  # noqa: F401
from torch.fx import GraphModule
from .fx.utils import graph_pretty_str  # noqa: F401
def _check_is_graph_module(model):
if not isinstance(model, GraphModule):
"
1009,"}
auto iter = TensorIteratorConfig()
    .set_check_mem_overlap(true)
.add_output(self)
.add_input(src)
.resize_outputs(false)
","}
auto iter = TensorIteratorConfig()
.add_output(self)
.add_input(src)
.resize_outputs(false)
"
1010,"static TensorIterator make_index_out_iterator(const AdvancedIndex& info, Tensor& result) {
TensorIteratorConfig config;
  config.check_all_same_dtype(false)
.add_output(result)
.add_input(info.src);
for (auto& index : info.indices) {
","static TensorIterator make_index_out_iterator(const AdvancedIndex& info, Tensor& result) {
TensorIteratorConfig config;
  // info.src is a restrided view of result
  config.set_check_mem_overlap(false)
        .check_all_same_dtype(false)
.add_output(result)
.add_input(info.src);
for (auto& index : info.indices) {
"
1011,"std::partial_sum(mask_long_data, mask_long_data + mask_long.numel(), mask_prefix_sum_data);
auto iter = TensorIteratorConfig()
.check_all_same_dtype(false)
.resize_outputs(false)
.add_output(result_strided)
","std::partial_sum(mask_long_data, mask_long_data + mask_long.numel(), mask_prefix_sum_data);
auto iter = TensorIteratorConfig()
    .set_check_mem_overlap(false)  // result is intenionally zero-strided above
.check_all_same_dtype(false)
.resize_outputs(false)
.add_output(result_strided)
"
1012,"Tensor ret = at::empty(self.sizes(), self.options());
auto iter = at::TensorIteratorConfig()
.check_all_same_dtype(false)
    .set_check_mem_overlap(true)
.add_output(ret)
.add_input(condition)
.add_input(self)
","Tensor ret = at::empty(self.sizes(), self.options());
auto iter = at::TensorIteratorConfig()
.check_all_same_dtype(false)
.add_output(ret)
.add_input(condition)
.add_input(self)
"
1013,"""unsupported operation: the input tensors cannot refer to any of the ""
""output memory locations. Found overlap in input tensor "", i);
}
auto should_skip = [](const Tensor& t) { return t.numel() == 0 && t.dim() == 1; };
for (auto const &tensor : tensors) {
","""unsupported operation: the input tensors cannot refer to any of the ""
""output memory locations. Found overlap in input tensor "", i);
}
  at::assert_no_internal_overlap(result);
auto should_skip = [](const Tensor& t) { return t.numel() == 0 && t.dim() == 1; };
for (auto const &tensor : tensors) {
"
1014,"replaceFallbackGraphWithFallbackFunction(copy->block());
GRAPH_DUMP(""Optimized Graph: "", copy);
// cache
optimized_plan_ =
ExecutionPlan(copy, function_name_, *remaining_bailout_depth_);
return *optimized_plan_;
","replaceFallbackGraphWithFallbackFunction(copy->block());
GRAPH_DUMP(""Optimized Graph: "", copy);
// cache
  GRAPH_DUMP(""Optimized Graph: "", copy);
optimized_plan_ =
ExecutionPlan(copy, function_name_, *remaining_bailout_depth_);
return *optimized_plan_;
"
1015,"int64_t messageId) mutable {
auto whenValueSet = rref->getFuture();
if (whenValueSet->hasError()) {
      responseFuture->setError(whenValueSet->error()->what());
return;
}
try {
","int64_t messageId) mutable {
auto whenValueSet = rref->getFuture();
if (whenValueSet->hasError()) {
      responseFuture->setError(whenValueSet->tryRetrieveErrorMessage());
return;
}
try {
"
1016,"insertQuantDequantNodes(self, observer, qparam_names, quantize_func);
}
observer_out->replaceAllUsesWith(original_val);

original_val->replaceAllUsesAfterNodeWith(dequant, dequant->output());
}
","insertQuantDequantNodes(self, observer, qparam_names, quantize_func);
}
observer_out->replaceAllUsesWith(original_val);
original_val->replaceAllUsesAfterNodeWith(dequant, dequant->output());
}
"
1017,"if (self.dim() > 1) {
int64_t n_dist = self.size(-2);
result.resize_({n_dist, n_sample});
} else {
result.resize_({n_sample});
}
","if (self.dim() > 1) {
int64_t n_dist = self.size(-2);
result.resize_({n_dist, n_sample});
    if (n_dist == 0) { return result; };
} else {
result.resize_({n_sample});
}
"
1018,"WithCPUCachingAllocatorGuard::WithCPUCachingAllocatorGuard(
CPUCachingAllocator* allocator) {
prev_caching_allocator_ptr_ = GetThreadLocalCachingAllocator();
}
","WithCPUCachingAllocatorGuard::WithCPUCachingAllocatorGuard(
CPUCachingAllocator* allocator) {
  caching_allocator_ptr = allocator;
prev_caching_allocator_ptr_ = GetThreadLocalCachingAllocator();
}
"
1019,"IntArrayRef padding,
IntArrayRef dilation,
bool ceil_mode) {
return _mkldnn_pooling(
input,
kernel_size,
","IntArrayRef padding,
IntArrayRef dilation,
bool ceil_mode) {
  TORCH_CHECK(std::all_of(dilation.cbegin(), dilation.cend(), [](int64_t i) { return 1 == i; }),
      ""mkldnn_max_pool2d does not support dilation case"");
return _mkldnn_pooling(
input,
kernel_size,
"
1020,"output, _, counts = _unique_impl(input, sorted, return_inverse, return_counts, dim)
return output, counts
def _return_output(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tensor
","output, _, counts = _unique_impl(input, sorted, return_inverse, return_counts, dim)
return output, counts

def _return_output(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tensor
"
1021,"output, inverse_indices, _ = _unique_consecutive_impl(input, return_inverse, return_counts, dim)
return output, inverse_indices
_consecutive_return_inverse_false = boolean_dispatch(
arg_name='return_counts',
arg_index=1,
","output, inverse_indices, _ = _unique_consecutive_impl(input, return_inverse, return_counts, dim)
return output, inverse_indices

_consecutive_return_inverse_false = boolean_dispatch(
arg_name='return_counts',
arg_index=1,
"
1022,"paramsDict.insert(nameTensorParamPair.second);
}
}
} // namespace jit
} // namespace torch
","paramsDict.insert(nameTensorParamPair.second);
}
}

Node* addNodeToBlock(Block* block, Value* input, Symbol kind) {
  auto new_node = block->appendNode(block->owningGraph()->create(kind));
  auto new_input = new_node->addInput(input);
  for (size_t i = 0; i < new_node->outputs().size(); i++) {
    auto output = new_node->outputs()[i];
    block->registerOutput(output);
  }
  return new_node;
}
} // namespace jit
} // namespace torch
"
1023,"value_dict[x] = str(key) + '_dynamic_axes_' + str(i + 1)
dynamic_axes[key] = value_dict
torch._C.Graph.op = _graph_op
torch._C.Graph.at = _graph_at
torch._C.Graph.constant = _graph_constant
","value_dict[x] = str(key) + '_dynamic_axes_' + str(i + 1)
dynamic_axes[key] = value_dict
def _add_block(node, input_node, op_name, **kwargs):
    new_block = node.addBlock()
    new_node = new_block.addNode(input_node, op_name)
    for k, v in kwargs.items():
        _add_attribute(new_node, k, v, False)

torch._C.Graph.op = _graph_op
torch._C.Graph.at = _graph_at
torch._C.Graph.constant = _graph_constant
"
1024,"See :func:`torch.mode`
"""""")
add_docstr_all('mul',
r""""""
mul(value) -> Tensor
","See :func:`torch.mode`
"""""")
add_docstr_all('movedim', r""""""
movedim(source, destination) -> Tensor

See :func:`torch.movedim`
"""""")

add_docstr_all('mul',
r""""""
mul(value) -> Tensor
"
1025,"See :func:`torch.ormqr`
"""""")

add_docstr_all('permute',
r""""""
permute(*dims) -> Tensor
","See :func:`torch.ormqr`
"""""")
add_docstr_all('permute',
r""""""
permute(*dims) -> Tensor
"
1026,"self.qconfig = qconfig
self.freeze_bn = freeze_bn if self.training else True
self.bn = nn.BatchNorm2d(out_channels, eps, momentum, True, True)
        self.activation_post_process = self.qconfig.activation()
self.weight_fake_quant = self.qconfig.weight()
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
","self.qconfig = qconfig
self.freeze_bn = freeze_bn if self.training else True
self.bn = nn.BatchNorm2d(out_channels, eps, momentum, True, True)
self.weight_fake_quant = self.qconfig.weight()
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
"
1027,"qconfig)
def forward(self, input):
        return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))
@classmethod
def from_float(cls, mod, qconfig=None):
","qconfig)
def forward(self, input):
        return F.relu(ConvBn2d._forward(self, input))
@classmethod
def from_float(cls, mod, qconfig=None):
"
1028,"class ConvReLU2d(nnqat.Conv2d):
r""""""
A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with
    FakeQuantize modules for both output activation and weight for
quantization aware training.
We combined the interface of :class:`~torch.nn.Conv2d` and
:class:`~torch.nn.BatchNorm2d`.
Attributes:
        activation_post_process: fake quant module for output activation
weight_fake_quant: fake quant module for weight
""""""
","class ConvReLU2d(nnqat.Conv2d):
r""""""
A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with
    FakeQuantize modules for weight for
quantization aware training.
We combined the interface of :class:`~torch.nn.Conv2d` and
:class:`~torch.nn.BatchNorm2d`.
Attributes:
weight_fake_quant: fake quant module for weight
""""""
"
1029,"default.
Attributes:
weight: fake quant module for weight
Examples::
","default.
Attributes:
        activation_post_process: fake quant module for output activation
weight: fake quant module for weight
Examples::
"
1030,"class LinearReLU(nnqat.Linear):
r""""""
A LinearReLU module fused from Linear and ReLU modules, attached with
    FakeQuantize modules for output activation and weight, used in
quantization aware training.
We adopt the same interface as :class:`torch.nn.Linear`.
","class LinearReLU(nnqat.Linear):
r""""""
A LinearReLU module fused from Linear and ReLU modules, attached with
    FakeQuantize modules for weight, used in
quantization aware training.
We adopt the same interface as :class:`torch.nn.Linear`.
"
1031,"outputs = engine.execute(roots, grads, keep_graph, create_graph, output_edges);
}
  if (inputs != nullptr) {
int num_inputs = PyTuple_GET_SIZE(inputs);
THPObjectPtr py_outputs {PyTuple_New(num_inputs)};
if (!py_outputs) return nullptr;
","outputs = engine.execute(roots, grads, keep_graph, create_graph, output_edges);
}
  if (!backward_api_called) {
int num_inputs = PyTuple_GET_SIZE(inputs);
THPObjectPtr py_outputs {PyTuple_New(num_inputs)};
if (!py_outputs) return nullptr;
"
1032,"}
ncclRedOp_t getNcclReduceOp(const ReduceOp reduceOp, at::Tensor& input) {
  if (reduceOp == ReduceOp::SUM && input.scalar_type() == at::kBool) {
    // For bool tensors, map sum to max, which both represent a bitwise or.
    // This is to prevent overflow issues with sum, since we use uint8 to
    // represent a bool (see ncclDataType mapping).
    return ncclMax;
}
  return ncclOp[reduceOp];
}
// Get the deviceList String from the list of devices
","}
ncclRedOp_t getNcclReduceOp(const ReduceOp reduceOp, at::Tensor& input) {
  try {
    if (reduceOp == ReduceOp::SUM && input.scalar_type() == at::kBool) {
      // For bool tensors, map sum to max, which both represent a bitwise or.
      // This is to prevent overflow issues with sum, since we use uint8 to
      // represent a bool (see ncclDataType mapping).
      return ncclMax;
    }
    return ncclOp.at(reduceOp);
  } catch (const std::out_of_range& e) {
    switch (reduceOp) {
      case ReduceOp::BAND:
        throw std::runtime_error(""Cannot use ReduceOp.BAND with NCCL"");
        break;
      case ReduceOp::BOR:
        throw std::runtime_error(""Cannot use ReduceOp.BOR with NCCL"");
        break;
      case ReduceOp::BXOR:
        throw std::runtime_error(""Cannot use ReduceOp.BXOR with NCCL"");
        break;
      default:
        throw std::runtime_error(""Unhandled ReduceOp"");
        break;
    }
}
}
// Get the deviceList String from the list of devices
"
1033,"return handle
class Module:
r""""""Base class for all neural network modules.
","return handle
# Trick mypy into not applying contravariance rules to inputs by defining
# forward as a value, rather than a function.  See also
# https://github.com/python/mypy/issues/8795
def _forward_unimplemented(self, *input: Any) -> None:
    raise NotImplementedError


class Module:
r""""""Base class for all neural network modules.
"
1034,"for declaration in aten_declarations:
formal_types = [arg['type'] for arg in declaration['arguments']]
type_declarations.append(METHOD_DECLARATION.substitute(declaration))
        if not declaration['manual_kernel_registration']:
body = emit_body(declaration)
type_definitions.append(METHOD_DEFINITION.substitute(
declaration, type_definition_body=body))
","for declaration in aten_declarations:
formal_types = [arg['type'] for arg in declaration['arguments']]
type_declarations.append(METHOD_DECLARATION.substitute(declaration))
        strategy = dispatch_strategy(declaration)
        if not declaration['manual_kernel_registration'] and strategy == 'use_derived':
body = emit_body(declaration)
type_definitions.append(METHOD_DEFINITION.substitute(
declaration, type_definition_body=body))
"
1035,"#include <torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h>

namespace torch {
namespace jit {

namespace onnx {
using namespace ::c10::onnx;
}

void FixupONNXIfs(Block* block) {
  for (auto* node : block->nodes()) {
    if (node->kind() == ::c10::onnx::If) {
      auto* if_node = node;
      auto* graph = if_node->owningGraph();
      for (Block* block : node->blocks()) {
        FixupONNXIfs(block);
        if (block->nodes().begin() == block->nodes().end()) {
          // ONNX does not support empty blocks, must use some op which does
          // nothing
          Value* output = block->outputs()[0];
          Node* id_node = graph->create(onnx::Identity);
          id_node->insertBefore(block->return_node());
          id_node->addInput(output);
          id_node->output()->copyMetadata(output);
          block->return_node()->replaceInputWith(output, id_node->output());
        }
      }
    } else {
      for (Block* block : node->blocks()) {
        FixupONNXIfs(block);
      }
    }
  }
}

void FixupONNXConditionals(std::shared_ptr<Graph>& graph) {
  FixupONNXIfs(graph->block());
}

} // namespace jit
} // namespace torch
","++ /dev/null
"
1036,"}
}
// This optimization does ONNX-specific peephole optimizations.
//
// At the moment, here are the optimizations it does:
","}
}
// This optimization removes consecutive SplitToSequence and ConcatFromSequence
// operators. The optimization only happens when
//  1. Output of SplitToSequence is not used by any other nodes.
//  2. The attribute keepdims and axis of SplitToSequence match
//     attribute new_axis and axis of ConcatFromSequence.
// In that case, the two ops combined are no-op, and can be safely removed.
static void removeSequenceSplitConcat(Block* b) {
  for (auto it = b->nodes().begin(), end = b->nodes().end(); it != end; ++it) {
    for (auto* child_block : it->blocks()) {
      removeSequenceSplitConcat(child_block);
    }
    if (it->kind() == onnx::ConcatFromSequence &&
        it->input()->node()->kind() == onnx::SplitToSequence) {
      if (it->input()->uses().size() > 1) {
        continue;
      }

      auto split_node = it->input()->node();
      auto concat_node = *it;

      const auto split_axis =
          split_node->hasAttribute(attr::axis) ? split_node->i(attr::axis) : 0;
      const auto split_keepdims = split_node->hasAttribute(attr::keepdims)
          ? split_node->i(attr::keepdims)
          : 1;
      const auto concat_axis = concat_node->i(attr::axis);
      const auto concat_new_axis = concat_node->hasAttribute(attr::new_axis)
          ? concat_node->i(attr::new_axis)
          : 0;
      const bool has_input_split = split_node->inputs().size() == 2;

      if (has_input_split) {
        continue;
      }

      if (split_keepdims == concat_new_axis) {
        continue;
      }

      if (split_axis != concat_axis) {
        continue;
      }

      concat_node->output()->replaceAllUsesWith(split_node->input());
    }
  }
}

// This optimization does ONNX-specific peephole optimizations.
//
// At the moment, here are the optimizations it does:
"
1037,"#include <torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h>
#include <torch/csrc/jit/passes/onnx/constant_fold.h>
#include <torch/csrc/jit/passes/onnx/eval_peephole.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_loop.h>
#include <torch/csrc/jit/passes/onnx/function_substitution.h>
#include <torch/csrc/jit/passes/onnx/peephole.h>
#include <torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h>
","#include <torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h>
#include <torch/csrc/jit/passes/onnx/constant_fold.h>
#include <torch/csrc/jit/passes/onnx/eval_peephole.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h>
#include <torch/csrc/jit/passes/onnx/function_substitution.h>
#include <torch/csrc/jit/passes/onnx/peephole.h>
#include <torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h>
"
1038,"}
}
Tensor masked_select_cpu(const Tensor & self, const Tensor & mask) {
  namedinference::compute_broadcast_outnames(self, mask);

  Tensor b_self, b_mask;
  std::tie(b_self, b_mask) = expand_outplace(self, mask, ""masked_select"");
  if (b_mask.dtype() == at::ScalarType::Byte) {
    TORCH_WARN(""masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,"" \
            ""please use a mask with dtype torch.bool instead."");
    return legacy::cpu::_th_masked_select(b_self, b_mask);
  } else {
    return legacy::cpu::_th_masked_select_bool(b_self, b_mask);
  }
}

Tensor & masked_select_out_cpu(Tensor & result, const Tensor & self, const Tensor & mask) {
  namedinference::compute_broadcast_outnames(self, mask);

  Tensor b_self, b_mask;
  std::tie(b_self, b_mask) = expand_outplace(self, mask, ""masked_select_out"");
  if (b_mask.dtype() == at::ScalarType::Bool) {
    return legacy::cpu::_th_masked_select_bool_out(result, b_self, b_mask);
  } else {
    return legacy::cpu::_th_masked_select_out(result, b_self, b_mask);
  }
}

Tensor argsort(const Tensor & self, int64_t dim, bool descending) {
return std::get<1>(at::sort(self, dim, descending));
}
","}
}
Tensor argsort(const Tensor & self, int64_t dim, bool descending) {
return std::get<1>(at::sort(self, dim, descending));
}
"
1039,"DEFINE_DISPATCH(index_put_accum_stub);
DEFINE_DISPATCH(masked_fill_stub);
REGISTER_NO_CPU_DISPATCH(index_put_accum_stub, index_put_accum_fn);
DEFINE_DISPATCH(gather_stub);
DEFINE_DISPATCH(scatter_stub);
","DEFINE_DISPATCH(index_put_accum_stub);
DEFINE_DISPATCH(masked_fill_stub);
REGISTER_NO_CPU_DISPATCH(index_put_accum_stub, index_put_accum_fn);
DEFINE_DISPATCH(masked_select_serial_stub);
DEFINE_DISPATCH(masked_select_stub);
DEFINE_DISPATCH(gather_stub);
DEFINE_DISPATCH(scatter_stub);
"
1040,"// Since we error out on svd_backward when we don't compute U and V, the backward pass for nuclear_norm
// would end up throwing an error as a result if U and V aren't computed.
// Due to this, we have to compute U and V conditionally.
  return at::sum(std::get<1>(at::svd(self, /*some=*/true,
/*compute_uv=*/at::GradMode::is_enabled() && self.requires_grad())), 0, keepdim);
}
Tensor &nuclear_norm_out(Tensor& result, const Tensor& self, bool keepdim) {
","// Since we error out on svd_backward when we don't compute U and V, the backward pass for nuclear_norm
// would end up throwing an error as a result if U and V aren't computed.
// Due to this, we have to compute U and V conditionally.
  Tensor result = at::sum(std::get<1>(at::svd(self, /*some=*/true,
/*compute_uv=*/at::GradMode::is_enabled() && self.requires_grad())), 0, keepdim);
  if (keepdim) {
    result.unsqueeze_(0);
  }
  return result;
}
Tensor &nuclear_norm_out(Tensor& result, const Tensor& self, bool keepdim) {
"
1041,"flattenTensors(backendType);
torch::jit::tensorexpr::LoopNest l(flatTensorOutputs_);
// Compute non-output tensors_ inline
for (auto& p : tensors_) {
","flattenTensors(backendType);
torch::jit::tensorexpr::LoopNest l(flatTensorOutputs_);
  GRAPH_DEBUG(""Original Stmt:\n"", std::to_string(l.root_stmt()), ""\n"");
// Compute non-output tensors_ inline
for (auto& p : tensors_) {
"
1042,"void VContext::createInstance() {
std::vector<const char*> enabledExtensions;
if (enableValidationLayers_) {
    uint32_t layerPresentCount;
    vkEnumerateInstanceLayerProperties(&layerPresentCount, nullptr);
std::vector<VkLayerProperties> layerProps(layerPresentCount);
    vkEnumerateInstanceLayerProperties(&layerPresentCount, layerProps.data());
std::array<const char*, 6> instanceLayers{
""VK_LAYER_GOOGLE_unique_objects"",
""VK_LAYER_GOOGLE_threading"",
","void VContext::createInstance() {
std::vector<const char*> enabledExtensions;
if (enableValidationLayers_) {
    uint32_t layerPresentCount = 0;
    VK_CHECK(vkEnumerateInstanceLayerProperties(&layerPresentCount, nullptr));
std::vector<VkLayerProperties> layerProps(layerPresentCount);
    VK_CHECK(vkEnumerateInstanceLayerProperties(&layerPresentCount, layerProps.data()));
std::array<const char*, 6> instanceLayers{
""VK_LAYER_GOOGLE_unique_objects"",
""VK_LAYER_GOOGLE_threading"",
"
1043,"}
void VImage::bind(
    VkDescriptorSet descriptorSet,
    uint32_t binding,
    VkDescriptorType descriptorType,
    VkImageLayout imageLayout) const {
  auto descrImageInfo = makeDescriptorImageInfo(imageLayout);
  auto writeDescrSet = makeWriteDescriptorSet(
descriptorSet, binding, descriptorType, &descrImageInfo);
vkUpdateDescriptorSets(context().device(), 1, &writeDescrSet, 0, nullptr);
}
void VImage::bindShaderRead(VkDescriptorSet descriptorSet, uint32_t binding)
    const {
bind(
descriptorSet,
binding,
","}
void VImage::bind(
    const VkDescriptorSet descriptorSet,
    const uint32_t binding,
    const VkDescriptorType descriptorType,
    const VkImageLayout imageLayout) const {
  const auto descrImageInfo = makeDescriptorImageInfo(imageLayout);
  const auto writeDescrSet = makeWriteDescriptorSet(
descriptorSet, binding, descriptorType, &descrImageInfo);
vkUpdateDescriptorSets(context().device(), 1, &writeDescrSet, 0, nullptr);
}
void VImage::bindShaderRead(
    const VkDescriptorSet descriptorSet, const uint32_t binding) const {
bind(
descriptorSet,
binding,
"
1044,"#ifdef USE_VULKAN_SHADERC_RUNTIME
void ComputeUnit::createComputePipelineCompile(
    std::string glslSrc,
    VkPipelineCache pipelineCache,
const VkDescriptorSetLayout& descrSetLayout,
    WorkGroupSize workGroupSize) {
  shaderc::Compiler compiler;
  shaderc::CompileOptions options;
options.SetGenerateDebugInfo();
options.SetTargetEnvironment(
shaderc_target_env_vulkan, shaderc_env_version_vulkan_1_0);
options.SetForcedVersionProfile(450, shaderc_profile_core);
  shaderc::SpvCompilationResult compilationResult = compiler.CompileGlslToSpv(
glslSrc.c_str(),
glslSrc.size(),
shaderc_compute_shader,
""vulkan_shader.comp"",
""main"",
options);
  auto compilationStatus = compilationResult.GetCompilationStatus();
TORCH_INTERNAL_ASSERT(
compilationStatus == shaderc_compilation_status_success,
""Shader compilation error: status:"",
compilationStatus,
compilationResult.GetErrorMessage());
  std::vector<uint32_t> shaderSpvCode(
compilationResult.cbegin(), compilationResult.cend());
const auto codeSizeBytes = 4 * shaderSpvCode.size();
createComputePipeline(
","#ifdef USE_VULKAN_SHADERC_RUNTIME
void ComputeUnit::createComputePipelineCompile(
    const std::string& glslSrc,
    const VkPipelineCache pipelineCache,
const VkDescriptorSetLayout& descrSetLayout,
    const WorkGroupSize workGroupSize) {
  shaderc::Compiler compiler{};
  shaderc::CompileOptions options[];
options.SetGenerateDebugInfo();
options.SetTargetEnvironment(
shaderc_target_env_vulkan, shaderc_env_version_vulkan_1_0);
options.SetForcedVersionProfile(450, shaderc_profile_core);
  const shaderc::SpvCompilationResult compilationResult = compiler.CompileGlslToSpv(
glslSrc.c_str(),
glslSrc.size(),
shaderc_compute_shader,
""vulkan_shader.comp"",
""main"",
options);
  const auto compilationStatus = compilationResult.GetCompilationStatus();
TORCH_INTERNAL_ASSERT(
compilationStatus == shaderc_compilation_status_success,
""Shader compilation error: status:"",
compilationStatus,
compilationResult.GetErrorMessage());
  const std::vector<uint32_t> shaderSpvCode(
compilationResult.cbegin(), compilationResult.cend());
const auto codeSizeBytes = 4 * shaderSpvCode.size();
createComputePipeline(
"
1045,"}
void ComputeUnit::dispatchCommandBuffer(
    uint32_t gridX,
    uint32_t gridY,
    uint32_t gridZ,
    WorkGroupSize workGroupSize) {
dispatchCommandBuffer(
UP_DIV(gridX, workGroupSize.x),
UP_DIV(gridY, workGroupSize.y),
","}
void ComputeUnit::dispatchCommandBuffer(
    const uint32_t gridX,
    const uint32_t gridY,
    const uint32_t gridZ,
    const WorkGroupSize workGroupSize) {
dispatchCommandBuffer(
UP_DIV(gridX, workGroupSize.x),
UP_DIV(gridY, workGroupSize.y),
"
1046,"Tensor& _clamp__vulkan(
Tensor& self,
    c10::optional<Scalar> min,
    c10::optional<Scalar> max) {
auto y = vulkan_clamp(self, min, max);
self.copy_(y);
return self;
}
Tensor vulkan_hardtanh(const Tensor& self, Scalar min, Scalar max) {
return vulkan_clamp(self, min, max);
}
Tensor& vulkan_hardtanh_(Tensor& self, Scalar min, Scalar max) {
return _clamp__vulkan(self, min, max);
}
Tensor mean_vulkan(
const Tensor& self,
    IntArrayRef dim,
    bool keepdim,
    optional<ScalarType> dtype) {
TORCH_INTERNAL_ASSERT(
self.is_vulkan(), ""mean_vulkan expects Vulkan tensor input"");
TORCH_INTERNAL_ASSERT(
self.dim() == 4 && dim.size() == 2 && dim[0] == 2 && dim[1] == 3);
VulkanTensor& x = vtensor_from_vulkan(self);
  auto sizes = self.sizes();
  std::vector<int64_t> outputSizes{sizes[0], sizes[1]};
  VulkanTensor output = VulkanTensor{outputSizes};
output.allocate_storage();
vulkan::detail::mean(output, x);
return new_with_vtensor_vulkan(std::move(output), self.options());
","Tensor& _clamp__vulkan(
Tensor& self,
    const c10::optional<Scalar> min,
    const c10::optional<Scalar> max) {
auto y = vulkan_clamp(self, min, max);
self.copy_(y);
return self;
}
Tensor vulkan_hardtanh(const Tensor& self, const Scalar min, const Scalar max) {
return vulkan_clamp(self, min, max);
}
Tensor& vulkan_hardtanh_(Tensor& self, const Scalar min, const Scalar max) {
return _clamp__vulkan(self, min, max);
}
Tensor mean_vulkan(
const Tensor& self,
    const IntArrayRef dim,
    const bool keepdim,
    const optional<ScalarType> dtype) {
TORCH_INTERNAL_ASSERT(
self.is_vulkan(), ""mean_vulkan expects Vulkan tensor input"");
TORCH_INTERNAL_ASSERT(
self.dim() == 4 && dim.size() == 2 && dim[0] == 2 && dim[1] == 3);
VulkanTensor& x = vtensor_from_vulkan(self);
  const auto sizes = self.sizes();
  VulkanTensor output = VulkanTensor{std::vector<int64_t>{sizes[0], sizes[1]}};
output.allocate_storage();
vulkan::detail::mean(output, x);
return new_with_vtensor_vulkan(std::move(output), self.options());
"
1047,"std::unordered_map<std::string, std::string>>;
// Referenced the logic in llvm-cxxfilt.cpp.
std::string demangle(const std::string& mangled) {
int status;
const char* decorated = mangled.c_str();
size_t decoratedLength = mangled.length();
","std::unordered_map<std::string, std::string>>;
// Referenced the logic in llvm-cxxfilt.cpp.
// Starting from LLVM 9 it provides a `demangle()` API. Here we keep our ad-hoc
// version for backward compatibility.
std::string _demangle(const std::string& mangled) {
int status;
const char* decorated = mangled.c_str();
size_t decoratedLength = mangled.length();
"
1048,"SET roots;
for (const auto& F : visibleFuncs) {
std::string name = F->getName();
      auto demangled = demangle(name);
if (RootSymbolPatternLoc.pattern->match(demangled)) {
roots.insert(name);
if (Verbose) {
","SET roots;
for (const auto& F : visibleFuncs) {
std::string name = F->getName();
      auto demangled = _demangle(name);
if (RootSymbolPatternLoc.pattern->match(demangled)) {
roots.insert(name);
if (Verbose) {
"
1049,"std::ostream& out, const SET& keys, const GRAPH& graph,
const PATH* path) {
for (const auto& K : keys) {
      out << ""- name: "" << demangle(K) << std::endl;
auto it = graph.find(K);
if (it == graph.end() || it->second.empty()) {
continue;
}
out << ""  depends:"" << std::endl;
for (const auto& value : it->second) {
        out << ""  - name: "" << demangle(value) << std::endl;
if (path) {
std::vector<std::string> rpath;
for (std::string prev = value;
","std::ostream& out, const SET& keys, const GRAPH& graph,
const PATH* path) {
for (const auto& K : keys) {
      out << ""- name: "" << _demangle(K) << std::endl;
auto it = graph.find(K);
if (it == graph.end() || it->second.empty()) {
continue;
}
out << ""  depends:"" << std::endl;
for (const auto& value : it->second) {
        out << ""  - name: "" << _demangle(value) << std::endl;
if (path) {
std::vector<std::string> rpath;
for (std::string prev = value;
"
1050,"TORCH_INTERNAL_ASSERT(false, ""NYI"");
}
int64_t VmapPhysicalView::numBatchDims() {
return levels_.count();
}
int64_t VmapPhysicalView::numLogicalDims() {
return /*physical*/tensor_.dim() - numBatchDims();
}
VmapDimVector VmapPhysicalView::getPhysicalDims(IntArrayRef logical_dims) {
auto logical_ndim = numLogicalDims();
// NB: fmap doesn't have a SmallVector variant, so we don't use it here.
VmapDimVector result;
","TORCH_INTERNAL_ASSERT(false, ""NYI"");
}
int64_t VmapPhysicalView::numBatchDims() const {
return levels_.count();
}
int64_t VmapPhysicalView::numLogicalDims() const {
return /*physical*/tensor_.dim() - numBatchDims();
}
VmapDimVector VmapPhysicalView::getPhysicalDims(IntArrayRef logical_dims) const {
auto logical_ndim = numLogicalDims();
// NB: fmap doesn't have a SmallVector variant, so we don't use it here.
VmapDimVector result;
"
1051,"const auto inferred_dispatch_key = denseTypeIdWithDefault(r, ARG_DEVICE, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(ARG_TYPE, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(ARG_DEVICE));
  Tensor values = internal_new_from_data(inferred_dispatch_key, inferred_scalar_type, r.deviceOptional(ARG_DEVICE), r.pyobject(ARG_VALUES), false, true, type_inference);
  Tensor indices = internal_new_from_data(legacyExtractDispatchKey(values.key_set()), kLong, r.deviceOptional(ARG_DEVICE), r.pyobject(ARG_INDICES), false, true, false);
return at::_sparse_coo_tensor_unsafe(indices, values, r.intlist(ARG_SIZE), values.options().layout(at::kSparse)).set_requires_grad(r.toBool(ARG_REQUIRES_GRAD));
}
","const auto inferred_dispatch_key = denseTypeIdWithDefault(r, ARG_DEVICE, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(ARG_TYPE, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(ARG_DEVICE));
  Tensor values = internal_new_from_data(inferred_dispatch_key, inferred_scalar_type, r.deviceOptional(ARG_DEVICE), r.pyobject(ARG_VALUES),
                                         /*copy_variables=*/false, /*copy_numpy=*/true,
                                         /*type_inference=*/type_inference);
  Tensor indices = internal_new_from_data(legacyExtractDispatchKey(values.key_set()), kLong, r.deviceOptional(ARG_DEVICE), r.pyobject(ARG_INDICES),
                                          /*copy_variables=*/false, /*copy_numpy=*/true,
                                          /*type_inference=*/false);
return at::_sparse_coo_tensor_unsafe(indices, values, r.intlist(ARG_SIZE), values.options().layout(at::kSparse)).set_requires_grad(r.toBool(ARG_REQUIRES_GRAD));
}
"
1052,"r.scalartypeWithDefault(1, scalar_type),
r.deviceOptional(2),
r.pyobject(0),
        false,
        false,
        type_inference);
}
throw std::runtime_error(""tensor(): invalid arguments"");
}
","r.scalartypeWithDefault(1, scalar_type),
r.deviceOptional(2),
r.pyobject(0),
        /*copy_variables=*/false,
        /*copy_numpy=*/false,
        /*type_inference=*/type_inference);
}
throw std::runtime_error(""tensor(): invalid arguments"");
}
"
1053,"add_docstr(torch.cross,
r""""""
cross(input, other, dim=-1, out=None) -> Tensor
Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input`
","add_docstr(torch.cross,
r""""""
cross(input, other, dim=None, out=None) -> Tensor
Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input`
"
1054,":meth:`~torch.nn.ModuleDict.update`).
Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict`` before Python verision 3.6) does not
preserve the order of the merged mapping.
Arguments:
",":meth:`~torch.nn.ModuleDict.update`).
Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict`` before Python version 3.6) does not
preserve the order of the merged mapping.
Arguments:
"
1055,":math:`\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}`
    where :math:`i, j` run over sparse tensor indicies and unspecified
    entries are ignores. This is equivalent to defining unspecifed
entries as negative infinity so that :max:`exp(x_k) = 0` when the
entry with index :math:`k` has not specified.
",":math:`\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}`
    where :math:`i, j` run over sparse tensor indices and unspecified
    entries are ignores. This is equivalent to defining unspecified
entries as negative infinity so that :max:`exp(x_k) = 0` when the
entry with index :math:`k` has not specified.
"
1056,"corresponding_topts.append(
{'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
'kwarg_only': True, 'default': 'at::kLong'})
def check_topt_representation(topt_representation):
for idx, supported_topt in enumerate(supported_topt_arguments):
","corresponding_topts.append(
{'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
'kwarg_only': True, 'default': 'at::kLong'})
    corresponding_topts.append(
        {'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
         'kwarg_only': True})
def check_topt_representation(topt_representation):
for idx, supported_topt in enumerate(supported_topt_arguments):
"
1057,"scale_factor_len = input.dim() - 2
scale_factor_list = torch.jit.annotate(List[Optional[float]], [None for _ in range(scale_factor_len)])
    if scale_factor is not None and recompute_scale_factor is False:
        if isinstance(scale_factor, list):
_scale_factor_repeated = scale_factor
else:
_scale_factor_repeated = [scale_factor for _ in range(scale_factor_len)]  # noqa: C416
","scale_factor_len = input.dim() - 2
scale_factor_list = torch.jit.annotate(List[Optional[float]], [None for _ in range(scale_factor_len)])
    # default value of recompute_scale_factor is False
    if scale_factor is not None and (recompute_scale_factor is False or recompute_scale_factor is None):
        if isinstance(scale_factor, (list, tuple)):
_scale_factor_repeated = scale_factor
else:
_scale_factor_repeated = [scale_factor for _ in range(scale_factor_len)]  # noqa: C416
"
1058,"return at::cuda::getPinnedMemoryAllocator();
}
bool CUDAHooks::compiledWithCuDNN() const {
return AT_CUDNN_ENABLED();
}
","return at::cuda::getPinnedMemoryAllocator();
}
Allocator* CUDAHooks::getCUDADeviceAllocator() const {
  return at::cuda::getCUDADeviceAllocator();
}

bool CUDAHooks::compiledWithCuDNN() const {
return AT_CUDNN_ENABLED();
}
"
1059,"from __future__ import absolute_import, division, print_function, unicode_literals


r""""""
`torch.distributed.launch` is a module that spawns up multiple distributed
training processes on each of the training nodes.
","r""""""
`torch.distributed.launch` is a module that spawns up multiple distributed
training processes on each of the training nodes.
"
1060,"if (auto type =
node->namedInput(attr::self)->type()->cast<TensorType>()) {
if (type->dim()) {
              return factory_with_ndim(node, *type->dim());
}
}
return {};
","if (auto type =
node->namedInput(attr::self)->type()->cast<TensorType>()) {
if (type->dim()) {
              return factory_like_with_ndim(node, *type->dim());
}
}
return {};
"
1061,">>> # Initialize random batch of input vectors, for *size = (T,N,C)
>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
        >>>
>>> # Initialize random batch of targets (0 = blank, 1:C = classes)
>>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)
>>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)
",">>> # Initialize random batch of input vectors, for *size = (T,N,C)
>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
        >>>
>>> # Initialize random batch of targets (0 = blank, 1:C = classes)
>>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)
>>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)
"
1062,"remapTypes(graph.get(), source, target, module_qconfig_map, type_remap_fn);
// remap self
graph->inputs()[0]->setType(target.type());
const auto this_method_name =
c10::QualifiedName(*target.type()->name(), method.name());
auto copied = target._ivalue()->compilation_unit()->create_function(
this_method_name, graph);
target.type()->addMethod(copied);
    // we'll use default schema for cloned method
}
};
","remapTypes(graph.get(), source, target, module_qconfig_map, type_remap_fn);
// remap self
graph->inputs()[0]->setType(target.type());
    // we only support %self being Module in the arguments of function
    auto schema_type_remap_fn = [&](TypePtr type_ptr) {
      return type_remap_fn(type_ptr, module_qconfig_map.at(source._ivalue()));
    };
    auto schema =
        method.getSchema().cloneWithRemappedTypes(schema_type_remap_fn);
const auto this_method_name =
c10::QualifiedName(*target.type()->name(), method.name());
auto copied = target._ivalue()->compilation_unit()->create_function(
this_method_name, graph);
target.type()->addMethod(copied);
    copied->setSchema(std::move(schema));
}
};
"
1063,"ANDROID_LOG_DEBUG, // VLOG(1)
ANDROID_LOG_VERBOSE, // VLOG(2) .. VLOG(N)
};
  int android_level_index = FATAL - std::min(FATAL, severity_);
int level = android_log_levels[std::min(android_level_index, 5)];
// Output the log string the Android log at the appropriate level.
__android_log_print(level, tag_, ""%s"", stream_.str().c_str());
// Indicate termination if needed.
  if (severity_ == FATAL) {
__android_log_print(ANDROID_LOG_FATAL, tag_, ""terminating.\n"");
}
#else // !ANDROID
","ANDROID_LOG_DEBUG, // VLOG(1)
ANDROID_LOG_VERBOSE, // VLOG(2) .. VLOG(N)
};
  int android_level_index =
      ::google::GLOG_FATAL - std::min(::google::GLOG_FATAL, severity_);
int level = android_log_levels[std::min(android_level_index, 5)];
// Output the log string the Android log at the appropriate level.
__android_log_print(level, tag_, ""%s"", stream_.str().c_str());
// Indicate termination if needed.
  if (severity_ == ::google::GLOG_FATAL) {
__android_log_print(ANDROID_LOG_FATAL, tag_, ""terminating.\n"");
}
#else // !ANDROID
"
1064,"If :math:`y = 1` then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for :math:`y = -1`.
    The loss function for each sample in the mini-batch is:
.. math::
        \text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})
Args:
margin (float, optional): Has a default value of :math:`0`.
","If :math:`y = 1` then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for :math:`y = -1`.
    The loss function for each pair of samples in the mini-batch is:
.. math::
        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})
Args:
margin (float, optional): Has a default value of :math:`0`.
"
1065,"[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""")
add_docstr(torch.reciprocal,
r""""""
","[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""".format(**common_args))
add_docstr(torch.reciprocal,
r""""""
"
1066,"EigenArrayMap<T> Y_arr(Y, N, M);
T tmp = T(0);
if (gamma != nullptr && beta != nullptr) {
ConstEigenVectorArrayMap<T> gamma_arr(gamma, N);
ConstEigenVectorArrayMap<T> beta_arr(beta, N);
for (int i = 0; i < M; ++i) {
      T normFactor = T(T(1) / std_arr[i]);
      fp16_wrap(&normFactor);
      for (int j = 0; j < N; ++j) {
        tmp = T(X_arr.col(i)[j] - mean[i]);
        fp16_wrap(&tmp);
        T normalized = tmp * normFactor;
        fp16_wrap(&normalized);
        tmp = normalized * gamma_arr[j] + beta_arr[j];
        fp16_wrap(&tmp);
        Y_arr.col(i)[j] = tmp;
}
    }
  } else {
    for (int i = 0; i < M; ++i) {
      T normFactor = T(T(1) / std_arr[i]);
      fp16_wrap(&normFactor);
      for (int j = 0; j < N; ++j) {
        tmp = T(X_arr.col(i)[j] - mean[i]);
        fp16_wrap(&tmp);
        tmp *= normFactor;
        fp16_wrap(&tmp);
        Y_arr.col(i)[j] = tmp;
}
}
}
","EigenArrayMap<T> Y_arr(Y, N, M);
T tmp = T(0);
  for (int i = 0; i < M; ++i) {
    T normFactor = T(T(1) / std_arr[i]);
    fp16_wrap(&normFactor);
    for (int j = 0; j < N; ++j) {
      T normalized = T(X_arr.col(i)[j] - mean[i]);
      fp16_wrap(&normalized);
      normalized *= normFactor;
      fp16_wrap(&normalized);
      Y_arr.col(i)[j] = normalized;
    }
  }

if (gamma != nullptr && beta != nullptr) {
ConstEigenVectorArrayMap<T> gamma_arr(gamma, N);
ConstEigenVectorArrayMap<T> beta_arr(beta, N);
for (int i = 0; i < M; ++i) {
      vector<float> res(N);
      for (int j = 0; j < N; j++) {
        res[j] = beta[j];
}
      fake_fp16::fma_fp16(N, &Y_arr.col(i)[0], gamma, res.data());
      for (int j = 0; j < N; j++) {
        Y_arr.col(i)[j] = res[j];
}
}
}
"
1067,"name = record.name()
if start_record is None and name == '__start_profile':
start_record = record
        elif name == '__cuda_start_event':
# N.B.: Each CUDA device has its own __cuda_start_event.
assert record.device() != -1
# key for cuda_records is (node_id, device) in case of multiple nodes
","name = record.name()
if start_record is None and name == '__start_profile':
start_record = record
        elif '__cuda_start_event' in name:
# N.B.: Each CUDA device has its own __cuda_start_event.
assert record.device() != -1
# key for cuda_records is (node_id, device) in case of multiple nodes
"
1068,"return profilerConfig_;
}
std::unique_ptr<RpcWithProfilingReq> RpcWithProfilingReq::fromMessage(
const rpc::Message& message) {
rpc::MessageType origMsgType = message.type();
","return profilerConfig_;
}
const rpc::ProfilingId& RpcWithProfilingReq::getProfilingId() const {
  return profilingKeyId_;
}

std::unique_ptr<RpcWithProfilingReq> RpcWithProfilingReq::fromMessage(
const rpc::Message& message) {
rpc::MessageType origMsgType = message.type();
"
1069,"RpcCommandBase& rpc = *rpcPtr;
auto& rpcWithProfilingResp =
static_cast<autograd::RpcWithProfilingResp&>(rpc);
      // read and reconstruct events
      // Check if the profiler is enabled
      auto enabled = torch::autograd::profiler::profilerEnabled();
      TORCH_CHECK(
          enabled,
          ""Profiler was expected to be enabled. This can happen in callback ""
          "" continutations that run in different threads, and the TLS of the ""
          "" profiler was not propagated."");
      std::vector<torch::autograd::profiler::Event> events =
          rpcWithProfilingResp.getProfiledEvents();
      wrappedMsgType = rpcWithProfilingResp.wrappedMessageType();

      torch::autograd::profiler::addEventList(std::move(events));
auto wrappedRPC = std::move(rpcWithProfilingResp).moveWrappedRpc();
return wrappedRPC;
}
","RpcCommandBase& rpc = *rpcPtr;
auto& rpcWithProfilingResp =
static_cast<autograd::RpcWithProfilingResp&>(rpc);
      // Process remotely profiled events.
      processRemoteProfiledEvents(rpcWithProfilingResp);
      wrappedMsgType = rpcWithProfilingResp.wrappedMessageType();
auto wrappedRPC = std::move(rpcWithProfilingResp).moveWrappedRpc();
return wrappedRPC;
}
"
1070,"return self->cast<InterfaceType>() != nullptr;
});
py::class_<AnyType, Type, std::shared_ptr<AnyType>>(m, ""AnyType"")
.def_static(""get"", &AnyType::get);
py::class_<NumberType, Type, std::shared_ptr<NumberType>>(m, ""NumberType"")
","return self->cast<InterfaceType>() != nullptr;
});
  using ::c10::NamedType;
  py::class_<NamedType, Type, std::shared_ptr<NamedType>>(m, ""NamedType"")
      .def(""name"", [](const NamedType& type) {
        if (type.name().has_value()) {
          return type.name().value().name();
        }
        return std::string();
      });
py::class_<AnyType, Type, std::shared_ptr<AnyType>>(m, ""AnyType"")
.def_static(""get"", &AnyType::get);
py::class_<NumberType, Type, std::shared_ptr<NumberType>>(m, ""NumberType"")
"
1071,".def(
""get_interface"",
[](const std::shared_ptr<CompilationUnit>& self,
             const std::string& name) { return self->get_interface(name); });
py::class_<StrongFunctionPtr>(m, ""ScriptFunction"", py::dynamic_attr())
.def(
",".def(
""get_interface"",
[](const std::shared_ptr<CompilationUnit>& self,
             const std::string& name) { return self->get_interface(name); })
      .def(
          ""get_type"",
          [](CompilationUnit& cu, const std::string& name) {
            return cu.get_type(name);
          })
      .def(
          ""register_type"",
          [](CompilationUnit& cu, const c10::NamedTypePtr& type) {
            cu.register_type(type);
          });
py::class_<StrongFunctionPtr>(m, ""ScriptFunction"", py::dynamic_attr())
.def(
"
1072,"[](Module& module,
const std::string& method_name,
bool inplace,
int quant_type_int) {
auto quant_type = static_cast<QuantType>(quant_type_int);
            return InsertQuantDeQuant(module, method_name, inplace, quant_type);
},
py::arg(""module""),
py::arg(""method_name""),
py::arg(""inplace""),
py::arg(""quant_type_int"") = 1)
.def(
""_jit_pass_insert_prepack_unpack"",
","[](Module& module,
const std::string& method_name,
bool inplace,
             bool debug,
int quant_type_int) {
auto quant_type = static_cast<QuantType>(quant_type_int);
            return InsertQuantDeQuant(
                module, method_name, inplace, debug, quant_type);
},
py::arg(""module""),
py::arg(""method_name""),
py::arg(""inplace""),
          py::arg(""debug""),
py::arg(""quant_type_int"") = 1)
.def(
""_jit_pass_insert_prepack_unpack"",
"
1073,"},
aliasAnalysisFromSchema()),
Operator(
         ""aten::local_value(RRef(t) self) -> t"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
TORCH_CHECK(
","},
aliasAnalysisFromSchema()),
Operator(
         ""aten::local_value(RRef(t) self) -> t(*)"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
TORCH_CHECK(
"
1074,"zero_prob_condition = (self.sum(1) == 0).sum().item().to<bool>();
}
TORCH_CHECK(!zero_prob_condition, ""invalid multinomial distribution (sum of probabilities <= 0)"");

auto rand = at::empty_like(self).uniform_(0, 1, gen);
rand.log_().div_(self); //save memory with inplace operations
auto vals = at::empty(result.sizes(), self.options());
","zero_prob_condition = (self.sum(1) == 0).sum().item().to<bool>();
}
TORCH_CHECK(!zero_prob_condition, ""invalid multinomial distribution (sum of probabilities <= 0)"");
auto rand = at::empty_like(self).uniform_(0, 1, gen);
rand.log_().div_(self); //save memory with inplace operations
auto vals = at::empty(result.sizes(), self.options());
"
1075,"// Implementations of historic symbol behaviors are defined here
// See note [Versioned Symbols]
auto _test_serialization_subcmul = R""SCRIPT(
def _test_serialization_subcmul_0_2(self: Tensor, other:Tensor, alpha: number=2) -> Tensor:
return other - (self * alpha)
)SCRIPT"";
struct BuiltinFunctionRegistry {
const std::vector<Function*>& getAllBuiltinFunctionsFor(Symbol name) {
const static std::vector<Function*> empty;
","// Implementations of historic symbol behaviors are defined here
// See note [Versioned Symbols]

// This builtin is for testing
auto _test_serialization_subcmul = R""SCRIPT(
def _test_serialization_subcmul_0_2(self: Tensor, other:Tensor, alpha: number=2) -> Tensor:
return other - (self * alpha)
)SCRIPT"";
// Division versioned symbols, for Torchscript programs serialized when
// division on integer tensors was floor division, not true division.

// Tensor x Tensor
// NOTE: testing for the tensors being float tensors is sufficient here,
// because the Torchscript versions this fix applies to (0 through 3)
// did not support complex tensors.
auto div_tensor = R""SCRIPT(
def div_0_3(self: Tensor, other: Tensor) -> Tensor:
  if (self.is_floating_point() or other.is_floating_point()):
    return self.true_divide(other)
  return self.floor_divide(other)
)SCRIPT"";

// Tensor x Scalar
auto div_tensor_scalar = R""SCRIPT(
def div_0_3(self: Tensor, other: number) -> Tensor:
  if (self.is_floating_point() or isinstance(other, float)):
    return self.true_divide(other)
  return self.floor_divide(other)
)SCRIPT"";

// Scalar x Scalar
auto div_scalar_scalar = R""SCRIPT(
def div_0_3(self: number, other: number) -> number:
  return self / other
)SCRIPT"";

// Tensor x Tensor with out kwarg
// NOTE: the JIT doesn't support Tensor x Scalar with the out kwarg
auto div_tensor_out = R""SCRIPT(
def div_0_3(self: Tensor, other: Tensor, *, out: Tensor) -> Tensor:
  if (self.is_floating_point() or other.is_floating_point() or out.is_floating_point()):
    return self.true_divide(other, out=out)
  return self.floor_divide(other, out=out)
)SCRIPT"";

// Tensor x Tensor inplace
auto div__tensor = R""SCRIPT(
def div__0_3(self: Tensor, other: Tensor) -> Tensor:
  if (self.is_floating_point() or other.is_floating_point()):
    return self.true_divide_(other)
  return self.floor_divide_(other)
)SCRIPT"";

// Tensor x Scalar inplace
auto div__scalar = R""SCRIPT(
def div__0_3(self: Tensor, other: number) -> Tensor:
  if (self.is_floating_point() or isinstance(other, float)):
    return self.true_divide_(other)
  return self.floor_divide_(other)
)SCRIPT"";

struct BuiltinFunctionRegistry {
const std::vector<Function*>& getAllBuiltinFunctionsFor(Symbol name) {
const static std::vector<Function*> empty;
"
1076,"case MessageType::PYTHON_CALL: {
auto& upc = static_cast<UnpickledPythonCall&>(rpc);
if (upc.isAsyncExecution()) {
        processAsyncExecution(
            upc.pythonUdf(),
            messageId,
            responseFuture,
            [](const py::object& result,
               const int64_t messageId,
               PythonRpcHandler& pythonRpcHandler,
               const std::shared_ptr<FutureMessage>& responseFuture) {
              auto serializedPyObj = pythonRpcHandler.serialize(result);
              py::gil_scoped_release release;
              auto m =
                  std::move(PythonResp(std::move(serializedPyObj))).toMessage();
              m.setId(messageId);
              responseFuture->markCompleted(std::move(m));
            });
} else {
auto& pythonRpcHandler = PythonRpcHandler::getInstance();
std::shared_ptr<SerializedPyObj> serializedPyObj;
","case MessageType::PYTHON_CALL: {
auto& upc = static_cast<UnpickledPythonCall&>(rpc);
if (upc.isAsyncExecution()) {
        try {
          processAsyncExecution(
              upc.pythonUdf(),
              messageId,
              responseFuture,
              [](const py::object& result,
                 const int64_t messageId,
                 PythonRpcHandler& pythonRpcHandler,
                 const std::shared_ptr<FutureMessage>& responseFuture) {
                auto serializedPyObj = pythonRpcHandler.serialize(result);
                py::gil_scoped_release release;
                auto m = std::move(PythonResp(std::move(serializedPyObj)))
                             .toMessage();
                m.setId(messageId);
                responseFuture->markCompleted(std::move(m));
              });
        } catch (std::exception& e) {
          responseFuture->markCompleted(
              createExceptionResponse(e.what(), messageId));
        }
} else {
auto& pythonRpcHandler = PythonRpcHandler::getInstance();
std::shared_ptr<SerializedPyObj> serializedPyObj;
"
1077,"rewriter.runOnGraph(graph, filter);
}
void checkCalculateQParamsResult(const IValue& qparams) {
TORCH_CHECK(
qparams.isTuple(),
","rewriter.runOnGraph(graph, filter);
}
void ReplicateClampScalarArgs(std::shared_ptr<Graph>& graph) {
  std::stack<Block*> blocks_to_visit;
  std::unordered_set<Node*> scalar_nodes_to_rewrite;
  ;
  blocks_to_visit.push(graph->block());
  while (!blocks_to_visit.empty()) {
    Block* b = blocks_to_visit.top();
    blocks_to_visit.pop();
    for (Node* n : b->nodes()) {
      for (Value* output : n->outputs()) {
        if (getClampScalarInputUse(output) && output->uses().size() > 1) {
          scalar_nodes_to_rewrite.insert(n);
        }
      }
      for (Block* subblock : n->blocks()) {
        blocks_to_visit.push(subblock);
      }
    }
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    const std::vector<Use> uses = n->output()->uses();
    for (const auto& use : uses) {
      Node* user = use.user;
      WithInsertPoint ins(user);
      Node* cloned_node = graph->createClone(n, [](Value* v) { return v; });
      graph->insertNode(cloned_node);
      user->replaceInput(use.offset, cloned_node->output());
    }
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    n->removeAllInputs();
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    n->destroy();
  }
}

void checkCalculateQParamsResult(const IValue& qparams) {
TORCH_CHECK(
qparams.isTuple(),
"
1078,"legacy_class_hints = []
for c in ('DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',
              'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor'):
legacy_class_hints.append('class {}(Tensor): ...'.format(c))
# Generate type signatures for dtype classes
","legacy_class_hints = []
for c in ('DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',
              'ShortTensor', 'HalfTensor', 'CharTensor', 'ByteTensor', 'BoolTensor'):
legacy_class_hints.append('class {}(Tensor): ...'.format(c))
# Generate type signatures for dtype classes
"
1079,"reduce_op->reduce_args().end()};
// Store loops below the target point.
  std::vector<const For*> init_loops;
while (st) {
if (For* f = dynamic_cast<For*>(st)) {
if (f->var() == reduction_var) {
target_for = f;
        init_loops.push_back(f);
}
if (reduce_args.count(f->var())) {
reduce_args.erase(f->var());
} else {
        init_loops.push_back(f);
}
if (reduce_args.empty()) {
","reduce_op->reduce_args().end()};
// Store loops below the target point.
  std::vector<const For*> output_loops;
while (st) {
if (For* f = dynamic_cast<For*>(st)) {
if (f->var() == reduction_var) {
target_for = f;
        output_loops.push_back(f);
}
if (reduce_args.count(f->var())) {
reduce_args.erase(f->var());
} else {
        output_loops.push_back(f);
}
if (reduce_args.empty()) {
"
1080,"new Store(tmp_buf, new_outer, init_it->second, new IntImm(1));
// Wrap it in any loops lower than the insertion point of the new reduction.
    for (auto* il : init_loops) {
      init_stmt = il->cloneWithNewBody(init_stmt);
}
    parent_block->prepend_stmt(init_stmt);
} else {
// We may support this but not possible now.
throw std::runtime_error(""can't rfactor reduction with no initializer\n"");
","new Store(tmp_buf, new_outer, init_it->second, new IntImm(1));
// Wrap it in any loops lower than the insertion point of the new reduction.
    for (auto* ol : output_loops) {
      init_stmt = ol->cloneWithNewBody(init_stmt);
}
    parent_block->insert_stmt_before(init_stmt, new_root_for);
} else {
// We may support this but not possible now.
throw std::runtime_error(""can't rfactor reduction with no initializer\n"");
"
1081,"return True
_handles = {}


def set_flags(_enabled, _benchmark, _deterministic):
    global benchmark, deterministic
orig_flags = (torch._C._get_cudnn_enabled(),
torch._C._get_cudnn_benchmark(),
torch._C._get_cudnn_deterministic())
","return True
def set_flags(_enabled, _benchmark, _deterministic):
orig_flags = (torch._C._get_cudnn_enabled(),
torch._C._get_cudnn_benchmark(),
torch._C._get_cudnn_deterministic())
"
1082,"""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
    auto sizes = self.sizes();
    if (sizes[dim.value()] == 1) {
      if (keepdim) {
        result = at::zeros(sizes, self.options().dtype(at::kLong));
      } else {
        auto sizes_vec = sizes.vec();
        sizes_vec.erase(sizes_vec.begin() + dim.value());
        result = at::zeros(sizes_vec, self.options().dtype(at::kLong));
      }
      return result;
    }
in = self;
} else {
in = self.reshape({-1});
","""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
in = self;
} else {
in = self.reshape({-1});
"
1083,"""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
    auto sizes = self.sizes();
    if (sizes[dim.value()] == 1) {
      if (keepdim) {
        result = at::zeros(sizes, self.options().dtype(at::kLong));
      } else {
        auto sizes_vec = sizes.vec();
        sizes_vec.erase(sizes_vec.begin() + dim.value());
        result = at::zeros(sizes_vec, self.options().dtype(at::kLong));
      }
      return result;
    }
in = self;
} else {
in = self.reshape({-1});
","""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
in = self;
} else {
in = self.reshape({-1});
"
1084,"}
TensorPipeAgent::TensorPipeAgent(
    std::shared_ptr<::c10d::Store> addressStore,
std::string selfName,
worker_id_t selfId,
int worldSize,
","}
TensorPipeAgent::TensorPipeAgent(
    const std::shared_ptr<::c10d::Store>& store,
std::string selfName,
worker_id_t selfId,
int worldSize,
"
1085,"""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
in = self;
} else {
in = self.reshape({-1});
","""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
    auto sizes = self.sizes();
    if (sizes[dim.value()] == 1) {
      if (keepdim) {
        result = at::zeros(sizes, self.options().dtype(at::kLong));
      } else {
        auto sizes_vec = sizes.vec();
        sizes_vec.erase(sizes_vec.begin() + dim.value());
        result = at::zeros(sizes_vec, self.options().dtype(at::kLong));
      }
      return result;
    }
in = self;
} else {
in = self.reshape({-1});
"
1086,"tensorpipe::ContextOptions().name(workerInfo_.name_))),
addressStore_(std::move(addressStore)),
worldSize_(worldSize),
      opts_(std::move(opts)) {
collectNames();
// Initialize the time-series metrics tracking map
","tensorpipe::ContextOptions().name(workerInfo_.name_))),
addressStore_(std::move(addressStore)),
worldSize_(worldSize),
      opts_(std::move(opts)),
      processGroup_(std::move(processGroup)) {
collectNames();
// Initialize the time-series metrics tracking map
"
1087,"bucketize(input, boundaries, out_int32=False, right=False, out=None) -> Tensor
Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the
boundaries of the buckets are set by :attr:`boundaries`. Return a new tensor with the same size
as :attr:`input`. If :attr:`right` is False (default), then the left boundary is closed. More
formally, the returned index satisfies the following rules:
.. list-table::
   :widths: 15 85
:header-rows: 1
* - :attr:`right`
","bucketize(input, boundaries, out_int32=False, right=False, out=None) -> Tensor
Returns the indices of the buckets to which each value in the :attr:`input` belongs, where the
boundaries of the buckets are set by :attr:`boundaries`. Return a new tensor with the same size
as :attr:`input`. If :attr:`right` is False (default), then the left boundary is closed. More
formally, the returned index satisfies the following rules:
.. list-table::
   :widths: 15 85
:header-rows: 1
* - :attr:`right`
"
1088,"Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
be a parent of two other intervals.
        If for any reason two intervals intersect only partialy, this function
will not record a parent child relationship between then.
""""""
if self.cpu_children_populated:
","Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
be a parent of two other intervals.
        If for any reason two intervals intersect only partially, this function
will not record a parent child relationship between then.
""""""
if self.cpu_children_populated:
"
1089,"def __str__(self):
if self.function_events is None:
return '<unfinished torch.autograd.profile>'
return str(self.function_events)
def _check_finish(self):
","def __str__(self):
if self.function_events is None:
return '<unfinished torch.autograd.profile>'
        self.function_events.populate_cpu_children()
return str(self.function_events)
def _check_finish(self):
"
1090,"os() << ""})"";
}
// === Stmt visitors below ===
// Some invariants to keep in mind when changing printer visitors for statement:
//  1) every statement first outputs the indendation with emitIndent
","os() << ""})"";
}
void IRPrinter::visit(const NoOp* v) {
  os() << ""NoOp"";
}

// === Stmt visitors below ===
// Some invariants to keep in mind when changing printer visitors for statement:
//  1) every statement first outputs the indendation with emitIndent
"
1091,"}
}
} // namespace tensorexpr
} // namespace jit
} // namespace torch
","}
}
void IRVisitor::visit(const NoOp* v) {
  // do nothing
}

} // namespace tensorexpr
} // namespace jit
} // namespace torch
"
1092,"return invoked_methods;
}
void propagateQuantizationParamsFromInput(
Value* original_output,
    const std::vector<Value*>& inputs) {
Node* n = original_output->node();
Graph* graph = n->owningGraph();
// for ops like average pool, we'll insert quant dequant after the op
","return invoked_methods;
}
void propagateQParams(
Value* original_output,
    const std::vector<Value*>& inputs,
    const c10::optional<std::tuple<c10::QScheme, QParamVector>>& qparams_opt =
        c10::nullopt) {
Node* n = original_output->node();
Graph* graph = n->owningGraph();
// for ops like average pool, we'll insert quant dequant after the op
"
1093,"bool is_nonzero(const Tensor& self) {
auto n = self.numel();
  AT_ASSERT(n >= 0);
  if (n == 0) {
    AT_ERROR(""bool value of Tensor with no values is ambiguous"");
  }
  if (n > 1) {
    AT_ERROR(""bool value of Tensor with more than one value is ambiguous"");
  }
Scalar localScalar = self.item();
if (localScalar.isFloatingPoint()) {
return localScalar.to<double>() != 0;
","bool is_nonzero(const Tensor& self) {
auto n = self.numel();
  TORCH_CHECK(n != 0, ""Boolean value of Tensor with no values is ambiguous"");
  TORCH_CHECK(n < 2, ""Boolean value of Tensor with more than one value is ambiguous"");

Scalar localScalar = self.item();
if (localScalar.isFloatingPoint()) {
return localScalar.to<double>() != 0;
"
1094,"auto sg_outputs = nn::getOutputs(sg);
auto sg_inputs_copy = sg_inputs;
for (const auto& input : node_inputs) {
sg_inputs_copy.erase(input);
}
assert(sg_inputs_copy.size() == 0 && ""Not all inputs were listed"");
  auto sg_outputs_copy = sg_outputs;
for (const auto& output : node_outputs) {
sg_outputs_copy.erase(output);
}
","auto sg_outputs = nn::getOutputs(sg);
auto sg_inputs_copy = sg_inputs;
  auto sg_outputs_copy = sg_outputs;

for (const auto& input : node_inputs) {
sg_inputs_copy.erase(input);
    // outputs may contain inputs that have additional
    // consumers external to the subgraph
    sg_outputs_copy.erase(input);
}
assert(sg_inputs_copy.size() == 0 && ""Not all inputs were listed"");
for (const auto& output : node_outputs) {
sg_outputs_copy.erase(output);
}
"
1095,"validateGraph(graph, operator_export_type);
}
auto* imp = model_proto_.add_opset_import();
// This is the version of ONNX operator set we are targeting
imp->set_version(onnx_opset_version);
","validateGraph(graph, operator_export_type);
}
  if (use_external_data_format) {
    TORCH_CHECK(
        !onnx_file_path.empty(),
        ""For large model export, f in torch.onnx.export must be a non-empty string ""
        ""specifying the location of the model."");
  }

auto* imp = model_proto_.add_opset_import();
// This is the version of ONNX operator set we are targeting
imp->set_version(onnx_opset_version);
"
1096,"for k, v in f_locals.items():
if isinstance(v, torch.Tensor) and var is v:
return k if k != 'self' else ''
        for k, v in f_globals.items():
            if isinstance(v, torch.Tensor) and var is v:
                return k if k != 'self' else ''
return ''
return _get_interpreter_name_for_var
","for k, v in f_locals.items():
if isinstance(v, torch.Tensor) and var is v:
return k if k != 'self' else ''
return ''
return _get_interpreter_name_for_var
"
1097,"return toIValue(getValue(name, match_vmap, vmap));
}
void replaceConvolutionWithConv2d(std::shared_ptr<Graph>& graph) {
ConstantPropagation(graph);
std::string convolution = R""(
graph(%a, %w, %b, %stride:int[], %padding:int[], %dilation:int[],
","return toIValue(getValue(name, match_vmap, vmap));
}
std::unordered_map<std::string, c10::IValue> getConvParams(
    const Match& match,
    const std::unordered_map<std::string, Value*>& vmap) {
  std::unordered_map<std::string, c10::IValue> calc_values;
  const auto& match_vmap = match.values_map;
  auto transposed_value = getIValue(""transposed"", match_vmap, vmap).value();
  calc_values[""transposed""] = transposed_value;
  auto benchmark_value = getIValue(""benchmark"", match_vmap, vmap).value();
  calc_values[""benchmark""] = benchmark_value;
  auto deterministic_value =
      getIValue(""deterministic"", match_vmap, vmap).value();
  calc_values[""deterministic""] = deterministic_value;
  auto cudnn_enabled_value =
      getIValue(""cudnn_enabled"", match_vmap, vmap).value();
  calc_values[""cudnn_enabled""] = cudnn_enabled_value;
  auto output_padding_value =
      getIValue(""output_padding"", match_vmap, vmap).value();
  calc_values[""output_padding""] = output_padding_value;
  auto stride_value = getIValue(""stride"", match_vmap, vmap).value();
  calc_values[""stride""] = stride_value;
  auto padding_value = getIValue(""padding"", match_vmap, vmap).value();
  calc_values[""padding""] = padding_value;
  auto dilation_value = getIValue(""dilation"", match_vmap, vmap).value();
  calc_values[""dilation""] = dilation_value;
  return calc_values;
}

void replaceConvolutionWithAtenConv(std::shared_ptr<Graph>& graph) {
ConstantPropagation(graph);
std::string convolution = R""(
graph(%a, %w, %b, %stride:int[], %padding:int[], %dilation:int[],
"
1098,"void insertPrePackedConv2dOp(std::shared_ptr<Graph>& graph) {
// Replace _convolution with conv2d
  graph_rewrite_helper::replaceConvolutionWithConv2d(graph);
std::string conv_2d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
","void insertPrePackedConv2dOp(std::shared_ptr<Graph>& graph) {
// Replace _convolution with conv2d
  graph_rewrite_helper::replaceConvolutionWithAtenConv(graph);
std::string conv_2d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
"
1099,"// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, const std::string& msg)
    : Error(msg, str("" ("", source_location, "")\n"", (*GetFetchStackTrace())())) {
}
using APIUsageLoggerType = std::function<void(const std::string&)>;
","// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, std::string msg)
    : Error(std::move(msg), str(""Exception raised from "", source_location, "" (most recent call first):\n"", (*GetFetchStackTrace())())) {
}
using APIUsageLoggerType = std::function<void(const std::string&)>;
"
1100,"#include <torch/csrc/jit/runtime/profiling_record.h>
#include <torch/csrc/jit/passes/clear_profiling.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/runtime/graph_executor.h>
","#include <torch/csrc/jit/runtime/profiling_record.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/clear_profiling.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/runtime/graph_executor.h>
"
1101,"for (const auto& observer_attrs : block_observer_map_.at(block)) {
const auto& name = std::get<0>(observer_attrs);
const auto& observer = std::get<1>(observer_attrs);
      module._ivalue()->setAttr(name, observer.clone_instance()._ivalue());
}
}
// NB: Why do we need to process the graph even if it's visited?
","for (const auto& observer_attrs : block_observer_map_.at(block)) {
const auto& name = std::get<0>(observer_attrs);
const auto& observer = std::get<1>(observer_attrs);
      module._ivalue()->setAttr(name, observer.deepcopy()._ivalue());
}
}
// NB: Why do we need to process the graph even if it's visited?
"
1102,"function_declarations = get_py_torch_functions(declarations)
for name in sorted(function_declarations.keys()):
        unsorted_function_hints[name] += generate_type_hints(name, function_declarations[name])
# Generate type signatures for deprecated functions
","function_declarations = get_py_torch_functions(declarations)
for name in sorted(function_declarations.keys()):
        unsorted_function_hints[name] += generate_type_hints(name, function_declarations[name], namedtuples)
# Generate type signatures for deprecated functions
"
1103,"checkpointed version won't be equivalent, and unfortunately it can't be
detected.
.. warning:
At least one of the inputs needs to have :code:`requires_grad=True` if
grads are needed for model inputs, otherwise the checkpointed part of the
","checkpointed version won't be equivalent, and unfortunately it can't be
detected.
    .. warning::
        If checkpointed segment contains tensors detached from the computational
        graph by `detach()` or `torch.no_grad()`, the backward pass will raise an
        error. This is because `checkpoint` makes all the outputs require
        gradients which causes issues when a tensor is defined to have no
        gradient in the model. To circumvent this, detach the tensors outside of
        the `checkpoint` function.

.. warning:
At least one of the inputs needs to have :code:`requires_grad=True` if
grads are needed for model inputs, otherwise the checkpointed part of the
"
1104,"scanTypeDependencies(node);
if (!print_const && node->kind() == prim::Constant)
return;
    splitLongInlines(node->inputs());
switch (node->kind()) {
case prim::Return:
if (enforce_importable_ && node->inputs().size() != 1) {
","scanTypeDependencies(node);
if (!print_const && node->kind() == prim::Constant)
return;
switch (node->kind()) {
case prim::Return:
if (enforce_importable_ && node->inputs().size() != 1) {
"
1105,"// Make node outputs a single tuple;
std::vector<TypePtr> types;
for (size_t i = 0; i < n->outputs().size(); ++i) {
          types.push_back(n->output(0)->type());
}
Value* tup_output = n->addOutput()->setType(TupleType::create(types));
Node* tup_unpack = g->createTupleUnpack(tup_output)->insertAfter(n);
","// Make node outputs a single tuple;
std::vector<TypePtr> types;
for (size_t i = 0; i < n->outputs().size(); ++i) {
          types.push_back(n->output(i)->type());
}
Value* tup_output = n->addOutput()->setType(TupleType::create(types));
Node* tup_unpack = g->createTupleUnpack(tup_output)->insertAfter(n);
"
1106,"auto wrap_dim = maybe_wrap_dim(dim, self.dim());
int64_t self_dim_size = ensure_nonempty_size(self, wrap_dim);
  AT_DISPATCH_ALL_TYPES_AND_COMPLEX(self.scalar_type(), ""cumsum_out_cpu"", [&] {
cpu_cum_base_kernel<scalar_t>(result, self, wrap_dim, [&] (
scalar_t* result_data, auto result_dim_stride,
const scalar_t* self_data, auto self_dim_stride, scalar_t init_val) {
","auto wrap_dim = maybe_wrap_dim(dim, self.dim());
int64_t self_dim_size = ensure_nonempty_size(self, wrap_dim);
  AT_DISPATCH_ALL_TYPES(self.scalar_type(), ""cumsum_out_cpu"", [&] {
cpu_cum_base_kernel<scalar_t>(result, self, wrap_dim, [&] (
scalar_t* result_data, auto result_dim_stride,
const scalar_t* self_data, auto self_dim_stride, scalar_t init_val) {
"
1107,"scanTypeDependencies(node);
if (!print_const && node->kind() == prim::Constant)
return;
switch (node->kind()) {
case prim::Return:
if (enforce_importable_ && node->inputs().size() != 1) {
","scanTypeDependencies(node);
if (!print_const && node->kind() == prim::Constant)
return;
    splitLongInlines(node->inputs());
switch (node->kind()) {
case prim::Return:
if (enforce_importable_ && node->inputs().size() != 1) {
"
1108,"// we will transfer the caffe2_log_level setting to glog to override that.
FLAGS_minloglevel = std::min(FLAGS_caffe2_log_level, FLAGS_minloglevel);
// If caffe2_log_level is explicitly set, let's also turn on logtostderr.
  if (FLAGS_caffe2_log_level < google::GLOG_WARNING) {
FLAGS_logtostderr = 1;
}
// Also, transfer the caffe2_log_level verbose setting to glog.
","// we will transfer the caffe2_log_level setting to glog to override that.
FLAGS_minloglevel = std::min(FLAGS_caffe2_log_level, FLAGS_minloglevel);
// If caffe2_log_level is explicitly set, let's also turn on logtostderr.
  if (FLAGS_caffe2_log_level < google::GLOG_ERROR) {
FLAGS_logtostderr = 1;
}
// Also, transfer the caffe2_log_level verbose setting to glog.
"
1109,"return result;
}
void dump(
const std::unordered_map<Node*, std::vector<Value*>>& liveness_sets) {
std::cout << ""Liveness info:\n"";
","return result;
}
  // temporary make loop counts live for the duration of the loop
  // as they are needed by BailOuts in the loop
  void insertExplicitUsesOfLoopCounters(
      Block* b,
      std::vector<Node*>& counters) {
    for (auto it : b->nodes()) {
      if (it->kind() == prim::Loop) {
        LoopView lv(it);
        WithInsertPoint guard(lv.bodyBlock());
        auto ctc = graph_->create(prim::Store, {lv.currentTripCount()}, 0);
        graph_->insertNode(ctc);
        counters.push_back(ctc);
        auto mtc = graph_->create(prim::Store, {lv.maxTripCount()}, 0);
        graph_->insertNode(mtc);
        counters.push_back(mtc);
      }

      for (auto ib : it->blocks()) {
        insertExplicitUsesOfLoopCounters(ib, counters);
      }
    }
  }

  void removeCounterNodes(std::vector<Node*>& counters) {
    for (auto n : counters) {
      n->destroy();
    }
  }

void dump(
const std::unordered_map<Node*, std::vector<Value*>>& liveness_sets) {
std::cout << ""Liveness info:\n"";
"
1110,"return qy;
}
#endif
 public:
  Tensor operator()(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
    check_inputs(qa, qb);
#ifdef USE_PYTORCH_QNNPACK
    if (at::globalContext().qEngine() == at::QEngine::QNNPACK &&
        qa.scalar_type() == kQUInt8 && qb.scalar_type() == kQUInt8) {
      return qnnpack_add(qa, qb, scale, zero_point);
    }
#endif
    auto qc = at::_empty_affine_quantized(
        qa.sizes(),
        at::device(kCPU)
           .dtype(qa.scalar_type())
           .memory_format(qa.suggest_memory_format()),
        scale,
        zero_point,
        c10::nullopt);
    return _add_out<ReLUFused>(qc, qa, qb);
}
};
template <bool ReLUFused = false>
class QAddOut final : public c10::OperatorKernel {
 public:
  Tensor operator()(Tensor qa, Tensor qb, Tensor out) {
    check_inputs(qa, qb);
    check_inputs(qa, out);
    return _add_out<ReLUFused>(out, qa, qb);
  }
};
template <bool ReLUFused = false>
class QAddScalar final : public c10::OperatorKernel {
 public:
  Tensor operator()(Tensor qa, Scalar b) {
TORCH_CHECK(qa.qscheme() == kPerTensorAffine ||
qa.qscheme() == kPerTensorSymmetric,
""Only per tensor quantization is suuported in Add."");
    auto qc = at::empty_like(qa, qa.suggest_memory_format());
    return _add_scalar_out<ReLUFused>(qc, qa, b);
  }
};
template <bool ReLUFused = false>
class QAddScalarOut final : public c10::OperatorKernel {
 public:
  Tensor operator()(Tensor qa, Scalar b, Tensor out) {
    check_inputs(qa, out);
    return _add_scalar_out<ReLUFused>(out, qa, b);
  }
};

static auto registry = c10::RegisterOperators()
.op(""quantized::add(Tensor qa, Tensor qb, float scale, int zero_point)""
     ""-> Tensor qc"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAdd</*ReLUFused=*/false>>(DispatchKey::QuantizedCPU))
.op(""_quantized::add(Tensor qa, Tensor qb, float scale, int zero_point)""
     ""-> Tensor qc"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAdd</*ReLUFused=*/false>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_relu(Tensor qa, Tensor qb, float scale, int zero_point)""
     ""-> Tensor qc"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAdd</*ReLUFused=*/true>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_out(Tensor qa, Tensor qb, Tensor(a!) out)""
     ""-> Tensor(a!) out"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddOut</*ReLUFused=*/false>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_relu_out(Tensor qa, Tensor qb, Tensor(a!) out)""
     ""-> Tensor(a!) out"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddOut</*ReLUFused=*/true>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_scalar(Tensor qa, Scalar b) -> Tensor qc"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddScalar</*ReLUFused=*/false>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_scalar_relu(Tensor qa, Scalar b) -> Tensor qc"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddScalar</*ReLUFused=*/true>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_scalar_out(Tensor qa, Scalar b, Tensor(a!) out)""
     ""-> Tensor(a!) out"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddScalarOut</*ReLUFused=*/false>>(DispatchKey::QuantizedCPU))
.op(""quantized::add_scalar_relu_out(Tensor qa, Scalar b, Tensor(a!) out)""
     ""-> Tensor(a!) out"",
    c10::RegisterOperators::options()
      .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
      .kernel<QAddScalarOut</*ReLUFused=*/true>>(DispatchKey::QuantizedCPU));
}  // namespace
}}  // namespace at::native
","return qy;
}
#endif

template <bool ReLUFused = false>
Tensor qadd(Tensor qa, Tensor qb, double scale, int64_t zero_point) {
  check_inputs(qa, qb);
#ifdef USE_PYTORCH_QNNPACK
  if (at::globalContext().qEngine() == at::QEngine::QNNPACK &&
      qa.scalar_type() == kQUInt8 && qb.scalar_type() == kQUInt8) {
    return qnnpack_add<ReLUFused>(qa, qb, scale, zero_point);
}
#endif
  auto qc = at::_empty_affine_quantized(
      qa.sizes(),
      at::device(kCPU)
         .dtype(qa.scalar_type())
         .memory_format(qa.suggest_memory_format()),
      scale,
      zero_point,
      c10::nullopt);
  return _add_out<ReLUFused>(qc, qa, qb);
}
template <bool ReLUFused = false>
Tensor qadd_out(Tensor qa, Tensor qb, Tensor out) {
  check_inputs(qa, qb);
  check_inputs(qa, out);
  return _add_out<ReLUFused>(out, qa, qb);
}
template <bool ReLUFused = false>
Tensor qadd_scalar(Tensor qa, Scalar b) {
TORCH_CHECK(qa.qscheme() == kPerTensorAffine ||
qa.qscheme() == kPerTensorSymmetric,
""Only per tensor quantization is suuported in Add."");
  auto qc = at::empty_like(qa, qa.suggest_memory_format());
  return _add_scalar_out<ReLUFused>(qc, qa, b);
}
template <bool ReLUFused = false>
Tensor qadd_scalar_out(Tensor qa, Scalar b, Tensor out) {
  check_inputs(qa, out);
  return _add_scalar_out<ReLUFused>(out, qa, b);
}

TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
  m.impl(""add"",                 qadd</*ReLUFused=*/false>);
  m.impl(""add_relu"",            qadd</*ReLUFused=*/true>);
  m.impl(""add_out"",             qadd_out</*ReLUFused=*/false>);
  m.impl(""add_relu_out"",        qadd_out</*ReLUFused=*/true>);
  m.impl(""add_scalar"",          qadd_scalar</*ReLUFused=*/false>);
  m.impl(""add_scalar_relu"",     qadd_scalar</*ReLUFused=*/true>);
  m.impl(""add_scalar_out"",      qadd_scalar_out</*ReLUFused=*/false>);
  m.impl(""add_scalar_relu_out"", qadd_scalar_out</*ReLUFused=*/true>);
}

TORCH_LIBRARY_IMPL(_quantized, QuantizedCPU, m) {
  m.impl(""add"", qadd</*ReLUFused=*/false>);
}

}  // namespace
}}  // namespace at::native
"
1111,"auto sizes = compute_sizes(data);
checkListInputType(elem_type, sizes.size() == 1 && sizes[0] == 0);
at::ScalarType initial_scalar_type = scalarTypeFromJitType(elem_type);
auto tensor =
at::empty(sizes, at::initialTensorOptions().dtype(initial_scalar_type));
","auto sizes = compute_sizes(data);
checkListInputType(elem_type, sizes.size() == 1 && sizes[0] == 0);
at::ScalarType initial_scalar_type = scalarTypeFromJitType(elem_type);
  if (initial_scalar_type == at::ScalarType::Double) {
    initial_scalar_type = typeMetaToScalarType(c10::get_default_dtype());
  }
auto tensor =
at::empty(sizes, at::initialTensorOptions().dtype(initial_scalar_type));
"
1112,"""operator FloatToFused4BitRowwiseQuantized"")
.Input(
1,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        2,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
    .Input(
        3,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
.Input(
4,
""COMPRESSED_INDICES_MAPPING"",
","""operator FloatToFused4BitRowwiseQuantized"")
.Input(
1,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
    .Input(
        2,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        3,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
.Input(
4,
""COMPRESSED_INDICES_MAPPING"",
"
1113,"""operator FloatToFused4BitRowwiseQuantized"")
.Input(
1,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        2,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
    .Input(
        3,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
.Input(
4,
""COMPRESSED_INDICES_MAPPING"",
","""operator FloatToFused4BitRowwiseQuantized"")
.Input(
1,
        ""WEIGHTS"",
        ""Vector of weights to scale rows of DATA with before reduction"")
    .Input(
        2,
""INDICES"",
""Integer vector containing indices of the first ""
""dimension of DATA for the slices that are being aggregated"")
.Input(
        3,
""LENGTHS"",
""Vector with the same sum of elements as the first dimension of DATA"")
.Input(
4,
""COMPRESSED_INDICES_MAPPING"",
"
1114,"// Compile the kernel.
stmt->accept(this);
irb_.CreateRet(value_);
#if DEBUG_PRINT
","// Compile the kernel.
stmt->accept(this);

  // If the kernel is empty, set a default return value.
  if (value_ == nullptr) {
    value_ = llvm::ConstantInt::get(IntTy_, 0);
  }

irb_.CreateRet(value_);
#if DEBUG_PRINT
"
1115,"Args:
hparam_dict (dict): Each key-value pair in the dictionary is the
name of the hyper parameter and it's corresponding value.
metric_dict (dict): Each key-value pair in the dictionary is the
name of the metric and it's corresponding value. Note that the key used
here should be unique in the tensorboard record. Otherwise the value
","Args:
hparam_dict (dict): Each key-value pair in the dictionary is the
name of the hyper parameter and it's corresponding value.
              The type of the value can be one of `bool`, `string`, `float`,
              `int`, or `None`.
metric_dict (dict): Each key-value pair in the dictionary is the
name of the metric and it's corresponding value. Note that the key used
here should be unique in the tensorboard record. Otherwise the value
"
1116,"namespace {

RegisterOperators reg(
    {
      Operator(
         ""prim::TupleUnpack(Any tup) -> ..."",
         [](Stack& stack) {
           tupleUnpack(stack);
           return 0;
         },
         aliasAnalysisSpecialCase()),
     Operator(
         ""prim::unchecked_cast(t x) -> t"",
         noop,
         aliasAnalysisSpecialCase()),
Operator(
""aten::format(str self, ...) -> str"",
[](Stack& stack) {
","namespace {
RegisterOperators reg({
    Operator(
        ""prim::TupleUnpack(Any tup) -> ..."",
        [](Stack& stack) {
          tupleUnpack(stack);
          return 0;
        },
        aliasAnalysisSpecialCase()),
    Operator(
        ""prim::unchecked_cast(t x) -> t"",
        noop,
        aliasAnalysisSpecialCase()),
Operator(
""aten::format(str self, ...) -> str"",
[](Stack& stack) {
"
1117,"return unshapedType(type);
case TypeKind::OptionalType:
return getMutableType(type->cast<OptionalType>()->getElementType());
case TypeKind::FutureType: {
if (auto elem =
getMutableType(type->cast<FutureType>()->getElementType())) {
","return unshapedType(type);
case TypeKind::OptionalType:
return getMutableType(type->cast<OptionalType>()->getElementType());
      case TypeKind::AnyType:
        return type;
case TypeKind::FutureType: {
if (auto elem =
getMutableType(type->cast<FutureType>()->getElementType())) {
"
1118,"// if the blob doesn't exist or is not initialized, return false
inline bool getShouldStop(const Blob* b) {
  if (!b || b->meta().id() == TypeIdentifier::uninitialized()) { // not exist or uninitialized
return false;
}
","// if the blob doesn't exist or is not initialized, return false
inline bool getShouldStop(const Blob* b) {
  if (!b ||
      b->meta().id() ==
          TypeIdentifier::uninitialized()) { // not exist or uninitialized
return false;
}
"
1119,"}
if (compiledStep->gotFailure) {
LOG(ERROR) << ""One of the workers failed."";
          if (first_exception.size()) {
            CAFFE_THROW(
                ""One of the workers died with an unhandled exception "",
                first_exception);
}
return false;
}
","}
if (compiledStep->gotFailure) {
LOG(ERROR) << ""One of the workers failed."";
          if (first_exception) {
            first_exception.rethrowException();
}
return false;
}
"
1120,"import sys
import platform
import ctypes
from ._utils import _import_dotted_name
from ._utils_internal import get_file_path, prepare_multiprocessing_environment, \
USE_RTLD_GLOBAL_WITH_LIBTORCH
","import sys
import platform
import ctypes

if sys.version_info < (3,):
    raise Exception(""Python 2 has reached end-of-life and is no longer supported by PyTorch."")

from ._utils import _import_dotted_name
from ._utils_internal import get_file_path, prepare_multiprocessing_environment, \
USE_RTLD_GLOBAL_WITH_LIBTORCH
"
1121,"void RpcAgent::cleanup() {
rpcAgentRunning_.store(false);
if (rpcRetryThread_.joinable()) {
    rpcRetryMapCV_.notify_one();
rpcRetryThread_.join();
}
}
","void RpcAgent::cleanup() {
rpcAgentRunning_.store(false);
  // We must notify the condition variable so it stops waiting in the
  // retry thread, otherwise this thread cannot be joined.
  rpcRetryMapCV_.notify_one();
if (rpcRetryThread_.joinable()) {
rpcRetryThread_.join();
}
}
"
1122,"XNNPackLinearOpContext::create_context(
at::Tensor&& weight,
c10::optional<at::Tensor>&& bias,
    const c10::optional<double> output_min,
    const c10::optional<double> output_max) {
auto linear_op_context =
c10::make_intrusive<XNNPackLinearOpContext>(
std::move(weight),
std::move(bias),
xnnpack::internal::linear::create(
weight,
bias,
              output_min ? static_cast<float>(*output_min)
: xnnpack::ContextLinear::kMin,
              output_max ? static_cast<float>(*output_max)
: xnnpack::ContextLinear::kMax)
);
return linear_op_context;
","XNNPackLinearOpContext::create_context(
at::Tensor&& weight,
c10::optional<at::Tensor>&& bias,
    const c10::optional<Scalar> output_min,
    const c10::optional<Scalar> output_max) {
auto linear_op_context =
c10::make_intrusive<XNNPackLinearOpContext>(
std::move(weight),
std::move(bias),
          output_min,
          output_max,
xnnpack::internal::linear::create(
weight,
bias,
              output_min ? output_min->to<float>()
: xnnpack::ContextLinear::kMin,
              output_max ? output_max->to<float>()
: xnnpack::ContextLinear::kMax)
);
return linear_op_context;
"
1123,"DispatchKey::CPUTensorId))
.op(""prepacked::conv2d_clamp_prepack(Tensor W, Tensor? B, int[2] stride, ""
""int[2] padding, int[2] dilation, int groups, ""
            ""float? output_min=None, float? output_max=None) ""
""-> __torch__.torch.classes.xnnpack.Conv2dOpContext"",
torch::RegisterOperators::options()
.aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
","DispatchKey::CPUTensorId))
.op(""prepacked::conv2d_clamp_prepack(Tensor W, Tensor? B, int[2] stride, ""
""int[2] padding, int[2] dilation, int groups, ""
            ""Scalar? output_min=None, Scalar? output_max=None) ""
""-> __torch__.torch.classes.xnnpack.Conv2dOpContext"",
torch::RegisterOperators::options()
.aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
"
1124,"c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
c10::ReplaceAll(doc, ""{extra}"", extra);
schema.SetDoc(doc);
    schema.Arg(""broadcast"", ""*(type: int; default: 0)* Pass 1 to enable broadcasting"");
schema.Arg(
        ""axis"",
        ""*(type: int; default: -1)* Axis to concatenate on."");
schema.Input(
0,
""A"",
","c10::ReplaceAll(doc, ""{broadcast_doc}"", kBroadcastDoc);
c10::ReplaceAll(doc, ""{extra}"", extra);
schema.SetDoc(doc);
schema.Arg(
        ""broadcast"", ""*(type: int; default: 0)* Pass 1 to enable broadcasting"");
    schema.Arg(""axis"", ""*(type: int; default: -1)* Axis to concatenate on."");
schema.Input(
0,
""A"",
"
1125,"""B"",
""*(type: Tensor`<float>`)* Second operand. With broadcasting can be of smaller size than A. ""
""If broadcasting is disabled it should be of the same size as A."");
    schema.Output(0, ""C"", ""*(type: Tensor`<float>`)* Output tensor with same dimensions and type as A."");
};
}
","""B"",
""*(type: Tensor`<float>`)* Second operand. With broadcasting can be of smaller size than A. ""
""If broadcasting is disabled it should be of the same size as A."");
    schema.Output(
        0,
        ""C"",
        ""*(type: Tensor`<float>`)* Output tensor with same dimensions and type as A."");
};
}
"
1126,"OPERATOR_SCHEMA(AddGradient)
.NumInputs(3)
.NumOutputs(2)
.AllowInplace({{0, 0}, {0, 1}});
OPERATOR_SCHEMA(Sub)
","OPERATOR_SCHEMA(AddGradient)
.NumInputs(3)
.NumOutputs(2)
    .TensorInferenceFunction(ElementwiseGradientOpShapeInference)
.AllowInplace({{0, 0}, {0, 1}});
OPERATOR_SCHEMA(Sub)
"
1127,"OPERATOR_SCHEMA(MulGradient)
.NumInputs(3)
.NumOutputs(2)
.AllowInplace({{0, 0}, {0, 1}});
OPERATOR_SCHEMA(Div)
","OPERATOR_SCHEMA(MulGradient)
.NumInputs(3)
.NumOutputs(2)
    .TensorInferenceFunction(ElementwiseGradientOpShapeInference)
.AllowInplace({{0, 0}, {0, 1}});
OPERATOR_SCHEMA(Div)
"
1128,"</details>
)DOC"";
std::function<void(OpSchema&)> ComparisonDocGenerator(
    const char* name,
    const char* desc,
    const char* extra) {
return [=](OpSchema& schema) {
string doc = R""DOC(
Performs element-wise {desc} comparison **{name}** (with limited broadcast support).
","</details>
)DOC"";
std::function<void(OpSchema&)>
ComparisonDocGenerator(const char* name, const char* desc, const char* extra) {
return [=](OpSchema& schema) {
string doc = R""DOC(
Performs element-wise {desc} comparison **{name}** (with limited broadcast support).
"
1129,"switch (inst.op) {
case OP: {
#if defined(PYTORCH_MOBILE_OPERATOR_OBSERVER)
        if (auto debug_info = at::getThreadLocalDebugInfo()) {
if (auto* mobile_debug_info =
dynamic_cast<MobileDebugInfo*>(debug_info.get())) {
mobile_debug_info->setOpIdx(pc);
","switch (inst.op) {
case OP: {
#if defined(PYTORCH_MOBILE_OPERATOR_OBSERVER)
        if (auto debug_info = at::ThreadLocalDebugInfo::get(
                at::DebugInfoKind::MOBILE_RUNTIME_INFO)) {
if (auto* mobile_debug_info =
dynamic_cast<MobileDebugInfo*>(debug_info.get())) {
mobile_debug_info->setOpIdx(pc);
"
1130,".NumOutputs(1)
.Input(0, ""X"", ""tensor of float"")
.Output(0, ""Y"", ""tensor of float"")
.SetDoc(R""DOC(
Converts each input element into either high_ or low_value
based on the given threshold.
",".NumOutputs(1)
.Input(0, ""X"", ""tensor of float"")
.Output(0, ""Y"", ""tensor of float"")
    .TensorInferenceFunction([](const OperatorDef&,
                                const vector<TensorShape>& input_types) {
      vector<TensorShape> out(1);
      out.at(0) = input_types.at(0);
      out.at(0).set_data_type(TensorProto_DataType::TensorProto_DataType_FLOAT);
      return out;
    })
.SetDoc(R""DOC(
Converts each input element into either high_ or low_value
based on the given threshold.
"
1131,"* ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.
* ``scaler.update()`` updates ``scaler``'s scale factor.
    Typical use::
# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()
","* ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.
* ``scaler.update()`` updates ``scaler``'s scale factor.
    Example::
# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()
"
1132,"}
// Write pendingSends to a global map so that they can be interrupted by
// ::shutdown().
  std::unique_lock<std::mutex> pendingSendGuard(pendingSendMutex_);

  for (auto& p : pendingSends) {
    currentPendingSends_[dst].insert(p);
}
  // Unlock to call into wait, otherwise shutdown() cannot interrupt here.
  pendingSendGuard.unlock();

for (auto& pendingSend : pendingSends) {
if (!rpcRunning_.load() || !pendingSend->wait()) {
// Send was interrupted or RPC is not running.
","}
// Write pendingSends to a global map so that they can be interrupted by
// ::shutdown().
  {
    std::lock_guard<std::mutex> pendingSendGuard(pendingSendMutex_);
    for (auto& p : pendingSends) {
      currentPendingSends_[dst].insert(p);
    }
}
for (auto& pendingSend : pendingSends) {
if (!rpcRunning_.load() || !pendingSend->wait()) {
// Send was interrupted or RPC is not running.
"
1133,"// the correct value
call->replaceInput(1, v);
observer_nodes_.emplace(call);
}
  observed_values_.insert(v);
}
void InsertObserversHelper::skipValuesInPattern(
","// the correct value
call->replaceInput(1, v);
observer_nodes_.emplace(call);
    observed_values_.insert(call->output());
}
}
void InsertObserversHelper::skipValuesInPattern(
"
1134,"#include <ATen/ATen.h>
#include <ATen/core/op_registration/op_registration.h>
#include <ATen/NativeFunctions.h>
#include <ATen/autocast_mode.h>

#include <c10/util/intrusive_ptr.h>
#include <c10/core/impl/LocalDispatchKeySet.h>

#include <iostream>
#include <exception>

namespace at {
namespace autocast {

bool is_enabled() {
  return c10::impl::tls_is_dispatch_key_included(DispatchKey::AutocastTensorId);
}

void set_enabled(bool new_enabled) {
  c10::impl::tls_set_dispatch_key_included(DispatchKey::AutocastTensorId, new_enabled);
}

namespace {
// Imitate Apex and cache some of the casts to streamline parameter reuse.
// Our heuristic is to cache fp16 casts of fp32 model weights (see cached_cast below).
//
// After discussion with @ezyang, the cache uses the following structure:
// The key is the source tensor's TensorImpl*, a proxy for a Tensor uuid that's unchanged
// across shallow copies.  The value is a tuple with a weakref to the source tensor's
// TensorImpl as the first element and the casted tensor as the second element.
//
// The weakref keeps the source's TensorImpl from being deleted.  We need to because we're
// using the source TensorImpl* as the key.  If it were deleted, another random Tensor could
// be allocated whose TensorImpl* happened to have the same value.  This TensorImpl* would
// then mistakenly hit in cache:  a rare, intermittent, unpredictable bug.
//
// I'm not using the weak_intrusive_ptr as the key because it's more difficult to compare
// directly against incoming TensorImpl*s.
using weakref_type = c10::weak_intrusive_ptr<TensorImpl, UndefinedTensorImpl>;
using val_type = std::tuple<weakref_type, Tensor>;
thread_local std::unordered_map<TensorImpl*, val_type> cached_casts;

// nesting tracks the nesting depth of the Python-side context manager.
// When the autocast context manager exits to a nesting level that's outside
// any instance of autocast (which should occur at the end of each forward pass)
// it calls clear_cache() to ensure cached Tensors don't leak outside the autocasting region.
thread_local int nesting = 0;
}

void clear_cache() {
  cached_casts.clear();
}

int increment_nesting() {
  return ++nesting;
}

int decrement_nesting() {
  return --nesting;
}

// Policies correspond to op categories that need code-divergent handling.
// Wrapper templates below are specialized based on a policy template parameter.
enum class CastPolicy : uint8_t {
  fp16 = 0, // Cast all inputs to at::kHalf before running the op.
  fp32, // Cast all inputs to at::kFloat before running the op.
  fp32_set_opt_dtype, // Treats functions (like softmax) that
                      //   1. we'd like to run in fp32 and
                      //   2. have a c10::optional<ScalarType> arg that controls the output type.
                      // fp32_set_opt_dtype wrappers' policy is:  if the output type is already set,
                      // don't touch it, otherwise, set it to at::kFloat.
  fp32_append_dtype, // Treats functions (like norm) that
                     //   1. we'd like to run in fp32 and
                     //   2. have some overloads that accept an output type and other overloads that don't.
                     // fp32_append_dtype wrappers wrap the overloads that don't have an output dtype.
                     // The wrapper policy is:  append at::kFloat to the args, and redispatch to the
                     // type-aware overload.
  promote, // Run in the widest dtype among several args.
};

/********************************************************************
Logic to extract the promote type from any Tensor or TensorList args.
********************************************************************/

// Overload to catch Tensor args.
// If nextArg is floating-point, compare its scalar_type with our
// current best guess for the promote type, and update if necessary.
inline at::ScalarType prioritize(at::ScalarType current, const Tensor& nextArg) {
  if (current == at::kDouble) {
    AT_ERROR(""promote type is double in at::autocast::prioritize"");
    return current;
  }
  if (nextArg.is_cuda() && nextArg.is_floating_point()) {
    auto next = nextArg.scalar_type();
    if (next == at::kDouble) {
      return current; // ignores double tensors
    } else if (current == at::kFloat || next == at::kFloat) {
      return at::kFloat; // prioritizes float over half
    } else if (current == at::kHalf && next == at::kHalf) {
      return at::kHalf;
    } else {
      AT_ERROR(""Unexpected floating ScalarType in at::autocast::prioritize"");
      return current;
    }
  } else {
    return current;
  }
}

// Overload to catch TensorList args (for e.g. cat, stack).
// Reuses the overload above to process each Tensor in the list.
inline at::ScalarType prioritize(at::ScalarType current, const TensorList& list) {
  for (const auto& tensor : list) {
    current = prioritize(current, tensor);
  }
  return current;
}

// Template to catch non-Tensor args (no-op that returns current best guess)
template<typename T>
inline at::ScalarType prioritize(at::ScalarType current, T nextArg) {
  return current;
}

// Overload for the tail case.
inline at::ScalarType promote_type(at::ScalarType current) {
  return current;
}

// Unpack args and determine if incoming float16 tensors need to be promoted to float32.
// Non-Tensor arguments are ignored.
template<typename Arg0, typename... Args>
inline at::ScalarType promote_type(at::ScalarType current, Arg0 arg0, Args... args) {
  auto new_current = prioritize(current, arg0);
  return promote_type(new_current, args...);
}

/****************************************************
Logic to apply cached casting to any Tensor argument.
****************************************************/
inline bool is_eligible(const Tensor& arg) {
  return (arg.is_cuda() && arg.is_floating_point() && (arg.scalar_type() != at::kDouble));
}

// Overload to catch Tensor args
inline Tensor cached_cast(at::ScalarType to_type, const Tensor& arg) {
  if (is_eligible(arg) && (arg.scalar_type() != to_type)) {
    // Heuristic:  Do what Apex does, and cache fp16 casts of fp32 model weights (leaves).
    // See cached_casts declaration above for detailed strategy.
    bool can_try_cache = (to_type == at::kHalf && arg.scalar_type() == at::kFloat && arg.requires_grad() && arg.is_leaf());
    if (can_try_cache) {
      auto it = cached_casts.find(arg.unsafeGetTensorImpl());
      if (it != cached_casts.end()) {
        return std::get<1>(it->second);
      } else {
        auto casted_arg = arg.to(to_type);
        cached_casts.emplace(arg.unsafeGetTensorImpl(), val_type{weakref_type(arg.getIntrusivePtr()), casted_arg});
        return casted_arg;
      }
    } else {
      return arg.to(to_type);
    }
  } else {
    return arg;
  }
}

// Overload to process TensorLists
std::vector<Tensor> cached_cast(at::ScalarType to_type, const TensorList& arg) {
  std::vector<Tensor> vec;
  vec.reserve(arg.size());
  for (const auto& t : arg) {
    vec.push_back(cached_cast(to_type, t));
  }
  return vec;
}

// Template to catch non-Tensor args.
template<typename T>
inline T cached_cast(at::ScalarType to_type, T arg) {
  return arg;
}

/*******************************************************
Logic to flip an output dtype flag.
Keep it simple for now by assuming only one such flag is
present in the argument list.  If I ever need a function
with more than flag I'll figure out something else.
The policy is:
If the user has explicity specified a dtype, respect it.
Otherwise, set it to the autocast type.
********************************************************/

// Overload to catch dtype flags
inline c10::optional<ScalarType> set_opt_dtype(at::ScalarType to_type, const c10::optional<ScalarType>& dtype) {
  return dtype.has_value() ? dtype : to_type;
}

// Template to catch other args
template<typename T>
inline T set_opt_dtype(at::ScalarType to_type, T arg) {
  return arg;
}

template<typename... Args>
inline bool firstarg_is_eligible(const Tensor& arg, Args... args) {
  return is_eligible(arg);
}

template<typename... Args>
inline at::ScalarType type_from_firstarg(at::ScalarType to_type, const Tensor& arg, Args... args) {
  return (is_eligible(arg) ? to_type : arg.scalar_type());
}

/********************************************************************************************************
Templates to provide wrapper functions

I'm copying the pattern used in core/boxing/kernel_function.h to extract args and return type.
(see also https://stackoverflow.com/questions/46533698/how-to-deduce-argument-list-from-function-pointer)

This strategy uses an exterior ""WrapFunction"" that extracts arguments on behalf of
(in my case several specializations of) an interior ""WrapFunction_"".
Interior WrapFunction_ specializations are defined for each CastPolicy.
********************************************************************************************************/

// Base template for WrapFunction_, which is specialized to contain a ""call"" method each CastPolicy
template<CastPolicy policy, class Redispatch, Redispatch* F, class Ret, class ArgList> struct WrapFunction_ {};

// CastPolicy::fp16
template<class Redispatch, Redispatch* F, class Ret, class... Args>
struct WrapFunction_<CastPolicy::fp16, Redispatch, F, Ret, guts::typelist::typelist<Args...>> {
  static Ret call(Args... args) {
    c10::impl::ExcludeDispatchKeyGuard no_autocasting(DispatchKey::AutocastTensorId);
    return (*F)(cached_cast(at::kHalf, args)...);
  }
};

// CastPolicy::fp32
template<class Redispatch, Redispatch* F, class Ret, class... Args>
struct WrapFunction_<CastPolicy::fp32, Redispatch, F, Ret, guts::typelist::typelist<Args...>> {
  static Ret call(Args... args) {
    c10::impl::ExcludeDispatchKeyGuard no_autocasting(DispatchKey::AutocastTensorId);
    return (*F)(cached_cast(at::kFloat, args)...);
  }
};

// CastPolicy::fp32_set_opt_dtype
template<class Redispatch, Redispatch* F, class Ret, class... Args>
struct WrapFunction_<CastPolicy::fp32_set_opt_dtype, Redispatch, F, Ret, guts::typelist::typelist<Args...>> {
  static Ret call(Args... args) {
    c10::impl::ExcludeDispatchKeyGuard no_autocasting(DispatchKey::AutocastTensorId);
    if (firstarg_is_eligible(args...)) {
      return (*F)(set_opt_dtype(at::kFloat, args)...);
    } else {
      // If ineligible, calls F with unaltered args.  Does not set opt dtype, because setting
      // opt dtype explicitly may interfere with internal implicit promotion decisions.
      return (*F)(args...);
    }
  }
};

// CastPolicy::fp32_append_dtype
template<class Redispatch, Redispatch* F, class Ret, class... Args>
struct WrapFunction_<CastPolicy::fp32_append_dtype, Redispatch, F, Ret, guts::typelist::typelist<Args...>> {
  static Ret call(Args... args) {
    c10::impl::ExcludeDispatchKeyGuard no_autocasting(DispatchKey::AutocastTensorId);
    at::ScalarType out_type = type_from_firstarg(at::kFloat, args...);
    return (*F)(args..., out_type);
  }
};

// CastPolicy::promote
template<class Redispatch, Redispatch* F, class Ret, class... Args>
struct WrapFunction_<CastPolicy::promote, Redispatch, F, Ret, guts::typelist::typelist<Args...>> {
  static Ret call(Args... args) {
    c10::impl::ExcludeDispatchKeyGuard no_autocasting(DispatchKey::AutocastTensorId);
    auto to_type = promote_type(at::kHalf, args...);
    return (*F)(cached_cast(to_type, args)...);
  }
};

// Wrapper to infer return_type and parameter_types for WrapFunction_ (imitating core/boxing/kernel_function.h)
template<CastPolicy policy,
         class Registered, // The signature for which we're registering.  The dispatcher's calling code invokes our
                           // registered functions with arguments matching Registered, so we register
                           // WrapFunction_::call methods with a matching signature to properly field those arguments.
                           // guts::function_traits below extracts return_type and parameter_types from Registered,
                           // which WrapFunction_ templates above use to declare their call methods.
         class Redispatch, // The signature for the function we're redispatching to.  In most cases this is the same
                           // as Registered, but for some ops (for example, ops where we append a dtype) it's useful
                           // to redispatch to a function with a different signature.
         Redispatch* F>    // The actual function we're redispatching to.
struct WrapFunction final {
  using type = WrapFunction_<policy,
                             Redispatch,
                             F,
                             typename guts::function_traits<Registered>::return_type,
                             typename guts::function_traits<Registered>::parameter_types>;
};

/*******************************
Banned functions
*******************************/

Tensor binary_cross_entropy_banned(const Tensor &, const Tensor &, const Tensor &, int64_t) {
  AT_ERROR(""torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\n""
           ""Many models use a sigmoid layer right before the binary cross entropy layer.\n""
           ""In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\n""
           ""or torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\n""
           ""safe to autocast."");
}

#ifndef USE_STATIC_DISPATCH
namespace {
/*****************************************************************************************************************
This section performs load-time registration for autocast wrappers.

It's debatable at what level operations should be patched.  We'd like casts to be autograd-exposed
and precede autograd history recording, so that for fp16 ops, input tensors are saved for backward
in fp16 rather than fp32.  Saving inputs in fp16 can significantly reduce a model's memory footprint.

Option 1 (strawman):  Patch only at the level of explicit calls into cudnn/cublas (cudnn_convolution, etc),
because those are the code paths that are guaranteed to use Tensor Cores, therefore they're the ones that
will benefit most from fp16.   Potential pitfall:  convolutions (and other ops) are wrapped in several
layers of at::* calls.  If one of those happens to record autograd history, then we've lost the
opportunity to save inputs in fp16.

Option 2:  Patch the Python-exposed surface of calls, to make 100% sure autograd history
recording can't sneak in ahead of autocast.  This mirrors Apex most closely.

I think Option 2 is the right answer for all ops, not just convolutions.  Option 2 is what I implement here.
*****************************************************************************************************************/

auto register_fallthrough = c10::Dispatcher::singleton()
  .registerBackendFallbackKernel(DispatchKey::AutocastTensorId,
                                 KernelFunction::makeFallthrough());

/********************************************************************************************************************
Explicit registration for out-of-place ops

The stuff below could be codegenned.  Ed said
> you are going to have to write the function definition at some point, I wouldn't try to get clever about it
Therefore, for the moment, this is all copy pasted in from VariableTypeEverything.cpp with appropriate substitutions.
********************************************************************************************************************/

// Workaround for a compiler bug in VS 2017 (versions < 15.8).  See comments in autocast_VS2017_helper.h.
#ifdef _MSC_VER
  #if _MSC_VER >= 1915
    // With VS 15.8+, template directly on at:: functions.
    #define ADD_NS(RAW_OP) at::RAW_OP
  #else
    // If we're compiling with the buggy VS, pull in local wrappers to template on.
    #include <ATen/autocast_VS2017_helper.h>
    #define ADD_NS(RAW_OP) autocastVS2017Helper::RAW_OP
  #endif
#else
  // With other compilers, template directly on at:: functions.
  #define ADD_NS(RAW_OP) at::RAW_OP
#endif

// Common cases where registration signature matches redispatch signature
// (that's why SIGNATURE is repeated in the WrapFunction instantiation)
#define KERNEL(FUNC, REGISTER_SCHEMA, SIGNATURE, POLICY) \
  .op(torch::RegisterOperators::options() \
    .schema(REGISTER_SCHEMA) \
    .kernel<SIGNATURE>(DispatchKey::AutocastTensorId, \
    &WrapFunction<CastPolicy::POLICY, SIGNATURE, SIGNATURE, &FUNC>::type::call))

#define KERNEL_UNBOXED_ONLY(FUNC, REGISTER_SCHEMA, SIGNATURE, POLICY) \
  .op(torch::RegisterOperators::options() \
    .schema(REGISTER_SCHEMA) \
    .impl_unboxedOnlyKernel<SIGNATURE, \
    &WrapFunction<CastPolicy::POLICY, SIGNATURE, SIGNATURE, &FUNC>::type::call \
    >(DispatchKey::AutocastTensorId))

// Less-common but still useful case: redispatching to a function with a new signature (e.g. appending a dtype)
#define KERNEL_UNBOXED_ONLY_DIFFERENT_REDISPATCH_SIGNATURE(REDISPATCH_FUNC, REGISTER_SCHEMA, REGISTER_SIGNATURE, REDISPATCH_SIGNATURE, POLICY) \
  .op(torch::RegisterOperators::options() \
    .schema(REGISTER_SCHEMA) \
    .impl_unboxedOnlyKernel<REGISTER_SIGNATURE, \
    &WrapFunction<CastPolicy::POLICY, REGISTER_SIGNATURE, REDISPATCH_SIGNATURE, &REDISPATCH_FUNC>::type::call \
    >(DispatchKey::AutocastTensorId))

/*****************************************
Explicit registration for out-of-place ops
*****************************************/
auto register_out_of_place = torch::RegisterOperators()
  // fp16
  KERNEL_UNBOXED_ONLY(ADD_NS(_convolution), ""aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t, bool, bool, bool), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(_convolution_nogroup), ""aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv1d), ""aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv2d), ""aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv3d), ""aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv_tbc), ""aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, int64_t), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv_transpose1d), ""aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, IntArrayRef), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv_transpose2d), ""aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, IntArrayRef), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(conv_transpose3d), ""aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, IntArrayRef), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(convolution), ""aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(cudnn_convolution), ""aten::cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(cudnn_convolution_transpose), ""aten::cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(cudnn_convolution), ""aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor"", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(cudnn_convolution_transpose), ""aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor"", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool), fp16)
 KERNEL(ADD_NS(prelu), ""aten::prelu(Tensor self, Tensor weight) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp16)
 KERNEL(ADD_NS(addmm), ""aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), fp16)
 KERNEL(ADD_NS(addmv), ""aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), fp16)
 KERNEL(ADD_NS(addr), ""aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), fp16)
 KERNEL(ADD_NS(matmul), ""aten::matmul(Tensor self, Tensor other) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp16)
 KERNEL(ADD_NS(mm), ""aten::mm(Tensor self, Tensor mat2) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp16)
 KERNEL(ADD_NS(mv), ""aten::mv(Tensor self, Tensor vec) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(linear), ""aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &), fp16)
 KERNEL(ADD_NS(addbmm), ""aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), fp16)
 KERNEL(ADD_NS(baddbmm), ""aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar), fp16)
 KERNEL(ADD_NS(bmm), ""aten::bmm(Tensor self, Tensor mat2) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp16)
 KERNEL_UNBOXED_ONLY(ADD_NS(chain_matmul), ""aten::chain_matmul(Tensor[] matrices) -> Tensor"", Tensor (TensorList), fp16)
 // fp32
 KERNEL(ADD_NS(acos), ""aten::acos(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(asin), ""aten::asin(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(cosh), ""aten::cosh(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(erfinv), ""aten::erfinv(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(exp), ""aten::exp(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(expm1), ""aten::expm1(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(log), ""aten::log(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(log10), ""aten::log10(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(log2), ""aten::log2(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(log1p), ""aten::log1p(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(reciprocal), ""aten::reciprocal(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(rsqrt), ""aten::rsqrt(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(sinh), ""aten::sinh(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(tan), ""aten::tan(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL(ADD_NS(pow), ""aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor"", Tensor (const Tensor &, Scalar), fp32)
 KERNEL(ADD_NS(pow), ""aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor"", Tensor (const Tensor &, const Tensor &), fp32)
 KERNEL(ADD_NS(pow), ""aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor"", Tensor (Scalar, const Tensor &), fp32)
 KERNEL(ADD_NS(softplus), ""aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor"", Tensor (const Tensor &, Scalar, Scalar), fp32)
 KERNEL(ADD_NS(gelu), ""aten::gelu(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(layer_norm), ""aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor"", Tensor (const Tensor &, IntArrayRef, const Tensor &, const Tensor &, double, bool), fp32)
 // The macro doesn't like this one so I had to write it out manually.
 .op(torch::RegisterOperators::options()
   .schema(""aten::native_layer_norm(Tensor input, Tensor? weight, Tensor? bias, int M, int N, float eps) -> (Tensor, Tensor, Tensor)"")
   .impl_unboxedOnlyKernel<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, double),
   &WrapFunction<CastPolicy::fp32, std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, double), std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, double), &ADD_NS(native_layer_norm)>::type::call
   >(DispatchKey::AutocastTensorId)
   .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA))
 KERNEL_UNBOXED_ONLY(ADD_NS(group_norm), ""aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor"", Tensor (const Tensor &, int64_t, const Tensor &, const Tensor &, double, bool), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(frobenius_norm), ""aten::frobenius_norm(Tensor self) -> Tensor"", Tensor (const Tensor &), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(frobenius_norm), ""aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor"", Tensor (const Tensor &, IntArrayRef, bool), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(nuclear_norm), ""aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor"", Tensor (const Tensor &, bool),
fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(nuclear_norm), ""aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor"", Tensor (const Tensor &, IntArrayRef, bool), fp32)
 KERNEL(ADD_NS(cosine_similarity), ""aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t, double), fp32)
 KERNEL(ADD_NS(poisson_nll_loss), ""aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor"", Tensor (const Tensor &, const Tensor &, bool, bool, double, int64_t), fp32)
 KERNEL(ADD_NS(cosine_embedding_loss), ""aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, double, int64_t), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(nll_loss), ""aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(nll_loss2d), ""aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t), fp32)
 KERNEL(ADD_NS(hinge_embedding_loss), ""aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, double, int64_t), fp32)
 KERNEL(ADD_NS(kl_div), ""aten::kl_div(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(l1_loss), ""aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(smooth_l1_loss), ""aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(mse_loss), ""aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(margin_ranking_loss), ""aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, double, int64_t), fp32)
 KERNEL(ADD_NS(multilabel_margin_loss), ""aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(soft_margin_loss), ""aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(triplet_margin_loss), ""aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, double, double, double, bool, int64_t), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(multi_margin_loss), ""aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, Scalar, Scalar, const Tensor &, int64_t), fp32)
 KERNEL_UNBOXED_ONLY(ADD_NS(binary_cross_entropy_with_logits), ""aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t), fp32)
 KERNEL(ADD_NS(dist), ""aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor"", Tensor (const Tensor &, const Tensor &, Scalar), fp32)
 KERNEL(ADD_NS(pdist), ""aten::pdist(Tensor self, float p=2) -> Tensor"", Tensor (const Tensor &, double), fp32)
 KERNEL(ADD_NS(cdist), ""aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor"", Tensor (const Tensor &, const Tensor &, double, c10::optional<int64_t>), fp32)
 KERNEL(ADD_NS(renorm), ""aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor"", Tensor (const Tensor &, Scalar, int64_t, Scalar), fp32)
 // fp32_set_opt_dtype
 KERNEL_UNBOXED_ONLY(ADD_NS(prod), ""aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(prod), ""aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, int64_t, bool, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(prod), ""aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, Dimname, bool, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(softmax), ""aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, int64_t, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(softmax), ""aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, Dimname, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(log_softmax), ""aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, int64_t, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(log_softmax), ""aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, Dimname, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(cumprod), ""aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, int64_t, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(cumprod), ""aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, Dimname, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(cumsum), ""aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, int64_t, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(cumsum), ""aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, Dimname, c10::optional<ScalarType>), fp32_set_opt_dtype)
 // commenting these out because they accept an explicit (not-optional) dtype, and we shouldn't try to flip that even
 // when autocasting.
 // KERNEL_UNBOXED_ONLY(ADD_NS(norm), ""aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor"", Tensor (const Tensor &, c10::optional<Scalar>, ScalarType), fp32_set_opt_dtype)
 // KERNEL_UNBOXED_ONLY(ADD_NS(norm), ""aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor"", Tensor (const Tensor &, c10::optional<Scalar>, IntArrayRef, bool, ScalarType), fp32_set_opt_dtype)
 // KERNEL_UNBOXED_ONLY(ADD_NS(norm), ""aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor"", Tensor (const Tensor &, c10::optional<Scalar>, DimnameList, bool, ScalarType), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(sum), ""aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(sum), ""aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, IntArrayRef, bool, c10::optional<ScalarType>), fp32_set_opt_dtype)
 KERNEL_UNBOXED_ONLY(ADD_NS(sum), ""aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"", Tensor (const Tensor &, DimnameList, bool, c10::optional<ScalarType>), fp32_set_opt_dtype)
 // fp32_append_dtype
 // The fp32_append_dtype wrapper overrides implicit promotion behavior.
 // norm does not implicitly promote, but be aware when adding new ops to this policy.
 KERNEL_UNBOXED_ONLY_DIFFERENT_REDISPATCH_SIGNATURE(ADD_NS(norm), ""aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor"", Tensor (const Tensor &, Scalar), Tensor (const Tensor &, c10::optional<Scalar>, ScalarType), fp32_append_dtype)
 KERNEL_UNBOXED_ONLY_DIFFERENT_REDISPATCH_SIGNATURE(ADD_NS(norm), ""aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor"", Tensor (const Tensor &, c10::optional<Scalar>, IntArrayRef, bool), Tensor (const Tensor &, c10::optional<Scalar>, IntArrayRef, bool, ScalarType), fp32_append_dtype)
 KERNEL_UNBOXED_ONLY_DIFFERENT_REDISPATCH_SIGNATURE(ADD_NS(norm), ""aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor"", Tensor (const Tensor &, c10::optional<Scalar>, DimnameList, bool), Tensor (const Tensor &, c10::optional<Scalar>, DimnameList, bool, ScalarType), fp32_append_dtype)
 // promote
 KERNEL(ADD_NS(addcdiv), ""aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar), promote)
 KERNEL(ADD_NS(addcmul), ""aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar), promote)
 KERNEL(ADD_NS(atan2), ""aten::atan2(Tensor self, Tensor other) -> Tensor"", Tensor (const Tensor &, const Tensor &), promote)
 KERNEL(ADD_NS(cross), ""aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor"", Tensor (const Tensor &, const Tensor &, c10::optional<int64_t>), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(bilinear), ""aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor"", Tensor (const Tensor &, const Tensor &, const Tensor &, const Tensor &), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), ""aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor"", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(dot), ""aten::dot(Tensor self, Tensor tensor) -> Tensor"", Tensor (const Tensor &, const Tensor &), promote)
 KERNEL(ADD_NS(equal), ""aten::equal(Tensor self, Tensor other) -> bool"", bool (const Tensor &, const Tensor &), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""aten::cat(Tensor[] tensors, int dim=0) -> Tensor"", Tensor (TensorList, int64_t), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(cat), ""aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor"", Tensor (TensorList, Dimname), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(_cat), ""aten::_cat(Tensor[] tensors, int dim=0) -> Tensor"", Tensor (TensorList, int64_t), promote)
 KERNEL_UNBOXED_ONLY(ADD_NS(stack), ""aten::stack(Tensor[] tensors, int dim=0) -> Tensor"", Tensor (TensorList, int64_t), promote)
  ;

auto register_banned = torch::RegisterOperators()
  .op(torch::RegisterOperators::options()
    .schema(""aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor"")
    .impl_unboxedOnlyKernel<Tensor (const Tensor &, const Tensor &, const Tensor &, int64_t), &at::autocast::binary_cross_entropy_banned>(DispatchKey::AutocastTensorId)
    .aliasAnalysis(c10::AliasAnalysisKind::FROM_SCHEMA));
}
#endif

} // namespace autocast
} // namespace at
","++ /dev/null
"
1135,"return ""BackendSelect"";
case DispatchKey::TESTING_ONLY_GenericModeTensorId:
return ""TESTING_ONLY_GenericModeTensorId"";
    case DispatchKey::AutocastTensorId:
      return ""AutocastTensorId"";
case DispatchKey::TESTING_ONLY_GenericWrapperTensorId:
return ""TESTING_ONLY_GenericWrapperTensorId"";
default:
","return ""BackendSelect"";
case DispatchKey::TESTING_ONLY_GenericModeTensorId:
return ""TESTING_ONLY_GenericModeTensorId"";
case DispatchKey::TESTING_ONLY_GenericWrapperTensorId:
return ""TESTING_ONLY_GenericWrapperTensorId"";
default:
"
1136,"namespace torch { namespace autograd {
static PyObject * set_autocast_enabled(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  if (!PyBool_Check(arg)) {
    throw TypeError(""enabled must be a bool (got %s)"", Py_TYPE(arg)->tp_name);
  }
  at::autocast::set_enabled(arg == Py_True);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

static PyObject * is_autocast_enabled(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  if (at::autocast::is_enabled()) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

static PyObject * clear_autocast_cache(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  at::autocast::clear_cache();
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

static PyObject * autocast_increment_nesting(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  return THPUtils_packInt64(at::autocast::increment_nesting());
  END_HANDLE_TH_ERRORS
}

static PyObject * autocast_decrement_nesting(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  return THPUtils_packInt64(at::autocast::decrement_nesting());
  END_HANDLE_TH_ERRORS
}

static PyObject * set_grad_enabled(PyObject* _unused, PyObject *arg) {
HANDLE_TH_ERRORS
if (!PyBool_Check(arg)) {
","namespace torch { namespace autograd {
static PyObject * set_grad_enabled(PyObject* _unused, PyObject *arg) {
HANDLE_TH_ERRORS
if (!PyBool_Check(arg)) {
"
1137,"backoff_factor=0.5,
growth_interval=2000,
enabled=True):
        self._enabled = enabled
        if enabled:
assert growth_factor > 1.0, ""The growth factor must be > 1.0.""
assert backoff_factor < 1.0, ""The backoff factor must be < 1.0.""
","backoff_factor=0.5,
growth_interval=2000,
enabled=True):
        if enabled and not torch.cuda.is_available():
            warnings.warn(""torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling."")
            self._enabled = False
        else:
            self._enabled = enabled

        if self._enabled:
assert growth_factor > 1.0, ""The growth factor must be > 1.0.""
assert backoff_factor < 1.0, ""The backoff factor must be < 1.0.""
"
1138,"/* need_inputs */ false,
/* sampled */ false);
#endif
    JITCallGuard guard;
}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
preModuleLoadSetup();
module_ = torch::jit::load(std::move(modelPath->toStdString()));
module_.eval();
}
","/* need_inputs */ false,
/* sampled */ false);
#endif
}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
preModuleLoadSetup();
    JITCallGuard guard;
module_ = torch::jit::load(std::move(modelPath->toStdString()));
module_.eval();
}
"
1139,"""Could not get buffer for asset '%s'"",
assetName->toStdString().c_str());
}
module_ = torch::jit::load(torch::make_unique<MemoryReadAdapter>(
assetBuffer, AAsset_getLength(asset)));
AAsset_close(asset);
","""Could not get buffer for asset '%s'"",
assetName->toStdString().c_str());
}
    JITCallGuard guard;
module_ = torch::jit::load(torch::make_unique<MemoryReadAdapter>(
assetBuffer, AAsset_getLength(asset)));
AAsset_close(asset);
"
1140,"}
auto output = [&]() {
      torch::autograd::AutoGradMode guard(false);
      at::AutoNonVariableTypeMode non_var_type_mode(true);
return module_.forward(inputs);
}();
return JIValue::newJIValueFromAtIValue(output);
","}
auto output = [&]() {
      LiteJITCallGuard guard;
return module_.forward(inputs);
}();
return JIValue::newJIValueFromAtIValue(output);
"
1141,"std::cerr << FLAGS_model <<  "":Model file is not provided\n"";
return -1;
}

torch::jit::mobile::Module bc = torch::jit::_load_for_mobile(FLAGS_model);
return 0;
}
","std::cerr << FLAGS_model <<  "":Model file is not provided\n"";
return -1;
}

  // TODO: avoid having to set this guard for custom mobile build with mobile
  // interpreter.
  torch::AutoNonVariableTypeMode non_var_guard{true};
torch::jit::mobile::Module bc = torch::jit::_load_for_mobile(FLAGS_model);
return 0;
}
"
1142,"Arguments:
to (str or WorkerInfo): id or name of the destination worker.
        func (callable): any callable function. python callable, builtin or annotated TorchScript
                         functions (like meth:`torch.add`) can be sent over RPC more efficiently.
args (tuple): the argument tuple for the ``func`` invocation.
kwargs (dict): is a dictionary of keyword arguments for the ``func``
invocation.
","Arguments:
to (str or WorkerInfo): id or name of the destination worker.
        func (callable): a callable function, such as Python callables, builtin
                         operators (e.g. :meth:`~torch.add`) and annotated
                         TorchScript functions.
args (tuple): the argument tuple for the ``func`` invocation.
kwargs (dict): is a dictionary of keyword arguments for the ``func``
invocation.
"
1143,">>> rpc.init_rpc(""worker1"", rank=1, world_size=2)
>>> rpc.shutdown()
        If invoking an annotated TorchScript function, then run the following
        code in two different processes:
        >>> # On worker 0:
>>> @torch.jit.script
>>> def my_script_add(t1, t2):
>>>    return torch.add(t1, t2)
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc(""worker0"", rank=0, world_size=2)
>>> fut = rpc.rpc_async(""worker1"", my_script_add, args=(torch.ones(2), 3))
",">>> rpc.init_rpc(""worker1"", rank=1, world_size=2)
>>> rpc.shutdown()
        Below is an example of running a TorchScript function using RPC.
        >>> # On both workers:
>>> @torch.jit.script
>>> def my_script_add(t1, t2):
>>>    return torch.add(t1, t2)

        >>> # On worker 0:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc(""worker0"", rank=0, world_size=2)
>>> fut = rpc.rpc_async(""worker1"", my_script_add, args=(torch.ones(2), 3))
"
1144,"def _list_with_default(out_size, defaults):
if isinstance(out_size, int):
return out_size
if len(defaults) <= len(out_size):
","def _list_with_default(out_size, defaults):
    # type: (List[int], List[int]) -> List[int]
if isinstance(out_size, int):
return out_size
if len(defaults) <= len(out_size):
"
1145,"for (ch = start + toBeSkipped_; ch < end; ++ch) {
if (*ch == escape_) {
if (!copied) {
        tokenized.modifiedStrings_.emplace_back(new std::string());
copied = tokenized.modifiedStrings_.back().get();
}
copied->append(currentStart, ch);
","for (ch = start + toBeSkipped_; ch < end; ++ch) {
if (*ch == escape_) {
if (!copied) {
        tokenized.modifiedStrings_.emplace_back(std::make_shared<std::string>());
copied = tokenized.modifiedStrings_.back().get();
}
copied->append(currentStart, ch);
"
1146,"std::vector<std::string> joinedKeys;
joinedKeys.reserve(keys.size());
for (const auto& key : keys) {
    joinedKeys.push_back(joinKey(key));
}
return joinedKeys;
}
","std::vector<std::string> joinedKeys;
joinedKeys.reserve(keys.size());
for (const auto& key : keys) {
    joinedKeys.emplace_back(joinKey(key));
}
return joinedKeys;
}
"
1147,"}
};
// NOTE [ Convolution design ]
//
// cuDNN convolutions does not handle bias. Bias is handled outside.
","}
};
inline Tensor allocate_workspace(size_t size, const Tensor &other) {
  // Sometimes cuDNN returns a workspace size > 2^63, this could makes the allocation of
  // workspace fail with some 64bit indexing error instead of an OOM error. In such case,
  // we manually fail with OOM.
  TORCH_CHECK_WITH(CUDAOutOfMemoryError, size < 1_TiB, ""Not enough memory for workspace!"");
  return at::empty({static_cast<int64_t>(size)}, other.options().dtype(kByte));
}

// NOTE [ Convolution design ]
//
// cuDNN convolutions does not handle bias. Bias is handled outside.
"
1148,"""SparseLengthsMeanFused8BitRowwiseFakeFP16AccFP16""},
{""BatchMatMul"", ""BatchMatMulFP16Acc32Fake""},
{""Sigmoid"", ""SigmoidFakeFp16""},
      // {""SpatialBN"", ""SpatialBNFakeLoweredFp16NNPI""},
{""Tanh"", ""TanhFakeFp16""},
{""Relu"", ""ReluFakeFp16""},
{""Add"", ""AddFakeFp16""},
","""SparseLengthsMeanFused8BitRowwiseFakeFP16AccFP16""},
{""BatchMatMul"", ""BatchMatMulFP16Acc32Fake""},
{""Sigmoid"", ""SigmoidFakeFp16""},
      {""SpatialBN"", ""SpatialBNFakeFp16Op""},
{""Tanh"", ""TanhFakeFp16""},
{""Relu"", ""ReluFakeFp16""},
{""Add"", ""AddFakeFp16""},
"
1149,"}
void Caffe2Annotation::setComponentLevels(std::vector<std::string> components) {
  component_levels_ = components;
}
std::vector<std::string> Caffe2Annotation::getComponentLevels() const {
return component_levels_;
","}
void Caffe2Annotation::setComponentLevels(std::vector<std::string> components) {
  component_levels_ = std::move(components);
}
std::vector<std::string> Caffe2Annotation::getComponentLevels() const {
return component_levels_;
"
1150,"char label_value;
std::vector<char> pixels(rows * cols);
int count = 0;
  const int kMaxKeyLength = 10;
char key_cstr[kMaxKeyLength];
string value;
","char label_value;
std::vector<char> pixels(rows * cols);
int count = 0;
  const int kMaxKeyLength = 11;
char key_cstr[kMaxKeyLength];
string value;
"
1151,"Py_TYPE(obj)->tp_name);
} else {
// foo(): argument 'other' (position 2) must be str, not int
        throw TypeError(""%s(): argument '%s' (position %d) must be %s, not %s"",
            name.c_str(), param.name.c_str(), arg_pos + 1,
param.type_name().c_str(), Py_TYPE(obj)->tp_name);
}
} else {
","Py_TYPE(obj)->tp_name);
} else {
// foo(): argument 'other' (position 2) must be str, not int
        throw TypeError(""%s(): argument '%s' (position %ld) must be %s, not %s"",
            name.c_str(), param.name.c_str(), static_cast<long>(arg_pos + 1),
param.type_name().c_str(), Py_TYPE(obj)->tp_name);
}
} else {
"
1152,"namespace caffe2 {
namespace {
size_t getDefaultNumThreads() {
CAFFE_ENFORCE(cpuinfo_initialize(), ""cpuinfo initialization failed"");
int numThreads = cpuinfo_get_processors_count();
","namespace caffe2 {
size_t getDefaultNumThreads() {
CAFFE_ENFORCE(cpuinfo_initialize(), ""cpuinfo initialization failed"");
int numThreads = cpuinfo_get_processors_count();
"
1153,"# TorchScript doesn't support constructors on named tuples, so we use this helper
# method to construct PackedSequence
def _packed_sequence_init_args(data, batch_sizes=None, sorted_indices=None, unsorted_indices=None):
    # type: (Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]) -> Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
# NB: if unsorted_indices is provided, it should be the inverse permutation
# to sorted_indices. Don't assert it here because the PackedSequence ctor
# should only be used internally.
","# TorchScript doesn't support constructors on named tuples, so we use this helper
# method to construct PackedSequence
def _packed_sequence_init_args(data,  # type: Tensor
                               batch_sizes=None,  # type: Optional[Tensor]
                               sorted_indices=None,  # type: Optional[Tensor]
                               unsorted_indices=None  # type: Optional[Tensor]
                               ):
    # type: (...) -> Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
# NB: if unsorted_indices is provided, it should be the inverse permutation
# to sorted_indices. Don't assert it here because the PackedSequence ctor
# should only be used internally.
"
1154,"import torch
import torch.nn as nn
from torch import Tensor  # noqa: F401
from torch.nn import _VF
from torch._jit_internal import Tuple, Optional, List  # noqa: F401
from torch.nn.utils.rnn import PackedSequence
import numbers
","import torch
import torch.nn as nn
from torch import Tensor  # noqa: F401
from torch import _VF
from torch._jit_internal import Tuple, Optional, List  # noqa: F401
from torch.nn.utils.rnn import PackedSequence
import numbers
"
1155,"calls.pop_back();
}
#else // defined C10_MOBILE
ErrorReport::ErrorReport()
    : context(c10::nullopt) {}
ErrorReport::ErrorReport(SourceRange r)
: context(std::move(r)) {}
","calls.pop_back();
}
#else // defined C10_MOBILE
ErrorReport::ErrorReport(SourceRange r)
: context(std::move(r)) {}
"
1156,"void GraphTask::set_exception(
std::exception& e,
const std::shared_ptr<Node>& fn) {
  std::lock_guard<std::mutex> lock(mutex_);
if (!has_error_.load()) {
if (AnomalyMode::is_enabled() && fn) {
fn->metadata()->print_stack();
}
has_error_ = true;
    if (!future_result_->completed()) {
      future_result_->setError(e.what());
} else {
      TORCH_INTERNAL_ASSERT(future_result_->hasError());
}
}
}
","void GraphTask::set_exception(
std::exception& e,
const std::shared_ptr<Node>& fn) {
  std::unique_lock<std::mutex> lock(mutex_);
if (!has_error_.load()) {
if (AnomalyMode::is_enabled() && fn) {
fn->metadata()->print_stack();
}
has_error_ = true;
    // Careful: setting the future_result_ can trigger DistAutogradContext to
    // resetGraphTask(), sometimes deleting this underlying GraphTask.
    // Don't touch *this after setError() below, and release the lock early, to
    // avoid unlocking this->mutex_ after setting the future.
    std::shared_ptr<FutureVariableList> future_result = future_result_;
    lock.unlock();
    if (!future_result->completed()) {
      future_result->setError(e.what());
} else {
      TORCH_INTERNAL_ASSERT(future_result->hasError());
}
}
}
"
1157,") {
for (int64_t i = 0; i < index_dim_size; ++i) {
int64_t idx_dim = index_data[i * index_dim_stride];
TORCH_CHECK(idx_dim >= 0 && idx_dim < self_dim_size,
          ""index "", idx_dim,
          "" is out of bounds for dimension "", dim,
          "" with size "", self_dim_size);
result_data[i * result_dim_stride] = self_data[idx_dim * self_dim_stride];
}
}, /*serial_exec=*/false
",") {
for (int64_t i = 0; i < index_dim_size; ++i) {
int64_t idx_dim = index_data[i * index_dim_stride];
        // we are not putting idx_dim in the error message because it disables
        // loop optimization in clang-7
TORCH_CHECK(idx_dim >= 0 && idx_dim < self_dim_size,
                    ""index "", index_data[i * index_dim_stride], "" is out of bounds for dimension "", dim,
                    "" with size "", self_dim_size);
result_data[i * result_dim_stride] = self_data[idx_dim * self_dim_stride];
}
}, /*serial_exec=*/false
"
1158,"py::object buffer_;
size_t size_;
};
py::class_<PyTorchStreamReader>(m, ""PyTorchFileReader"")
","py::object buffer_;
size_t size_;
    size_t start_offset_;
    bool use_readinto_;
};
py::class_<PyTorchStreamReader>(m, ""PyTorchFileReader"")
"
1159,"/**
* Cache normal random in float
 *
* See Note [Acquire lock when using random generators]
*/
void CPUGenerator::set_next_float_normal_sample(c10::optional<float> randn) {
","/**
* Cache normal random in float
 *
* See Note [Acquire lock when using random generators]
*/
void CPUGenerator::set_next_float_normal_sample(c10::optional<float> randn) {
"
1160,"throw ErrorReport(stmt.range()) << ""Expected RHS for assignment"";
}
const auto lhs = Select(stmt.lhs());
    const auto basename = Var(lhs.value()).name();
const auto rhsValue = emitSugaredExpr(stmt.rhs().get(), 1)
>asValue(stmt.rhs().range(), method);
    auto userObject = environment_stack->getSugaredVar(basename);
    userObject->setAttr(stmt.range(), method, lhs.selector().name(), rhsValue);
}
NodeKind getNodeKind(int kind, int ninputs) {
","throw ErrorReport(stmt.range()) << ""Expected RHS for assignment"";
}
const auto lhs = Select(stmt.lhs());
    auto lhsObject = emitSugaredExpr(lhs.value(), 1);
const auto rhsValue = emitSugaredExpr(stmt.rhs().get(), 1)
    lhsObject->setAttr(stmt.range(), method, lhs.selector().name(), rhsValue);
}
NodeKind getNodeKind(int kind, int ninputs) {
"
1161,"class _NormBase(Module):
""""""Common base of _InstanceNorm and _BatchNorm""""""
_version = 2
    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',
                     'running_mean', 'running_var', 'num_batches_tracked',
'num_features', 'affine']
def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,
","class _NormBase(Module):
""""""Common base of _InstanceNorm and _BatchNorm""""""
_version = 2
    __constants__ = ['track_running_stats', 'momentum', 'eps',
'num_features', 'affine']
def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,
"
1162,"Tensor batch_sizes;
};
// TODO: Remove this once https://github.com/pytorch/pytorch/issues/30987 is closed
// This is used to avoid limitations with the autograd inplace verification check
// that cannot detect non-overlapping changes.
std::vector<Tensor> unsafe_chunk_no_version_check(const Tensor& self, int chunks, int dim) {
  const auto results = self.chunk(chunks, dim);
  // Each result gets its own version counter as they don't overlap.
  // This is still unsafe as changes to self will not be properly tracked as modifying results
  for (auto& t: results) {
    t.unsafeGetTensorImpl()->set_version_counter(c10::VariableVersion());
  }
  return results;
}

// Pretty much all cells we support take the same set of arguments, but threading those
// 4 arguments manually is really annoying. Their lifetime is externally managed, so we only
// pass this struct of references around.
","Tensor batch_sizes;
};
// Pretty much all cells we support take the same set of arguments, but threading those
// 4 arguments manually is really annoying. Their lifetime is externally managed, so we only
// pass this struct of references around.
"
1163,"# this list contains both the root view functions and any that are purely composed
# of viewing functions, and is used by the JIT to determine when an operator
# returns a view of its inputs
RETURNS_VIEWS_OF_INPUT = set(VIEW_FUNCTIONS.keys()).union({'chunk', 'narrow'})
def format_return_type(returns):
","# this list contains both the root view functions and any that are purely composed
# of viewing functions, and is used by the JIT to determine when an operator
# returns a view of its inputs
RETURNS_VIEWS_OF_INPUT = set(VIEW_FUNCTIONS.keys()).union({'chunk', 'split'})
def format_return_type(returns):
"
1164,"//     and would appear in the same order when the expression tree is
//     reparsed.
// The last case can be checked
  // becuase when we emit a expresion tree in the parser,
// we do a left-to-right postorder traversal of the expression tree (emit
// children, then emit op). The reverse of this is a right-to-left preorder
// traversal of the tree. By doing a right-to-left preorder traversal of the
","//     and would appear in the same order when the expression tree is
//     reparsed.
// The last case can be checked
  // because when we emit a expresion tree in the parser,
// we do a left-to-right postorder traversal of the expression tree (emit
// children, then emit op). The reverse of this is a right-to-left preorder
// traversal of the tree. By doing a right-to-left preorder traversal of the
"
1165,"if not modifies_arguments and not returns_void:
call = '{} = {}'.format(tie_return_values(), call)
call = call + ';'
call = enforce_same_tensorimpl_and_storage(env, call)
return call
","if not modifies_arguments and not returns_void:
call = '{} = {}'.format(tie_return_values(), call)
call = call + ';'
        for stmt in extra_wrapping_stmts:
            call += '\n' + stmt
call = enforce_same_tensorimpl_and_storage(env, call)
return call
"
1166,"Tensor batch_sizes;
};
// Pretty much all cells we support take the same set of arguments, but threading those
// 4 arguments manually is really annoying. Their lifetime is externally managed, so we only
// pass this struct of references around.
","Tensor batch_sizes;
};
// TODO: Remove this once https://github.com/pytorch/pytorch/issues/30987 is closed
// This is used to avoid limitations with the autograd inplace verification check
// that cannot detect non-overlapping changes.
std::vector<Tensor> unsafe_chunk_no_version_check(const Tensor& self, int chunks, int dim) {
  const auto results = self.chunk(chunks, dim);
  // Each result gets its own version counter as they don't overlap.
  // This is still unsafe as changes to self will not be properly tracked as modifying results
  for (auto& t: results) {
    t.unsafeGetTensorImpl()->set_version_counter(c10::VariableVersion());
  }
  return results;
}

// Pretty much all cells we support take the same set of arguments, but threading those
// 4 arguments manually is really annoying. Their lifetime is externally managed, so we only
// pass this struct of references around.
"
1167,"const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = gates.chunk(4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
","const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = unsafe_chunk_no_version_check(gates, 4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
"
1168,"}
auto current_version = self._version();
if (diff_view_meta->attr_version != current_version) {
      AT_ASSERT(diff_view_meta->output_nr_ == 0);
auto fn = std::make_shared<torch::autograd::generated::AsStridedBackward>();
fn->self_geometry = at::TensorGeometry(diff_view_meta->base_);
fn->size = self.sizes().vec();
","}
auto current_version = self._version();
if (diff_view_meta->attr_version != current_version) {
auto fn = std::make_shared<torch::autograd::generated::AsStridedBackward>();
fn->self_geometry = at::TensorGeometry(diff_view_meta->base_);
fn->size = self.sizes().vec();
"
1169,"*      `apply_fn` will be called multiple times, and together cover the entire
*      output spatial space.
*
 *  Now you should be able tp understand everything about the implementaion of
*  2D forward kernel shown at the beginning of this note.
*
**/
","*      `apply_fn` will be called multiple times, and together cover the entire
*      output spatial space.
*
 *  Now you should be able tp understand everything about the implementation of
*  2D forward kernel shown at the beginning of this note.
*
**/
"
1170,"struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
  // a bufer to e.g. run RNNs in test mode.
at::Tensor buffer;
c10::optional<cuda::CUDAEvent> event;
std::mutex mutex;
","struct DropoutState {
// Both buffer and event are lazily instantiated when a dropout state is needed
// for the first time. Note that in this case needed != used, as we don't need
  // a buffer to e.g. run RNNs in test mode.
at::Tensor buffer;
c10::optional<cuda::CUDAEvent> event;
std::mutex mutex;
"
1171,"from_slice_data += (output.size(d) - 1) * output.stride(d);
break;
} else {
          // Substract. Doesn't carry over to previous dimension
from_slice_data -= output.stride(d);
break;
}
","from_slice_data += (output.size(d) - 1) * output.stride(d);
break;
} else {
          // Subtract. Doesn't carry over to previous dimension
from_slice_data -= output.stride(d);
break;
}
"
1172,"R_2 = [0.3, 1.2];
We will build R = [0.1, 0.4, 0.5, 0.3, 1.2]; besides, we have
lengths = [3, 2]
    to indicate the boundries of the percentile information.
)DOC"")
.Arg(
","R_2 = [0.3, 1.2];
We will build R = [0.1, 0.4, 0.5, 0.3, 1.2]; besides, we have
lengths = [3, 2]
    to indicate the boundaries of the percentile information.
)DOC"")
.Arg(
"
1173,"auto& blob_states = *blob_states_ptr;
if (blob_states.count(key) == 0) {
// We reset the blob so that any existing content is destroyed. This
    // is to guaranee correct device placement: if we are deserializing
// into a TensorCUDA, without explicit Reset we might be loading data
// into an existing TensorCUDA that has pre-allocated memory on a
// different GPU.
","auto& blob_states = *blob_states_ptr;
if (blob_states.count(key) == 0) {
// We reset the blob so that any existing content is destroyed. This
    // is to guarantee correct device placement: if we are deserializing
// into a TensorCUDA, without explicit Reset we might be loading data
// into an existing TensorCUDA that has pre-allocated memory on a
// different GPU.
"
1174,"""Index_High"",
""tensor of int64 indices for elements above threshold"")
.SetDoc(R""DOC(
Split the elemnts and return the indices based on the given threshold.
)DOC"");
NO_GRADIENT(StumpFuncIndex);
","""Index_High"",
""tensor of int64 indices for elements above threshold"")
.SetDoc(R""DOC(
Split the elements and return the indices based on the given threshold.
)DOC"");
NO_GRADIENT(StumpFuncIndex);
"
1175,"for (const auto& i : n.input()) {
bool is_new = used_inputs.emplace(i).second;
// The input is not seen and it's not referred by any nodes before as
        // output, we count it as an boudary input
if (is_new && !used_outputs.count(i)) {
boundary_inputs.emplace_back(i);
}
","for (const auto& i : n.input()) {
bool is_new = used_inputs.emplace(i).second;
// The input is not seen and it's not referred by any nodes before as
        // output, we count it as an boundary input
if (is_new && !used_outputs.count(i)) {
boundary_inputs.emplace_back(i);
}
"
1176,"input_dims,
"""",
""The path of the file that ""
    ""stores input dimesions of all the operators in the run net. ""
""Each element of the array is a mapping from ""
""operator index to its input dimension."");
C10_DEFINE_string(
","input_dims,
"""",
""The path of the file that ""
    ""stores input dimensions of all the operators in the run net. ""
""Each element of the array is a mapping from ""
""operator index to its input dimension."");
C10_DEFINE_string(
"
1177,"if schema.inf == max_output:
raise ValueError(
""For operators with max_output == inf,\
                        user should pass num_output explicity.""
)
output_names = get_name_list(
output_prefix, max_output, max_output
","if schema.inf == max_output:
raise ValueError(
""For operators with max_output == inf,\
                        user should pass num_output explicitly.""
)
output_names = get_name_list(
output_prefix, max_output, max_output
"
1178,"model, blob_in, blob_out, w_csr, iw, jw, bias,
**kwargs
):
    """"""FC_Sparse: Only takes in alocated weights""""""
if not (w_csr and iw and jw and bias):
print(""Warning..."")
model.AddParameter(w_csr)
","model, blob_in, blob_out, w_csr, iw, jw, bias,
**kwargs
):
    """"""FC_Sparse: Only takes in allocated weights""""""
if not (w_csr and iw and jw and bias):
print(""Warning..."")
model.AddParameter(w_csr)
"
1179,"self.axis = axis
assert len(self.input_shape) >= 1, (
            ""This layer supports only >= 2D tesnors"")
input_dims = self.input_shape[0]
self.output_schema = schema.Scalar(
","self.axis = axis
assert len(self.input_shape) >= 1, (
            ""This layer supports only >= 2D tensors"")
input_dims = self.input_shape[0]
self.output_schema = schema.Scalar(
"
1180,"graph.add_edge(pydot.Edge(input_node, op_node))
for output_name in op.output:
if output_name in pydot_nodes:
                # we are overwriting an existing blob. need to updat the count.
pydot_node_counts[output_name] += 1
output_node = blob_node_producer(
_escape_label(
","graph.add_edge(pydot.Edge(input_node, op_node))
for output_name in op.output:
if output_name in pydot_nodes:
                # we are overwriting an existing blob. need to update the count.
pydot_node_counts[output_name] += 1
output_node = blob_node_producer(
_escape_label(
"
1181,"start_bin = next_start_bin;
end_bin = next_end_bin;
}
  VLOG(2) << ""best quantiation range "" << start_bin << "","" << end_bin + 1 << "",""
<< norm_min;
double selected_sum = 0;
","start_bin = next_start_bin;
end_bin = next_end_bin;
}
  VLOG(2) << ""best quantization range "" << start_bin << "","" << end_bin + 1 << "",""
<< norm_min;
double selected_sum = 0;
"
1182,"))
else:
# Okay, we are abusing the definition of 'unpack' here a bit,
            # although it's stll getting the non-variable from the variable
# (in this case via TensorOptions rather than Variable/Tensor).
body.append(UNPACK_OPTIONS.substitute(arg_name=arg['name']))
","))
else:
# Okay, we are abusing the definition of 'unpack' here a bit,
            # although it's still getting the non-variable from the variable
# (in this case via TensorOptions rather than Variable/Tensor).
body.append(UNPACK_OPTIONS.substitute(arg_name=arg['name']))
"
1183,"}
}
if (notifyThread) {
      // Notify the wathdog thread only after releasing the lock,
// so watchdog can acquire lock on waking up.
futureTimeoutCV_.notify_one();
}
","}
}
if (notifyThread) {
      // Notify the watchdog thread only after releasing the lock,
// so watchdog can acquire lock on waking up.
futureTimeoutCV_.notify_one();
}
"
1184,"if (v->node()->kind() != prim::Constant || v->type()->cast<FunctionType>()) {
return c10::nullopt;
}
  // use implemenation of prim::Constant to compute the output IValue
auto op = getOperation(v->node());
Stack stack;
op(stack);
","if (v->node()->kind() != prim::Constant || v->type()->cast<FunctionType>()) {
return c10::nullopt;
}
  // use implementation of prim::Constant to compute the output IValue
auto op = getOperation(v->node());
Stack stack;
op(stack);
"
1185,"// Run the user-supplied function
py_func_output = f(*args_tup);
        // Convert the output of the user-supplied funciton to IValue. The type
// information of this IValue is used both to record the correct type in
// the trace.
output_ivalue = toTypeInferredIValue(py_func_output);
","// Run the user-supplied function
py_func_output = f(*args_tup);
        // Convert the output of the user-supplied function to IValue. The type
// information of this IValue is used both to record the correct type in
// the trace.
output_ivalue = toTypeInferredIValue(py_func_output);
"
1186,"case prim::ListUnpack:
case prim::PythonOp:
case prim::GetAttr:
    case prim::unchecked_cast:
return analyzeExtractor(node);
case prim::ConstantChunk:
return analyzeChunk(node);
case prim::BroadcastingChunk:
","case prim::ListUnpack:
case prim::PythonOp:
case prim::GetAttr:
return analyzeExtractor(node);
    case prim::unchecked_cast:
      return makePointerTo(node->output(), node->input());
case prim::ConstantChunk:
return analyzeChunk(node);
case prim::BroadcastingChunk:
"
1187,"// kill outputs
liveness -= toSparseBitVector(it->outputs());
if (it->kind() == prim::Loop) {
        auto loop_block = liveness;
// loop's outputs aren't live inside the loop
// loop's block outputs, OTOH, will be considered
// as uses

        LoopView lv(it);
WithInsertPoint guard(*lv.bodyBlock()->nodes().end());
// temporary make loop counts live for the duration of the loop
// as they are needed by BailOuts in the loop
","// kill outputs
liveness -= toSparseBitVector(it->outputs());
if (it->kind() == prim::Loop) {
        LoopView lv(it);
        // N.B. merge in changes from the loop header
        auto loop_header = *lv.bodyBlock()->nodes().begin();
        auto loop_block = liveness | liveness_sets_[loop_header];
// loop's outputs aren't live inside the loop
// loop's block outputs, OTOH, will be considered
// as uses
WithInsertPoint guard(*lv.bodyBlock()->nodes().end());
// temporary make loop counts live for the duration of the loop
// as they are needed by BailOuts in the loop
"
1188,"expr = py_args.vararg
ctx_range = ctx.make_range(expr.lineno, expr.col_offset - 1, expr.col_offset + len(expr.arg))
raise NotSupportedError(ctx_range, _vararg_kwarg_err)
    if not PY2 and py_args.kw_defaults:
        raise NotSupportedError(ctx_range, _vararg_kwarg_err)
result = [build_param(ctx, arg, self_name, False) for arg in py_args.args]
if not PY2:
        result += [build_params(ctx, arg, self_name, True) for arg in py_args.kwonlyargs]
return result
","expr = py_args.vararg
ctx_range = ctx.make_range(expr.lineno, expr.col_offset - 1, expr.col_offset + len(expr.arg))
raise NotSupportedError(ctx_range, _vararg_kwarg_err)
    if not PY2 and len(py_args.kw_defaults) > 0:
        # kw_defaults is a list of the values for the kwargs (which default to None),
        # so they don't actually have line numbers.
        for arg in py_args.kw_defaults:
            if arg is not None:
                ctx_range = build_expr(ctx, arg).range()
                raise NotSupportedError(ctx_range, _vararg_kwarg_err)
result = [build_param(ctx, arg, self_name, False) for arg in py_args.args]
if not PY2:
        result += [build_param(ctx, arg, self_name, True) for arg in py_args.kwonlyargs]
return result
"
1189,"# and later one we can config docker.pytorch.org to point to the location
client.put_object(
        Bucket=""ossci-docker"",
ACL=""public-read"",
Key=""{project}.html"".format(project=project),
Body=html_body,
","# and later one we can config docker.pytorch.org to point to the location
client.put_object(
        Bucket=""docker.pytorch.org"",
ACL=""public-read"",
Key=""{project}.html"".format(project=project),
Body=html_body,
"
