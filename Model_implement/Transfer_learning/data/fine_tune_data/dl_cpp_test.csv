,source,target
0,"std::vector<py::obj<Dim>> dims;
size_t size = s.size();
dims.reserve(size);
            for (ssize_t i = 0; i < size; ++i) {
auto r = s[i];
if (py::is_int(r)) {
dims.emplace_back(Dim::create(py::unicode_from_format(""%S%i"", self->name_.ptr(), (int)i),  py::to_int(r)));
","std::vector<py::obj<Dim>> dims;
size_t size = s.size();
dims.reserve(size);
            for (size_t i = 0; i < size; ++i) {
auto r = s[i];
if (py::is_int(r)) {
dims.emplace_back(Dim::create(py::unicode_from_format(""%S%i"", self->name_.ptr(), (int)i),  py::to_int(r)));
"
1,"} else {
dim_name_str = ""dim"";
}
    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction) patched_dim_method, std::move(dim_name_str));
if (dim_offset.ptr()) {
info->dim_offset = py::to_int(dim_offset);
}
","} else {
dim_name_str = ""dim"";
}
    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction)(void*) patched_dim_method, std::move(dim_name_str));
if (dim_offset.ptr()) {
info->dim_offset = py::to_int(dim_offset);
}
"
2,"size_t size_at_dim = 0;
for (const auto i : c10::irange(materialized.size())) {
const Tensor& t = materialized[i];
if (!at::native::cat_should_skip_tensor(t)) {
at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);
size_at_dim += t.size(dim);
all_contiguous = all_contiguous && t.is_contiguous(memory_format);
        all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();
all_same_sizes_and_stride = all_same_sizes_and_stride &&
t.sizes() == materialized[valid].get().sizes() &&
t.strides() == materialized[valid].get().strides();
","size_t size_at_dim = 0;
for (const auto i : c10::irange(materialized.size())) {
const Tensor& t = materialized[i];
      all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();
if (!at::native::cat_should_skip_tensor(t)) {
at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);
size_at_dim += t.size(dim);
all_contiguous = all_contiguous && t.is_contiguous(memory_format);
all_same_sizes_and_stride = all_same_sizes_and_stride &&
t.sizes() == materialized[valid].get().sizes() &&
t.strides() == materialized[valid].get().strides();
"
3,"const auto input_ = moveBatchDimToFront(input, input_bdim);
auto weight_flatten = moveBatchDimToFront(weight, weight_bdim);
if (weight_flatten.dim() > 1) {
// for an input [N, C, ...]
// weight can be a non-vector but the total number of elements must be the same as C
","const auto input_ = moveBatchDimToFront(input, input_bdim);
auto weight_flatten = moveBatchDimToFront(weight, weight_bdim);
  const auto weight_logical_dim = rankWithoutBatchDim(weight, weight_bdim);
  TORCH_CHECK(weight_logical_dim == 0 || weight_logical_dim == 1,
      ""prelu: Expected `weight` to be a scalar or 1D tensor, but got ndim = "",
      weight_logical_dim);

if (weight_flatten.dim() > 1) {
// for an input [N, C, ...]
// weight can be a non-vector but the total number of elements must be the same as C
"
4,"// End event should only be recorded after the ncclGroupEnd()
for (const auto i : c10::irange(devices.size())) {
at::cuda::CUDAStream& ncclStream = ncclStreams[i];
    (*work->ncclEndEvents_)[i].record(ncclStream);
work->ncclComms_[i] = ncclComms[i];
}
","// End event should only be recorded after the ncclGroupEnd()
for (const auto i : c10::irange(devices.size())) {
at::cuda::CUDAStream& ncclStream = ncclStreams[i];
    if (!coalescing_active_) {
      (*work->ncclEndEvents_)[i].record(ncclStream);
    }
work->ncclComms_[i] = ncclComms[i];
}
"
5,".def(
""_get_backend_name"",
&::c10d::ProcessGroup::getBackendName,
py::call_guard<py::gil_scoped_release>());
// base ProcessGroup::Options binding
",".def(
""_get_backend_name"",
&::c10d::ProcessGroup::getBackendName,
              py::call_guard<py::gil_scoped_release>())
          .def(
              ""_start_coalescing"",
              &::c10d::ProcessGroup::startCoalescing,
              py::call_guard<py::gil_scoped_release>())
          .def(
              ""_end_coalescing"",
              &::c10d::ProcessGroup::endCoalescing,
              py::arg(""reqs""),
py::call_guard<py::gil_scoped_release>());
// base ProcessGroup::Options binding
"
6,"auto* output_data = output.data_ptr<uint8_t>();
#ifdef USE_FBGEMM
  if (weight.scalar_type() == at::ScalarType::Half) {
    const auto weight_data = static_cast<fbgemm::float16*>(weight.data_ptr());
at::parallel_for(
0, embedding_rows, 1, [&](int64_t start_idx, int64_t end_idx) {
fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<
","auto* output_data = output.data_ptr<uint8_t>();
#ifdef USE_FBGEMM
  if (weight_contig->scalar_type() == at::ScalarType::Half) {
    const auto weight_data =
        static_cast<fbgemm::float16*>(weight_contig->data_ptr());
at::parallel_for(
0, embedding_rows, 1, [&](int64_t start_idx, int64_t end_idx) {
fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<
"
7,"output_data + start_idx * output_columns);
});
} else {
    const auto weight_data = weight.data_ptr<float>();
at::parallel_for(
0, embedding_rows, 1, [&](int64_t start_idx, int64_t end_idx) {
fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
","output_data + start_idx * output_columns);
});
} else {
    const auto weight_data = weight_contig->data_ptr<float>();
at::parallel_for(
0, embedding_rows, 1, [&](int64_t start_idx, int64_t end_idx) {
fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
"
8,"namespace at { namespace functorch {
static void handleScalarTypePromotion(Tensor& logical_scalar_tensor, Tensor& second) {
  auto result_type = at::native::result_type(logical_scalar_tensor[0], second);
  if (logical_scalar_tensor.scalar_type() != result_type) {
    logical_scalar_tensor = logical_scalar_tensor.to(result_type);
  }
  if (second.scalar_type() != result_type) {
    second = second.to(result_type);
  }
}

std::tuple<Tensor, Tensor> _binary_pointwise_helper(
    const Tensor& tensor, optional<int64_t> tensor_batch_dim,
    const Tensor& other, optional<int64_t> other_batch_dim) {
  // compute max logical rank
  auto tensor_logical_rank = rankWithoutBatchDim(tensor, tensor_batch_dim);
  auto other_logical_rank = rankWithoutBatchDim(other, other_batch_dim);
  auto max_logical_rank = std::max(tensor_logical_rank, other_logical_rank);

  auto tensor_ = moveBatchDimToFront(tensor, tensor_batch_dim);
  auto other_ = moveBatchDimToFront(other, other_batch_dim);

  // In the (0D, ND) case, type promotion semantics are different :/
  auto tensor_is_logical_scalar = (tensor_logical_rank == 0 && tensor_batch_dim.has_value());
  auto other_is_logical_scalar = (other_logical_rank == 0 && other_batch_dim.has_value());
  if (tensor_is_logical_scalar && !other_is_logical_scalar) {
    handleScalarTypePromotion(tensor_, other_);
  }
  if (other_is_logical_scalar && !tensor_is_logical_scalar) {
    handleScalarTypePromotion(other_, tensor_);
  }

  // If the dimensions aren't aligned, we need to line them up.
  // Tensor[B, 3] + Tensor[2, 5, 3] -> Tensor[B, 1, 1, 3] + Tensor[2, 5, 3]
  // Note that only tensors that have a batch dim need to be modified.
  // Tensor[B, 2, 3, 5] + Tensor[5] -> no changes needed
  tensor_ = maybePadToLogicalRank(tensor_, tensor_batch_dim, max_logical_rank);
  other_ = maybePadToLogicalRank(other_, other_batch_dim, max_logical_rank);

  return std::make_tuple(tensor_, other_);
}

template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
const Tensor& tensor, optional<int64_t> tensor_batch_dim,
","namespace at { namespace functorch {
template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> _binary_pointwise_batch_rule(
const Tensor& tensor, optional<int64_t> tensor_batch_dim,
"
9,"// even though it is possible, though rare, for someone to mutate them
return toSugaredValue(member, m, loc, /*is_constant=*/true);
}
#endif
Value* ModuleValue::asValue(const SourceRange& loc, GraphFunction& m) {
return self_;
","// even though it is possible, though rare, for someone to mutate them
return toSugaredValue(member, m, loc, /*is_constant=*/true);
}
Value* ModuleValue::asValue(const SourceRange& loc, GraphFunction& m) {
return self_;
"
10,"}
Tensor narrow_copy_symint(const Tensor& self, int64_t dim, int64_t start, SymInt sym_length) {
  return narrow_copy(self, dim, start, sym_length.expect_int());
}
Tensor narrow_copy_dense(const Tensor& self, int64_t dim, int64_t start, int64_t length) {
","}
Tensor narrow_copy_symint(const Tensor& self, int64_t dim, int64_t start, SymInt sym_length) {
  return self.narrow_copy(dim, start, sym_length.expect_int());
}
Tensor narrow_copy_dense(const Tensor& self, int64_t dim, int64_t start, int64_t length) {
"
11,"return true;
}
} // anonymous namespace
#if !defined(USE_ROCM)
","return true;
}

bool isEmptyContainer(const py::handle self) {
  bool is_empty_list =
      PySequence_Check(self.ptr()) && !PySequence_Size(self.ptr());
  return is_empty_list;
}
} // anonymous namespace
#if !defined(USE_ROCM)
"
12,"#ifndef USE_NUMPY
namespace torch {
namespace utils {
PyObject* tensor_to_numpy(const at::Tensor& tensor) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
at::Tensor tensor_from_numpy(
","#ifndef USE_NUMPY
namespace torch {
namespace utils {
PyObject* tensor_to_numpy(const at::Tensor&, bool) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
at::Tensor tensor_from_numpy(
"
13,"VMAP_SUPPORT2(slice, Tensor, slice_batch_rule);
VMAP_SUPPORT2(transpose, int, transpose_int_batch_rule);
VMAP_SUPPORT(diag_embed, diag_embed_batch_rule);
}
}}
","VMAP_SUPPORT2(slice, Tensor, slice_batch_rule);
VMAP_SUPPORT2(transpose, int, transpose_int_batch_rule);
VMAP_SUPPORT(diag_embed, diag_embed_batch_rule);
  m.impl(""expand.SymInt"", expand_symint_decomp_hack);
}
}}
"
14,"if (self.dim() < 2) {
channel_dim = 0;
}
  auto self_ = self;
auto target_ = target.unsqueeze(channel_dim);
auto grad_output_ = grad_output;
","if (self.dim() < 2) {
channel_dim = 0;
}
auto target_ = target.unsqueeze(channel_dim);
auto grad_output_ = grad_output;
"
15,"std::string schema_name = op.schema().name();
std::string vjp_fn_name = schema_name + ""_vjp"";
  auto vjp_fn = c10::Dispatcher::singleton()
    .findSchemaOrThrow(vjp_fn_name.c_str(), """");
std::shared_ptr<GenericPythonBackward> grad_fn;
if (_any_requires_grad) {
grad_fn = std::shared_ptr<GenericPythonBackward>(new GenericPythonBackward(), deleteNode);
grad_fn->set_next_edges(collect_next_edges(tensors));
    grad_fn->backward_fn_ = std::move(vjp_fn);
grad_fn->num_inputs_ = tensors_.size();
}
","std::string schema_name = op.schema().name();
std::string vjp_fn_name = schema_name + ""_vjp"";
std::shared_ptr<GenericPythonBackward> grad_fn;
if (_any_requires_grad) {
grad_fn = std::shared_ptr<GenericPythonBackward>(new GenericPythonBackward(), deleteNode);
grad_fn->set_next_edges(collect_next_edges(tensors));
    grad_fn->backward_fn_ = c10::Dispatcher::singleton().findSchemaOrThrow(vjp_fn_name.c_str(), """");
grad_fn->num_inputs_ = tensors_.size();
}
"
16,"std::vector<Tensor> makeBatchedVector(const std::vector<Tensor>& tensors, optional<int64_t> bdim, int64_t level) {
std::vector<Tensor> res;
  for (size_t idx = 0; idx < tensors.size(); idx++) {
    res.emplace_back(makeBatched(tensors[idx], bdim, level));
}
return res;
}
","std::vector<Tensor> makeBatchedVector(const std::vector<Tensor>& tensors, optional<int64_t> bdim, int64_t level) {
std::vector<Tensor> res;
  for (const auto & tensor : tensors) {
    res.emplace_back(makeBatched(tensor, bdim, level));
}
return res;
}
"
17,"variable_list grad_inputs(num_inputs_);
std::vector<Tensor> args;
  for (const auto& g : grads) {
    args.push_back(g);
}
for (const auto& saved : saved_tensors_) {
    args.push_back(saved.unpack(shared_from_this()));
}
if (should_compute_output({ tensors_ix })) {
","variable_list grad_inputs(num_inputs_);
std::vector<Tensor> args;
  for (auto& g : grads) {
    args.emplace_back(std::move(g));
}
for (const auto& saved : saved_tensors_) {
    args.emplace_back(saved.unpack(shared_from_this()));
}
if (should_compute_output({ tensors_ix })) {
"
18,"return self.squeeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
  TORCH_CHECK(batched->bdim() == 0);
auto logical_dim = self.dim();
auto dim_physical = 1 + maybe_wrap_dim(dim, logical_dim);
batched->value().squeeze_(dim_physical);
","return self.squeeze_(dim);
}
auto* batched = maybeGetBatchedImpl(self);
  TORCH_CHECK(batched && batched->bdim() == 0);
auto logical_dim = self.dim();
auto dim_physical = 1 + maybe_wrap_dim(dim, logical_dim);
batched->value().squeeze_(dim_physical);
"
19,"BINARY_POINTWISE(leaky_relu_backward);
BINARY_POINTWISE(logit_backward);
POINTWISE_BOXED(log_sigmoid_backward);
  BINARY_POINTWISE(gelu_backward);
BINARY_POINTWISE(sigmoid_backward);
POINTWISE_BOXED(softplus_backward);
BINARY_POINTWISE(softshrink_backward);
","BINARY_POINTWISE(leaky_relu_backward);
BINARY_POINTWISE(logit_backward);
POINTWISE_BOXED(log_sigmoid_backward);
  VMAP_SUPPORT(gelu_backward, gelu_backward_batch_rule);
BINARY_POINTWISE(sigmoid_backward);
POINTWISE_BOXED(softplus_backward);
BINARY_POINTWISE(softshrink_backward);
"
20,"return at::native_group_norm_backward(grad_out, input, mean, rstd, weight_opt, N, C, HxW, group, output_mask);
}
  Tensor grad_out_value;
  optional<int64_t> grad_out_bdim;
  std::tie(grad_out_value, grad_out_bdim) = unwrapTensorAtLevel(grad_out, cur_level);
Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
","return at::native_group_norm_backward(grad_out, input, mean, rstd, weight_opt, N, C, HxW, group, output_mask);
}
Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
"
21,"TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
UNSUPPORTED_DYNAMIC(nonzero);
UNSUPPORTED_DYNAMIC(unique);
m.impl(""_local_scalar_dense"", torch::CppFunction::makeFromBoxedFunction<&unsupportedLocalScalarDense>());
m.impl(""item"", torch::CppFunction::makeFromBoxedFunction<&unsupportedItem>());
","TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
UNSUPPORTED_DYNAMIC(nonzero);
    UNSUPPORTED_DYNAMIC(where);
UNSUPPORTED_DYNAMIC(unique);
m.impl(""_local_scalar_dense"", torch::CppFunction::makeFromBoxedFunction<&unsupportedLocalScalarDense>());
m.impl(""item"", torch::CppFunction::makeFromBoxedFunction<&unsupportedItem>());
"
22,"TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
","TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  if (!areAnyBatchedAtLevel({input, weight_opt, bias_opt}, cur_level)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return at::native_group_norm(input, weight_opt, bias_opt, N, C, HxW, group, eps);
  }

Tensor input_value;
optional<int64_t> input_bdim;
std::tie(input_value, input_bdim) = unwrapTensorAtLevel(input, cur_level);
"
23,"const auto& schema = op.schema();
const auto num_returns = schema.returns().size();
const auto num_arguments = schema.arguments().size();
  auto arguments = torch::jit::pop(*stack, num_arguments);
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
std::vector<std::pair<Tensor, optional<int64_t>>> tensor_inputs;
std::vector<int64_t> tensor_pos;
","const auto& schema = op.schema();
const auto num_returns = schema.returns().size();
const auto num_arguments = schema.arguments().size();
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  auto orig_arguments = torch::jit::last(*stack, num_arguments);
  if (std::none_of(orig_arguments.begin(), orig_arguments.end(), ivalueParticipatesInCurrentLevel)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    op.callBoxed(stack);
    return;
  }

  auto arguments = torch::jit::pop(*stack, num_arguments);
std::vector<std::pair<Tensor, optional<int64_t>>> tensor_inputs;
std::vector<int64_t> tensor_pos;
"
24,"return apply_loss_reduction(loss, reduction);
}
TORCH_LIBRARY_IMPL(aten, FT_DYNAMIC_LAYER_FRONT_MODE_KEY, m) {
m.impl(""value_selecting_reduction_backward"", value_selecting_reduction_backward_hack);
m.impl(""index_select_backward"", index_select_backward_hack);
","return apply_loss_reduction(loss, reduction);
}
Tensor trace_backward_decomp(const Tensor& grad, IntArrayRef sizes) {
  if (sizes.size() != 2) {
    throw std::runtime_error(""expected matrix input"");
  }
  auto grad_input = at::zeros(sizes[0] * sizes[1], grad.options());
  auto indices = at::arange(0, grad_input.numel(), sizes[1] + 1, grad.options().dtype(at::kLong));
  // Workaround using index_put instead of yet unsupported index_fill_
  grad_input = grad_input.index_put({indices}, grad);
  return grad_input.view(sizes);
}

TORCH_LIBRARY_IMPL(aten, FT_DYNAMIC_LAYER_FRONT_MODE_KEY, m) {
m.impl(""value_selecting_reduction_backward"", value_selecting_reduction_backward_hack);
m.impl(""index_select_backward"", index_select_backward_hack);
"
25,"}
// ['_masked_scale', 'native_dropout_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_14_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double);
template <>
Tensor lowerToNextLayer<batch_rule_14_t,Tensor,const Tensor &, const Tensor &, double>(
  batch_rule_14_t batch_rule,
const Tensor & self, const Tensor & mask, double scale
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_masked_scale', 'native_dropout_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_15_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double);
template <>
Tensor lowerToNextLayer<batch_rule_15_t,Tensor,const Tensor &, const Tensor &, double>(
  batch_rule_15_t batch_rule,
const Tensor & self, const Tensor & mask, double scale
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
26,"}
// ['addmv', 'addr', 'baddbmm', 'sspaddmm', '_sparse_addmm', 'sparse_sampled_addmm', 'addmm', 'addbmm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_23_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_23_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &>(
  batch_rule_23_t batch_rule,
const Tensor & self, const Tensor & mat, const Tensor & vec, const Scalar & beta, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['addmv', 'addr', 'baddbmm', 'sspaddmm', '_sparse_addmm', 'sparse_sampled_addmm', 'addmm', 'addbmm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_24_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_24_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &>(
  batch_rule_24_t batch_rule,
const Tensor & self, const Tensor & mat, const Tensor & vec, const Scalar & beta, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
27,"}
// ['binary_cross_entropy']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_37_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_37_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_37_t batch_rule,
const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['binary_cross_entropy']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_38_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_38_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_38_t batch_rule,
const Tensor & self, const Tensor & target, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
28,"if (ggb) {
std::tie(ggb_value, ggb_bdim) = unwrapTensorAtLevel(ggb.value(), cur_level);
}
  auto results = batch_rule(ggI_value, ggI_bdim, ggW_value, ggW_bdim, ggb_value, ggb_bdim, gO_value, gO_bdim, weight_value, weight_bdim, self_value, self_bdim, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, output_mask);
return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['conv1d', 'conv2d', 'conv3d', 'cudnn_convolution_relu', 'mkldnn_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_57_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_57_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t>(
  batch_rule_57_t batch_rule,
const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","if (ggb) {
std::tie(ggb_value, ggb_bdim) = unwrapTensorAtLevel(ggb.value(), cur_level);
}
  auto results = batch_rule(ggI_value, ggI_bdim, ggW_value, ggW_bdim, ggb_value, ggb_bdim, gO_value, gO_bdim, weight_value, weight_bdim, self_value, self_bdim, stride, padding, dilation, transposed, output_padding, groups, output_mask);
return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['conv1d', 'conv2d', 'conv3d', 'cudnn_convolution_relu', 'mkldnn_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_58_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_58_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t>(
  batch_rule_58_t batch_rule,
const Tensor & input, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
29,"}
// ['conv_tbc', 'cummaxmin_backward', '_make_per_channel_quantized_tensor', 'mse_loss_backward', 'l1_loss_backward', 'soft_margin_loss_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_58_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_58_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_58_t batch_rule,
const Tensor & self, const Tensor & weight, const Tensor & bias, int64_t pad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['conv_tbc', 'cummaxmin_backward', '_make_per_channel_quantized_tensor', 'mse_loss_backward', 'l1_loss_backward', 'soft_margin_loss_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_59_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_59_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_59_t batch_rule,
const Tensor & self, const Tensor & weight, const Tensor & bias, int64_t pad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
30,"Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution_backward_input', 'cudnn_convolution_backward_weight', 'cudnn_convolution_transpose_backward_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_71_t)(IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_71_t,Tensor,IntArrayRef, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool>(
batch_rule_71_t batch_rule,
  IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_size, grad_output_value, grad_output_bdim, weight_value, weight_bdim, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_72_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool, ::std::array<bool,2>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_72_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool, ::std::array<bool,2>>(
batch_rule_72_t batch_rule,
  const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, ::std::array<bool,2> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_convolution_add_relu']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_71_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_71_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const c10::optional<Scalar> &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t>(
batch_rule_71_t batch_rule,
  const Tensor & self, const Tensor & weight, const Tensor & z, const c10::optional<Scalar> & alpha, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  Tensor z_value;
  optional<int64_t> z_bdim;
  std::tie(z_value, z_bdim) = unwrapTensorAtLevel(z, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, z_value, z_bdim, alpha, bias_value, bias_bdim, stride, padding, dilation, groups);
return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['cudnn_grid_sampler_backward', 'prelu_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_72_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_72_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &>(
batch_rule_72_t batch_rule,
  const Tensor & self, const Tensor & grid, const Tensor & grad_output
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
31,"}
// ['cumulative_trapezoid.dx', 'trapezoid.dx']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_83_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_83_t,Tensor,const Tensor &, const Scalar &, int64_t>(
  batch_rule_83_t batch_rule,
const Tensor & y, const Scalar & dx, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['cumulative_trapezoid.dx', 'trapezoid.dx']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_77_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_77_t,Tensor,const Tensor &, const Scalar &, int64_t>(
  batch_rule_77_t batch_rule,
const Tensor & y, const Scalar & dx, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
32,"}
// ['gradient.scalarrayarray']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_95_t)(const Tensor &, c10::optional<int64_t>, ArrayRef<Scalar>, IntArrayRef, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_95_t,::std::vector<Tensor>,const Tensor &, ArrayRef<Scalar>, IntArrayRef, int64_t>(
  batch_rule_95_t batch_rule,
const Tensor & self, ArrayRef<Scalar> spacing, IntArrayRef dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['gradient.scalarrayarray']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_89_t)(const Tensor &, c10::optional<int64_t>, ArrayRef<Scalar>, IntArrayRef, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_89_t,::std::vector<Tensor>,const Tensor &, ArrayRef<Scalar>, IntArrayRef, int64_t>(
  batch_rule_89_t batch_rule,
const Tensor & self, ArrayRef<Scalar> spacing, IntArrayRef dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
33,"}
// ['new_empty', 'new_zeros', 'new_ones']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_108_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_108_t,Tensor,const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_108_t batch_rule,
const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['new_empty', 'new_zeros', 'new_ones']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_102_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_102_t,Tensor,const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_102_t batch_rule,
const Tensor & self, IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
34,"}
// ['flatten.named_out_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_115_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_115_t,Tensor,const Tensor &, int64_t, int64_t, Dimname>(
  batch_rule_115_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['flatten.named_out_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_109_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, Dimname);
template <>
Tensor lowerToNextLayer<batch_rule_109_t,Tensor,const Tensor &, int64_t, int64_t, Dimname>(
  batch_rule_109_t batch_rule,
const Tensor & self, int64_t start_dim, int64_t end_dim, Dimname out_dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
35,"}
// ['_fft_c2r', 'select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_128_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_128_t,Tensor,const Tensor &, IntArrayRef, int64_t, int64_t>(
  batch_rule_128_t batch_rule,
const Tensor & self, IntArrayRef dim, int64_t normalization, int64_t last_dim_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_fft_c2r', 'select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_122_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_122_t,Tensor,const Tensor &, IntArrayRef, int64_t, int64_t>(
  batch_rule_122_t batch_rule,
const Tensor & self, IntArrayRef dim, int64_t normalization, int64_t last_dim_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
36,"}
// ['isclose', 'pairwise_distance']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_133_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_133_t,Tensor,const Tensor &, const Tensor &, double, double, bool>(
  batch_rule_133_t batch_rule,
const Tensor & self, const Tensor & other, double rtol, double atol, bool equal_nan
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['isclose', 'pairwise_distance']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_127_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, double, double, bool);
template <>
Tensor lowerToNextLayer<batch_rule_127_t,Tensor,const Tensor &, const Tensor &, double, double, bool>(
  batch_rule_127_t batch_rule,
const Tensor & self, const Tensor & other, double rtol, double atol, bool equal_nan
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
37,"}
// ['kl_div_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_139_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_139_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, bool>(
  batch_rule_139_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, bool log_target
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['kl_div_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_132_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_132_t,Tensor,const Tensor &, const Tensor &, const Tensor &, int64_t, bool>(
  batch_rule_132_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, bool log_target
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
38,"}
// ['native_layer_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_143_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_143_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, IntArrayRef, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_143_t batch_rule,
const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['native_layer_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_136_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_136_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, IntArrayRef, const c10::optional<Tensor> &, const c10::optional<Tensor> &, double>(
  batch_rule_136_t batch_rule,
const Tensor & input, IntArrayRef normalized_shape, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, double eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
39,"}
// ['logsumexp.names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_156_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_156_t,Tensor,const Tensor &, DimnameList, bool>(
  batch_rule_156_t batch_rule,
const Tensor & self, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['logsumexp.names']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_150_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool);
template <>
Tensor lowerToNextLayer<batch_rule_150_t,Tensor,const Tensor &, DimnameList, bool>(
  batch_rule_150_t batch_rule,
const Tensor & self, DimnameList dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
40,"Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(self_value, self_bdim, grad_output_value, grad_output_bdim, weight_value, weight_bdim, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['miopen_convolution_transpose_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_175_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_175_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, ::std::array<bool,3>>(
  batch_rule_175_t batch_rule,
  const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","Tensor self_value;
optional<int64_t> self_bdim;
std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, bias_value, bias_bdim, padding, stride, dilation, groups, benchmark, deterministic);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['miopen_convolution_transpose']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_165_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_165_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
  batch_rule_165_t batch_rule,
  const Tensor & self, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
41,"}
// ['batch_norm_backward_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_183_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_183_t,std::tuple<Tensor,Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, bool, bool, bool>(
  batch_rule_183_t batch_rule,
const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const c10::optional<Tensor> & weight, bool input_g, bool weight_g, bool bias_g
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['batch_norm_backward_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_173_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_173_t,std::tuple<Tensor,Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, bool, bool, bool>(
  batch_rule_173_t batch_rule,
const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const c10::optional<Tensor> & weight, bool input_g, bool weight_g, bool bias_g
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
42,"}
// ['slice_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_209_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_209_t,Tensor,const Tensor &, IntArrayRef, int64_t, int64_t, int64_t, int64_t>(
  batch_rule_209_t batch_rule,
const Tensor & grad_output, IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['slice_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_198_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_198_t,Tensor,const Tensor &, IntArrayRef, int64_t, int64_t, int64_t, int64_t>(
  batch_rule_198_t batch_rule,
const Tensor & grad_output, IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
43,"}
// ['rot90']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_230_t)(const Tensor &, c10::optional<int64_t>, int64_t, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_230_t,Tensor,const Tensor &, int64_t, IntArrayRef>(
  batch_rule_230_t batch_rule,
const Tensor & self, int64_t k, IntArrayRef dims
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['rot90']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_219_t)(const Tensor &, c10::optional<int64_t>, int64_t, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_219_t,Tensor,const Tensor &, int64_t, IntArrayRef>(
  batch_rule_219_t batch_rule,
const Tensor & self, int64_t k, IntArrayRef dims
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
44,"}
// ['_weight_norm_cuda_interface_backward', '_weight_norm_differentiable_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_242_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_242_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_242_t batch_rule,
const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_weight_norm_cuda_interface_backward', '_weight_norm_differentiable_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_231_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_231_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_231_t batch_rule,
const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
45,"}
// ['_fake_quantize_per_tensor_affine_cachemask_tensor_qparams']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_270_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_270_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t>(
  batch_rule_270_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, const Tensor & fake_quant_enabled, int64_t quant_min, int64_t quant_max
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_fake_quantize_per_tensor_affine_cachemask_tensor_qparams']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_259_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_259_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t>(
  batch_rule_259_t batch_rule,
const Tensor & self, const Tensor & scale, const Tensor & zero_point, const Tensor & fake_quant_enabled, int64_t quant_min, int64_t quant_max
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
46,"}
// ['_autocast_to_full_precision']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_280_t)(const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_280_t,Tensor,const Tensor &, bool, bool>(
  batch_rule_280_t batch_rule,
const Tensor & self, bool cuda_enabled, bool cpu_enabled
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_autocast_to_full_precision']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_269_t)(const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_269_t,Tensor,const Tensor &, bool, bool>(
  batch_rule_269_t batch_rule,
const Tensor & self, bool cuda_enabled, bool cpu_enabled
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
47,"}
// ['result_type.Scalar']
typedef std::tuple<ScalarType> (*batch_rule_288_t)(const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
ScalarType lowerToNextLayer<batch_rule_288_t,ScalarType,const Tensor &, const Scalar &>(
  batch_rule_288_t batch_rule,
const Tensor & tensor, const Scalar & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['result_type.Scalar']
typedef std::tuple<ScalarType> (*batch_rule_277_t)(const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
ScalarType lowerToNextLayer<batch_rule_277_t,ScalarType,const Tensor &, const Scalar &>(
  batch_rule_277_t batch_rule,
const Tensor & tensor, const Scalar & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
48,"}
// ['gru_cell', 'rnn_tanh_cell', 'rnn_relu_cell']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_296_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_296_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &>(
  batch_rule_296_t batch_rule,
const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih, const c10::optional<Tensor> & b_hh
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['gru_cell', 'rnn_tanh_cell', 'rnn_relu_cell']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_285_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_285_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &>(
  batch_rule_285_t batch_rule,
const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const c10::optional<Tensor> & b_ih, const c10::optional<Tensor> & b_hh
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
49,"}
// ['quantized_gru_cell', 'quantized_rnn_relu_cell', 'quantized_rnn_tanh_cell']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_297_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_297_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, const Scalar &, const Scalar &>(
  batch_rule_297_t batch_rule,
const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['quantized_gru_cell', 'quantized_rnn_relu_cell', 'quantized_rnn_tanh_cell']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_286_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Scalar &, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_286_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, const Scalar &, const Scalar &>(
  batch_rule_286_t batch_rule,
const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, const Scalar & scale_ih, const Scalar & scale_hh, const Scalar & zero_point_ih, const Scalar & zero_point_hh
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
50,"}
// ['cross', 'take_along_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_310_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_310_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>>(
  batch_rule_310_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<int64_t> dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['cross', 'take_along_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_299_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_299_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>>(
  batch_rule_299_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<int64_t> dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
51,"}
// ['index_select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_313_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_313_t,Tensor,const Tensor &, IntArrayRef, int64_t, const Tensor &>(
  batch_rule_313_t batch_rule,
const Tensor & grad, IntArrayRef self_sizes, int64_t dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['index_select_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_302_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_302_t,Tensor,const Tensor &, IntArrayRef, int64_t, const Tensor &>(
  batch_rule_302_t batch_rule,
const Tensor & grad, IntArrayRef self_sizes, int64_t dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
52,"}
// ['lu_unpack']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_324_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_324_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_324_t batch_rule,
const Tensor & LU_data, const Tensor & LU_pivots, bool unpack_data, bool unpack_pivots
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['lu_unpack']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_313_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_313_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_313_t batch_rule,
const Tensor & LU_data, const Tensor & LU_pivots, bool unpack_data, bool unpack_pivots
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
53,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['quantile.scalar', 'nanquantile.scalar']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_332_t)(const Tensor &, c10::optional<int64_t>, double, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_332_t,Tensor,const Tensor &, double, c10::optional<int64_t>, bool>(
  batch_rule_332_t batch_rule,
  const Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  auto results = batch_rule(self_value, self_bdim, q, dim, keepdim);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['quantile', 'nanquantile']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_333_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_333_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, bool>(
  batch_rule_333_t batch_rule,
  const Tensor & self, const Tensor & q, c10::optional<int64_t> dim, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['quantile', 'nanquantile']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_321_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, bool, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_321_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, bool, c10::string_view>(
  batch_rule_321_t batch_rule,
  const Tensor & self, const Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
54,"}
// ['sort.dimname_stable']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_337_t)(const Tensor &, c10::optional<int64_t>, c10::optional<bool>, Dimname, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_337_t,std::tuple<Tensor,Tensor>,const Tensor &, c10::optional<bool>, Dimname, bool>(
  batch_rule_337_t batch_rule,
const Tensor & self, c10::optional<bool> stable, Dimname dim, bool descending
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['sort.dimname_stable']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_324_t)(const Tensor &, c10::optional<int64_t>, c10::optional<bool>, Dimname, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_324_t,std::tuple<Tensor,Tensor>,const Tensor &, c10::optional<bool>, Dimname, bool>(
  batch_rule_324_t batch_rule,
const Tensor & self, c10::optional<bool> stable, Dimname dim, bool descending
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
55,"}
// ['renorm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_339_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, int64_t, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_339_t,Tensor,const Tensor &, const Scalar &, int64_t, const Scalar &>(
  batch_rule_339_t batch_rule,
const Tensor & self, const Scalar & p, int64_t dim, const Scalar & maxnorm
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['renorm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_326_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, int64_t, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_326_t,Tensor,const Tensor &, const Scalar &, int64_t, const Scalar &>(
  batch_rule_326_t batch_rule,
const Tensor & self, const Scalar & p, int64_t dim, const Scalar & maxnorm
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
56,"}
// ['multi_margin_loss']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_343_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_343_t,Tensor,const Tensor &, const Tensor &, const Scalar &, const Scalar &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_343_t batch_rule,
const Tensor & self, const Tensor & target, const Scalar & p, const Scalar & margin, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['multi_margin_loss']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_330_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_330_t,Tensor,const Tensor &, const Tensor &, const Scalar &, const Scalar &, const c10::optional<Tensor> &, int64_t>(
  batch_rule_330_t batch_rule,
const Tensor & self, const Tensor & target, const Scalar & p, const Scalar & margin, const c10::optional<Tensor> & weight, int64_t reduction
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
57,"}
// ['max_pool2d_with_indices_backward', 'max_pool3d_with_indices_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_361_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_361_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool, const Tensor &>(
  batch_rule_361_t batch_rule,
const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['max_pool2d_with_indices_backward', 'max_pool3d_with_indices_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_347_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_347_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool, const Tensor &>(
  batch_rule_347_t batch_rule,
const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
58,"}
// ['max_unpool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_363_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_363_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_363_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['max_unpool3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_350_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_350_t,Tensor,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_350_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
59,"}
// ['upsample_trilinear3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_373_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_373_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>>(
  batch_rule_373_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_trilinear3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_360_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_360_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>>(
  batch_rule_360_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
60,"}
// ['upsample_nearest1d_backward', '_upsample_nearest_exact1d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_375_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_375_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>>(
  batch_rule_375_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_nearest1d_backward', '_upsample_nearest_exact1d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_362_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_362_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, c10::optional<double>>(
  batch_rule_362_t batch_rule,
const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
61,"Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  Tensor finput_value;
  optional<int64_t> finput_bdim;
  std::tie(finput_value, finput_bdim) = unwrapTensorAtLevel(finput, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, weight_value, weight_bdim, kernel_size, stride, padding, finput_value, finput_bdim, output_mask);
return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['_conv_depthwise2d', 'conv_depthwise3d', 'slow_conv_dilated2d', 'slow_conv_dilated3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_386_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_386_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_386_t batch_rule,
const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","Tensor weight_value;
optional<int64_t> weight_bdim;
std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, weight_value, weight_bdim, kernel_size, stride, padding, output_mask);
return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['_conv_depthwise2d', 'conv_depthwise3d', 'slow_conv_dilated2d', 'slow_conv_dilated3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_371_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_371_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_371_t batch_rule,
const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
62,"}
// ['fft_fftn', 'fft_ifftn', 'fft_rfftn', 'fft_irfftn', 'fft_hfftn', 'fft_ihfftn']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_395_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, c10::optional<IntArrayRef>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_395_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, c10::optional<IntArrayRef>, c10::optional<c10::string_view>>(
  batch_rule_395_t batch_rule,
const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<c10::string_view> norm
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fft_fftn', 'fft_ifftn', 'fft_rfftn', 'fft_irfftn', 'fft_hfftn', 'fft_ihfftn']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_376_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, c10::optional<IntArrayRef>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_376_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, c10::optional<IntArrayRef>, c10::optional<c10::string_view>>(
  batch_rule_376_t batch_rule,
const Tensor & self, c10::optional<IntArrayRef> s, c10::optional<IntArrayRef> dim, c10::optional<c10::string_view> norm
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
63,"}
// ['linalg_matrix_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_405_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, IntArrayRef, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_405_t,Tensor,const Tensor &, const Scalar &, IntArrayRef, bool, c10::optional<ScalarType>>(
  batch_rule_405_t batch_rule,
const Tensor & self, const Scalar & ord, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['linalg_matrix_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_385_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, IntArrayRef, bool, c10::optional<ScalarType>);
template <>
Tensor lowerToNextLayer<batch_rule_385_t,Tensor,const Tensor &, const Scalar &, IntArrayRef, bool, c10::optional<ScalarType>>(
  batch_rule_385_t batch_rule,
const Tensor & self, const Scalar & ord, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
64,"}
// ['linalg_pinv.atol_rtol_tensor', 'linalg_matrix_rank.atol_rtol_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_409_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_409_t,Tensor,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool>(
  batch_rule_409_t batch_rule,
const Tensor & self, const c10::optional<Tensor> & atol, const c10::optional<Tensor> & rtol, bool hermitian
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['linalg_pinv.atol_rtol_tensor', 'linalg_matrix_rank.atol_rtol_tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_389_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_389_t,Tensor,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool>(
  batch_rule_389_t batch_rule,
const Tensor & self, const c10::optional<Tensor> & atol, const c10::optional<Tensor> & rtol, bool hermitian
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
65,"VARIADIC_BDIMS(pinverse);
VARIADIC_BDIMS(inverse);
VARIADIC_BDIMS_BOXED(slogdet);
  VARIADIC_BDIMS_BOXED(_svd_helper);
VARIADIC_BDIMS_BOXED(solve);
VARIADIC_BDIMS_BOXED(symeig);
VARIADIC_BDIMS_BOXED(triangular_solve);
","VARIADIC_BDIMS(pinverse);
VARIADIC_BDIMS(inverse);
VARIADIC_BDIMS_BOXED(slogdet);
  VARIADIC_BDIMS_BOXED(_linalg_svd);
VARIADIC_BDIMS_BOXED(solve);
VARIADIC_BDIMS_BOXED(symeig);
VARIADIC_BDIMS_BOXED(triangular_solve);
"
66,"for (int i = 0; i < numTensorArgs; ++i) {
PyObject *arg = PyTuple_GET_ITEM(args, i);
if (!THPVariable_Check(arg)) {
        throw std::runtime_error(""Encountered a non-tensor argument. Set up ""
                                 ""static_argnums correctly."");
}
tensorArgs[i] = THPVariable_Unpack(arg);
}
","for (int i = 0; i < numTensorArgs; ++i) {
PyObject *arg = PyTuple_GET_ITEM(args, i);
if (!THPVariable_Check(arg)) {
        std::string dtype = Py_TYPE(arg)->tp_name;
        std::string index = std::to_string(i);
        throw std::runtime_error(""Found an argument of type "" + dtype +
                                 "" at index "" + index +
                                 "". Non-tensor arguments must be marked static.""
                                 "" Please set the static_argnums correctly to ""
                                 ""mark the argument at index "" +
                                 index + "" static."");
}
tensorArgs[i] = THPVariable_Unpack(arg);
}
"
67,"UNARY_POINTWISE_ALL(cosh);
UNARY_POINTWISE(_conj);
UNARY_POINTWISE_ALL(deg2rad);
UNARY_POINTWISE_ALL(digamma);
UNARY_POINTWISE_ALL(erf);
UNARY_POINTWISE_ALL(exp);
","UNARY_POINTWISE_ALL(cosh);
UNARY_POINTWISE(_conj);
UNARY_POINTWISE_ALL(deg2rad);
  UNARY_POINTWISE(detach);
UNARY_POINTWISE_ALL(digamma);
UNARY_POINTWISE_ALL(erf);
UNARY_POINTWISE_ALL(exp);
"
68,"#include <functorch/csrc/Constants.h>
#include <torch/library.h>
#include <ATen/ATen.h>
namespace at { namespace functorch {
","#include <functorch/csrc/Constants.h>
#include <torch/library.h>
#include <ATen/ATen.h>
#include <functorch/csrc/TensorWrapper.h>
#include <functorch/csrc/BatchedTensorImpl.h>
namespace at { namespace functorch {
"
69,"dynamicLayerStack.pop_back();
if (dynamicLayerStack.size() == 0) {
    // std::cout << ""DynamicLayer off"" << std::endl;
c10::impl::tls_set_dispatch_key_included(kDynamicLayerFrontModeKey, false);
c10::impl::tls_set_dispatch_key_included(kDynamicLayerBackModeKey, false);
}
","dynamicLayerStack.pop_back();
if (dynamicLayerStack.size() == 0) {
#ifdef HAS_TORCH_SHOW_DISPATCH_TRACE
    if (c10::show_dispatch_trace_enabled()) {
      std::cout << ""DynamicLayer off"" << std::endl;
    }
#endif
c10::impl::tls_set_dispatch_key_included(kDynamicLayerFrontModeKey, false);
c10::impl::tls_set_dispatch_key_included(kDynamicLayerBackModeKey, false);
}
"
70,"}
}
static bool allTensors(
ArrayRef<IValue> args,
std::function<bool(const Tensor&)> pred) {
","}
}
std::ostream& operator<< (std::ostream& os, const DynamicLayer& layer) {
  os << layer.layerId() << "":"" << layer.key();
  return os;
}
std::ostream& operator<< (std::ostream& os, const std::vector<DynamicLayer>& dls) {
  os << ""DynamicLayerStack[ "";
  for (const auto& layer : dls) {
    os << layer << "" "";
  }
  os << ""]"";
  return os;
}

static bool allTensors(
ArrayRef<IValue> args,
std::function<bool(const Tensor&)> pred) {
"
71,"DynamicLayerStackHolder() {}
virtual ~DynamicLayerStackHolder() {}
  std::vector<DynamicLayer> dynamicLayerStack = { DynamicLayer(DispatchKey::Autograd, 1) };
};
thread_local std::shared_ptr<DynamicLayerStackHolder> kDynamicLayerStack;
","DynamicLayerStackHolder() {}
virtual ~DynamicLayerStackHolder() {}
  std::vector<DynamicLayer> dynamicLayerStack = { DynamicLayer(DispatchKey::Autograd, 1, nullopt, true) };
};
thread_local std::shared_ptr<DynamicLayerStackHolder> kDynamicLayerStack;
"
72,"return layerId;
}
static int64_t pushDynamicLayer(DispatchKey key, optional<int64_t> batch_size = nullopt) {
auto& dynamicLayerStack = dynamicLayerStackAccessor();
TORCH_INTERNAL_ASSERT(key != DispatchKey::Undefined);
TORCH_INTERNAL_ASSERT(key != DispatchKey::Batched);
auto layerId = 1 + dynamicLayerStack.size();
  dynamicLayerStack.emplace_back(key, layerId, batch_size);
if (layerId == 2) {
// std::cout << ""DynamicLayer on"" << std::endl;
","return layerId;
}
static int64_t pushDynamicLayer(
    DispatchKey key,
    optional<int64_t> batch_size = nullopt,
    optional<bool> prev_grad_mode = nullopt) {
auto& dynamicLayerStack = dynamicLayerStackAccessor();
TORCH_INTERNAL_ASSERT(key != DispatchKey::Undefined);
TORCH_INTERNAL_ASSERT(key != DispatchKey::Batched);
auto layerId = 1 + dynamicLayerStack.size();
  dynamicLayerStack.emplace_back(key, layerId, batch_size, prev_grad_mode);
if (layerId == 2) {
// std::cout << ""DynamicLayer on"" << std::endl;
"
73,"c10::impl::tls_set_dispatch_key_included(kDynamicLayerBackModeKey, true);
// Re-dispatch
  op.callBoxed(stack);
// Step 4, 5, 6
if (cur_key == DispatchKey::Autograd) {
","c10::impl::tls_set_dispatch_key_included(kDynamicLayerBackModeKey, true);
// Re-dispatch
  if (cur_key == DispatchKey::Autograd && *prev_grad_mode == false) {
    // See NOTE [grad and vjp interaction with no_grad]
    c10::AutoGradMode guard(*prev_grad_mode);
    op.callBoxed(stack);
  } else {
    op.callBoxed(stack);
  }
// Step 4, 5, 6
if (cur_key == DispatchKey::Autograd) {
"
74,"STOP_DECOMPOSE(range.step);
STOP_DECOMPOSE(ravel);
STOP_DECOMPOSE(refine_names);
  STOP_DECOMPOSE(relu6_);
STOP_DECOMPOSE(rename);
STOP_DECOMPOSE(rename_);
STOP_DECOMPOSE(repeat_interleave.self_Tensor);
","STOP_DECOMPOSE(range.step);
STOP_DECOMPOSE(ravel);
STOP_DECOMPOSE(refine_names);
STOP_DECOMPOSE(rename);
STOP_DECOMPOSE(rename_);
STOP_DECOMPOSE(repeat_interleave.self_Tensor);
"
75,"return std::make_tuple(at::_log_softmax_backward_data(grad_output_, output_, dim, self_), 0);
}

TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
REDUCTION_BOXED(amax);
REDUCTION_BOXED(amin);
","return std::make_tuple(at::_log_softmax_backward_data(grad_output_, output_, dim, self_), 0);
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
REDUCTION_BOXED(amax);
REDUCTION_BOXED(amin);
"
76,"return std::make_tuple( std::move(result), 0 );
}
std::tuple<Tensor,optional<int64_t>> pow_scalar_tensor_batch_rule(
    const Scalar& other,
    const Tensor& tensor, optional<int64_t> tensor_batch_dim) {
  return std::make_tuple( at::pow(other, tensor), tensor_batch_dim );
}

std::tuple<Tensor,optional<int64_t>> _s_where_batch_rule(
const Tensor& condition, optional<int64_t> condition_bdim,
const Tensor& self, optional<int64_t> self_bdim, const Tensor& other, optional<int64_t> other_bdim) {
","return std::make_tuple( std::move(result), 0 );
}
std::tuple<Tensor,optional<int64_t>> _s_where_batch_rule(
const Tensor& condition, optional<int64_t> condition_bdim,
const Tensor& self, optional<int64_t> self_bdim, const Tensor& other, optional<int64_t> other_bdim) {
"
77,"return std::make_tuple( result, 0 );
}
template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> reduction_dimarray_batch_rule(
const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, ExtraArgs... extra_args) {
","return std::make_tuple( result, 0 );
}
// For reductions that take in an array of dimensions to reduce over.
template <typename F, F Func, typename... ExtraArgs>
std::tuple<Tensor,optional<int64_t>> reduction_dimarray_batch_rule(
const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, ExtraArgs... extra_args) {
"
78,"TORCH_LIBRARY_IMPL(aten, FuncTorchVmapMode, m) {
  // NB: I'd really like to register a special kernel like
  // CppFunction::makeNamedNotSupported() to avoid listing out the types of everything.
  // However, registering e.g. CppFunction::makeNamedNotSupported() as an implementation
  // only works for operators that support boxing.
  // random operations (out-of-place)
UNSUPPORTED_RANDOM(bernoulli);
UNSUPPORTED_RANDOM2(bernoulli, out);
UNSUPPORTED_RANDOM2(bernoulli, p);
","TORCH_LIBRARY_IMPL(aten, FuncTorchVmapMode, m) {
UNSUPPORTED_RANDOM(bernoulli);
UNSUPPORTED_RANDOM2(bernoulli, out);
UNSUPPORTED_RANDOM2(bernoulli, p);
"
79,"// Taken from https://stackoverflow.com/a/41301717
template<typename R, typename... A>
R ret(R(*)(A...));
// Optional implies the weird case with 0-dim tensors i.e. torch.sum(torch.randn(()), 0)
template <typename F, F Func, typename... ExtraArgs>
optional<std::tuple<decltype(ret(Func)), optional<int64_t>>> reduction_dimarray_batch_rule_impl(    const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, ExtraArgs... extra_args) {
auto logical_dim = rankWithoutBatchDim(self, self_bdim);
// If the dim intlist is empty, that's equivalent to passing in a dim on all dimensions.
","// Taken from https://stackoverflow.com/a/41301717
template<typename R, typename... A>
R ret_type(R(*)(A...));
// Optional implies the weird case with 0-dim tensors i.e. torch.sum(torch.randn(()), 0)
template <typename F, F Func, typename... ExtraArgs>
optional<std::tuple<decltype(ret_type(Func)), optional<int64_t>>> reduction_dimarray_batch_rule_impl(    const Tensor& self, optional<int64_t> self_bdim, IntArrayRef dims, ExtraArgs... extra_args) {
auto logical_dim = rankWithoutBatchDim(self, self_bdim);
// If the dim intlist is empty, that's equivalent to passing in a dim on all dimensions.
"
80,"#include <functorch/csrc/BatchRulesHelper.h>
#include <iostream>
namespace at { namespace functorch {
","#include <functorch/csrc/BatchRulesHelper.h>
#include <iostream>
#include <ATen/Operators.h>

namespace at { namespace functorch {
"
81,"// [start, start + 1, ..., stop - 1]
static VmapDimVector range(int64_t start, int64_t stop) {
  TORCH_INTERNAL_ASSERT(stop > start);
VmapDimVector dims;
dims.reserve(stop - start);
for (int64_t i = start; i < stop; i++) {
","// [start, start + 1, ..., stop - 1]
static VmapDimVector range(int64_t start, int64_t stop) {
  TORCH_INTERNAL_ASSERT(stop >= start);
VmapDimVector dims;
dims.reserve(stop - start);
for (int64_t i = start; i < stop; i++) {
"
82,"// TODO: need to reset sizes/strides on mutation
TORCH_INTERNAL_ASSERT(use_value_sizes_strides);
  refreshSizesAndStrides();
}
// The following are some internal inherited methods that we do not support.
","// TODO: need to reset sizes/strides on mutation
TORCH_INTERNAL_ASSERT(use_value_sizes_strides);
  refreshMetadata();
}
// The following are some internal inherited methods that we do not support.
"
83,".getElementType());
py::list pyL;
        for (int jdx = 0; jdx < l.size(); jdx++) {
auto nv = l.get(jdx);
if (nv.isTensor() && isPythonTensor(nv.toTensor())) {
auto pyTensor = getPythonImpl(nv.toTensor());
",".getElementType());
py::list pyL;
        for (unsigned jdx = 0; jdx < l.size(); jdx++) {
auto nv = l.get(jdx);
if (nv.isTensor() && isPythonTensor(nv.toTensor())) {
auto pyTensor = getPythonImpl(nv.toTensor());
"
84,"#include <functorch/csrc/BatchRulesHelper.h>
namespace at { namespace functorch {
","#include <functorch/csrc/BatchRulesHelper.h>
#include <iostream>
namespace at { namespace functorch {
"
85,"int64_t dim) {
auto self_ = moveBatchDimToFront(self, self_bdim);
auto rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, rank + 1) + 1;
  return { self.unsqueeze(dim), valIfNonempty(self_bdim, 0) };
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
","int64_t dim) {
auto self_ = moveBatchDimToFront(self, self_bdim);
auto rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, rank + 1);
  if (self_bdim) {
    dim += 1;
  }
  return { self_.unsqueeze(dim), valIfNonempty(self_bdim, 0) };
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
"
86,"node, /*num_reduced_dim=*/0, /*upcast_integer=*/false, opt_dtype);
}};
    static const auto factory_with_ndim = [](Node* node,
                                             int dim) -> type_vec_t {
at::optional<IValue> maybe_layout_option = node->get(attr::layout);
if (!maybe_layout_option)
return {};
","node, /*num_reduced_dim=*/0, /*upcast_integer=*/false, opt_dtype);
}};
    static const auto factory_with_ndim =
        [](Node* node, int dim, at::ScalarType default_dtype) -> type_vec_t {
at::optional<IValue> maybe_layout_option = node->get(attr::layout);
if (!maybe_layout_option)
return {};
"
87,"auto self_working_copy = cloneBatchedColumnMajor(self);
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""symeig_cpu"", [&]{
    apply_symeig<scalar_t>(self_working_copy, eigvals, eigenvectors, upper, infos);
});
  if (self.dim() > 2) {
    batchCheckErrors(infos, ""symeig_cpu"");
  } else {
    singleCheckErrors(infos[0], ""symeig_cpu"");
  }
if (eigenvectors) {
return std::tuple<Tensor, Tensor>(eigvals, self_working_copy);
} else {
","auto self_working_copy = cloneBatchedColumnMajor(self);
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""symeig_cpu"", [&]{
    apply_symeig<scalar_t>(self_working_copy, eigvals, eigenvectors, upper, infos.data_ptr<int>());
});
  at::_linalg_check_errors(infos, ""symeig"", self.dim() == 2);
if (eigenvectors) {
return std::tuple<Tensor, Tensor>(eigvals, self_working_copy);
} else {
"
88,"template <typename scalar_t>
static void apply_eig(const Tensor& self, bool eigenvectors, Tensor& out_eigvals, Tensor& out_eigvecs,
                      int64_t *info_ptr) {
#if !AT_MAGMA_ENABLED()
TORCH_CHECK(false, ""Calling torch.eig on a CUDA tensor requires compiling PyTorch with MAGMA. ""
""Either transfer the tensor to the CPU before calling torch.eig or recompile with MAGMA."");
","template <typename scalar_t>
static void apply_eig(const Tensor& self, bool eigenvectors, Tensor& out_eigvals, Tensor& out_eigvecs,
                      int* info_ptr) {
#if !AT_MAGMA_ENABLED()
TORCH_CHECK(false, ""Calling torch.eig on a CUDA tensor requires compiling PyTorch with MAGMA. ""
""Either transfer the tensor to the CPU before calling torch.eig or recompile with MAGMA."");
"
89,"//! 2) MergeAdjacentSingletonAxes class merges or reduces any
//!    adjacent singleton dimensions.
class MergeAxesInterface {
protected:
// See addMergeTransform for ""is_index_merge_rhs"" and
// ""is_last_axis_rfactor"" descriptions
","//! 2) MergeAdjacentSingletonAxes class merges or reduces any
//!    adjacent singleton dimensions.
class MergeAxesInterface {
   public:
    virtual ~MergeAxesInterface() = default;

protected:
// See addMergeTransform for ""is_index_merge_rhs"" and
// ""is_last_axis_rfactor"" descriptions
"
90,"mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
","mtsa.handle(false /* is_index_merge_rhs */, is_last_axis_rfactor);
}
    virtual ~MergeThenSplitAxes() = default;

private:
MergeThenSplitAxes(
AnalyzeViewTransformation* avt,
"
91,"return self;
}
Tensor& clamp_(
Tensor& self,
const c10::optional<Scalar>& min,
const c10::optional<Scalar>& max) {
  return _clamp_(self, min, max, ""aten::clamp_"");
}
Tensor activation(
","return self;
}
Tensor threshold(
    const Tensor& self,
    const Scalar& threshold,
    const Scalar& value) {
  return _clamp(self, threshold, value, VK_KERNEL(threshold), ""aten::threshold"");
}

Tensor& clamp_(
Tensor& self,
const c10::optional<Scalar>& min,
const c10::optional<Scalar>& max) {
  return _clamp_(self, min, max, VK_KERNEL(clamp_), ""aten::clamp_"");
}
Tensor activation(
"
92,"if (n != 2 && n != 0) {
TORCH_WARN_ONCE(
""The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated "",
        ""and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices"",
""or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor.""
);
}
","if (n != 2 && n != 0) {
TORCH_WARN_ONCE(
""The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated "",
        ""and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices "",
""or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor.""
);
}
"
93,"}
case aten::_batch_norm_impl_index_backward:
case aten::native_batch_norm_backward: {
        int weight_index = -1;
int mask_index = -1;
if (node->kind() ==
c10::Symbol::fromQualString(
""aten::_batch_norm_impl_index_backward"")) {
          weight_index = 3;
mask_index = 10;
} else if (
node->kind() ==
c10::Symbol::fromQualString(""aten::native_batch_norm_backward"")) {
          weight_index = 2;
mask_index = 9;
} else {
TORCH_INTERNAL_ASSERT(
","}
case aten::_batch_norm_impl_index_backward:
case aten::native_batch_norm_backward: {
int mask_index = -1;
if (node->kind() ==
c10::Symbol::fromQualString(
""aten::_batch_norm_impl_index_backward"")) {
mask_index = 10;
} else if (
node->kind() ==
c10::Symbol::fromQualString(""aten::native_batch_norm_backward"")) {
mask_index = 9;
} else {
TORCH_INTERNAL_ASSERT(
"
94,"const auto num_arguments = arguments.size();
const auto num_returns = returns.size();
const auto stack_start = stack->size() - num_arguments;
  bool any_is_inplace = false;
at::Tensor aliased_input;
","const auto num_arguments = arguments.size();
const auto num_returns = returns.size();
const auto stack_start = stack->size() - num_arguments;
at::Tensor aliased_input;
"
95,"}
return name;
}
} // namespace
const std::vector<at::Tensor> Module::parameters() const {
","}
return name;
}
#endif

} // namespace
const std::vector<at::Tensor> Module::parameters() const {
"
96,"std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
","std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::Backend get_backend(bool is_cuda, bool is_sparse) {
  if (is_cuda) {
    if (is_sparse){
      return at::Backend::SparseCUDA;
    } else {
      return at::Backend::CUDA;
    }
  } else {
    if (is_sparse){
      return at::Backend::SparseCPU;
    } else {
      return at::Backend::CPU;
    }
  }
}

at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
"
97,"#include <torch/library.h>
namespace at {
namespace native {

// These are still needed because we don't have C++ conversions from number
// types (int, float, etc.) to Tensor (only to Scalar). They're not exposed
// to Python.

static void check_convert(const Scalar& scalar, ScalarType scalarType) {
  // Validate that is possible to convert scalar to tensor dtype without
  // overflow
  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(
      at::ScalarType::Bool,
      at::ScalarType::BFloat16,
      at::ScalarType::Half,
      at::ScalarType::ComplexHalf,
      scalarType,
      ""check_convert"",
      [&] { scalar.to<scalar_t>(); });
}

} // namespace native
namespace meta {
","#include <torch/library.h>
namespace at {
namespace meta {
"
98,"return index;
}
// TODO: temporary hack to resolve my is_constructible issue;
std::vector<size_t> toVector(const at::DimVector& small_vec) {
  return std::vector<size_t>(small_vec.begin(), small_vec.end());
}

void encodeBuffer(size_t value, std::string& buffer) {
const char* v = reinterpret_cast<char*>(&value);
for (const auto i : c10::irange(sizeof(size_t))) {
","return index;
}
void encodeBuffer(size_t value, std::string& buffer) {
const char* v = reinterpret_cast<char*>(&value);
for (const auto i : c10::irange(sizeof(size_t))) {
"
99,"}
}
std::string getTopModuleTypeName(const Module& m) {
std::string name;
if (m._ivalue()->type() && m._ivalue()->type()->name()) {
","}
}
#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
std::string getTopModuleTypeName(const Module& m) {
std::string name;
if (m._ivalue()->type() && m._ivalue()->type()->name()) {
"
100,"});
}
at::Tensor signed_log1p(const at::Tensor& input) {
  auto out = create_empty_from(input);
  signed_log1p_out(out, input);
  return out;
}

} // namespace
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
","});
}
} // namespace
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
"
101,"}
at::Tensor& _unsafe_view_copy_out(const at::Tensor & self, at::IntArrayRef size, at::Tensor & out) {
  auto tmp = at::_unsafe_view(self, size);
  out.copy_(tmp);
  return out;
}


at::Tensor& unsqueeze_copy_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
auto tmp = self.unsqueeze(dim);
out.copy_(tmp);
","}
at::Tensor& unsqueeze_copy_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
auto tmp = self.unsqueeze(dim);
out.copy_(tmp);
"
102,"cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {
TORCH_CHECK(b != 0, ""ZeroDivisionError"");
scalar_t r = a % b;
        if ((r != 0) && ((r < 0) != (b < 0))) {
r += b;
}
return r;
","cpu_kernel(iter, [](scalar_t a, scalar_t b) -> scalar_t {
TORCH_CHECK(b != 0, ""ZeroDivisionError"");
scalar_t r = a % b;
        if ((r != 0) && (c10::is_negative(r) != c10::is_negative(b))) {
r += b;
}
return r;
"
103,"ExpressionEvaluator const_eval(tv->fusion());
for (auto axis_index : c10::irange(axis.size())) {
TORCH_INTERNAL_ASSERT(
        ((axis[axis_index] + tv->nDims()) >= 0) &&
(axis[axis_index] < (int)tv->nDims()),
""CheckDimSize: axis position out of bound "",
axis[axis_index],
","ExpressionEvaluator const_eval(tv->fusion());
for (auto axis_index : c10::irange(axis.size())) {
TORCH_INTERNAL_ASSERT(
        ((axis[axis_index] + static_cast<int>(tv->nDims())) >= 0) &&
(axis[axis_index] < (int)tv->nDims()),
""CheckDimSize: axis position out of bound "",
axis[axis_index],
"
104,"const std::vector<IterDomain*>& new_root_domain,
std::vector<IterDomain*>& rfactor_domain) override {
TORCH_INTERNAL_ASSERT(
        index_ >= 0 && index_ < new_root_domain.size(),
""Index: \t"",
index_,
""\t Domain Size:\t"",
","const std::vector<IterDomain*>& new_root_domain,
std::vector<IterDomain*>& rfactor_domain) override {
TORCH_INTERNAL_ASSERT(
        index_ < new_root_domain.size(),
""Index: \t"",
index_,
""\t Domain Size:\t"",
"
105,"ScalarType output_type = ScalarType::Long;
checkLinalgCompatibleDtype(""torch.linalg.matrix_rank"", result.scalar_type(), output_type);
  // Matrices or batch of matrices are allowed
  TORCH_CHECK(input.dim() >= 2, ""torch.linalg.matrix_rank: Expected as input a matrix or a batch of matrices, but got a tensor of size: "", input.sizes());

checkNotComplexTolerance(atol, ""torch.linalg.matrix_rank"", ""atol"");
checkNotComplexTolerance(rtol, ""torch.linalg.matrix_rank"", ""rtol"");
  // matrix_rank assigns a scalar value for each matrix in the batch so
  // result's shape is equal to input.shape[0:input.ndim-2]
  // for single matrix result_shape = {}
  auto result_shape = IntArrayRef(input.sizes().cbegin(), input.sizes().cend() - 2);
  at::native::resize_output(result, result_shape);

// NumPy doesn't take into account possible input with no elements and it errors on max not defined for this case
// Let's output 0 for this case, since that kind of matrices have zero number of non-zero rows, hence rank is 0.
if (input.numel() == 0) {
","ScalarType output_type = ScalarType::Long;
checkLinalgCompatibleDtype(""torch.linalg.matrix_rank"", result.scalar_type(), output_type);
checkNotComplexTolerance(atol, ""torch.linalg.matrix_rank"", ""atol"");
checkNotComplexTolerance(rtol, ""torch.linalg.matrix_rank"", ""rtol"");
// NumPy doesn't take into account possible input with no elements and it errors on max not defined for this case
// Let's output 0 for this case, since that kind of matrices have zero number of non-zero rows, hence rank is 0.
if (input.numel() == 0) {
"
106,"Tensor hinge_embedding_loss(const Tensor& self, const Tensor& target, double margin, int64_t reduction) {
auto zeros = at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
  auto margin_clamp = (margin - self).clamp_min_(0);
auto output_margin = at::where(target != 1, margin_clamp, zeros);
auto output_self = at::where(target != -1, self, zeros);
auto output = output_margin + output_self;
","Tensor hinge_embedding_loss(const Tensor& self, const Tensor& target, double margin, int64_t reduction) {
auto zeros = at::zeros_like(self, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
  auto margin_diff = (margin - self);
  // For Composite Compliance,
  // In Forward AD, if `margin_diff` is a CCT but its tangent isn't,
  // using inplace clamp_min doesn't work because we end up writing
  // the CCT in-place to the tangent
  auto margin_clamp = (margin_diff._fw_grad(/*level*/ 0).defined() &&
                       isTensorSubclassLike(margin_diff))
      ? margin_diff.clamp_min(0)
      : margin_diff.clamp_min_(0);
auto output_margin = at::where(target != 1, margin_clamp, zeros);
auto output_self = at::where(target != -1, self, zeros);
auto output = output_margin + output_self;
"
107,"}
Tensor margin_ranking_loss(const Tensor& input1, const Tensor& input2, const Tensor& target, double margin, int64_t reduction) {
  auto output =  (-target * (input1 - input2) + margin).clamp_min_(0);
return apply_loss_reduction(output, reduction);
}
","}
Tensor margin_ranking_loss(const Tensor& input1, const Tensor& input2, const Tensor& target, double margin, int64_t reduction) {
  auto unclamped_output = (-target * (input1 - input2) + margin);
  // For Composite Compliance,
  // In Forward AD, if `margin_diff` is a CCT but its tangent isn't,
  // using inplace clamp_min doesn't work because we end up writing
  // the CCT in-place to the tangent
  auto output = (unclamped_output._fw_grad(/*level*/ 0).defined() &&
                 isTensorSubclassLike(unclamped_output))
      ? unclamped_output.clamp_min(0)
      : unclamped_output.clamp_min_(0);
return apply_loss_reduction(output, reduction);
}
"
108,"#include <torch/csrc/jit/serialization/flatbuffer_serializer_jit.h>
#include <torch/csrc/jit/mobile/flatbuffer_loader.h>
#include <torch/csrc/jit/serialization/export.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#include <torch/csrc/jit/serialization/flatbuffer_serializer.h>
","#include <torch/csrc/jit/serialization/flatbuffer_serializer_jit.h>
#include <torch/csrc/jit/mobile/flatbuffer_loader.h>
#include <torch/csrc/jit/operator_upgraders/upgraders_entry.h>
#include <torch/csrc/jit/serialization/export.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#include <torch/csrc/jit/serialization/flatbuffer_serializer.h>
"
109,"Tensor d = at::full_like(sizes, D);
// N * 2, ([[size1, D], [size2, D], ..., [sizeN, D]])
    sizes = at::cat({sizes, d}, 1);
return at::_nested_from_padded(t, sizes, false);
}
","Tensor d = at::full_like(sizes, D);
// N * 2, ([[size1, D], [size2, D], ..., [sizeN, D]])
    sizes = at::cat({sizes, d}, 1).to(kCPU);
return at::_nested_from_padded(t, sizes, false);
}
"
110,"if (is_concretized) {
// Keep track of all the origin domains as concretized
for (auto origin : producer_origins) {
            // concretized_root_domains_.insert(origin);
            markAsConcretized(origin);
}
} else {
// Not concretized yet. Propagate forward the origin info.
","if (is_concretized) {
// Keep track of all the origin domains as concretized
for (auto origin : producer_origins) {
            markAsConcretized(origin, c_id);
}
} else {
// Not concretized yet. Propagate forward the origin info.
"
111,"}
PyTuple_SET_ITEM(tuple.get(), 2, torch::autograd::utils::wrap(non_blocking));
if (opt_memory_format.has_value()) {
    PyTuple_SET_ITEM(tuple.get(), 3, THPMemoryFormat_New(opt_memory_format.value(), ""unused_name""));
} else {
Py_INCREF(Py_None);
PyTuple_SET_ITEM(tuple.get(), 3, Py_None);
","}
PyTuple_SET_ITEM(tuple.get(), 2, torch::autograd::utils::wrap(non_blocking));
if (opt_memory_format.has_value()) {
    PyTuple_SET_ITEM(tuple.get(), 3, torch::utils::getTHPMemoryFormat(opt_memory_format.value()).release().ptr());
} else {
Py_INCREF(Py_None);
PyTuple_SET_ITEM(tuple.get(), 3, Py_None);
"
112,"throw python_error();
}
  _ADD_MEMORY_FORMAT(at::MemoryFormat::Preserve, ""preserve_format"");
  _ADD_MEMORY_FORMAT(at::MemoryFormat::Contiguous, ""contiguous_format"");
  _ADD_MEMORY_FORMAT(at::MemoryFormat::ChannelsLast, ""channels_last"");
  _ADD_MEMORY_FORMAT(at::MemoryFormat::ChannelsLast3d, ""channels_last_3d"");
}
","throw python_error();
}
  auto add_memory_format = [&](at::MemoryFormat format, const char* name) {
    std::string module_name = ""torch."";
    PyObject* memory_format = THPMemoryFormat_New(format, module_name + name);
    Py_INCREF(memory_format);
    if (PyModule_AddObject(torch_module, name, memory_format) != 0) {
      Py_DECREF(memory_format);
      throw python_error();
    }
    Py_INCREF(memory_format);
    memory_format_registry[static_cast<size_t>(format)] = memory_format;
  };

  add_memory_format(at::MemoryFormat::Preserve, ""preserve_format"");
  add_memory_format(at::MemoryFormat::Contiguous, ""contiguous_format"");
  add_memory_format(at::MemoryFormat::ChannelsLast, ""channels_last"");
  add_memory_format(at::MemoryFormat::ChannelsLast3d, ""channels_last_3d"");
}
"
113,"assertFuserCanBeEnabled(*getCachedFuserEnabledEnvVar());
}
});
    // 0. opportunity to force disable NVFuser
    if (getCachedNNCNotNVFuser()) {
      return false;
    }
// 1. if user has explicitly assigned fuser value, that value takes
// precedence.
if (runtime_assigned_fuser_enabled_.has_value()) {
","assertFuserCanBeEnabled(*getCachedFuserEnabledEnvVar());
}
});
// 1. if user has explicitly assigned fuser value, that value takes
// precedence.
if (runtime_assigned_fuser_enabled_.has_value()) {
"
114,"return false;
#endif
#else
   return false;
 #endif
}
bool Context::hasOpenMP() {
","return false;
#endif
#else
  return false;
#endif
}
bool Context::hasOpenMP() {
"
115,"return output;
}
// The default implementation of lift is a no-op.
// If TLS is set appropriately (for wrapper-tensor keys like Functionalize or functorch transforms),
// then we'll dispatch to one of their implementations, which will properly lift the tensor into a wrapper.
at::Tensor lift(const at::Tensor& self) {
    return self;
}

} // namespace native
} // namespace at
","return output;
}
} // namespace native
} // namespace at
"
116,"} initializer;
}  // namespace (anonymous)
#else
const bool use_magma_ = false;
","} initializer;
}  // namespace (anonymous)
#define AT_MAGMA_VERSION MAGMA_VERSION_MAJOR*100 + MAGMA_VERSION_MINOR*10 + MAGMA_VERSION_MICRO

// Check that MAGMA never releases MAGMA_VERSION_MINOR >= 10 or MAGMA_VERSION_MICRO >= 10
#if MAGMA_VERSION_MINOR >= 10 || MAGMA_VERSION_MICRO >= 10
#error ""MAGMA release minor or micro version >= 10, please correct AT_MAGMA_VERSION""
#endif

#else
const bool use_magma_ = false;
"
117,"TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
namespace {

// DO NOT USE THIS -- it's just an implementation detail of wrapped_scalar tensor below.
at::Tensor scalar_to_tensor_default_dtype(
    const Scalar& s,
    const Device device = at::kCPU) {
  if (s.isFloatingPoint()) {
    return at::scalar_tensor(
        s, at::device(device).dtype(at::get_default_dtype()));
  } else if (s.isBoolean()) {
    return at::scalar_tensor(s, at::device(device).dtype(at::kBool));
  } else if (s.isComplex()) {
    return at::scalar_tensor(
        s, at::device(device).dtype(at::get_default_complex_dtype()));
  } else {
    TORCH_INTERNAL_ASSERT(s.isIntegral(false));
    return at::scalar_tensor(s, at::device(device).dtype(at::kLong));
  }
}

// TLDR: Don't call `wrapped_scalar_tensor_default_dtype` -- this function is only necessary to support the partial
// type-promotion that torch.where supports.  Once torch.where fully supports type promotion, we
// won't need this function.
//
// Longer explanation:
// `wrapped_scalar_tensor_default_dtype` is a bit of a hack because torch.where doesn't support type promotion, but
// does support `torch.where(tensor, scalar1, scalar2)` with default scalar types.  The trickiness is we
// usually convert double scalars to doubles, and `set_wrapped_number` defines type promotion priority
// as being below tensor types rather than as the default dtype (perhaps we should?).  This wouldn't matter
// if we just supported type normal type promotion on torch.where, however.
Tensor wrapped_scalar_tensor_default_dtype(
    const Scalar& scalar,
    Device device) {
  at::Tensor tensor;
  tensor = scalar_to_tensor_default_dtype(scalar, device);
  tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
  return tensor;
}

} // anonymous namespace

// Sorting-based algorithm for isin(); used when the number of test elements is large.
static void isin_sorting(
const Tensor& elements,
","TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
// Sorting-based algorithm for isin(); used when the number of test elements is large.
static void isin_sorting(
const Tensor& elements,
"
118,"const auto source_n = n->sourceRange().source();
const auto source_m = m->sourceRange().source();
return (
      (source_n->text() == source_m->text()) &&
(source_n->starting_line_no() == source_m->starting_line_no()));
}
","const auto source_n = n->sourceRange().source();
const auto source_m = m->sourceRange().source();
return (
      (source_n->text_str() == source_m->text_str()) &&
(source_n->starting_line_no() == source_m->starting_line_no()));
}
"
119,"new_size[0] = bias_.value().size(0);
broadcasted_bias = bias_.value().reshape(new_size);
broadcasted_bias.value() = broadcasted_bias.value().broadcast_to(quantized_output.sizes());
    broadcasted_bias.value() = broadcasted_bias.value().contiguous(c10::MemoryFormat::ChannelsLast);
bias_multiplier_tensor = at::empty(quantized_output.sizes(), at::device(at::kCUDA).dtype(at::kFloat), at::MemoryFormat::ChannelsLast);
auto bias_multiplier = 1.0 / (act_scale * weight_scale);
bias_multiplier_tensor.value().fill_(bias_multiplier);
","new_size[0] = bias_.value().size(0);
broadcasted_bias = bias_.value().reshape(new_size);
broadcasted_bias.value() = broadcasted_bias.value().broadcast_to(quantized_output.sizes());
    broadcasted_bias.value() = broadcasted_bias.value().to(c10::MemoryFormat::ChannelsLast);
bias_multiplier_tensor = at::empty(quantized_output.sizes(), at::device(at::kCUDA).dtype(at::kFloat), at::MemoryFormat::ChannelsLast);
auto bias_multiplier = 1.0 / (act_scale * weight_scale);
bias_multiplier_tensor.value().fill_(bias_multiplier);
"
120,"if (!pctx) {
std::unique_lock<std::mutex> cudaFreeMutexLock(
*(c10::cuda::CUDACachingAllocator::getFreeMutex()));
    C10_CUDA_CHECK(cudaFree(nullptr));
}
}
","if (!pctx) {
std::unique_lock<std::mutex> cudaFreeMutexLock(
*(c10::cuda::CUDACachingAllocator::getFreeMutex()));
    cudaFree(nullptr);
}
}
"
121,"void onEachDevice(std::function<void(int)> op) const override {
at::cuda::OptionalCUDAGuard device_guard;
    for(const auto i : c10::irange(at::cuda::device_count())) {
device_guard.set_index(i);
op(i);
}
}
void synchronize() const override {
    TORCH_CUDA_CHECK(cudaDeviceSynchronize());
}
bool enabled() const override {
","void onEachDevice(std::function<void(int)> op) const override {
at::cuda::OptionalCUDAGuard device_guard;
    // NOLINTNEXTLINE(bugprone-signed-char-misuse)
    int count = at::cuda::device_count();
    for(const auto i : c10::irange(count)) {
device_guard.set_index(i);
op(i);
}
}
void synchronize() const override {
    cudaDeviceSynchronize();
}
bool enabled() const override {
"
122,"try {
if (event_sync_required_) {
at::cuda::CUDAGuard device_guard(device_.index());
      cudaEventDestroy(event_);
if(!CudaIPCGlobalEntities::alive) {
return;
}
","try {
if (event_sync_required_) {
at::cuda::CUDAGuard device_guard(device_.index());
      C10_CUDA_CHECK(cudaEventDestroy(event_));
if(!CudaIPCGlobalEntities::alive) {
return;
}
"
123,"c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    cudaMemGetInfo(&device_free, &device_total);
return {device_free, device_total};
});
}
","c10::cuda::CUDAGuard guard(device);
size_t device_free = 0;
size_t device_total = 0;
    C10_CUDA_CHECK(cudaMemGetInfo(&device_free, &device_total));
return {device_free, device_total};
});
}
"
124,"if (at::cuda::currentStreamCaptureStatusMayInitCtx() ==
at::cuda::CaptureStatus::None) {
#endif
    return cudaMalloc(p, size);
#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
} else {
// It's ok to capture cudaMallocs, as long as we never cudaFree those
","if (at::cuda::currentStreamCaptureStatusMayInitCtx() ==
at::cuda::CaptureStatus::None) {
#endif
    return C10_CUDA_ERROR_HANDLED(cudaMalloc(p, size));
#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
} else {
// It's ok to capture cudaMallocs, as long as we never cudaFree those
"
125,"X_->data_ptr<scalar_t>(),
CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());
});
if (!X.is_same(*X_)) {
X.copy_(*X_);
","X_->data_ptr<scalar_t>(),
CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());

        bsrsv2_bsrsm2_may_need_to_sync();
});
if (!X.is_same(*X_)) {
X.copy_(*X_);
"
126,"ldx,
CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());
});
if (!X.is_same(*X_)) {
X.copy_(*X_);
","ldx,
CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());

        bsrsv2_bsrsm2_may_need_to_sync();
});
if (!X.is_same(*X_)) {
X.copy_(*X_);
"
127,"namespace torch {
namespace jit {
std::atomic<PrintHandler> print_handler([](const std::string& str) {
  std::cout << str;
});
PrintHandler getPrintHandler() {
return print_handler.load();
","namespace torch {
namespace jit {
namespace {

std::atomic<PrintHandler> print_handler(getDefaultPrintHandler());

} // namespace

PrintHandler getDefaultPrintHandler() {
  return [](const std::string& s) { std::cout << s; };
}
PrintHandler getPrintHandler() {
return print_handler.load();
"
128,"ClearProfilingInformation(subgraph);
};
auto applyOptimizations = [](std::shared_ptr<Graph>& subgraph) {
runOptimization(
subgraph,
/* unroll_non_constant_loops? */ false,
","ClearProfilingInformation(subgraph);
};
auto applyOptimizations = [](std::shared_ptr<Graph>& subgraph) {
#ifndef C10_MOBILE
      Autocast(subgraph);
#endif
runOptimization(
subgraph,
/* unroll_non_constant_loops? */ false,
"
129,"#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/file_format.h>
#if defined(ENABLE_FLATBUFFER)
#include <torch/csrc/jit/mobile/flatbuffer_loader.h>
#endif
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
","#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
"
130,"mobile::serialization::Module* module,
ExtraFilesMap& extra_files) {
auto extra_files_offsets = module->extra_files();
  parseExtraFilesFromVector(module->extra_files(), &extra_files);
}
mobile::Module FlatbufferLoader::parseModule(
","mobile::serialization::Module* module,
ExtraFilesMap& extra_files) {
auto extra_files_offsets = module->extra_files();
  parseExtraFilesFromVector(extra_files_offsets, &extra_files);
}
mobile::Module FlatbufferLoader::parseModule(
"
131,"void Module::_save_for_mobile(
std::ostream& out,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info) const {
ExportModule(
*this,
out,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info);
}
void Module::_save_for_mobile(
const std::string& filename,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info) const {
ExportModule(
*this,
filename,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info);
}
} // namespace jit
","void Module::_save_for_mobile(
std::ostream& out,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info,
    bool use_flatbuffer) const {
ExportModule(
*this,
out,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info,
      use_flatbuffer);
}
void Module::_save_for_mobile(
const std::string& filename,
const ExtraFilesMap& extra_files,
    bool save_mobile_debug_info,
    bool use_flatbuffer) const {
ExportModule(
*this,
filename,
extra_files,
true /* bytecode_format */,
      save_mobile_debug_info,
      use_flatbuffer);
}
} // namespace jit
"
132,"#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
","#include <caffe2/serialize/inline_container.h>
#include <caffe2/serialize/versions.h>
#include <torch/csrc/jit/api/compilation_unit.h>
#include <torch/csrc/jit/mobile/file_format.h>
#if defined(ENABLE_FLATBUFFER)
#include <torch/csrc/jit/mobile/flatbuffer_loader.h>
#endif
#include <torch/csrc/jit/mobile/interpreter.h>
#include <torch/csrc/jit/mobile/observer.h>
#include <torch/csrc/jit/mobile/type_parser.h>
"
133,"#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/unpickler.h>
#include <torch/custom_class.h>

#include <exception>
#include <fstream>
#include <string>
","#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/unpickler.h>
#include <torch/custom_class.h>
#include <exception>
#include <fstream>
#include <string>
"
134,"i = 0;
for (const auto p : c10::irange(params.size())) {
const auto& t = params[p];
    // I'd like to include which process we are in the message,
    // but ProcessGroup::getRank is not public!
for (const auto& sz : t.sizes()) {
      auto msg = c10::str(""params["", p, ""] in this process"",
"" with sizes "",
t.sizes(),
"" appears not to match sizes of the same param in process 0."");
","i = 0;
for (const auto p : c10::irange(params.size())) {
const auto& t = params[p];
for (const auto& sz : t.sizes()) {
      auto msg = c10::str(""["", process_group->getRank(),
                        ""]: params["", p, ""] in this process"",
"" with sizes "",
t.sizes(),
"" appears not to match sizes of the same param in process 0."");
"
135,"ASSERT_TRUE(set_module_attr(""DisableTorchFunction"", (PyObject*)THPModule_DisableTorchFunctionType(), /* incref= */ false));
torch::set_disabled_torch_function_impl(PyObject_GetAttrString(module, ""_disabled_torch_function_impl""));
ASSERT_TRUE(torch::disabled_torch_function_impl() != nullptr);
return module;
END_HANDLE_TH_ERRORS
}
","ASSERT_TRUE(set_module_attr(""DisableTorchFunction"", (PyObject*)THPModule_DisableTorchFunctionType(), /* incref= */ false));
torch::set_disabled_torch_function_impl(PyObject_GetAttrString(module, ""_disabled_torch_function_impl""));
ASSERT_TRUE(torch::disabled_torch_function_impl() != nullptr);
  torch::set_disabled_torch_dispatch_impl(PyObject_GetAttrString(module, ""_disabled_torch_dispatch_impl""));
  ASSERT_TRUE(torch::disabled_torch_dispatch_impl() != nullptr);
return module;
END_HANDLE_TH_ERRORS
}
"
136,"py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
  py::class_<DisableTorchDispatch>(_C_m, ""_DisableTorchDispatch"")
.def(py::init<>());
py::class_<torch::autograd::SavedVariable>(m, ""SavedTensor"")
","py::class_<c10::InferenceMode>(_C_m, ""_InferenceMode"")
.def(py::init<bool>());
  // TODO: line up this binding with DisableTorchFunction
  py::class_<torch::DisableTorchDispatch>(_C_m, ""_DisableTorchDispatch"")
.def(py::init<>());
py::class_<torch::autograd::SavedVariable>(m, ""SavedTensor"")
"
137,"{},
c10::parseType)
.toTuple();
  const auto& ivalues = ivaluesTuple->elements();
unpickled_records = std::make_shared<SourceRangeRecords>();
  for (auto& val : ivalues) {
const auto& tup_elems = val.toTupleRef().elements();
int64_t offset = tup_elems[kByteOffsetIndex].toInt();
auto source_range = deserializer->deserialize(tup_elems[kSourceRangeIndex]);
","{},
c10::parseType)
.toTuple();
  const auto& ivalues = ivaluesTuple->elements();
unpickled_records = std::make_shared<SourceRangeRecords>();
  IValue lines;
  if (ivalues[0].isString() &&
      kFormatWithStringTable == ivalues[0].toStringRef()) {
    deserializer.reset(new SourceRangeDeserializer(ivalues[1]));
    lines = ivalues[2];
  } else {
    deserializer.reset(new SourceRangeDeserializer());
    lines = ivaluesTuple;
  }
  for (auto& val : lines.toTuple()->elements()) {
const auto& tup_elems = val.toTupleRef().elements();
int64_t offset = tup_elems[kByteOffsetIndex].toInt();
auto source_range = deserializer->deserialize(tup_elems[kSourceRangeIndex]);
"
138,"std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.emplace_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.emplace_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
","std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.push_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.push_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
"
139,"SharedParserData& shared;
};
Parser::Parser(const std::shared_ptr<Source>& src)
: pImpl(new ParserImpl(src)) {}
Parser::~Parser() = default;
","SharedParserData& shared;
};
Parser::Parser(const std::shared_ptr<SourceView>& src)
: pImpl(new ParserImpl(src)) {}
Parser::~Parser() = default;
"
140,"if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer->deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
","if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer.deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
"
141,"std::unique_ptr<SocketImpl> SocketConnectOp::run() {
if (opts_->prefer_ipv6()) {
    C10D_INFO(""The client socket will attempt to connect to an IPv6 address of ({}, {})."",
              host_,
              port_);
if (tryConnect(AF_INET6)) {
return std::move(socket_);
}
    C10D_INFO(""The client socket will attempt to connect to an IPv4 address of ({}, {})."",
              host_,
              port_);
if (tryConnect(AF_INET)) {
return std::move(socket_);
}
} else {
    C10D_INFO(""The client socket will attempt to connect to an IPv4 or IPv6 address of ({}, {})."",
              host_,
              port_);
if (tryConnect(AF_UNSPEC)) {
return std::move(socket_);
","std::unique_ptr<SocketImpl> SocketConnectOp::run() {
if (opts_->prefer_ipv6()) {
    C10D_DEBUG(""The client socket will attempt to connect to an IPv6 address of ({}, {})."",
               host_,
               port_);
if (tryConnect(AF_INET6)) {
return std::move(socket_);
}
    C10D_DEBUG(""The client socket will attempt to connect to an IPv4 address of ({}, {})."",
               host_,
               port_);
if (tryConnect(AF_INET)) {
return std::move(socket_);
}
} else {
    C10D_DEBUG(""The client socket will attempt to connect to an IPv4 or IPv6 address of ({}, {})."",
               host_,
               port_);
if (tryConnect(AF_UNSPEC)) {
return std::move(socket_);
"
142,"auto desc = canonicalize_fft_shape_and_dim_args(self, s, dims);
TORCH_CHECK(desc.shape.size() > 0, fname, "" must transform at least one axis"");
last_dim_size = [&] {
// Fixup default shape handling in the last dimension,
if (!s.has_value() || (s->back() == -1)) {
","auto desc = canonicalize_fft_shape_and_dim_args(self, s, dims);
TORCH_CHECK(desc.shape.size() > 0, fname, "" must transform at least one axis"");
  // Expected output size of the hermitian-symmetric dimension
last_dim_size = [&] {
// Fixup default shape handling in the last dimension,
if (!s.has_value() || (s->back() == -1)) {
"
143,"const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qx);
  auto ResultBuf = makeQBufHandleNLC(
""quantized_conv1d"",
outputShape,
Dtype(out_qdtype),
","const auto out_qzero = c10::get<int64_t>(inputs[3]);
// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qx);
  auto ResultBuf = makeQBufHandleChannelsLast(
""quantized_conv1d"",
outputShape,
Dtype(out_qdtype),
"
144,"const double out_qscale = 1.0f / 256.0f;
const int64_t out_qzero = (out_qdtype == ScalarType::QInt8) ? -128 : 0;
  auto ResultBuf = makeQBufHandleNHWC(
""quantized_sigmoid"",
outputShape,
Dtype(out_qdtype),
","const double out_qscale = 1.0f / 256.0f;
const int64_t out_qzero = (out_qdtype == ScalarType::QInt8) ? -128 : 0;
  auto ResultBuf = makeQBufHandleChannelsLast(
""quantized_sigmoid"",
outputShape,
Dtype(out_qdtype),
"
145,"auto tuple = pop(stack).toTuple();
auto norm_index = normalizeIndex(index, tuple->elements().size());
if (norm_index < 0 ||
      norm_index > static_cast<int64_t>(tuple->elements().size())) {
throw std::out_of_range(""Tuple list index out of range"");
}
stack.emplace_back(tuple->elements()[norm_index]);
","auto tuple = pop(stack).toTuple();
auto norm_index = normalizeIndex(index, tuple->elements().size());
if (norm_index < 0 ||
      norm_index >= static_cast<int64_t>(tuple->elements().size())) {
throw std::out_of_range(""Tuple list index out of range"");
}
stack.emplace_back(tuple->elements()[norm_index]);
"
146,"std::vector<IValue> sample_inputs) {
// Enable TensorExpr fusion with dynamic shapes
setTensorExprDynamicShapeFusionEnabled(true);
  GRAPH_DEBUG(""Graph before tracing: "", graph);
auto traced_graph = TraceGraph(graph, sample_inputs);
  GRAPH_DEBUG(""Graph after tracing: "", traced_graph);
FuseTensorExprs(
traced_graph,
/*min_group_size*/ 2,
","std::vector<IValue> sample_inputs) {
// Enable TensorExpr fusion with dynamic shapes
setTensorExprDynamicShapeFusionEnabled(true);
  GRAPH_DEBUG(""Graph before tracing: "", *graph);
auto traced_graph = TraceGraph(graph, sample_inputs);
  GRAPH_DEBUG(""Graph after tracing: "", *traced_graph);
FuseTensorExprs(
traced_graph,
/*min_group_size*/ 2,
"
147,"} // namespace native
} // namespace at
#endif // AT_MKLDNN_EBABLED
","} // namespace native
} // namespace at
#endif // AT_MKLDNN_ENABLED
"
148,"} // namespace native
} // namespace at
#else // AT_MKLDNN_EBABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
","} // namespace native
} // namespace at
#else // AT_MKLDNN_ENABLED
#include <ATen/native/mkldnn/MKLDNNCommon.h>
"
149,"if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
    auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
150,"if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","if (bag_size.defined()) {
bag_size_data = bag_size.data_ptr<index_t>();
}
    auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
151,"VarPtr func_callee_arg = producer_index_vars_.at(i);
ExprPtr func_caller_param = dims.at(i);
if (func_callee_arg == nullptr) {
        auto param_val = evalInt(func_caller_param);
        if (!param_val || *param_val != 0) {
          // We are implicitly assuming that if you have an index of 0, that
          // must also be inlined into an index of 0.
          success_ = false;
          return nullptr;
        }
continue;
}
auto iter = inline_mapping_.find(func_callee_arg);
","VarPtr func_callee_arg = producer_index_vars_.at(i);
ExprPtr func_caller_param = dims.at(i);
if (func_callee_arg == nullptr) {
continue;
}
auto iter = inline_mapping_.find(func_callee_arg);
"
152,"if (optional_error_index) {
AT_ERROR(""Found an invalid max index: "", optional_error_index.value(),
"" (output volumes are of size "", output_height,
        ""x"", output_width);
}
if (!output_.is_contiguous(memory_format)) {
","if (optional_error_index) {
AT_ERROR(""Found an invalid max index: "", optional_error_index.value(),
"" (output volumes are of size "", output_height,
        ""x"", output_width, "")"");
}
if (!output_.is_contiguous(memory_format)) {
"
153,"// Since test elements is not an input of the TensorIterator, type promotion
// must be done manually.
ScalarType common_type = at::result_type(elements, test_elements);
  Tensor test_elements_flat = test_elements.to(common_type).ravel();
Tensor promoted_elements = elements.to(common_type);
auto iter = TensorIteratorConfig()
.add_output(out)
.add_input(promoted_elements)
","// Since test elements is not an input of the TensorIterator, type promotion
// must be done manually.
ScalarType common_type = at::result_type(elements, test_elements);
Tensor promoted_elements = elements.to(common_type);
  Tensor test_elements_flat = test_elements.to(common_type).view(-1);
  auto test_elements_stride = test_elements_flat.stride(0);

auto iter = TensorIteratorConfig()
.add_output(out)
.add_input(promoted_elements)
"
154,"values_below.unsqueeze_(0).transpose_(0, -1).squeeze_(-1);
}
  out.copy_(values_below);
}
std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cpu(
","values_below.unsqueeze_(0).transpose_(0, -1).squeeze_(-1);
}
  return values_below;
}

} // namespace

void quantile_out_impl(
    Tensor& out,
    const Tensor& self,
    const Tensor& q,
    const optional<int64_t> original_dim,
    const bool keepdim,
    const QUANTILE_INTERPOLATION_MODE& interpolation,
    const bool ignore_nan) {
  quantile_checks(self, q);
  TORCH_CHECK(
      self.scalar_type() == out.scalar_type(),
      ""quantile() out tensor must be same dtype as the input tensor"");
  TORCH_CHECK(
      self.device() == out.device(),
      ""quantile() out tensor must be on the same device as the input tensor"");

  int64_t wrapped_dim = at::maybe_wrap_dim(original_dim.value_or(0), self.dim());

  auto out_shape = quantile_output_shape(original_dim, self, q, keepdim, wrapped_dim);
  resize_output(out, out_shape);

  auto quantile = quantile_compute(
      self, q, original_dim, keepdim, interpolation, ignore_nan, wrapped_dim, out_shape);
  out.copy_(quantile);
}

Tensor quantile_impl(
    const Tensor& self,
    const Tensor& q,
    const optional<int64_t> original_dim,
    const bool keepdim,
    const QUANTILE_INTERPOLATION_MODE& interpolation,
    const bool ignore_nan) {
  quantile_checks(self, q);

  int64_t wrapped_dim = at::maybe_wrap_dim(original_dim.value_or(0), self.dim());

  auto out_shape = quantile_output_shape(original_dim, self, q, keepdim, wrapped_dim);

  return quantile_compute(
      self, q, original_dim, keepdim, interpolation, ignore_nan, wrapped_dim, out_shape);
}
std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cpu(
"
155,"#ifndef CPU_CAPABILITY_AVX512
REGISTER_DISPATCH(nansum_stub, &nansum_kernel_impl);
#else
REGISTER_NO_AVX512_DISPATCH(nansum_stub, reduce_fn);
#endif
}}  // namespace at::native
","#ifndef CPU_CAPABILITY_AVX512
REGISTER_DISPATCH(nansum_stub, &nansum_kernel_impl);
#else
REGISTER_NO_AVX512_DISPATCH(nansum_stub);
#endif
}}  // namespace at::native
"
156,"TORCH_CHECK(false, ""mkldnn_convolution_backward: ATen not compiled with MKLDNN support"");
}
REGISTER_NO_CPU_DISPATCH(mkldnn_convolution_backward_stub, mkldnn_convolution_backward_fn);
}}
","TORCH_CHECK(false, ""mkldnn_convolution_backward: ATen not compiled with MKLDNN support"");
}
REGISTER_NO_CPU_DISPATCH(mkldnn_convolution_backward_stub);
}}
"
157,"auto diff_graph = std::move(dnode->g(attr::Subgraph));
Gradient gradient = differentiate(diff_graph);
RemoveTensorTypeSpecializations(gradient.f);
      RemoveProfilingNodes(gradient.f);
GRAPH_DEBUG(""Forward graph:\n"", *(gradient.f));
GRAPH_DEBUG(""Backward graph:\n"", *(gradient.df));
// just like inside autograd.Functions, the forward of a differentiable
","auto diff_graph = std::move(dnode->g(attr::Subgraph));
Gradient gradient = differentiate(diff_graph);
RemoveTensorTypeSpecializations(gradient.f);
      ProfilingRecord::removeProfilingNodes(gradient.f->block());
GRAPH_DEBUG(""Forward graph:\n"", *(gradient.f));
GRAPH_DEBUG(""Backward graph:\n"", *(gradient.df));
// just like inside autograd.Functions, the forward of a differentiable
"
158,"copy,
getAutodiffSubgraphInlining() ? autodiffSubgraphNodeThreshold : 1);
replaceFallbackGraphWithFallbackFunction(copy->block());
    RemoveProfilingNodes(copy);
GRAPH_DEBUG(
""After InlineAutodiffSubgraphs and Removing Profiling Nodes\n"", *copy);
} else {
","copy,
getAutodiffSubgraphInlining() ? autodiffSubgraphNodeThreshold : 1);
replaceFallbackGraphWithFallbackFunction(copy->block());
    ProfilingRecord::removeProfilingNodes(copy->block());
GRAPH_DEBUG(
""After InlineAutodiffSubgraphs and Removing Profiling Nodes\n"", *copy);
} else {
"
159,"void StaticRuntime::set_inputs(
std::vector<IValue>&& args,
    const std::unordered_map<std::string, c10::IValue>& kwargs) {
if (!kwargs.empty()) {
// This is not ideal
TORCH_CHECK(
","void StaticRuntime::set_inputs(
std::vector<IValue>&& args,
    const KeywordArgs& kwargs) {
if (!kwargs.empty()) {
// This is not ideal
TORCH_CHECK(
"
160,"// to ProcessedNode.
AliasDb alias_db(
graph_, /*isFrozen=*/false, /*enablePreciseTupleContainerAnalysis=*/true);
// Construct constant and function nodes
for (Node* node : graph_->nodes()) {
","// to ProcessedNode.
AliasDb alias_db(
graph_, /*isFrozen=*/false, /*enablePreciseTupleContainerAnalysis=*/true);
  GRAPH_DEBUG(""AliasDb: "", alias_db.toString());
// Construct constant and function nodes
for (Node* node : graph_->nodes()) {
"
161,"#include <ATen/cuda/CUDABlas.h>
#include <ATen/cuda/CUDAEvent.h>
#include <c10/cuda/CUDAStream.h>
#include <ATen/native/LinearAlgebraUtils.h>
#include <ATen/native/cuda/MiscUtils.h>
","#include <ATen/cuda/CUDABlas.h>
#include <ATen/cuda/CUDAEvent.h>
#include <c10/cuda/CUDAStream.h>
#include <c10/util/irange.h>
#include <ATen/native/LinearAlgebraUtils.h>
#include <ATen/native/cuda/MiscUtils.h>
"
162,"""__torch__ types other than torchbind (__torch__.torch.classes)""
""are not supported in lite interpreter. "",
""Workaround: instead of using arbitrary class type (class Foo()), "",
          ""define a pytorch class (class Foo(torch.nn.Module))."");
}
types.emplace_back(type_str);
}
","""__torch__ types other than torchbind (__torch__.torch.classes)""
""are not supported in lite interpreter. "",
""Workaround: instead of using arbitrary class type (class Foo()), "",
          ""define a pytorch class (class Foo(torch.nn.Module)). The problematic type is: "",
          type_str);
}
types.emplace_back(type_str);
}
"
163,"size_t offset = 0;
uint8_t* start = static_cast<uint8_t*>(buffer_.get());
reused_tensors_ = 0;
auto group_idx = 0;
","size_t offset = 0;
uint8_t* start = static_cast<uint8_t*>(buffer_.get());
  buffer_start_ = start;
  buffer_end_ = start + managed_bytes_;
reused_tensors_ = 0;
auto group_idx = 0;
"
164,"#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
if (error_message.empty()) {
error_message = owner_->getDebugTable().getSourceDebugString(
          function_->getExceptionDebugHandle(), getTopModuleTypeName(*owner_));
}
#endif
","#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
if (error_message.empty()) {
error_message = owner_->getDebugTable().getSourceDebugString(
          function_->getExceptionDebugHandles(), getTopModuleTypeName(*owner_));
}
#endif
"
165,"return at::linalg_det(self);
}
Tensor& linalg_det_out(const Tensor& self, Tensor& out) {
checkSameDevice(""torch.linalg.det"", out, self, ""out"");
checkLinalgCompatibleDtype(""torch.linalg.det"", out, self, ""out"");
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point or complex tensor as input"");
IntArrayRef out_sizes(self.sizes().data(), self.dim() - 2);
at::native::resize_output(out, out_sizes);
  auto det = std::get<0>(at::native::_det_lu_based_helper(self));
out.copy_(det);
return out;
}
Tensor linalg_det(const Tensor& self) {
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point or complex tensor as input"");

  return std::get<0>(at::_det_lu_based_helper(self));
}

Tensor logdet(const Tensor& self) {
  squareCheckInputs(self);
  TORCH_CHECK((at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type())),
              ""Expected a floating point tensor as input"");
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
","return at::linalg_det(self);
}
Tensor linalg_det(const Tensor& self) {
  squareCheckInputs(self, ""linalg.det"");
  checkFloatingOrComplex(self, ""linalg.det"");

  return std::get<0>(at::_det_lu_based_helper(self));
}

Tensor& linalg_det_out(const Tensor& self, Tensor& out) {
checkSameDevice(""torch.linalg.det"", out, self, ""out"");
checkLinalgCompatibleDtype(""torch.linalg.det"", out, self, ""out"");
IntArrayRef out_sizes(self.sizes().data(), self.dim() - 2);
at::native::resize_output(out, out_sizes);
  auto det = at::native::linalg_det(self);
out.copy_(det);
return out;
}
Tensor logdet(const Tensor& self) {
  squareCheckInputs(self, ""logdet"");
  checkFloatingOrComplex(self, ""logdet"");
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
"
166,"}
std::tuple<Tensor, Tensor> linalg_slogdet(const Tensor& self) {
  squareCheckInputs(self);
ScalarType t = self.scalar_type();
TORCH_CHECK(t == ScalarType::Double || t == ScalarType::Float || t == ScalarType::ComplexFloat || t == ScalarType::ComplexDouble,
              ""linalg_slogdet: expected a tensor of float, double, cfloat or cdouble types but got "", t);
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
","}
std::tuple<Tensor, Tensor> linalg_slogdet(const Tensor& self) {
  squareCheckInputs(self, ""linalg.slogdet"");
ScalarType t = self.scalar_type();
TORCH_CHECK(t == ScalarType::Double || t == ScalarType::Float || t == ScalarType::ComplexFloat || t == ScalarType::ComplexDouble,
              ""linalg.slogdet: expected a tensor of float, double, cfloat or cdouble types but got "", t);
c10::ExclusivelyOwned<Tensor> det_P, diag_U;
std::tie(det_P, diag_U) = _lu_det_P_diag_U(self);
"
167,"const optional<Tensor>& rtol,
bool hermitian,
Tensor& result) {
  checkSameDevice(""linalg_pinv"", result, input);
  checkLinalgCompatibleDtype(""linalg_pinv"", result, input);
Tensor result_tmp = at::linalg_pinv(input, atol, rtol, hermitian);
at::native::resize_output(result, result_tmp.sizes());
result.copy_(result_tmp);
","const optional<Tensor>& rtol,
bool hermitian,
Tensor& result) {
  checkSameDevice(""linalg.pinv"", result, input);
  checkLinalgCompatibleDtype(""linalg.pinv"", result, input);
Tensor result_tmp = at::linalg_pinv(input, atol, rtol, hermitian);
at::native::resize_output(result, result_tmp.sizes());
result.copy_(result_tmp);
"
168,"// TODO: implement _out variant avoiding copy and using already allocated storage directly
Tensor& linalg_cond_out(const Tensor& self, c10::string_view ord, Tensor& result) {
  checkSameDevice(""linalg_cond"", result, self);
ScalarType real_dtype = toValueType(self.scalar_type());
  checkLinalgCompatibleDtype(""linalg_cond"", result.scalar_type(), real_dtype);
Tensor result_tmp = at::linalg_cond(self, ord);
at::native::resize_output(result, result_tmp.sizes());
","// TODO: implement _out variant avoiding copy and using already allocated storage directly
Tensor& linalg_cond_out(const Tensor& self, c10::string_view ord, Tensor& result) {
  checkSameDevice(""linalg.cond"", result, self);
ScalarType real_dtype = toValueType(self.scalar_type());
  checkLinalgCompatibleDtype(""linalg.cond"", result.scalar_type(), real_dtype);
Tensor result_tmp = at::linalg_cond(self, ord);
at::native::resize_output(result, result_tmp.sizes());
"
169,"auto exp_a = a;
auto exp_b = b;
// mkldnn tensors only support reshape, not expand or view operators
if (a_size.equals(out_size)) {
push(stack, a);
} else if (out_numel == a.numel()) {
exp_a = a.reshape(out_size);
} else {
","auto exp_a = a;
auto exp_b = b;
      int stacked = 0;
// mkldnn tensors only support reshape, not expand or view operators
if (a_size.equals(out_size)) {
push(stack, a);
        ++stacked;
} else if (out_numel == a.numel()) {
exp_a = a.reshape(out_size);
} else {
"
170,"if (b_size.equals(out_size)) {
push(stack, b);
} else if (out_numel == b.numel()) {
exp_b = b.reshape(out_size);
} else {
exp_b = b.to_dense().expand(out_size).to_mkldnn();
}
      {
// If one of the inputs was expanded and converted to nchw/nhwc
// we might end up in a very bad spot if the second argument
// is in a blocked format. In this case, MKLDNN uses its
","if (b_size.equals(out_size)) {
push(stack, b);
        ++stacked;
} else if (out_numel == b.numel()) {
exp_b = b.reshape(out_size);
} else {
exp_b = b.to_dense().expand(out_size).to_mkldnn();
}
      if (stacked < 2) {
        if (stacked == 1) {
          pop(stack);
        }
// If one of the inputs was expanded and converted to nchw/nhwc
// we might end up in a very bad spot if the second argument
// is in a blocked format. In this case, MKLDNN uses its
"
171,"unmanaged_ivalues.erase(output);
}
// copy to unmanaged_ivalues_
unmanaged_ivalues_.reserve(unmanaged_ivalues.size());
unmanaged_ivalues_.insert(
","unmanaged_ivalues.erase(output);
}
  GRAPH_DEBUG(""managed_tensor_values: "", dumpValueSet(managed_tensor_values));

// copy to unmanaged_ivalues_
unmanaged_ivalues_.reserve(unmanaged_ivalues.size());
unmanaged_ivalues_.insert(
"
172,"checkBackend(c, {*log_probs}, Backend::CUDA);
checkBackend(c, {*targets}, Backend::CPU);
const auto batch_size = log_probs->size(1);
  TORCH_CHECK(input_lengths_.size() == batch_size, ""input_lengths needs to have size to match batch_size"");
  TORCH_CHECK(target_lengths_.size() == batch_size, ""target_lengths needs to have size to match batch_size"");
std::vector<int> input_lengths(input_lengths_.begin(), input_lengths_.end());
std::vector<int> target_lengths(target_lengths_.begin(), target_lengths_.end());
","checkBackend(c, {*log_probs}, Backend::CUDA);
checkBackend(c, {*targets}, Backend::CPU);
const auto batch_size = log_probs->size(1);
  TORCH_CHECK(static_cast<int64_t>(input_lengths_.size()) == batch_size, ""input_lengths needs to have size to match batch_size"");
  TORCH_CHECK(static_cast<int64_t>(target_lengths_.size()) == batch_size, ""target_lengths needs to have size to match batch_size"");
std::vector<int> input_lengths(input_lengths_.begin(), input_lengths_.end());
std::vector<int> target_lengths(target_lengths_.begin(), target_lengths_.end());
"
173,"auto stride = tensor_indices_or_sections.stride(0);
auto numel = tensor_indices_or_sections.numel();
std::vector<int64_t> indices(numel);
    for (size_t offset = 0; offset < numel; offset++) {
// indices tensor could be non-contiguous
indices[offset] = *(indices_data + offset * stride);
}
","auto stride = tensor_indices_or_sections.stride(0);
auto numel = tensor_indices_or_sections.numel();
std::vector<int64_t> indices(numel);
    for (const auto offset : c10::irange(numel)) {
// indices tensor could be non-contiguous
indices[offset] = *(indices_data + offset * stride);
}
"
174,"}
Tensor cosine_similarity(const Tensor& x1, const Tensor& x2, int64_t dim, double eps) {
  TORCH_CHECK(x1.ndimension() == x2.ndimension(), ""cosine_similarity requires both inputs to have the same number of dimensions, but x1 has "",
              x1.ndimension(), "" and x2 has "", x2.ndimension());
  TORCH_CHECK(x1.ndimension() == 0 || x1.size(dim) == x2.size(dim), ""cosine_similarity requires both inputs to have the same size at dimension "", dim, ""but x1 has "",
  x1.size(dim), "" and x2 has "", x2.size(dim));
auto commonDtype = at::result_type(x1, x2);
TORCH_CHECK(at::isFloatingType(commonDtype), ""expected common dtype to be floating point, yet common dtype is "", commonDtype);
  Tensor x1_ = x1.to(commonDtype);
  Tensor x2_ = x2.to(commonDtype);
// Follow scipy impl to improve numerical precision
// Use x / sqrt(x * x) instead of x / (sqrt(x) * sqrt(x))
Tensor w12 = at::sum(x1_ * x2_, dim);
","}
Tensor cosine_similarity(const Tensor& x1, const Tensor& x2, int64_t dim, double eps) {
  auto common_size = at::infer_size_dimvector(x1.sizes(), x2.sizes());
auto commonDtype = at::result_type(x1, x2);
TORCH_CHECK(at::isFloatingType(commonDtype), ""expected common dtype to be floating point, yet common dtype is "", commonDtype);
  Tensor x1_ = x1.to(commonDtype).expand(common_size);
  Tensor x2_ = x2.to(commonDtype).expand(common_size);
// Follow scipy impl to improve numerical precision
// Use x / sqrt(x * x) instead of x / (sqrt(x) * sqrt(x))
Tensor w12 = at::sum(x1_ * x2_, dim);
"
175,"namespace c10 {
namespace {
inline bool is_contiguous_strides(
    const IntArrayRef sizes,
    const IntArrayRef strides) {
  int n_dim = static_cast<int>(sizes.size());

  if (n_dim == 0 || strides[n_dim-1] != 1) {
    return false;
  }

  for (int i = n_dim - 2; i >= 0; i--) {
    if (strides[i] != strides[i+1] * sizes[i+1]) {
      return false;
    }
  }
  return true;
}

} // namespace

TypeVerbosity type_verbosity() {
static const char* c_verbosity = std::getenv(""PYTORCH_JIT_TYPE_VERBOSITY"");
static TypeVerbosity verbosity = c_verbosity ?
","namespace c10 {
TypeVerbosity type_verbosity() {
static const char* c_verbosity = std::getenv(""PYTORCH_JIT_TYPE_VERBOSITY"");
static TypeVerbosity verbosity = c_verbosity ?
"
176,"}
}

","}
}
"
177,"VK_CHECK(vkCreateInstance(&instance_create_info, nullptr, &instance));
TORCH_CHECK(instance, ""Invalid Vulkan instance!"");
return instance;
}
","VK_CHECK(vkCreateInstance(&instance_create_info, nullptr, &instance));
TORCH_CHECK(instance, ""Invalid Vulkan instance!"");
#ifdef USE_VULKAN_WRAPPER
#ifdef USE_VULKAN_VOLK
  volkLoadInstance(instance);
#endif
#endif

return instance;
}
"
178,"size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
auto ivalues =
          jit::unpickle(
              reinterpret_cast<const char*>(debug_data.get()), debug_size)
              .toTuple()
              ->elements();
SourceRangeDeserializer deserializer;
for (auto& val : ivalues) {
        auto tup_elems = val.toTuple()->elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
// byte_offset, debug_handle (=source range tag), source range
","size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
auto ivalues =
          std::move(
              *jit::unpickle(
                   reinterpret_cast<const char*>(debug_data.get()), debug_size)
                   .toTuple())
              .elements();
SourceRangeDeserializer deserializer;
for (auto& val : ivalues) {
        auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
// byte_offset, debug_handle (=source range tag), source range
"
179,"} else {
x_viewed = x;
}
    // Note the .slice operation reduces the dimension along 'dim' by 1.
    // The sizes of other dimensions are untouched.
Tensor x_left = x_viewed.slice(dim, 0, -1);
Tensor x_right = x_viewed.slice(dim, 1);
","} else {
x_viewed = x;
}
    // Note the .slice operation reduces the dimension along 'dim' by 1,
    // while the sizes of other dimensions are untouched.
Tensor x_left = x_viewed.slice(dim, 0, -1);
Tensor x_right = x_viewed.slice(dim, 1);
"
180,"// Since we are cloning user, its result should be the same scalar type
// as the user. But the dims should correspond to that of the input.
auto input_tensor_type = inp->type()->cast<c10::TensorType>();
    TORCH_INTERNAL_ASSERT(input_tensor_type);
auto new_input_type =
input_tensor_type->withScalarType(user_tensor_type->scalarType());
new_cat_input->output()->setType(new_input_type);
","// Since we are cloning user, its result should be the same scalar type
// as the user. But the dims should correspond to that of the input.
auto input_tensor_type = inp->type()->cast<c10::TensorType>();
    TORCH_INTERNAL_ASSERT(
        input_tensor_type, buildErrorMessage(""Unexpected input tensor type""));
auto new_input_type =
input_tensor_type->withScalarType(user_tensor_type->scalarType());
new_cat_input->output()->setType(new_input_type);
"
181,"StmtPtr TermExpander::mutate(AllocatePtr v) {
BufPtr buf = v->buf();
BufPtr buf_new = to<Buf>(v->buf()->accept_mutator(this));
  TORCH_INTERNAL_ASSERT(buf_new);
ExprPtr flattened = buf_flat_size(buf_new);
if (flattened->isConstant() && immediateEquals(flattened, 0)) {
","StmtPtr TermExpander::mutate(AllocatePtr v) {
BufPtr buf = v->buf();
BufPtr buf_new = to<Buf>(v->buf()->accept_mutator(this));
  TORCH_INTERNAL_ASSERT(
      buf_new,
      buildErrorMessage(""TermExpander mutation produced null for Buf.""));
ExprPtr flattened = buf_flat_size(buf_new);
if (flattened->isConstant() && immediateEquals(flattened, 0)) {
"
182,"throw malformed_input(msg);
}
  TORCH_INTERNAL_ASSERT(tt->sizes().concrete_sizes());
auto sizes = *tt->sizes().concrete_sizes();
std::vector<int64_t> default_strides = TensorType::contiguousStridesOf(sizes);
if (!tt->strides().concrete_sizes()) {
return Tensor(buf, nullptr);
}
  TORCH_INTERNAL_ASSERT(tt->strides().concrete_sizes());
const std::vector<int64_t> strides = *tt->strides().concrete_sizes();
// All Tensors in NNC are layed out in default, contiguous layout.
// If the output is also default contiguous we don't need to do anything
","throw malformed_input(msg);
}
  TORCH_INTERNAL_ASSERT(
      tt->sizes().concrete_sizes(),
      buildErrorMessage(""Output shapes are unknown.""));
auto sizes = *tt->sizes().concrete_sizes();
std::vector<int64_t> default_strides = TensorType::contiguousStridesOf(sizes);
if (!tt->strides().concrete_sizes()) {
return Tensor(buf, nullptr);
}
  TORCH_INTERNAL_ASSERT(
      tt->strides().concrete_sizes(),
      buildErrorMessage(""Output strides are unknown.""));
const std::vector<int64_t> strides = *tt->strides().concrete_sizes();
// All Tensors in NNC are layed out in default, contiguous layout.
// If the output is also default contiguous we don't need to do anything
"
183,"void replaceConv1dWithConv2d(std::shared_ptr<Graph>& graph) {
std::string conv_1d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
        %r = aten::conv1d(%input, %weight, %bias, %stride, %padding, %dilation, %groups)
        return (%r) )"";
std::string conv_2d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
","void replaceConv1dWithConv2d(std::shared_ptr<Graph>& graph) {
std::string conv_1d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
        %res = aten::conv1d(%input, %weight, %bias, %stride, %padding, %dilation, %groups)
        return (%res) )"";
std::string conv_2d_pattern = R""(
graph(%input, %weight, %bias, %stride:int[], %padding:int[], %dilation:int[], %groups:int):
"
184,"%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %r = aten::relu_(%conv2d_res)
        return (%r) )"";
rewriter.RegisterRewritePattern(
      linear_prepack_run_relu_inplace, linear_prepack_run_relu_fused);
rewriter.RegisterRewritePattern(
      conv2d_prepack_run_relu_inplace, conv2d_prepack_run_relu_fused);
rewriter.runOnGraph(graph, torch::jit::graph_rewrite_helper::isClampFusable);
}
","%weight, %bias, %stride, %padding, %dilation, %groups,
%dummy_min_max, %dummy_min_max)
%conv2d_res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        %res = aten::relu_(%conv2d_res)
        return (%res) )"";

  value_mappings = {
      {""output_min"", ""packed_weight_bias""},
      {""output_max"", ""packed_weight_bias""},
      {""packed_weight_bias"", ""packed_weight_bias""},
      {""res"", ""res""}};
rewriter.RegisterRewritePattern(
      linear_prepack_run_relu_inplace,
      linear_prepack_run_relu_fused,
      value_mappings);

  value_mappings = {
      {""output_min"", ""packed_weight_bias""},
      {""output_max"", ""packed_weight_bias""},
      {""packed_weight_bias"", ""packed_weight_bias""},
      {""res"", ""res""}};

rewriter.RegisterRewritePattern(
      conv2d_prepack_run_relu_inplace,
      conv2d_prepack_run_relu_fused,
      value_mappings);
rewriter.runOnGraph(graph, torch::jit::graph_rewrite_helper::isClampFusable);
}
"
185,"return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(old_indices_.size() == v->indices().size());
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
","return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(
        old_indices_.size() == v->indices().size(),
        buildErrorMessage(
            ""Expected ranks to match in RfactorStoreRewriter in the fuser.""));
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
"
186,"if (aStrides.empty() || oStrides.empty()) {
return false;
}
  TORCH_INTERNAL_ASSERT(info->bounds().size() == other->bounds().size());
for (size_t b = 0; b < info->bounds().size(); ++b) {
ExprPtr aIndexStride = aStrides[b];
ExprPtr oIndexStride = oStrides[b];
","if (aStrides.empty() || oStrides.empty()) {
return false;
}
  TORCH_INTERNAL_ASSERT(
      info->bounds().size() == other->bounds().size(),
      buildErrorMessage(
          ""Dimension mismatch for two accesses in mem dep checker in the fuser.""));
for (size_t b = 0; b < info->bounds().size(); ++b) {
ExprPtr aIndexStride = aStrides[b];
ExprPtr oIndexStride = oStrides[b];
"
187,"for (auto const& input : node->inputs()) {
if (auto tt = input->type()->cast<TensorType>()) {
if (auto inputDevice = tt->device()) {
          TORCH_INTERNAL_ASSERT(!device || *device == *inputDevice);
device = inputDevice;
}
}
}
}
  TORCH_INTERNAL_ASSERT(device);
return device;
}
","for (auto const& input : node->inputs()) {
if (auto tt = input->type()->cast<TensorType>()) {
if (auto inputDevice = tt->device()) {
          TORCH_INTERNAL_ASSERT(
              !device || *device == *inputDevice,
              buildErrorMessage(
                  ""Different devices specified for inputs to the fuser.""));
device = inputDevice;
}
}
}
}
  TORCH_INTERNAL_ASSERT(
      device,
      buildErrorMessage(""Could not find device in fuser graph inputs.""));
return device;
}
"
188,"Instruction inst = inst_with_handle.instruction;
DebugHandle debug_handle = inst_with_handle.debug_handle;
      //    std::cout << ""RUNNING "" << pc << "" "" << code_->instructions_[pc];
      //    if (inst.op == OP) {
      //      std::cout << "", "" << code_->op_names_[inst.X].name;
      //      if (!code_->op_names_[inst.X].overload_name.empty()) {
      //        std::cout << ""."" << code_->op_names_[inst.X].overload_name;
      //      }
      //    }
      //    std::cout << std::endl;
// TODO(iliacher): remove the workaround after RecordFunction is in
// Dispatcher
","Instruction inst = inst_with_handle.instruction;
DebugHandle debug_handle = inst_with_handle.debug_handle;
      // std::cout << ""RUNNING "" << pc << "" ""
      //           << code_->instructions_with_handles_[pc].instruction;
      // if (inst.op == OP) {
      //   std::cout << "", "" << code_->op_names_[inst.X].name;
      //   if (!code_->op_names_[inst.X].overload_name.empty()) {
      //     std::cout << ""."" << code_->op_names_[inst.X].overload_name;
      //   }
      // }
      // std::cout << std::endl;
// TODO(iliacher): remove the workaround after RecordFunction is in
// Dispatcher
"
189,"}
}
}
inline_mapping_.erase(v);
}
return result;
","}
}
}
      GRAPH_DEBUG(""ComputeInline: Inline mapping: erasing"", std::to_string(v));
inline_mapping_.erase(v);
}
return result;
"
190,"TORCH_INTERNAL_ASSERT(relevant_store);
FunctionInliner inliner(relevant_store, output_bufs_);
root_stmt_ = root_stmt_->accept_mutator(&inliner);
","TORCH_INTERNAL_ASSERT(relevant_store);
  GRAPH_DEBUG(""ComputeInline: Def: "", std::to_string(relevant_store));
FunctionInliner inliner(relevant_store, output_bufs_);
root_stmt_ = root_stmt_->accept_mutator(&inliner);
"
191,"// Calculate the size of the dimension we need to quantize over,
// For per-channel quant we default to axis 0, since it is only for
// weight quantization currently.
  int64_t size = per_row_fake_quant ? self.size(0) : 1;
  if (per_row_fake_quant && running_min.numel() == 0) {
    float inf = std::numeric_limits<float>::infinity();
    running_min.resize_(size).fill_(inf);
    running_max.resize_(size).fill_(-inf);
    scale.resize_(size);
    zero_point.resize_(size);
  }
  if (observe) {
    calculate_moving_average(
        self,
        running_min,
        running_max,
        averaging_const,
        per_row_fake_quant,
        ch_axis);
}
// Calculate qparams and fake_quantize
auto fake_quant = fake_quant_on.item().toInt();
","// Calculate the size of the dimension we need to quantize over,
// For per-channel quant we default to axis 0, since it is only for
// weight quantization currently.
  if (per_row_fake_quant) {
    at::Tensor y = self;
    if (self.dim() != 2) {
      auto res = DimVector(self.sizes());
      std::iota(res.begin(), res.end(), 0);
      res[ch_axis] = 0;
      res[0] = ch_axis;

      y = self.permute(res);
      y = y.flatten(1);
    }
    int64_t size = self.size(ch_axis);
    if (running_min.numel() == 0) {
      float inf = std::numeric_limits<float>::infinity();
      running_min.resize_(size).fill_(inf);
      running_max.resize_(size).fill_(-inf);
      scale.resize_(size);
      zero_point.resize_(size);
    }
    if (observe) {
      calculate_moving_average(
          y,
          running_min,
          running_max,
          averaging_const,
          per_row_fake_quant,
          ch_axis);
    }
  } else {
    if (observe) {
      calculate_moving_average(
          self,
          running_min,
          running_max,
          averaging_const,
          per_row_fake_quant,
          ch_axis);
    }
}
// Calculate qparams and fake_quantize
auto fake_quant = fake_quant_on.item().toInt();
"
192,"}
#ifdef _WIN32
  struct timeval timeoutTV = {value.count() / 1000,
                              (value.count() % 1000) * 1000};
#else
struct timeval timeoutTV = {.tv_sec = value.count() / 1000,
.tv_usec = (value.count() % 1000) * 1000};
","}
#ifdef _WIN32
  struct timeval timeoutTV = {static_cast<long>(value.count() / 1000),
                              static_cast<long>((value.count() % 1000) * 1000)};
#else
struct timeval timeoutTV = {.tv_sec = value.count() / 1000,
.tv_usec = (value.count() % 1000) * 1000};
"
193,"Py_XDECREF(data_);
}
}
}}
","Py_XDECREF(data_);
}
}

  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::pack_hook_ = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
  PyObject* PyDefaultSavedVariableHooks::unpack_hook_ = nullptr;

  void PyDefaultSavedVariableHooks::set_hooks(py::function &pack_hook, py::function &unpack_hook) {
    TORCH_CHECK(!pack_hook_ && !unpack_hook_,
        ""Setting default hooks but they have already been set. ""
        ""Hint: only one pair of hooks is allowed at a time."");
    pack_hook_ = pack_hook.release().ptr();
    unpack_hook_ = unpack_hook.release().ptr();
  }

  void PyDefaultSavedVariableHooks::reset_hooks() {
    if (Py_IsInitialized()) {
      py::gil_scoped_acquire gil;
      Py_XDECREF(pack_hook_);
      Py_XDECREF(unpack_hook_);
    }
    pack_hook_ = nullptr;
    unpack_hook_ = nullptr;
  }

  std::unique_ptr<SavedVariableHooks> PyDefaultSavedVariableHooks::get_hooks() {
    if (!pack_hook_ || !unpack_hook_) {
      return nullptr;
    }
    py::gil_scoped_acquire gil;
    py::function pack_hook = py::reinterpret_borrow<py::function>(pack_hook_);
    py::function unpack_hook = py::reinterpret_borrow<py::function>(unpack_hook_);
    return std::make_unique<PySavedVariableHooks>(pack_hook, unpack_hook);
  }

}}
"
194,"}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
  if (!hooks) {
    return;
  }
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
","}
void SavedVariable::register_hooks(std::unique_ptr<SavedVariableHooks>&& hooks) {
TORCH_CHECK(!hooks_,
""Calling register_hooks on a saved tensor whose hooks have already been set. ""
""Hint: only one pair of hooks is allowed at a time."");
"
195,"auto* lc_node = input->node();
TypePtr elem =
lc_node->output()->type()->castRaw<ListType>()->getElementType();
      if (elem->cast<IntType>()) {
        // ListConstruct Int[] output case, we need to transform to ONNX
        // Concat to ensure the output is a single tensor(dynamic) type in
        // order to be consumed as inputs
        std::vector<Value*> unsqueezed;
        Graph* g = block->owningGraph();
        for (auto* input : lc_node->inputs()) {
          Node* unsqueezed_node =
              createONNXUnsqueeze(g, lc_node, input, 0, opset_version);
          unsqueezed.emplace_back(unsqueezed_node->output());
        }
        Node* concat_node = g->create(onnx::Concat, 1);
        concat_node->i_(attr::axis, 0);
        for (auto v : unsqueezed) {
          concat_node->addInput(v);
        }
        concat_node->insertBefore(lc_node);

// make concat node output as new input, then ListConstruct should
// become dead
replacements.emplace_back(
i, std::vector<Value*>({concat_node->output()}));

} else {
if (opset_version >= OPSET_VERSION_11) {
c10::Symbol seq_node_kind = lc_node->inputs().size() > 0
","auto* lc_node = input->node();
TypePtr elem =
lc_node->output()->type()->castRaw<ListType>()->getElementType();
      if (elem->cast<IntType>() &&
          isValidToTransformToONNXConcatNode(lc_node)) {
        auto concat_node = transformToONNXConcatNode(
            block->owningGraph(), input->node(), false, opset_version);
// make concat node output as new input, then ListConstruct should
// become dead
replacements.emplace_back(
i, std::vector<Value*>({concat_node->output()}));
} else {
if (opset_version >= OPSET_VERSION_11) {
c10::Symbol seq_node_kind = lc_node->inputs().size() > 0
"
196,"#include <pthread.h>
#include <algorithm>
#include <atomic>
#include <chrono>
#include <iostream>
#include <sstream>
#include <thread>
#include <vector>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <torch/deploy.h>

#include <ATen/ATen.h>
#include <ATen/TypeDefault.h>
#include <c10/util/irange.h>

#include <torch/script.h>

typedef void (*function_type)(const char*);
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
","#include <torch/deploy.h>

#include <ATen/ATen.h>
#include <ATen/TypeDefault.h>
#include <c10/util/irange.h>

#include <torch/script.h>

#include <pthread.h>
#include <algorithm>
#include <atomic>
#include <cassert>
#include <chrono>
#include <iostream>
#include <sstream>
#include <thread>
#include <vector>
typedef void (*function_type)(const char*);
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
"
197,"#define PY_SSIZE_T_CLEAN
#include <Python.h>
#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <iostream>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <pybind11/embed.h>
#include <pybind11/functional.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <stdio.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>
#include <iostream>
#include <map>
#include <thread>
","#define PY_SSIZE_T_CLEAN
#include <Python.h>
#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <pybind11/embed.h>
#include <pybind11/functional.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>

#include <cassert>
#include <cstdio>
#include <iostream>
#include <map>
#include <thread>
"
198,"""write_files"",
&ScriptModuleSerializer::writeFiles,
py::arg(""code_dir"") = "".data/ts_code/code/"")
      .def(""storage_context"", &ScriptModuleSerializer::storage_context);
// Used by torch.package to coordinate sharing of storages between eager
// and ScriptModules.
","""write_files"",
&ScriptModuleSerializer::writeFiles,
py::arg(""code_dir"") = "".data/ts_code/code/"")
      .def(
          ""storage_context"",
          &ScriptModuleSerializer::storage_context,
          pybind11::return_value_policy::reference_internal);
// Used by torch.package to coordinate sharing of storages between eager
// and ScriptModules.
"
199,"sizeof(THPFunction),                         /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPFunction_dealloc,             /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","sizeof(THPFunction),                         /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPFunction_dealloc,             /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
200,"0,                                           /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","0,                                           /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
201,"0,                                     /* tp_itemsize */
(destructor)THCPStream_dealloc,        /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_getattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_setattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_reserved */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_repr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_as_number */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_as_sequence */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_as_mapping */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_hash  */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_call */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_str */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_getattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_setattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_traverse */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_clear */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_iter */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_iternext */
THCPStream_methods,                    /* tp_methods */
THCPStream_members,                    /* tp_members */
THCPStream_properties,                 /* tp_getset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_base */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_dict */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_descr_get */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_descr_set */
0,                                     /* tp_dictoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_init */
  // NOLINTNEXTLINE(modernize-use-nullptr)
  0,                                     /* tp_alloc */
THCPStream_pynew,                      /* tp_new */
};
","0,                                     /* tp_itemsize */
(destructor)THCPStream_dealloc,        /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
  nullptr,                               /* tp_getattr */
  nullptr,                               /* tp_setattr */
  nullptr,                               /* tp_reserved */
  nullptr,                               /* tp_repr */
  nullptr,                               /* tp_as_number */
  nullptr,                               /* tp_as_sequence */
  nullptr,                               /* tp_as_mapping */
  nullptr,                               /* tp_hash  */
  nullptr,                               /* tp_call */
  nullptr,                               /* tp_str */
  nullptr,                               /* tp_getattro */
  nullptr,                               /* tp_setattro */
  nullptr,                               /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
  nullptr,                               /* tp_traverse */
  nullptr,                               /* tp_clear */
  nullptr,                               /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
  nullptr,                               /* tp_iter */
  nullptr,                               /* tp_iternext */
THCPStream_methods,                    /* tp_methods */
THCPStream_members,                    /* tp_members */
THCPStream_properties,                 /* tp_getset */
  nullptr,                               /* tp_base */
  nullptr,                               /* tp_dict */
  nullptr,                               /* tp_descr_get */
  nullptr,                               /* tp_descr_set */
0,                                     /* tp_dictoffset */
  nullptr,                               /* tp_init */
  nullptr,                               /* tp_alloc */
THCPStream_pynew,                      /* tp_new */
};
"
202,"has_random),
device_(device) {
// Initializes driver's API context (if necessary)
  // NOLINTNEXTLINE(modernize-use-nullptr)
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  CUcontext pctx = 0;
AT_CUDA_DRIVER_CHECK(nvrtc().cuCtxGetCurrent(&pctx));
if (!pctx) {
std::unique_lock<std::mutex> cudaFreeMutexLock(
*(c10::cuda::CUDACachingAllocator::getFreeMutex()));
    // NOLINTNEXTLINE(modernize-use-nullptr)
    cudaFree(0);
}
// Note: hacked at::DeviceGuard since at::DeviceGuard was failing to work
","has_random),
device_(device) {
// Initializes driver's API context (if necessary)
  CUcontext pctx = nullptr;
AT_CUDA_DRIVER_CHECK(nvrtc().cuCtxGetCurrent(&pctx));
if (!pctx) {
std::unique_lock<std::mutex> cudaFreeMutexLock(
*(c10::cuda::CUDACachingAllocator::getFreeMutex()));
    cudaFree(nullptr);
}
// Note: hacked at::DeviceGuard since at::DeviceGuard was failing to work
"
203,"// Any additional post process that are specific to individual node kind.
void SpecialPostProcess(Node* n) {
switch (n->kind()) {
    case ::c10::onnx::If: {
      if (!IsBlockReturnTypeSame(n) && IsStaticConditionONNX(n)) {
        auto cond = ConditionValueONNX(n);
        auto block_idx = cond ? 0 : 1;
        for (const auto i : c10::irange(n->outputs().size())) {
          n->outputs()[i]->setType(
              n->blocks()[block_idx]->outputs()[i]->type());
        }
      }
      break;
    }
case ::c10::onnx::SequenceInsert: {
// Special case when input sequence to SequenceInsert is empty.
// onnx Sequence type requires element type to be set.
","// Any additional post process that are specific to individual node kind.
void SpecialPostProcess(Node* n) {
switch (n->kind()) {
case ::c10::onnx::SequenceInsert: {
// Special case when input sequence to SequenceInsert is empty.
// onnx Sequence type requires element type to be set.
"
204,"return false;
}
bool LoopNest::fuseLoops(const std::vector<For*>& loops, For** fused) {
if (loops.empty()) {
return false;
}
","return false;
}
bool LoopNest::unsafeFuseLoops(const std::vector<For*>& loops, For** fused) {
if (loops.empty()) {
return false;
}
"
205,"}
}
// NOLINTNEXTLINE(modernize-use-equals-default)
MemoryReportingInfoBase::MemoryReportingInfoBase() {}
} // namespace c10
","}
}
MemoryReportingInfoBase::MemoryReportingInfoBase() = default;
} // namespace c10
"
206,"}
struct C10_API DefaultCPUAllocator final : at::Allocator {
  // NOLINTNEXTLINE(modernize-use-equals-default)
  DefaultCPUAllocator() {}
  // NOLINTNEXTLINE(modernize-use-equals-default)
  ~DefaultCPUAllocator() override {}
at::DataPtr allocate(size_t nbytes) const override {
void* data = alloc_cpu(nbytes);
profiledCPUMemoryReporter().New(data, nbytes);
","}
struct C10_API DefaultCPUAllocator final : at::Allocator {
  DefaultCPUAllocator() = default;
at::DataPtr allocate(size_t nbytes) const override {
void* data = alloc_cpu(nbytes);
profiledCPUMemoryReporter().New(data, nbytes);
"
207,"} else {
dtype = p_node->Input(1).toScalarType();
}
          // handle memory format
          c10::optional<c10::MemoryFormat> memory_format = c10::nullopt;
          if (p_node->inputs().size() == 5) {
            memory_format = p_node->Input(4).toOptional<c10::MemoryFormat>();
          }
          if (memory_format.value_or(c10::MemoryFormat::Preserve) ==
              c10::MemoryFormat::Preserve) {
if (self.is_non_overlapping_and_dense()) {
memory_format = c10::nullopt;
} else {
memory_format = self.suggest_memory_format();
}
}
// See Note [Explicit nullopt MemoryFormat argument]
p_node->Output(0) = at::detail::empty_cpu(
              {0}, dtype, layout, self.device(), c10::nullopt, memory_format);
}
        // ignore input 3 (copy)
        auto non_blocking = p_node->Input(2).toBool(); // non_blocking
auto& out_t = p_node->Output(0).toTensor();
fastResizeToZero(out_t);
        at::native::to_copy_out(out_t, self, non_blocking);
};
});
","} else {
dtype = p_node->Input(1).toScalarType();
}

          if (memory_format == c10::MemoryFormat::Preserve) {
if (self.is_non_overlapping_and_dense()) {
memory_format = c10::nullopt;
              copy_strides = true;
} else {
memory_format = self.suggest_memory_format();
}
}

// See Note [Explicit nullopt MemoryFormat argument]
          // Can't use size {0} if memory_format is ChannelLast
p_node->Output(0) = at::detail::empty_cpu(
              self.sizes(),
              dtype,
              layout,
              self.device(),
              c10::nullopt,
              memory_format);
}
        copy_strides = copy_strides ||
            (memory_format == c10::MemoryFormat::Preserve &&
             self.is_non_overlapping_and_dense());

auto& out_t = p_node->Output(0).toTensor();
fastResizeToZero(out_t);
        at::native::to_copy_out(out_t, self, non_blocking, copy_strides);
};
});
"
208,"in0_t, dtype, layout, device, pin_memory, memory_format);
}
auto& out_t = p_node->Output(0).toTensor();
at::native::fill_out(out_t, in1_s);
};
});
","in0_t, dtype, layout, device, pin_memory, memory_format);
}
auto& out_t = p_node->Output(0).toTensor();
    at::native::resize_(out_t, in0_t.sizes(), c10::nullopt);
at::native::fill_out(out_t, in1_s);
};
});
"
209,"//    dimensions We do the last part by reducing to bmm.
Tensor einsum(c10::string_view equation, TensorList operands) {
TORCH_CHECK(!operands.empty(), ""einsum(): must provide at least one operand"");
  checkDeviceType(""einsum():"", operands, operands[0].device().type());
// Code used to identify ELLIPSIS (""..."")
constexpr uint8_t ELLIPSIS = 52;
","//    dimensions We do the last part by reducing to bmm.
Tensor einsum(c10::string_view equation, TensorList operands) {
TORCH_CHECK(!operands.empty(), ""einsum(): must provide at least one operand"");
// Code used to identify ELLIPSIS (""..."")
constexpr uint8_t ELLIPSIS = 52;
"
210,"$extra_cuda_headers
$legacy_th_headers
$external_backend_headers
namespace at {
","$extra_cuda_headers
$legacy_th_headers
$external_backend_headers
$namespaced_headers
namespace at {
"
211,"}
GRAPH_DUMP(""Graph before fixing controlflow: "", node->owningGraph());
auto* if_node = node;
  // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores,clang-diagnostic-unused-variable)
  auto* graph = if_node->owningGraph();
FixupONNXSubblockOutputs(node);
ONNXFixupUninitializedOutput(if_node);
// Copy type of block output to node output.
","}
GRAPH_DUMP(""Graph before fixing controlflow: "", node->owningGraph());
auto* if_node = node;
FixupONNXSubblockOutputs(node);
ONNXFixupUninitializedOutput(if_node);
// Copy type of block output to node output.
"
212,"// Search if this is already known,
// and assign the same Symbol.
GRAPH_UPDATE(""Got dim_param:"", dim.dim_param());
          // NOLINTNEXTLINE(performance-for-range-copy)
          for (auto pair : symbol_map) {
if (pair.second == dim.dim_param()) {
sym = pair.first;
break;
","// Search if this is already known,
// and assign the same Symbol.
GRAPH_UPDATE(""Got dim_param:"", dim.dim_param());
          for (const auto& pair : symbol_map) {
if (pair.second == dim.dim_param()) {
sym = pair.first;
break;
"
213,"index_check();
if (THPVariable_Check(output_obj)) {
    // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
    at::Tensor var = THPVariable_Unpack(output_obj);
ONNXUpdateTypeFromTensor(
graph->outputs().at(outputs_index), var, onnx_shape_inference);
outputs_index++;
","index_check();
if (THPVariable_Check(output_obj)) {
    const at::Tensor& var = THPVariable_Unpack(output_obj);
ONNXUpdateTypeFromTensor(
graph->outputs().at(outputs_index), var, onnx_shape_inference);
outputs_index++;
"
214,"// The default tanh is quite slow, use the Eigen version from here:
// https://bitbucket.org/eigen/eigen/src/94875feeeeb9abe5509b314197da1991ba2070f5/Eigen/src/Core/MathFunctionsImpl.h#lines-26
ExprHandle fast_tanh(const ExprHandle& v) {
  // NOLINTNEXTLINE(clang-diagnostic-unused-variable)
  Dtype dtype = v.dtype();
// TODO: use a dedicated bind-var to make sure v is not evalualted multiple
// times. Clamp the input expression to [-9, 9]
ExprHandle plus_9 = FloatImm::make(9.0f);
","// The default tanh is quite slow, use the Eigen version from here:
// https://bitbucket.org/eigen/eigen/src/94875feeeeb9abe5509b314197da1991ba2070f5/Eigen/src/Core/MathFunctionsImpl.h#lines-26
ExprHandle fast_tanh(const ExprHandle& v) {
// TODO: use a dedicated bind-var to make sure v is not evalualted multiple
// times. Clamp the input expression to [-9, 9]
ExprHandle plus_9 = FloatImm::make(9.0f);
"
215,"bool list = PyList_Check(arg);
if (tuple || list) {
// NOLINTNEXTLINE(bugprone-branch-clone)
    int nDim = tuple ? PyTuple_GET_SIZE(arg) : PyList_GET_SIZE(arg);
THLongStoragePtr storage(THLongStorage_newWithSize(nDim));
for (int i = 0; i != nDim; ++i) {
PyObject* item = tuple ? PyTuple_GET_ITEM(arg, i) : PyList_GET_ITEM(arg, i);
","bool list = PyList_Check(arg);
if (tuple || list) {
// NOLINTNEXTLINE(bugprone-branch-clone)
    const auto nDim = tuple ? PyTuple_GET_SIZE(arg) : PyList_GET_SIZE(arg);
THLongStoragePtr storage(THLongStorage_newWithSize(nDim));
for (int i = 0; i != nDim; ++i) {
PyObject* item = tuple ? PyTuple_GET_ITEM(arg, i) : PyList_GET_ITEM(arg, i);
"
216,"const auto in1_i = p_node->Input(1).toScalarType();
p_node->Output(0) = at::native::to(in0_t, in1_i, in2_i, in3_i, in4_o);
}
};
}
return nullptr;
","const auto in1_i = p_node->Input(1).toScalarType();
p_node->Output(0) = at::native::to(in0_t, in1_i, in2_i, in3_i, in4_o);
}
      // in case that Output(0) is an alias of in0_t, copy the tensor.
      if (p_node->Output(0).toTensor().unsafeGetTensorImpl() ==
          in0_t.unsafeGetTensorImpl()) {
        p_node->Output(0) = in0_t.clone();
      }
};
}
return nullptr;
"
217,"at::Tensor& /*unused */,
at::Tensor& /*unused */,
const AllgatherOptions& /*unused */) {
  throw std::runtime_error(
""no support for _allgather_base in Gloo process group"");
}
","at::Tensor& /*unused */,
at::Tensor& /*unused */,
const AllgatherOptions& /*unused */) {
  TORCH_CHECK(false,
""no support for _allgather_base in Gloo process group"");
}
"
218,"std::vector<at::Tensor>& outputs,
std::vector<std::vector<at::Tensor>>& inputs,
const ReduceScatterOptions& opts) {
  throw std::runtime_error(""ProcessGroupGloo does not support reduce_scatter"");
}
namespace {
","std::vector<at::Tensor>& outputs,
std::vector<std::vector<at::Tensor>>& inputs,
const ReduceScatterOptions& opts) {
  TORCH_CHECK(false, ""ProcessGroupGloo does not support reduce_scatter"");
}
namespace {
"
219,"void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
","void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false,
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
"
220,"std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupMPI does not support allgather_coalesced"");
}
","std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllgatherOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupMPI does not support allgather_coalesced"");
}
"
221,"listenPort = ntohs(addr->sin6_port);
} else {
    throw std::runtime_error(""unsupported protocol"");
}
return listenPort;
}
","listenPort = ntohs(addr->sin6_port);
} else {
    TORCH_CHECK(false, ""unsupported protocol"");
}
return listenPort;
}
"
222,"while (true) {
int res = tcputil::poll(events.get(), 1, timeout.count());
if (res == 0) {
      throw std::runtime_error(
""waiting for processes to ""
""connect has timed out"");
} else if (res == -1) {
","while (true) {
int res = tcputil::poll(events.get(), 1, timeout.count());
if (res == 0) {
      TORCH_CHECK(false,
""waiting for processes to ""
""connect has timed out"");
} else if (res == -1) {
"
223,"const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout_ != kNoTimeout && elapsed > timeout_) {
        TORCH_CHECK(false, ""Timeout waiting for key: "" + key);
}
std::this_thread::sleep_for(std::chrono::milliseconds(10));
continue;
","const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout_ != kNoTimeout && elapsed > timeout_) {
        throw std::runtime_error(""Timeout waiting for key: "" + key);
}
std::this_thread::sleep_for(std::chrono::milliseconds(10));
continue;
"
224,"const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout != kNoTimeout && elapsed > timeout) {
      TORCH_CHECK(false, ""Wait timeout"");
}
/* sleep override */
","const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
std::chrono::steady_clock::now() - start);
if (timeout != kNoTimeout && elapsed > timeout) {
      throw std::runtime_error(""Wait timeout"");
}
/* sleep override */
"
225,"if (rank_ != opts.rootRank) {
if (outputTensors.size() > 0) {
      TORCH_CHECK(false,
""Gather: number of output tensors should be 0 ""
""for non-root"");
}
} else {
if (outputTensors.size() != 1) {
      TORCH_CHECK(false, ""Gather: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
      TORCH_CHECK(false,
""Gather: number of output tensors should equal ""
""to the world size"");
}
","if (rank_ != opts.rootRank) {
if (outputTensors.size() > 0) {
      throw std::runtime_error(
""Gather: number of output tensors should be 0 ""
""for non-root"");
}
} else {
if (outputTensors.size() != 1) {
      throw std::runtime_error(""Gather: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
      throw std::runtime_error(
""Gather: number of output tensors should equal ""
""to the world size"");
}
"
226,"for (auto i = size_t{}; i < num_devices; ++i) {
if (tensor_lists[i].size() != world_size * num_devices) {
      TORCH_CHECK(false,
""Tensor list input to scatter/gather must match number of collective""
"" participants"");
}
","for (auto i = size_t{}; i < num_devices; ++i) {
if (tensor_lists[i].size() != world_size * num_devices) {
      throw std::runtime_error(
""Tensor list input to scatter/gather must match number of collective""
"" participants"");
}
"
227,"std::vector<at::Tensor>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllToAllOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
","std::vector<at::Tensor>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const AllToAllOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
"
228,"listenPort = ntohs(addr->sin6_port);
} else {
    TORCH_CHECK(false, ""unsupported protocol"");
}
return listenPort;
}
","listenPort = ntohs(addr->sin6_port);
} else {
    throw std::runtime_error(""unsupported protocol"");
}
return listenPort;
}
"
229,"pg_name) { return pg_name.second == *group_name; });
if (it != pg_names_.end()) {
    TORCH_CHECK(false,
""The specified group name has already been ""
""created, please use a different group name"");
}
","pg_name) { return pg_name.second == *group_name; });
if (it != pg_names_.end()) {
    throw std::runtime_error(
""The specified group name has already been ""
""created, please use a different group name"");
}
"
230,"// Accumulates sizes of all memory blocks for given device in given pool
void cache_info_aux(const BlockPool& pool, size_t* total, size_t* largest) {
for (const auto& block : pool.blocks) {
      size_t blocksize = block->size;
*total += blocksize;
if (blocksize > *largest) {
*largest = blocksize;
","// Accumulates sizes of all memory blocks for given device in given pool
void cache_info_aux(const BlockPool& pool, size_t* total, size_t* largest) {
for (const auto& block : pool.blocks) {
      const auto blocksize = block->size;
*total += blocksize;
if (blocksize > *largest) {
*largest = blocksize;
"
231,"makeDeviceForHostname(const std::string& hostname) {
auto device = makeGlooDevice("""", hostname);
if (!device) {
    throw std::runtime_error(""makeDeviceForHostname(): unsupported gloo device"");
}
return device;
}
","makeDeviceForHostname(const std::string& hostname) {
auto device = makeGlooDevice("""", hostname);
if (!device) {
    TORCH_CHECK(false, ""makeDeviceForHostname(): unsupported gloo device"");
}
return device;
}
"
232,"std::vector<std::vector<at::Tensor>>& /* usused */,
std::vector<at::Tensor>& /* usused */,
const AllgatherOptions& /* usused */) {
  throw std::runtime_error(
""no support for allgather_coalesced in this process group"");
}
","std::vector<std::vector<at::Tensor>>& /* usused */,
std::vector<at::Tensor>& /* usused */,
const AllgatherOptions& /* usused */) {
  TORCH_CHECK(false,
""no support for allgather_coalesced in this process group"");
}
"
233,"func<int64_t>(args);                             \
break;                                           \
default:                                           \
      throw std::runtime_error(""Invalid scalar type""); \
}
#endif
","func<int64_t>(args);                             \
break;                                           \
default:                                           \
      TORCH_CHECK(false, ""Invalid scalar type""); \
}
#endif
"
234,"case ReduceOp::MAX:
return ReduceFunc(&::gloo::max<T>);
case ReduceOp::BAND:
      throw std::runtime_error(
""Cannot use ReduceOp.BAND with non-integral dtype"");
break;
case ReduceOp::BOR:
      throw std::runtime_error(
""Cannot use ReduceOp.BOR with non-integral dtype"");
break;
case ReduceOp::BXOR:
      throw std::runtime_error(
""Cannot use ReduceOp.BXOR with non-integral dtype"");
break;
case ReduceOp::UNUSED:
break;
}
  throw std::runtime_error(""Unhandled ReduceOp"");
}
// Bitwise AND with SFINAE guard for integral types.
","case ReduceOp::MAX:
return ReduceFunc(&::gloo::max<T>);
case ReduceOp::BAND:
      TORCH_CHECK(false,
""Cannot use ReduceOp.BAND with non-integral dtype"");
break;
case ReduceOp::BOR:
      TORCH_CHECK(false,
""Cannot use ReduceOp.BOR with non-integral dtype"");
break;
case ReduceOp::BXOR:
      TORCH_CHECK(false,
""Cannot use ReduceOp.BXOR with non-integral dtype"");
break;
case ReduceOp::UNUSED:
break;
}
  TORCH_CHECK(false, ""Unhandled ReduceOp"");
}
// Bitwise AND with SFINAE guard for integral types.
"
235,"at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    throw std::runtime_error(""input tensor has to be dense"");
}
return tensor;
}
","at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false, ""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    TORCH_CHECK(false, ""input tensor has to be dense"");
}
return tensor;
}
"
236,"ProcessGroupMPI::ProcessGroupMPI(int rank, int size, MPI_Comm pgComm)
: ProcessGroup(rank, size), stop_(false), pgComm_(pgComm) {
if (pgComm_ == MPI_COMM_NULL) {
    throw std::runtime_error(""pgComm_ must not be MPI_COMM_NULL"");
}
// Start the worker thread accepting MPI calls
","ProcessGroupMPI::ProcessGroupMPI(int rank, int size, MPI_Comm pgComm)
: ProcessGroup(rank, size), stop_(false), pgComm_(pgComm) {
if (pgComm_ == MPI_COMM_NULL) {
    TORCH_CHECK(false, ""pgComm_ must not be MPI_COMM_NULL"");
}
// Start the worker thread accepting MPI calls
"
237,"if (rank_ != opts.rootRank) {
if (inputTensors.size() > 0) {
      throw std::runtime_error(
""Scatter: number of input tensors should be 0 ""
""for non-root"");
}
} else {
if (inputTensors.size() != 1) {
      throw std::runtime_error(
""Scatter: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != inputTensors[0].size()) {
      throw std::runtime_error(
""Scatter: number of input tensors should equal ""
""to the world size"");
}
","if (rank_ != opts.rootRank) {
if (inputTensors.size() > 0) {
      TORCH_CHECK(false,
""Scatter: number of input tensors should be 0 ""
""for non-root"");
}
} else {
if (inputTensors.size() != 1) {
      TORCH_CHECK(false,
""Scatter: multi-GPU collective is not supported"");
}
if (static_cast<size_t>(size_) != inputTensors[0].size()) {
      TORCH_CHECK(false,
""Scatter: number of input tensors should equal ""
""to the world size"");
}
"
238,"bool isSendRecvSelf) {
// Sanity check
if (devicesKey.empty()) {
    throw std::runtime_error(
""Not able to create/get the NCCL Communicator since ""
""the GPU devices are not known"");
}
","bool isSendRecvSelf) {
// Sanity check
if (devicesKey.empty()) {
    TORCH_CHECK(false,
""Not able to create/get the NCCL Communicator since ""
""the GPU devices are not known"");
}
"
239,"// across distinct GPUs.
void check_gpu_tensors(const std::vector<at::Tensor>& tensors) {
if (tensors.size() == 0) {
    throw std::runtime_error(""Tensor list must be nonempty"");
}
if (tensors.size() > static_cast<size_t>(at::cuda::getNumGPUs())) {
    throw std::runtime_error(
""Tensor list mustn't be larger than the number of available GPUs"");
}
","// across distinct GPUs.
void check_gpu_tensors(const std::vector<at::Tensor>& tensors) {
if (tensors.size() == 0) {
    TORCH_CHECK(false, ""Tensor list must be nonempty"");
}
if (tensors.size() > static_cast<size_t>(at::cuda::getNumGPUs())) {
    TORCH_CHECK(false,
""Tensor list mustn't be larger than the number of available GPUs"");
}
"
240,"std::vector<int64_t>& /* unused */,
std::vector<int64_t>& /* unused */,
const AllToAllOptions& /* unused */) {
  throw std::runtime_error(
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
","std::vector<int64_t>& /* unused */,
std::vector<int64_t>& /* unused */,
const AllToAllOptions& /* unused */) {
  TORCH_CHECK(false,
""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"");
}
"
241,"std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const GatherOptions& /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support gather"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::scatter(
std::vector<at::Tensor>& /* unused */,
std::vector<std::vector<at::Tensor>>& /* unused */,
const ScatterOptions& /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support scatter"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::recvAnysource(
std::vector<at::Tensor>& /* unused */,
int /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support recvAnysource"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::_allgather_base(
","std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const GatherOptions& /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support gather"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::scatter(
std::vector<at::Tensor>& /* unused */,
std::vector<std::vector<at::Tensor>>& /* unused */,
const ScatterOptions& /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support scatter"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::recvAnysource(
std::vector<at::Tensor>& /* unused */,
int /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support recvAnysource"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::_allgather_base(
"
242,"if (jitFuture.hasError()) {
auto errMsg = jitFuture.tryRetrieveErrorMessage();
VLOG(1) << ""Got exception: "" << errMsg;
    throw std::runtime_error(errMsg);
}
}
","if (jitFuture.hasError()) {
auto errMsg = jitFuture.tryRetrieveErrorMessage();
VLOG(1) << ""Got exception: "" << errMsg;
    TORCH_CHECK(false, errMsg);
}
}
"
243,"TensorOptions linspace_logspace_infer_options(
const Scalar& start,
const Scalar& end,
    const TensorOptions& options) {
  auto result_options = options;
if (start.isComplex() || end.isComplex()) {
    // Since result_options.has_dtype() returns true (dtype is default type),
    // even if the user hasn't specified the dtype.
    // We just check to see if either `start` or `end` is complex,
    // and if the `result_dtype` is not complex (be it default float type or
    // user provided), we cast it to default complex dtype with a Warning!.
    auto result_dtype = c10::typeMetaToScalarType(options.dtype());
    if (!at::isComplexType(result_dtype)) {
      TORCH_WARN(
          ""As either `start` or `stop` is complex, return type will be the complex dtype corresponding to default dtype."",
          ""In future, this may throw an error when a non-complex dtype arg is passed as input along "",
          ""with complex valued start or end value."");
      result_options = result_options.dtype(c10::get_default_complex_dtype());
}
}
  return result_options;
}
} // anonymous namespace
","TensorOptions linspace_logspace_infer_options(
const Scalar& start,
const Scalar& end,
    const TensorOptions& options,
    const char* fn_name) {
if (start.isComplex() || end.isComplex()) {
    const auto default_complex_dtype = c10::get_default_complex_dtype();
    if (options.has_dtype()) {
      auto dtype = c10::typeMetaToScalarType(options.dtype());
      TORCH_CHECK(at::isComplexType(dtype),
          fn_name, "": inferred dtype "", default_complex_dtype, "" can't be safely cast to passed dtype "", dtype);
    } else {
      return options.dtype(default_complex_dtype);
}
}
  return options.has_dtype() ? options : options.dtype(c10::get_default_dtype());
}
} // anonymous namespace
"
244,"#include <c10/util/Exception.h>
#include <c10/util/hash.h>
#include <c10d/comm.hpp>
#include <torch/csrc/autograd/engine.h>
#include <torch/csrc/autograd/function_hook.h>
#include <torch/csrc/autograd/functions/accumulate_grad.h>
","#include <c10/util/Exception.h>
#include <c10/util/hash.h>
#include <c10d/comm.hpp>
#include <c10d/logger.hpp>
#include <torch/csrc/autograd/engine.h>
#include <torch/csrc/autograd/function_hook.h>
#include <torch/csrc/autograd/functions/accumulate_grad.h>
"
245,"const at::Tensor& grad,
const at::Tensor& bucket_view) {
// Ensure that the gradient type matches the bucket type.
  TORCH_CHECK(
      grad.options().type_equal(bucket_view.options()),
""Expected "",
bucket_view.toString(),
"", got "",
      grad.toString());
TORCH_INTERNAL_ASSERT(grad.device() == bucket_view.device());
TORCH_INTERNAL_ASSERT(grad.numel() == bucket_view.numel());
// AccumulateGrad doesn't HAVE to obey the grad layout contract.
","const at::Tensor& grad,
const at::Tensor& bucket_view) {
// Ensure that the gradient type matches the bucket type.
  REDUCER_CHECK(
    grad.options().type_equal(bucket_view.options()),
    logger_,
    c10::str(
""Expected "",
bucket_view.toString(),
"", got "",
      grad.toString())
  );

TORCH_INTERNAL_ASSERT(grad.device() == bucket_view.device());
TORCH_INTERNAL_ASSERT(grad.numel() == bucket_view.numel());
// AccumulateGrad doesn't HAVE to obey the grad layout contract.
"
246,"CAFFE_ENFORCE(nnapi_.Model_finish);
int ret = nnapi_.Model_finish(model);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_addOperand(ANeuralNetworksModel* model, const ANeuralNetworksOperandType* type) {
CAFFE_ENFORCE(nnapi_.Model_addOperand);
int ret = nnapi_.Model_addOperand(model,type);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_setOperandValue(ANeuralNetworksModel* model, int32_t index, const void* buffer, size_t length) {
CAFFE_ENFORCE(nnapi_.Model_setOperandValue);
int ret = nnapi_.Model_setOperandValue(model,index,buffer,length);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_setOperandValueFromMemory(ANeuralNetworksModel* model, int32_t index, const ANeuralNetworksMemory* memory, size_t offset, size_t length) {
CAFFE_ENFORCE(nnapi_.Model_setOperandValueFromMemory);
int ret = nnapi_.Model_setOperandValueFromMemory(model,index,memory,offset,length);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_addOperation(ANeuralNetworksModel* model, ANeuralNetworksOperationType type, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs) {
CAFFE_ENFORCE(nnapi_.Model_addOperation);
int ret = nnapi_.Model_addOperation(model,type,inputCount,inputs,outputCount,outputs);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_identifyInputsAndOutputs(ANeuralNetworksModel* model, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs) {
CAFFE_ENFORCE(nnapi_.Model_identifyInputsAndOutputs);
int ret = nnapi_.Model_identifyInputsAndOutputs(model,inputCount,inputs,outputCount,outputs);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Model_relaxComputationFloat32toFloat16(ANeuralNetworksModel* model, bool allow) {
CAFFE_ENFORCE(nnapi_.Model_relaxComputationFloat32toFloat16);
int ret = nnapi_.Model_relaxComputationFloat32toFloat16(model,allow);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
int check_Compilation_create(ANeuralNetworksModel* model, ANeuralNetworksCompilation** compilation) {
CAFFE_ENFORCE(nnapi_.Compilation_create);
int ret = nnapi_.Compilation_create(model,compilation);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(ret == ANEURALNETWORKS_NO_ERROR);
return ret;
}
void check_Compilation_free(ANeuralNetworksCompilation* compilation) {
","CAFFE_ENFORCE(nnapi_.Model_finish);
int ret = nnapi_.Model_finish(model);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_finish"", ""failed with error "", ret
  );
return ret;
}
int check_Model_addOperand(ANeuralNetworksModel* model, const ANeuralNetworksOperandType* type) {
CAFFE_ENFORCE(nnapi_.Model_addOperand);
int ret = nnapi_.Model_addOperand(model,type);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_addOperand"", ""failed with error "", ret
  );
return ret;
}
int check_Model_setOperandValue(ANeuralNetworksModel* model, int32_t index, const void* buffer, size_t length) {
CAFFE_ENFORCE(nnapi_.Model_setOperandValue);
int ret = nnapi_.Model_setOperandValue(model,index,buffer,length);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_setOperandValue"", ""failed with error "", ret
  );
return ret;
}
int check_Model_setOperandValueFromMemory(ANeuralNetworksModel* model, int32_t index, const ANeuralNetworksMemory* memory, size_t offset, size_t length) {
CAFFE_ENFORCE(nnapi_.Model_setOperandValueFromMemory);
int ret = nnapi_.Model_setOperandValueFromMemory(model,index,memory,offset,length);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_setOperandValueFromMemory"", ""failed with error "", ret
  );
return ret;
}
int check_Model_addOperation(ANeuralNetworksModel* model, ANeuralNetworksOperationType type, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs) {
CAFFE_ENFORCE(nnapi_.Model_addOperation);
int ret = nnapi_.Model_addOperation(model,type,inputCount,inputs,outputCount,outputs);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_addOperation"", ""failed with error "", ret
  );
return ret;
}
int check_Model_identifyInputsAndOutputs(ANeuralNetworksModel* model, uint32_t inputCount, const uint32_t* inputs, uint32_t outputCount, const uint32_t* outputs) {
CAFFE_ENFORCE(nnapi_.Model_identifyInputsAndOutputs);
int ret = nnapi_.Model_identifyInputsAndOutputs(model,inputCount,inputs,outputCount,outputs);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_identifyInputsAndOutputs"", ""failed with error "", ret
  );
return ret;
}
int check_Model_relaxComputationFloat32toFloat16(ANeuralNetworksModel* model, bool allow) {
CAFFE_ENFORCE(nnapi_.Model_relaxComputationFloat32toFloat16);
int ret = nnapi_.Model_relaxComputationFloat32toFloat16(model,allow);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Model_relaxComputationFloat32toFloat16"", ""failed with error "", ret
  );
return ret;
}
int check_Compilation_create(ANeuralNetworksModel* model, ANeuralNetworksCompilation** compilation) {
CAFFE_ENFORCE(nnapi_.Compilation_create);
int ret = nnapi_.Compilation_create(model,compilation);
// TODO: Maybe add better logging here.
  CAFFE_ENFORCE(
    ret == ANEURALNETWORKS_NO_ERROR,
    ""Compilation_create"", ""failed with error "", ret
  );
return ret;
}
void check_Compilation_free(ANeuralNetworksCompilation* compilation) {
"
247,"#ifndef __HIP_PLATFORM_HCC__
ss << nvfuser_resources::fp16_support_cu;
#endif
ss << nvfuser_resources::tensor_cu;
","#ifndef __HIP_PLATFORM_HCC__
ss << nvfuser_resources::fp16_support_cu;
#else
  ss << R""(
#ifndef __noinline__
#define __noinline__ __attribute__((noinline))
#endif
#ifndef __forceinline__
#define __forceinline__ inline __attribute__((always_inline))
#endif
#ifndef assert
#define assert(expr) ((void)0)
#endif
#ifndef __align__
#define __align__(x) __attribute__((aligned(x)))
#endif
  )"";
#endif
ss << nvfuser_resources::tensor_cu;
"
248,"return out;
}
#ifdef USE_ROCM
static const char* device_resource_string = R""(
#define POS_INFINITY INFINITY
#define NEG_INFINITY -INFINITY

)"";
#else
static const char* device_resource_string = R""(
#define NAN __int_as_float(0x7fffffff)
#define POS_INFINITY __int_as_float(0x7f800000)
#define NEG_INFINITY __int_as_float(0xff800000)
)"";
#endif
static const char* shared_resource_string = R""(
template<typename T>
","return out;
}
static const char* device_resource_string = R""(
#define NAN __int_as_float(0x7fffffff)
#define POS_INFINITY __int_as_float(0x7f800000)
#define NEG_INFINITY __int_as_float(0xff800000)
)"";
static const char* shared_resource_string = R""(
template<typename T>
"
249,"}
}
std::unique_ptr<ProfilingRecord> ProfilingRecord::instrumentGraph(
const std::shared_ptr<Graph>& graph) {
auto new_g = graph->copy();
","}
}
bool ProfilingRecord::ready() const {
  std::lock_guard<std::mutex> lock(this->mutex_);
  return profiling_count_ == 0;
}

std::unique_ptr<ProfilingRecord> ProfilingRecord::instrumentGraph(
const std::shared_ptr<Graph>& graph) {
auto new_g = graph->copy();
"
250,"SourceRange source_range)
: callee_(std::move(callee)),
fn_(fn),
      source_range_(std::move(source_range)) {}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
","SourceRange source_range)
: callee_(std::move(callee)),
fn_(fn),
      source_range_(std::move(source_range)) {
  if (fn_) {
    set_function_name(fn_->name());
  }
}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
"
251,"cs_ptr = c10::make_intrusive<InlinedCallStack>(
nullptr, source_range, module_instance_info);
}
cached_inlined_callstacks_[tup] = cs_ptr;
// Invoking move constructor
// It is not clear if copy-ellision can happen since
","cs_ptr = c10::make_intrusive<InlinedCallStack>(
nullptr, source_range, module_instance_info);
}
  cs_ptr->set_function_name(function_name);
cached_inlined_callstacks_[tup] = cs_ptr;
// Invoking move constructor
// It is not clear if copy-ellision can happen since
"
252,"}
for (const auto& index_pair : sm.output_indices()) {
    // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
    int node_idx;
    // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
    int out_idx;
std::tie(node_idx, out_idx) = index_pair;
if (node_idx == StaticModule::INPUT_VALUE) {
outputs_.emplace_back(&inputs_[out_idx]);
","}
for (const auto& index_pair : sm.output_indices()) {
    int node_idx = 0;
    int out_idx = 0;
std::tie(node_idx, out_idx) = index_pair;
if (node_idx == StaticModule::INPUT_VALUE) {
outputs_.emplace_back(&inputs_[out_idx]);
"
253,"if (static_module_.opts().cleanup_activations) {
// MemoryPlanner is created after the first invocation of `run()`. This is
    // done intentionally because MemoryPlanner uses `TensorStorageImpl`
    // object sizes of the previous `run()` for memory planning of subsequent
    // runs
if (!planner_) {
planner_ = std::make_unique<MemoryPlanner>(
this,
","if (static_module_.opts().cleanup_activations) {
// MemoryPlanner is created after the first invocation of `run()`. This is
    // done intentionally because MemoryPlanner uses `Tensor` sizes of the
    // previous `run()` for memory planning of subsequent runs
if (!planner_) {
planner_ = std::make_unique<MemoryPlanner>(
this,
"
254,"namespace {
// This function construct stacktrace with module hierarchy
// Module hierarchy will contain information about where in the
// module hierarchy this source is. For example if conv2d op
// exist in hierarcy A->B->C->Conv2d with type annotations of
// A -> TopM, B->MyModule, C->SomeModule, then module hierarchy
// will be TopM(A).MyModule(B).SomeModule(C).Conv2d(conv)
// Source level stack information will be from model source code.
std::pair<std::string, std::string> getStackTraceWithModuleHierarchy(
    const DebugInfoPair& source_callstack,
    const std::string& root_scope_string,
    const std::string& top_module_type_name) {
constexpr size_t kSourceRange = 1;
constexpr size_t kModuleInstanceInfo = 2;
std::vector<StackEntry> entries;
const SourceRange& range = source_callstack.first;
InlinedCallStackPtr callstack_ptr = source_callstack.second;
  std::string module_info =
      root_scope_string + ""("" + top_module_type_name + "")"";
  std::ostringstream ss;
if (!callstack_ptr) {
// If not cs then top level node
entries.emplace_back(StackEntry{""FunctionName_UNKNOWN"", range});
} else {
for (const auto& element : callstack_ptr->vec()) {
const auto& opt_module_instance_info =
","namespace {
std::pair<std::vector<StackEntry>, std::string> getStackTraceWithModuleHierarchy(
    const DebugInfoPair& source_callstack) {
constexpr size_t kSourceRange = 1;
constexpr size_t kModuleInstanceInfo = 2;
std::vector<StackEntry> entries;
const SourceRange& range = source_callstack.first;
InlinedCallStackPtr callstack_ptr = source_callstack.second;
  std::string module_info;
if (!callstack_ptr) {
// If not cs then top level node
entries.emplace_back(StackEntry{""FunctionName_UNKNOWN"", range});
    return {std::move(entries), std::move(module_info)};
} else {
for (const auto& element : callstack_ptr->vec()) {
const auto& opt_module_instance_info =
"
255,"int64_t source_range_tag = tup_elems[1].toInt();
auto source_range_it = source_range_map.find(source_range_tag);
TORCH_CHECK(
      source_range_tag == -1 || source_range_it != source_range_map.end(),
""Source range tag must exist in deserialized source range map.""
"" Not found source range tag:"",
source_range_tag);
  auto source_range = source_range_it->second;
auto callee = deserialize(tup_elems[2], source_range_map, cu);
InlinedCallStackPtr cs_ptr;
if (callee) {
","int64_t source_range_tag = tup_elems[1].toInt();
auto source_range_it = source_range_map.find(source_range_tag);
TORCH_CHECK(
      source_range_tag == kInvalidSourceRangeTag ||
          source_range_it != source_range_map.end(),
""Source range tag must exist in deserialized source range map.""
"" Not found source range tag:"",
source_range_tag);
  SourceRange source_range;
  if (source_range_tag != kInvalidSourceRangeTag) {
    source_range = source_range_it->second;
  }
auto callee = deserialize(tup_elems[2], source_range_map, cu);
InlinedCallStackPtr cs_ptr;
if (callee) {
"
256,"std::pair<IValue, IValue> getFunctionTuple(
const Module& module,
const Function& func,
    BackendDebugHandleManager& debug_handle_manager) {
auto graph = func.graph()->copy();
Inline(*graph);
","std::pair<IValue, IValue> getFunctionTuple(
const Module& module,
const Function& func,
    BackendDebugInfoRecorder& debug_info_recorder) {
auto graph = func.graph()->copy();
Inline(*graph);
"
257,"elements,
qn_cache,
debug_info_elements,
          debug_handle_manager);
}
}
}
} // namespace
void moduleMethodsTuple(
const Module& module,
std::vector<c10::IValue>& elements, // note: appended to in-place
std::vector<c10::IValue>& debug_info_elements,
    BackendDebugHandleManager& debug_handle_manager) {
auto methods = module.get_methods();
std::unordered_set<std::string> qn_cache;
// top level methods
","elements,
qn_cache,
debug_info_elements,
          debug_info_recorder);
}
}
}

// Check if the global static map of backend debug info
// contains debug info for this module and any of its children.
// If so combine all the maps together and return one.
void getBackendDebugInfoMap(
    const Module& m,
    BackendDebugInfoMapType& debug_map) {
  c10::QualifiedName type_name;
  if (m.type()->name()) {
    type_name = m.type()->name().value();
  }
  if (c10::string_view(type_name.name()).ends_with(""LoweredModule"")) {
    auto backend_debug_info =
        m.attr(""__backend_debug_info"").toCustomClass<PyTorchBackendDebugInfo>();
    const auto& map = backend_debug_info->getDebugInfoMap();
    if (map) {
      debug_map.insert(map.value().begin(), map.value().end());
    }
  }
  for (const auto& c : m.children()) {
    getBackendDebugInfoMap(c, debug_map);
  }
}

SourceRangeRecords getBackendSourceRanges(const Module& m) {
  SourceRangeRecords sr_records;
  c10::QualifiedName type_name;
  if (m.type()->name()) {
    type_name = m.type()->name().value();
  }
  if (c10::string_view(type_name.name()).ends_with(""LoweredModule"")) {
    constexpr size_t kSourceRange = 1;
    auto backend_debug_info =
        m.attr(""__backend_debug_info"").toCustomClass<PyTorchBackendDebugInfo>();
    const auto& map = backend_debug_info->getDebugInfoMap();
    if (map) {
      const auto& map_val = map.value();
      // This map is map of debug handle-to-delegateDebugInfoType
      // DebugInfoPair = <source range, inlined_cs_ptr>
      for (const auto& it : map_val) {
        sr_records.emplace_back(
            std::numeric_limits<size_t>::max(), it.second.first);
        auto cs_ptr = it.second.second;
        if (cs_ptr) {
          for (const auto& e : cs_ptr->vec()) {
            const auto sr = std::get<kSourceRange>(e);
            sr_records.emplace_back(std::numeric_limits<size_t>::max(), sr);
          }
        }
      }
    }
  }
  for (const auto& c : m.children()) {
    const auto& child_sr_records = getBackendSourceRanges(c);
    sr_records.reserve(sr_records.size() + child_sr_records.size());
    std::move(
        child_sr_records.begin(),
        child_sr_records.end(),
        std::back_inserter(sr_records));
  }
  return sr_records;
}

} // namespace
void moduleMethodsTuple(
const Module& module,
std::vector<c10::IValue>& elements, // note: appended to in-place
std::vector<c10::IValue>& debug_info_elements,
    BackendDebugInfoRecorder& debug_info_recorder) {
auto methods = module.get_methods();
std::unordered_set<std::string> qn_cache;
// top level methods
"
258,"{
// adds a __doc__ string to a function, similar to numpy's arr_add_docstring
static std::vector<std::string> all_docs;
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  PyObject *obj;
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  PyObject *doc_obj;
if (!PyArg_ParseTuple(args, ""OO"", &obj, &doc_obj)) {
return nullptr;
}
","{
// adds a __doc__ string to a function, similar to numpy's arr_add_docstring
static std::vector<std::string> all_docs;
  PyObject *obj = nullptr;
  PyObject *doc_obj = nullptr;
if (!PyArg_ParseTuple(args, ""OO"", &obj, &doc_obj)) {
return nullptr;
}
"
259,"}
}
  if (newTerms.empty()) {
    return scalar;
}
  return new Polynomial(hasher_, scalar, std::move(newTerms));
}
// Does multiplying these two expressions make a Rounding Off operation.
","}
}
  // If the scalar in poly is not 0, it must be multiplied by term.
  // If there are no variables in term, this becomes the scalar in the result
  // polynomial. If there are variables in term, this becomes a new term in
  // the result polynomial.
  if (!immediateEquals(poly->scalar(), 0)) {
    const Expr* scalar = evaluateOp(new Mul(poly->scalar(), term->scalar()));
    if (term->variables().empty()) {
      return new Polynomial(hasher_, scalar, newTerms);
    }
    newTerms.push_back(new Term(hasher_, scalar, term->variables()));
}
  // The only case when the result polynomial has a scalar is when the input
  // term does not have any variables and the input polynomial has a non-zero
  // scalar. That case is handled above. So, at this point, we do not have any
  // scalars in the result polynomial.
  return new Polynomial(hasher_, std::move(newTerms));
}
// Does multiplying these two expressions make a Rounding Off operation.
"
260,"MobileCode::MobileCode(
const std::shared_ptr<Graph>& graph,
std::string function_name,
size_t remaining_bailout_depth)
: Code(new interpreter::MobileCodeImpl(
graph,
std::move(function_name),
remaining_bailout_depth)) {}
MobileCode::~MobileCode() = default;
","MobileCode::MobileCode(
const std::shared_ptr<Graph>& graph,
std::string function_name,
    bool emit_default_input_instructions,
size_t remaining_bailout_depth)
: Code(new interpreter::MobileCodeImpl(
graph,
std::move(function_name),
          emit_default_input_instructions,
remaining_bailout_depth)) {}
MobileCode::~MobileCode() = default;
"
261,"}
return result;
}
Tensor _th_var(const Tensor & self, bool unbiased) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);
switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_var"", false, DeviceType::CPU, dispatch_scalar_type);
            return at::scalar_tensor(convert<double>(THDoubleTensor_var_all(self_, unbiased)), options(ScalarType::Double));
break;
}
case ScalarType::Float: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_var"", false, DeviceType::CPU, dispatch_scalar_type);
            return at::scalar_tensor(convert<float>(THFloatTensor_var_all(self_, unbiased)), options(ScalarType::Float));
break;
}
default:
AT_ERROR(""_th_var not supported on CPUType for "", dispatch_scalar_type);
}
}
Tensor _th_std(const Tensor & self, bool unbiased) {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);

    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_std"", false, DeviceType::CPU, dispatch_scalar_type);
            return at::scalar_tensor(convert<double>(THDoubleTensor_std_all(self_, unbiased)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_std"", false, DeviceType::CPU, dispatch_scalar_type);
            return at::scalar_tensor(convert<float>(THFloatTensor_std_all(self_, unbiased)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR(""_th_std not supported on CPUType for "", dispatch_scalar_type);
    }
}
Tensor & _th_renorm_out(const Tensor & self, const Scalar& p, int64_t dim, const Scalar& maxnorm, Tensor & result) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);
","}
return result;
}
Scalar _th_std_var(const Tensor& self, int64_t correction, bool take_sqrt) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);
switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_var"", false, DeviceType::CPU, dispatch_scalar_type);
            return convert<double>(THDoubleTensor_std_var_all(self_, correction, take_sqrt));
break;
}
case ScalarType::Float: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_var"", false, DeviceType::CPU, dispatch_scalar_type);
            return convert<float>(THFloatTensor_std_var_all(self_, correction, take_sqrt));
break;
}
default:
AT_ERROR(""_th_var not supported on CPUType for "", dispatch_scalar_type);
}
}
Tensor & _th_renorm_out(const Tensor & self, const Scalar& p, int64_t dim, const Scalar& maxnorm, Tensor & result) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);
"
262,"#include <ATen/ATen.h>
#include <ATen/AccumulateType.h>
#include <ATen/ExpandUtils.h>
#include <ATen/NativeFunctions.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
","#include <ATen/ATen.h>
#include <ATen/AccumulateType.h>
#include <ATen/ExpandUtils.h>
#include <ATen/LegacyTHFunctionsCPU.h>
#include <ATen/NativeFunctions.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
"
263,"c10::raw::intrusive_ptr::decref(rowS);
}
accreal THTensor_(var_all)(THTensor *tensor, bool unbiased)
{
accreal mean = THTensor_wrap(tensor).mean().item<accreal>();
accreal sum = 0;
TH_TENSOR_APPLY(scalar_t, tensor, sum += (*tensor_data - mean)*(*tensor_data - mean););
  sum /= std::max<int64_t>(0, THTensor_(nElement)(tensor) - (unbiased ? 1 : 0));
  return sum;
}

accreal THTensor_(std_all)(THTensor *tensor, bool unbiased)
{
  return sqrt(THTensor_(var_all)(tensor, unbiased));
}
void THTensor_(histc)(THTensor *hist, THTensor *tensor, int64_t nbins, scalar_t minvalue, scalar_t maxvalue)
","c10::raw::intrusive_ptr::decref(rowS);
}
accreal THTensor_(std_var_all)(THTensor* tensor, int64_t correction, bool take_sqrt)
    __ubsan_ignore_float_divide_by_zero__ {
accreal mean = THTensor_wrap(tensor).mean().item<accreal>();
accreal sum = 0;
TH_TENSOR_APPLY(scalar_t, tensor, sum += (*tensor_data - mean)*(*tensor_data - mean););
  sum /= std::max(int64_t{0}, THTensor_(nElement)(tensor) - correction);
  if (take_sqrt) {
    return std::sqrt(sum);
  } else {
    return sum;
  }
}
void THTensor_(histc)(THTensor *hist, THTensor *tensor, int64_t nbins, scalar_t minvalue, scalar_t maxvalue)
"
264,"#include <torch/csrc/jit/mobile/module.h>
#include <torch/csrc/jit/passes/inliner.h>
#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
#include <torch/csrc/jit/serialization/pickle.h>
","#include <torch/csrc/jit/mobile/module.h>
#include <torch/csrc/jit/passes/inliner.h>
#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/callstack_debug_info_serialization.h>
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
#include <torch/csrc/jit/serialization/pickle.h>
"
265,"py::call_guard<py::gil_scoped_release>())
.def(
""_get_device_map"",
          (std::unordered_map<c10::Device, c10::Device>(
              ProcessGroupAgent::*)(const WorkerInfo& dst) const) &
ProcessGroupAgent::getDeviceMap,
py::call_guard<py::gil_scoped_release>())
.def(
","py::call_guard<py::gil_scoped_release>())
.def(
""_get_device_map"",
          (DeviceMap(ProcessGroupAgent::*)(const WorkerInfo& dst) const) &
ProcessGroupAgent::getDeviceMap,
py::call_guard<py::gil_scoped_release>())
.def(
"
266,"return profilingEnabled_.load();
}
std::unordered_map<c10::Device, c10::Device> RpcAgent::getDeviceMap(
    const WorkerInfo& /* unused */) const {
// Default implementation has no device map.
return {};
}
","return profilingEnabled_.load();
}
DeviceMap RpcAgent::getDeviceMap(const WorkerInfo& /* unused */) const {
// Default implementation has no device map.
return {};
}
"
267,"at::Tensor buffer;
c10::optional<cuda::CUDAEvent> event;
std::mutex mutex;
// Every time we use a dropout state, we need to synchronize with its event,
// to make sure all previous uses finish running before this one starts. Once
","at::Tensor buffer;
c10::optional<cuda::CUDAEvent> event;
std::mutex mutex;
#if CUDA_VERSION >= 11000
  // cudaStreamGetCaptureInfo will never give back a capture id of 0, so 0 can serve
  // as a sentinel value that capture was not underway.
  cuda::CaptureId_t capture_id_last_lock = 0;
  cuda::CaptureId_t capture_id_last_unlock = 0;
#endif
// Every time we use a dropout state, we need to synchronize with its event,
// to make sure all previous uses finish running before this one starts. Once
"
268,"template<class scalar_t>
void lapackCholeskySolve(char uplo, int n, int nrhs, scalar_t *a, int lda, scalar_t *b, int ldb, int *info);
template<class scalar_t>
void lapackCholesky(char uplo, int n, scalar_t *a, int lda, int *info);

template<class scalar_t, class value_t=scalar_t>
void lapackSymeig(char jobz, char uplo, int n, scalar_t *a, int lda, value_t *w, scalar_t *work, int lwork, value_t *rwork, int *info);
","template<class scalar_t>
void lapackCholeskySolve(char uplo, int n, int nrhs, scalar_t *a, int lda, scalar_t *b, int ldb, int *info);
template<class scalar_t, class value_t=scalar_t>
void lapackSymeig(char jobz, char uplo, int n, scalar_t *a, int lda, value_t *w, scalar_t *work, int lwork, value_t *rwork, int *info);
"
269,"namespace {
// Taken from codegened version
Tensor _fw_primal(const Tensor & self, int64_t level) {
auto& self_ = unpack(self, ""self"", 0);
std::shared_ptr<Identity> grad_fn;
if (compute_requires_grad( self )) {
grad_fn = std::make_shared<Identity>();
grad_fn->set_next_edges(collect_next_edges( self ));
}
  auto tmp = ([&]() {
    at::AutoDispatchBelowAutograd mode;
    return self_.alias();
})();
  std::function<at::Tensor(const at::Tensor&)> func=nullptr;
  if (!self.unsafeGetTensorImpl()->support_as_strided()) {
    auto size_vec = self.sizes().vec();
    func = [=](const at::Tensor& input_base) {
      return input_base.view(size_vec);
    };
  }
  auto result = as_view(/* base */ self, /* output */ tmp, /* is_bw_differentiable */ true,
                        /* is_fw_differentiable */ false, /* view_func */ func, /* creation_meta */ CreationMeta::DEFAULT);
if (grad_fn) {
set_history(flatten_tensor_args( result ), grad_fn);
}
","namespace {
// Taken from codegened version
Tensor _fw_primal(c10::DispatchKeySet ks, const Tensor & self, int64_t level) {
auto& self_ = unpack(self, ""self"", 0);
std::shared_ptr<Identity> grad_fn;
if (compute_requires_grad( self )) {
grad_fn = std::make_shared<Identity>();
grad_fn->set_next_edges(collect_next_edges( self ));
}

  auto result = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::_fw_primal(ks & c10::after_autograd_keyset, self_, level);
})();

if (grad_fn) {
set_history(flatten_tensor_args( result ), grad_fn);
}
"
270,"return self;
}
Tensor detach(const Tensor & self) {
RECORD_FUNCTION(""detach"", std::vector<c10::IValue>({self}));
  std::function<at::Tensor(const at::Tensor&)> func=nullptr;
  auto result = as_view(/* base */ self, /* output */ self, /* is_bw_differentiable */ false,
                        /* is_fw_differentiable */ true, /* view_func */ func, /* creation_meta */ CreationMeta::DEFAULT,
                        /*allow_tensor_metadata_change=*/false);
namedinference::propagate_names(result, self);
// detach only backward gradients for both primal and tangent
","return self;
}
Tensor detach(c10::DispatchKeySet ks, const Tensor & self) {
  auto& self_ = unpack(self, ""self"", 0);
RECORD_FUNCTION(""detach"", std::vector<c10::IValue>({self}));
  auto result = ([&]() {
    at::AutoDispatchBelowAutograd guard;
    return at::redispatch::detach(ks & c10::after_autograd_keyset, self_);
  })();
namedinference::propagate_names(result, self);
// detach only backward gradients for both primal and tangent
"
271,"static PyObject* THPFInfo_dtype(THPFInfo* self, void*) {
std::string primary_name, legacy_name;
std::tie(primary_name, legacy_name) = torch::utils::getDtypeNames(self->type);
return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::kHalf, at::ScalarType::BFloat16, self->type, ""dtype"", [primary_name] {
return PyUnicode_FromString((char*)primary_name.data());
});
","static PyObject* THPFInfo_dtype(THPFInfo* self, void*) {
std::string primary_name, legacy_name;
std::tie(primary_name, legacy_name) = torch::utils::getDtypeNames(self->type);
  // NOLINTNEXTLINE(clang-diagnostic-unused-local-typedef)
return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::kHalf, at::ScalarType::BFloat16, self->type, ""dtype"", [primary_name] {
return PyUnicode_FromString((char*)primary_name.data());
});
"
272,"// Massage the Python results tuple back into a C++ variable_list
variable_list results;
results.reserve(num_outputs);
auto& input_info = py_fn->input_info;
for (int i = 0; i != num_outputs; ++i) {
PyObject* output = PyTuple_GET_ITEM(r.get(), i);
","// Massage the Python results tuple back into a C++ variable_list
variable_list results;
results.reserve(num_outputs);
  // NOLINTNEXTLINE(clang-diagnostic-unused-variable)
auto& input_info = py_fn->input_info;
for (int i = 0; i != num_outputs; ++i) {
PyObject* output = PyTuple_GET_ITEM(r.get(), i);
"
273,"throw python_error();
}
auto stream = at::cuda::CUDAStream::unpack(bits);
auto device = static_cast<int>(c10::cuda::current_device());
if (device != stream.device_index()) {
THCPModule_setDevice(stream.device_index());
","throw python_error();
}
auto stream = at::cuda::CUDAStream::unpack(bits);
  // NOLINTNEXTLINE(bugprone-signed-char-misuse)
auto device = static_cast<int>(c10::cuda::current_device());
if (device != stream.device_index()) {
THCPModule_setDevice(stream.device_index());
"
274,"auto num_gpus = c10::cuda::device_count();
auto default_cuda_generators = PyTuple_New(static_cast<Py_ssize_t>(num_gpus));
for(int i = 0; i < num_gpus; i++) {
auto gen = at::cuda::detail::getDefaultCUDAGenerator(i);
auto cast_gen = (THPGenerator*)THPGenerator_initDefaultGenerator(gen);
// This reference is meant to be given away, so no need to incref here.
","auto num_gpus = c10::cuda::device_count();
auto default_cuda_generators = PyTuple_New(static_cast<Py_ssize_t>(num_gpus));
for(int i = 0; i < num_gpus; i++) {
    // NOLINTNEXTLINE(performance-unnecessary-copy-initialization)
auto gen = at::cuda::detail::getDefaultCUDAGenerator(i);
auto cast_gen = (THPGenerator*)THPGenerator_initDefaultGenerator(gen);
// This reference is meant to be given away, so no need to incref here.
"
275,"} else if (stream == Py_None) {
streams.emplace_back();
} else {
std::runtime_error(""Unknown data type found in stream list. Need torch.cuda.Stream or None"");
}
}
","} else if (stream == Py_None) {
streams.emplace_back();
} else {
      // NOLINTNEXTLINE(bugprone-throw-keyword-missing)
std::runtime_error(""Unknown data type found in stream list. Need torch.cuda.Stream or None"");
}
}
"
276,"#include <thread>
#include <vector>
#include <assert.h>
#include <torch/deploy.h>
","#include <thread>
#include <vector>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <torch/deploy.h>
"
277,"#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <iostream>
#include <assert.h>
#include <pybind11/embed.h>
#include <stdio.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>
","#include <torch/csrc/deploy/interpreter/interpreter_impl.h>
#include <iostream>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <assert.h>
#include <pybind11/embed.h>
// NOLINTNEXTLINE(modernize-deprecated-headers)
#include <stdio.h>
#include <torch/csrc/autograd/generated/variable_factories.h>
#include <torch/csrc/jit/python/pybind_utils.h>
"
278,"}
void ProcessGroupAgent::sendToSelf(Message&& message) {
threadPool_.run(std::bind(
[this](const Message& message) {
// Unlike the other cases, need to add a tensor deleter, since the
","}
void ProcessGroupAgent::sendToSelf(Message&& message) {
  // NOLINTNEXTLINE(modernize-avoid-bind)
threadPool_.run(std::bind(
[this](const Message& message) {
// Unlike the other cases, need to add a tensor deleter, since the
"
279,"payload.resize(indexToRead);
TORCH_INTERNAL_ASSERT(
payload.size() > additionalPayloadSize,
""Wrong payload sizes: payload.size() is "",
payload.size(),
","payload.resize(indexToRead);
TORCH_INTERNAL_ASSERT(
      // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
payload.size() > additionalPayloadSize,
""Wrong payload sizes: payload.size() is "",
payload.size(),
"
280,"within_alloc = true;
}
}
return loop_to_ind_map;
}
","within_alloc = true;
}
}
  // NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)
return loop_to_ind_map;
}
"
281,"}
}
auto index_map = generateIndexAndExtentMap(
tv_stack,
std::deque<kir::ForLoop*>(loops.begin(), loops.end()),
","}
}
  // NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)
auto index_map = generateIndexAndExtentMap(
tv_stack,
std::deque<kir::ForLoop*>(loops.begin(), loops.end()),
"
282,"default:
TORCH_CHECK(false, ""Unexpected expression type"");
}
}
void handle(const Statement* node) override {
","default:
TORCH_CHECK(false, ""Unexpected expression type"");
}
    // NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)
}
void handle(const Statement* node) override {
"
283,"TargetGroupMapT& computed_at_exprs,
ExprScoreMapT& scores) {
TensorView* target_tensor = nullptr;
ScoreT score;
findTargetTensor(expr, target_tensor, score);
scores.emplace(expr, score);
","TargetGroupMapT& computed_at_exprs,
ExprScoreMapT& scores) {
TensorView* target_tensor = nullptr;
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
ScoreT score;
findTargetTensor(expr, target_tensor, score);
scores.emplace(expr, score);
"
284,"return c10::nullopt;
}
auto steps_a = inputTensorValues[4].accessor<int64_t, 1>();
for (size_t i = 0; i < inputTensorValues[4].sizes()[0]; ++i) {
// Only steps == 1 are supported for constant-folding.
if (steps_a[i] != 1) {
","return c10::nullopt;
}
auto steps_a = inputTensorValues[4].accessor<int64_t, 1>();
    // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
for (size_t i = 0; i < inputTensorValues[4].sizes()[0]; ++i) {
// Only steps == 1 are supported for constant-folding.
if (steps_a[i] != 1) {
"
285,"throw std::runtime_error(""support only 3D gpu_block_index"");
}
const Expr* prev = nullptr;
if (gpu_block_extents_.size() <= gpu_block_index) {
gpu_block_extents_.resize(gpu_block_index + 1);
} else {
","throw std::runtime_error(""support only 3D gpu_block_index"");
}
const Expr* prev = nullptr;
    // NOLINTNEXTLINE(clang-diagnostic-sign-compare)
if (gpu_block_extents_.size() <= gpu_block_index) {
gpu_block_extents_.resize(gpu_block_index + 1);
} else {
"
286,"os() << cudaDtypeCppString(dtype) << (buffer_arg.isVar() ? "" "" : ""* "")
<< name_manager()->get_unique_name(var);
}
const Var* rand_seed;
const Var* rand_offset;
if (has_random_) {
// TODO: switch to kUint64 when it is available.
","os() << cudaDtypeCppString(dtype) << (buffer_arg.isVar() ? "" "" : ""* "")
<< name_manager()->get_unique_name(var);
}
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
const Var* rand_seed;
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
const Var* rand_offset;
if (has_random_) {
// TODO: switch to kUint64 when it is available.
"
287,"#include <ATen/Config.h>
#include <ATen/cuda/CUDAConfig.h>
#include <ATen/cuda/CUDAEvent.h>
#include <ATen/cuda/Exceptions.h>
#include <ATen/InitialTensorOptions.h>
#include <ATen/MatrixRef.h>
","#include <ATen/Config.h>
#include <ATen/cuda/CUDAConfig.h>
#include <ATen/cuda/CUDAEvent.h>
#include <ATen/cuda/CUDAGraphsUtils.cuh>
#include <ATen/cuda/Exceptions.h>
#include <ATen/InitialTensorOptions.h>
#include <ATen/MatrixRef.h>
"
288,"if (db.mayContainAlias({out}, graph->outputs())) {
continue;
}
    auto new_symbol = supported.at(n->kind());
auto* new_node = graph->create(new_symbol, n->outputs().size());
new_node->insertBefore(n);
for (auto* input : n->inputs()) {
","if (db.mayContainAlias({out}, graph->outputs())) {
continue;
}
auto* new_node = graph->create(new_symbol, n->outputs().size());
new_node->insertBefore(n);
for (auto* input : n->inputs()) {
"
289,"return result;
}
Tensor& resize_(
    Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
if (self.has_names()) {
","return result;
}
const Tensor& resize_(
    const Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
if (self.has_names()) {
"
290,"* reshaping methods
******************************************************************************/
SparseTensor& sparse_resize_(
    SparseTensor& self,
ArrayRef<int64_t> size,
int64_t sparse_dim,
int64_t dense_dim) {
","* reshaping methods
******************************************************************************/
const SparseTensor& sparse_resize_(
    const SparseTensor& self,
ArrayRef<int64_t> size,
int64_t sparse_dim,
int64_t dense_dim) {
"
291,"return self;
}
Tensor& resize_(
c10::DispatchKeySet ks,
    Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
","return self;
}
const Tensor& resize_(
c10::DispatchKeySet ks,
    const Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
auto& self_ = unpack(self, ""self"", 0);
"
292,"c10::optional<QualifiedName> qualifiedName,
std::weak_ptr<CompilationUnit> cu,
bool is_module,
    std::string doc_string) {
  return ClassTypePtr(
      new ClassType(std::move(qualifiedName), std::move(cu), is_module, std::move(doc_string)));
}
ClassType::ClassType(
c10::optional<QualifiedName> name,
std::weak_ptr<CompilationUnit> cu,
    bool is_module = false,
    std::string doc_string = """")
: NamedType(TypeKind::ClassType, std::move(name)),
compilation_unit_(std::move(cu)),
isModule_(is_module),
      doc_string_(std::move(doc_string)) {}
const std::vector<torch::jit::Function*>& ClassType::methods() const {
return methods_;
","c10::optional<QualifiedName> qualifiedName,
std::weak_ptr<CompilationUnit> cu,
bool is_module,
    std::string doc_string,
    std::vector<std::string> unresolved_class_attributes) {
  return ClassTypePtr(new ClassType(
      std::move(qualifiedName),
      std::move(cu),
      is_module,
      std::move(doc_string),
      std::move(unresolved_class_attributes)));
}
ClassType::ClassType(
c10::optional<QualifiedName> name,
std::weak_ptr<CompilationUnit> cu,
    bool is_module,
    std::string doc_string,
    std::vector<std::string> unresolved_class_attributes)
: NamedType(TypeKind::ClassType, std::move(name)),
compilation_unit_(std::move(cu)),
isModule_(is_module),
      doc_string_(std::move(doc_string)),
      unresolved_class_attributes_(std::move(unresolved_class_attributes)) {}
const std::vector<torch::jit::Function*>& ClassType::methods() const {
return methods_;
"
293,"#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/util/Logging.h>
#include <torch/csrc/cuda/nccl.h>
#include <c10d/Utils.hpp>
","#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/util/Logging.h>
#include <c10d/ParamCommsUtils.hpp>
#include <torch/csrc/cuda/nccl.h>
#include <c10d/Utils.hpp>
"
294,"Tensor rad2deg_backward(const Tensor& grad) {
constexpr double M_180_PI = 57.295779513082320876798154814105170332405472466564;
  return at::mul(grad, wrapped_scalar_tensor(Scalar(M_180_PI)));
}
Tensor deg2rad_backward(const Tensor& grad) {
constexpr double M_PI_180 = 0.017453292519943295769236907684886127134428718885417;
  return at::mul(grad, wrapped_scalar_tensor(Scalar(M_PI_180)));
}
Tensor unsqueeze_multiple(const Tensor & t, IntArrayRef dim, size_t n_dims) {
","Tensor rad2deg_backward(const Tensor& grad) {
constexpr double M_180_PI = 57.295779513082320876798154814105170332405472466564;
  return at::mul(grad, at::native::wrapped_scalar_tensor(Scalar(M_180_PI)));
}
Tensor deg2rad_backward(const Tensor& grad) {
constexpr double M_PI_180 = 0.017453292519943295769236907684886127134428718885417;
  return at::mul(grad, at::native::wrapped_scalar_tensor(Scalar(M_PI_180)));
}
Tensor unsqueeze_multiple(const Tensor & t, IntArrayRef dim, size_t n_dims) {
"
295,"int i = 0;
#ifdef CPU_CAPABILITY_AVX2
  __m256i sum_v_epi32 = _mm256_setzero_si256();
// vectorized
  for (; i < len / 16 * 16; i += 16) {
    // (i15, ..., i0)
    __m128i src_epi8 = _mm_loadu_si128(reinterpret_cast<__m128i const*>(A + i));
    __m256i src_epi16 = _mm256_cvtepi8_epi16(src_epi8);
    // (i15 ^ 2, ..., i0 ^ 2)
    __m256i sq_epi16 = _mm256_mullo_epi16(src_epi16, src_epi16);
    // (i7 ^ 2, ..., i0 ^ 2)
    __m128i sq_lo_epi16 = _mm256_castsi256_si128(sq_epi16);
    // (i15 ^ 2, ..., i8 ^ 2)
    __m128i sq_hi_epi16 = _mm256_extractf128_si256(sq_epi16, 1);
    // widen to epi32
    __m256i sq_lo_epi32 = _mm256_cvtepi16_epi32(sq_lo_epi16);
    __m256i sq_hi_epi32 = _mm256_cvtepi16_epi32(sq_hi_epi16);
    // add to running sum
    sum_v_epi32 = _mm256_add_epi32(sum_v_epi32, sq_lo_epi32);
    sum_v_epi32 = _mm256_add_epi32(sum_v_epi32, sq_hi_epi32);
  }

alignas(64) int32_t temp[8];
  _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_v_epi32);
  for (int k = 0; k < 8; ++k) {
    row_sum += temp[k];
}
#endif // CPU_CAPABILITY_AVX2
","int i = 0;
#ifdef CPU_CAPABILITY_AVX2
// vectorized
  __m256i sum_v_epi32 = _mm256_setzero_si256();
alignas(64) int32_t temp[8];

  int overflow_threshold = 1048576; //2147483647/(128*128)*8 = 1048576
  int loop = len / overflow_threshold + 1;

  for(int j=0; j<=loop; j++){
    for (; ((i < overflow_threshold * j) && (i < len / 16 * 16)); i += 16) {
      // (i15, ..., i0)
      __m128i src_epi8 = _mm_loadu_si128(reinterpret_cast<__m128i const*>(A + i));
      __m256i src_epi16 = _mm256_cvtepi8_epi16(src_epi8);
      // (i15 ^ 2, ..., i0 ^ 2)
      __m256i sq_epi16 = _mm256_mullo_epi16(src_epi16, src_epi16);
      // (i7 ^ 2, ..., i0 ^ 2)
      __m128i sq_lo_epi16 = _mm256_castsi256_si128(sq_epi16);
      // (i15 ^ 2, ..., i8 ^ 2)
      __m128i sq_hi_epi16 = _mm256_extractf128_si256(sq_epi16, 1);
      // widen to epi32
      __m256i sq_lo_epi32 = _mm256_cvtepi16_epi32(sq_lo_epi16);
      __m256i sq_hi_epi32 = _mm256_cvtepi16_epi32(sq_hi_epi16);
      // add to running sum
      sum_v_epi32 = _mm256_add_epi32(sum_v_epi32, sq_lo_epi32);
      sum_v_epi32 = _mm256_add_epi32(sum_v_epi32, sq_hi_epi32);
    }
    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_v_epi32);

    for (int k = 0; k < 8; ++k) {
      row_sum += temp[k];
    }
    sum_v_epi32 = _mm256_setzero_si256();
}
#endif // CPU_CAPABILITY_AVX2
"
296,"}
}
std::shared_ptr<Graph> graph_;
std::unique_ptr<AliasDb> aliasDb_ = nullptr;
// Minimal size of a fusion group
size_t min_group_size_;
// If true, shapes are ignored
","}
}
  // This function parses the option provided by the environment variable
  // ""PYTORCH_TENSOREXPR_DONT_FUSE"".
  // This variable allows users to disable fusion on a list of specified
  // operators that are separated by ':'. e.g.,
  // 'PYTORCH_TENSOREXPR_DONT_FUSE=""clamp:mul:add""' disables fusion on
  // aten::clamp, aten::mul and aten::add.
  void parseTENotFuseOption() {
    const char* option = std::getenv(""PYTORCH_TENSOREXPR_DONT_FUSE"");
    std::stringstream in_ss;
    if (option) {
      in_ss << option;
    }

    std::string line;
    while (std::getline(in_ss, line, ':')) {
      if (line.size() == 0) {
        continue;
      }
      operators_not_to_fuse.insert(c10::Symbol::aten(line));
    }
  }

std::shared_ptr<Graph> graph_;
std::unique_ptr<AliasDb> aliasDb_ = nullptr;
  std::set<NodeKind> operators_not_to_fuse;
// Minimal size of a fusion group
size_t min_group_size_;
// If true, shapes are ignored
"
297,"}
static void frac_kernel(TensorIterator& iter) {
  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), ""frac_cpu"", [&]() {
    cpu_kernel_vec(
        iter,
        [=](scalar_t a) -> scalar_t { return a - std::trunc(a); },
        [=](Vec256<scalar_t> a) { return a.frac(); });
  });
}
static void logical_not_kernel(TensorIterator& iter) {
","}
static void frac_kernel(TensorIterator& iter) {
  AT_DISPATCH_FLOATING_TYPES_AND2(
      kBFloat16, kHalf, iter.dtype(), ""frac_cpu"", [&]() {
        cpu_kernel_vec(
            iter,
            [=](scalar_t a) -> scalar_t {
              if (std::isinf(a)) {
                return std::copysign(static_cast<scalar_t>(0), a);
              }
              return a - ::trunc(a);
            },
            [=](Vec256<scalar_t> a) { return a.frac(); });
      });
}
static void logical_not_kernel(TensorIterator& iter) {
"
298,"#endif
Tensor& _lstsq_helper_cpu(
    Tensor& b, Tensor& rank, Tensor& singular_values, const Tensor& a, double cond, std::string driver_name) {
#ifndef USE_LAPACK
TORCH_CHECK(false, ""torch.linalg.lstsq: LAPACK library not found in compilation"");
#else
","#endif
Tensor& _lstsq_helper_cpu(
    Tensor& b, Tensor& rank, Tensor& singular_values, Tensor& infos, const Tensor& a, double cond, std::string driver_name) {
#ifndef USE_LAPACK
TORCH_CHECK(false, ""torch.linalg.lstsq: LAPACK library not found in compilation"");
#else
"
299,"}
auto& out_t = p_node->Output(0).toTensor();
fastResizeToZero(out_t);
        return at::native::embedding_bag_byte_rowwise_offsets_out(
out_t,
weight,
indices,
","}
auto& out_t = p_node->Output(0).toTensor();
fastResizeToZero(out_t);
        return at::native::embedding_bag_4bit_rowwise_offsets_out(
out_t,
weight,
indices,
"
300,".def(py::init([](const std::string& qualified_name) {
return get_python_cu()->get_class(c10::QualifiedName(qualified_name));
}))
      .def(""name"", [](ClassType& self) { return self.name()->name(); });
py::class_<EnumType, Type, std::shared_ptr<EnumType>>(m, ""EnumType"")
.def(py::init([](const std::string& qualified_name,
TypePtr value_type,
",".def(py::init([](const std::string& qualified_name) {
return get_python_cu()->get_class(c10::QualifiedName(qualified_name));
}))
      .def(""name"", [](ClassType& self) { return self.name()->name(); })
      .def(""qualified_name"", [](ClassType& self) {
        return self.name()->qualifiedName();
      });
py::class_<EnumType, Type, std::shared_ptr<EnumType>>(m, ""EnumType"")
.def(py::init([](const std::string& qualified_name,
TypePtr value_type,
"
301,"check_scalar_type_device_layout_equal(indices, at::empty({0}, self.options().dtype(at::kLong)));
{
NoNamesGuard guard;
    values.resize_(self.sizes());
    indices.resize_(self.sizes());
if(self.dim() == 0) {
values.fill_(self);
indices.fill_(0);
","check_scalar_type_device_layout_equal(indices, at::empty({0}, self.options().dtype(at::kLong)));
{
NoNamesGuard guard;
    resize_output(values, self.sizes());
    resize_output(indices, self.sizes());
if(self.dim() == 0) {
values.fill_(self);
indices.fill_(0);
"
302,"auto& src_ = unpack(src, ""src"", 1);
std::shared_ptr<CopyBackwards> grad_fn;
auto requires_grad = compute_requires_grad(self, src);
  assert_no_inference_tensor(self, src);
requires_grad &= isDifferentiableType(self.scalar_type());
check_inplace(self, requires_grad);
if (requires_grad) {
","auto& src_ = unpack(src, ""src"", 1);
std::shared_ptr<CopyBackwards> grad_fn;
auto requires_grad = compute_requires_grad(self, src);
requires_grad &= isDifferentiableType(self.scalar_type());
check_inplace(self, requires_grad);
if (requires_grad) {
"
303,"}
}
}} // namespace c10::impl
","}
}
bool tls_is_dispatch_keyset_excluded(DispatchKeySet ks) {
  return raw_local_dispatch_key_set.excluded().isSupersetOf(ks);
}

bool tls_is_dispatch_keyset_included(DispatchKeySet ks) {
  return raw_local_dispatch_key_set.included().isSupersetOf(ks);
}
}} // namespace c10::impl
"
304,""" can clarify your code and remove this warning by moving both the view and the inplace either both""
"" inside the no_grad block (if you don't want the inplace to be tracked) or both outside (if you want""
"" the inplace to be tracked)."");
} else if (creation_meta == CreationMeta::IN_CUSTOM_FUNCTION) {
msg = c10::str(msg, "" This view was created inside a custom Function (or because an input was returned as-is) and the""
"" autograd logic to handle view+inplace would override the custom backward associated with the custom""
",""" can clarify your code and remove this warning by moving both the view and the inplace either both""
"" inside the no_grad block (if you don't want the inplace to be tracked) or both outside (if you want""
"" the inplace to be tracked)."");
      } else if (creation_meta == CreationMeta::INFERENCE_MODE) {
        TORCH_INTERNAL_ASSERT(!grad_fn);
        msg = c10::str(msg, "" Given that this use case is ambiguous and error-prone, it is forbidden. ""
                       "" You can clarify your code by moving both the view and the inplace either both""
                       "" inside the inference_mode block (if you don't want the inplace to be tracked) or both outside (if you want""
                       "" the inplace to be tracked)."");
        TORCH_CHECK(false, msg);
} else if (creation_meta == CreationMeta::IN_CUSTOM_FUNCTION) {
msg = c10::str(msg, "" This view was created inside a custom Function (or because an input was returned as-is) and the""
"" autograd logic to handle view+inplace would override the custom backward associated with the custom""
"
305,"//     no dispatch keys are available we just slide into the undefined handler which would then raise
//     the error message.
// In the old world of catchAll, the only way to ""register"" a kernel to Undefined is by registering it to
  // catchAll. After catchAllKernel_ is removed, Undefined now can get a kernel from either DefaultBackend
// or CompositeImplicitAutograd alias key so that we don't break the support. Ideally isIncludedInAlias(Undefined, CompositeImplicitAutograd)
// should return true, it returns false because Undefined cannot be represented in a DispatchKeySet.
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
","//     no dispatch keys are available we just slide into the undefined handler which would then raise
//     the error message.
// In the old world of catchAll, the only way to ""register"" a kernel to Undefined is by registering it to
  // catchAll. After catchAllKernel_ is removed, Undefined now can get a kernel from either CompositeExplicitAutograd
// or CompositeImplicitAutograd alias key so that we don't break the support. Ideally isIncludedInAlias(Undefined, CompositeImplicitAutograd)
// should return true, it returns false because Undefined cannot be represented in a DispatchKeySet.
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
"
306,"namespace c10 {
// backend_dispatch_keyset should include all runtime backend keys.
// Alias key DispatchKey::DefaultBackend maps to backend_dispatch_keyset
// NestedTensor has been explicitly removed due to incompatibility with some
// kernels, such as structured kernels, that use the DefaultBackend key.
constexpr DispatchKeySet backend_dispatch_keyset = autogradother_backends |
","namespace c10 {
// backend_dispatch_keyset should include all runtime backend keys.
// Alias key DispatchKey::CompositeExplicitAutograd maps to backend_dispatch_keyset
// NestedTensor has been explicitly removed due to incompatibility with some
// kernels, such as structured kernels, that use the DefaultBackend key.
constexpr DispatchKeySet backend_dispatch_keyset = autogradother_backends |
"
307,"// Ops in the following registration list are registered as
//   (1) CompositeImplicitAutograd kernels
//   (2) Autograd kernels
//   (3) DefaultBackend kernels and additionally Autograd kernels
// The reason for (3) is that ops that also use dispatch (e.g. register CPU/CUDA/QuantizedCPU
// kernels) will skip picking up CompositeImplicitAutograd kernels for Autograd, so we register them to both
// DefaultBackend and Autograd instead. See
// https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword
// for more details.
// Invariant:
// - Ops registered to CompositeImplicitAutograd or DefaultBackend below must match `MANUAL_BACKEND` set in tools/autograd/gen_variable_type.py.
//   and they have manual_kernel_registration=True in native_functions.yaml.
// - Ops registered to DispatchKey::Autograd below must be included in `MANUAL_AUTOGRAD` in tools/autograd/gen_variable_type.py
","// Ops in the following registration list are registered as
//   (1) CompositeImplicitAutograd kernels
//   (2) Autograd kernels
//   (3) CompositeExplicitAutograd kernels and additionally Autograd kernels
// The reason for (3) is that ops that also use dispatch (e.g. register CPU/CUDA/QuantizedCPU
// kernels) will skip picking up CompositeImplicitAutograd kernels for Autograd, so we register them to both
// CompositeExplicitAutograd and Autograd instead. See
// https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword
// for more details.
// Invariant:
// - Ops registered to CompositeImplicitAutograd or CompositeExplicitAutograd below must match `MANUAL_BACKEND` set in tools/autograd/gen_variable_type.py.
//   and they have manual_kernel_registration=True in native_functions.yaml.
// - Ops registered to DispatchKey::Autograd below must be included in `MANUAL_AUTOGRAD` in tools/autograd/gen_variable_type.py
"
308,"const int L = TH_DESC_BUFF_LEN;
THDescBuff buf;
char *str = buf.str;
  int i, n = 0;
n += snprintf(str, L-n, ""["");
for (i = 0; i < ndim; i++) {
","const int L = TH_DESC_BUFF_LEN;
THDescBuff buf;
char *str = buf.str;
  int64_t i;
  int64_t n = 0;
n += snprintf(str, L-n, ""["");
for (i = 0; i < ndim; i++) {
"
309,"auto weight_grad_collector_data = weight_grad_collector.data_ptr<scalar_t>();
auto loop = [&](int64_t start, int64_t end) {
    for (auto i = start; i < end; i++) {
      for (auto j = 0; j < channel_size; j++) {
        for (auto k = 0; k < input_stride1; k++) {
int64_t pos = i * input_stride0 + j * input_stride1 + k;
scalar_t weight_data_val = weight_data[j];
scalar_t input_data_val = input_data[pos];
","auto weight_grad_collector_data = weight_grad_collector.data_ptr<scalar_t>();
auto loop = [&](int64_t start, int64_t end) {
    for (const auto i : c10::irange(start, end)) {
      for (const auto j : c10::irange(channel_size)) {
        for (const auto k : c10::irange(input_stride1)) {
int64_t pos = i * input_stride0 + j * input_stride1 + k;
scalar_t weight_data_val = weight_data[j];
scalar_t input_data_val = input_data[pos];
"
310,""", expected bias to be 1-dimensional with "", weight_sizes[0], "" elements"",
"", but got bias of size "", bias.sizes(), "" instead"");
    for (int i = 2; i < k; ++i) {
input_shape.push_back(input.size(i) + 2 * padding[i-2]);
// log new kernel size considering dilation
kernel_shape.push_back(dilation[i-2] * (weight_sizes[i]-1) + 1);
",""", expected bias to be 1-dimensional with "", weight_sizes[0], "" elements"",
"", but got bias of size "", bias.sizes(), "" instead"");
    for (const auto i : c10::irange(2, k)) {
input_shape.push_back(input.size(i) + 2 * padding[i-2]);
// log new kernel size considering dilation
kernel_shape.push_back(dilation[i-2] * (weight_sizes[i]-1) + 1);
"
311,"#include <ATen/TensorUtils.h>
#include <ATen/NativeFunctions.h>
#include <cstring>
#include <memory>
#include <sstream>
","#include <ATen/TensorUtils.h>
#include <ATen/NativeFunctions.h>
#include <c10/util/irange.h>

#include <cstring>
#include <memory>
#include <sstream>
"
312,"scalar_t scale = static_cast<scalar_t>(scale_);
// For each elt in batch, do:
        for (int elt = 0; elt < batch_size; elt++) {
// Matrix mulitply per output:
grad_output_n = grad_output.select(0, elt);
","scalar_t scale = static_cast<scalar_t>(scale_);
// For each elt in batch, do:
        for (const auto elt : c10::irange(batch_size)) {
// Matrix mulitply per output:
grad_output_n = grad_output.select(0, elt);
"
313,"// appears in the jth element of tensor.
std::vector<int64_t> tensor_idx_for(order.size(), not_found);
  for (auto order_idx = 0U; order_idx < order.size(); ++order_idx) {
const auto name = order[order_idx];
TORCH_CHECK(name.isBasic(),
""align_to: the desired order of dimensions cannot contain a None name, got "",
","// appears in the jth element of tensor.
std::vector<int64_t> tensor_idx_for(order.size(), not_found);
  for (const auto order_idx : c10::irange(order.size())) {
const auto name = order[order_idx];
TORCH_CHECK(name.isBasic(),
""align_to: the desired order of dimensions cannot contain a None name, got "",
"
314,"#include <ATen/ATen.h>
#include <ATen/NativeFunctions.h>
namespace at { namespace native {
void checkLongTensor(const Tensor& tensor) {
","#include <ATen/ATen.h>
#include <ATen/NativeFunctions.h>
#include <c10/util/irange.h>

namespace at { namespace native {
void checkLongTensor(const Tensor& tensor) {
"
315,"TORCH_CHECK(lengths[batch_size - 1] > 0,
""Length of all samples has to be greater than 0, but found an element ""
""in 'lengths' that is <= 0"");
  for(auto i = 0; i < batch_size - 1; i++) {
if (lengths[batch_size - 1 - i] > lengths[batch_size - 2 - i]) {
// NB: enforce_sorted is implemented at a Python level, but the sortedness
// check lives here. If enforce_sorted=False then this error should never
","TORCH_CHECK(lengths[batch_size - 1] > 0,
""Length of all samples has to be greater than 0, but found an element ""
""in 'lengths' that is <= 0"");
  for (const auto i : c10::irange(batch_size - 1)) {
if (lengths[batch_size - 1 - i] > lengths[batch_size - 2 - i]) {
// NB: enforce_sorted is implemented at a Python level, but the sortedness
// check lives here. If enforce_sorted=False then this error should never
"
316,"const int output_channels_per_group = output_channels / groups;
const int inner_size =
kernel_d * kernel_h * kernel_w * input_channels_per_group;
  for (int g = 0; g < groups; ++g) {
for (int i = 0; i < output_channels_per_group; ++i) {
const int c = g * output_channels_per_group + i;
int32_t sum = 0;
","const int output_channels_per_group = output_channels / groups;
const int inner_size =
kernel_d * kernel_h * kernel_w * input_channels_per_group;
  for (const auto g : c10::irange(groups)) {
for (int i = 0; i < output_channels_per_group; ++i) {
const int c = g * output_channels_per_group + i;
int32_t sum = 0;
"
317,"#include <torch/custom_class.h>
#include <torch/library.h>
#include <algorithm>
#include <string>
","#include <torch/custom_class.h>
#include <torch/library.h>
#include <c10/util/irange.h>

#include <algorithm>
#include <string>
"
318,"int num_tasks = at::get_num_threads();
at::parallel_for(0, num_tasks, 1, [&](int64_t begin, int64_t end) {
    for (int task_id = begin; task_id < end; ++task_id) {
// This operation does the following:
// 1) Creates a ""row buffer"" vector with offset values that must be
//    added to the integer matrix multiplication operation to ensure
","int num_tasks = at::get_num_threads();
at::parallel_for(0, num_tasks, 1, [&](int64_t begin, int64_t end) {
    for (const auto task_id : c10::irange(begin, end)) {
// This operation does the following:
// 1) Creates a ""row buffer"" vector with offset values that must be
//    added to the integer matrix multiplication operation to ensure
"
319,"// This is the end of the pipeline, pass the resulting matrix through.
fbgemm::DoNothing<float, float> doNothingObj{};
    for (int task_id = begin; task_id < end; ++task_id) {
if (q_scheme == c10::kPerTensorAffine) {
// Process the per tensor quantization.
//
","// This is the end of the pipeline, pass the resulting matrix through.
fbgemm::DoNothing<float, float> doNothingObj{};
    for (const auto task_id : c10::irange(begin, end)) {
if (q_scheme == c10::kPerTensorAffine) {
// Process the per tensor quantization.
//
"
320,"llvm::BasicBlock* bb_;
llvm::Value* value_{nullptr};
llvm::JITTargetAddress kernelAddress_;
  std::unique_ptr<void* []> argv_ { nullptr };
#define LLVM_TYPE_DECLARE(_1, Name) llvm::Type* Name##Ty_;
AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, LLVM_TYPE_DECLARE);
","llvm::BasicBlock* bb_;
llvm::Value* value_{nullptr};
llvm::JITTargetAddress kernelAddress_;
#define LLVM_TYPE_DECLARE(_1, Name) llvm::Type* Name##Ty_;
AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, LLVM_TYPE_DECLARE);
"
321,"jit_->addModule(std::move(module_), std::move(context_));
auto sym = jit_->findSymbol(""wrapper"");
kernelAddress_ = assertSuccess(sym.getAddress());
  argv_ = std::make_unique<void*[]>(params.size());
USE_TRIGGER(llvm_codegen_created);
}
","jit_->addModule(std::move(module_), std::move(context_));
auto sym = jit_->findSymbol(""wrapper"");
kernelAddress_ = assertSuccess(sym.getAddress());
USE_TRIGGER(llvm_codegen_created);
}
"
322,"TORCH_INTERNAL_ASSERT(result.scalar_type() == output().dtype());
}
TORCH_IMPL_FUNC(mul_out) (
const Tensor& self, const Tensor& other, const Tensor& result
) {
","TORCH_INTERNAL_ASSERT(result.scalar_type() == output().dtype());
}
TORCH_IMPL_FUNC(sub_out) (
  const Tensor& self, const Tensor& other, const Scalar& alpha, const Tensor& result
) {
  sub_stub(device_type(), *this, alpha);
  TORCH_INTERNAL_ASSERT(result.scalar_type() == output().dtype());
}

TORCH_IMPL_FUNC(mul_out) (
const Tensor& self, const Tensor& other, const Tensor& result
) {
"
323,"return at::sparse_coo_tensor(indices, values, values.options().layout(at::kSparse)).set_requires_grad(r.toBool(4));
} else if (r.idx == 1) {
bool type_inference = r.isNone(3);
    const auto inferred_dispatch_key = denseTypeIdWithDefault(r, 4, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(3, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(4));
Tensor values = internal_new_from_data(inferred_dispatch_key, inferred_scalar_type, r.deviceOptional(4), r.pyobject(1),
","return at::sparse_coo_tensor(indices, values, values.options().layout(at::kSparse)).set_requires_grad(r.toBool(4));
} else if (r.idx == 1) {
bool type_inference = r.isNone(3);
    const auto inferred_dispatch_key = typeIdWithDefault(r, 4, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(3, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(4));
Tensor values = internal_new_from_data(inferred_dispatch_key, inferred_scalar_type, r.deviceOptional(4), r.pyobject(1),
"
324,"ddp_logging_data_->world_size = reducer_->process_group_->getSize();
ddp_logging_data_->rank = reducer_->process_group_->getRank();
ddp_logging_data_->iteration = 0;
  ddp_logging_data_->dtype =
      std::string(reducer_->replicas_[0][0].dtype().name());
ddp_logging_data_->is_multi_device_module = reducer_->is_multi_device_module_;
set_parameter_stats();
","ddp_logging_data_->world_size = reducer_->process_group_->getSize();
ddp_logging_data_->rank = reducer_->process_group_->getRank();
ddp_logging_data_->iteration = 0;
ddp_logging_data_->is_multi_device_module = reducer_->is_multi_device_module_;
set_parameter_stats();
"
325,"data,
""DefaultCPUAllocator: not enough memory: you tried to allocate "",
nbytes,
      "" bytes. Buy new RAM!"");
// move data to a thread's NUMA node
NUMAMove(data, nbytes, GetCurrentNUMANode());
","data,
""DefaultCPUAllocator: not enough memory: you tried to allocate "",
nbytes,
      "" bytes."");
// move data to a thread's NUMA node
NUMAMove(data, nbytes, GetCurrentNUMANode());
"
326,"""nccl_socket_ifname"", &c10::DDPLoggingData::nccl_socket_ifname)
.def_readwrite(
""nccl_blocking_wait"", &c10::DDPLoggingData::nccl_blocking_wait)
.def_readwrite(""nccl_debug"", &c10::DDPLoggingData::nccl_debug)
.def_readwrite(""nccl_nthreads"", &c10::DDPLoggingData::nccl_nthreads)
.def_readwrite(""nccl_ib_timeout"", &c10::DDPLoggingData::nccl_ib_timeout)
","""nccl_socket_ifname"", &c10::DDPLoggingData::nccl_socket_ifname)
.def_readwrite(
""nccl_blocking_wait"", &c10::DDPLoggingData::nccl_blocking_wait)
      .def_readwrite(""nccl_async_error_handling"", &c10::DDPLoggingData::nccl_async_error_handling)
.def_readwrite(""nccl_debug"", &c10::DDPLoggingData::nccl_debug)
.def_readwrite(""nccl_nthreads"", &c10::DDPLoggingData::nccl_nthreads)
.def_readwrite(""nccl_ib_timeout"", &c10::DDPLoggingData::nccl_ib_timeout)
"
327,"#include <torch/csrc/python_headers.h>
#include <c10/util/intrusive_ptr.h>
#include <c10d/FileStore.hpp>
#include <c10d/TCPStore.hpp>
#ifndef _WIN32
","#include <torch/csrc/python_headers.h>
#include <c10/util/intrusive_ptr.h>
#include <c10d/Utils.hpp>
#include <c10d/FileStore.hpp>
#include <c10d/TCPStore.hpp>
#ifndef _WIN32
"
328,"py::arg(""tensor_indices"") = std::vector<int64_t>(),
py::call_guard<py::gil_scoped_release>());
module.def(
""_broadcast_coalesced"",
// Define a lambda such that the pybind11 prototype can take a std::vector
","py::arg(""tensor_indices"") = std::vector<int64_t>(),
py::call_guard<py::gil_scoped_release>());
  module.def(
      ""_verify_replicas_within_process"",
      &::c10d::verify_replicas_within_process,
      py::arg(""replicas""),
      py::arg(""expect_sparse_gradient""),
      py::call_guard<py::gil_scoped_release>());

  module.def(
      ""_verify_model_across_ranks"",
      &::c10d::verify_replica0_across_processes,
      py::arg(""process_group""),
      py::arg(""replicas""),
      py::call_guard<py::gil_scoped_release>());

module.def(
""_broadcast_coalesced"",
// Define a lambda such that the pybind11 prototype can take a std::vector
"
329,"}
}
// Verifies replicas in this process treat the same number of params,
// all params require grad, and corresponding params across replicas
// have the same dtype/size/layout.
void Reducer::verify_replicas_within_process() {
  const auto replica_count = replicas_.size();
  for (size_t replica_index = 0; replica_index < replica_count;
       replica_index++) {
    const auto variable_count = replicas_[replica_index].size();
    TORCH_CHECK(
        replicas_[replica_index].size() == replicas_[0].size(),
        ""Model replicas must have an equal number of parameters."");
    TORCH_CHECK(
        expect_sparse_gradients_[replica_index].size() ==
            expect_sparse_gradients_[0].size(),
        ""Expected number of entries in expect_sparse_gradients "",
        ""to be equal across replicas."");
    for (size_t variable_index = 0; variable_index < variable_count;
         variable_index++) {
      TORCH_CHECK(
          replicas_[replica_index][variable_index].requires_grad(),
          ""Variables must require gradients (have `requires_grad` set)."");
      TORCH_CHECK(
          replicas_[replica_index][variable_index].sizes() ==
              replicas_[0][variable_index].sizes(),
          ""Variables across model replicas must have identical sizes."");
      TORCH_CHECK(
          replicas_[replica_index][variable_index].strides() ==
              replicas_[0][variable_index].strides(),
          ""Variables across model replicas must have identical strides."");
      TORCH_CHECK(
          replicas_[replica_index][variable_index].dtype() ==
              replicas_[0][variable_index].dtype(),
          ""Variables across model replicas must have identical dtype."");
      TORCH_CHECK(
          expect_sparse_gradients_[replica_index][variable_index] ==
              expect_sparse_gradients_[0][variable_index],
          ""Expected the same variables across replicas to either both "",
          ""or neither expect a sparse gradient."");
    }
  }
}

// Verifies corresponding params in replica 0 have the same sizes/strides
// across processes.
void Reducer::verify_replica0_across_processes() {
  size_t i = 0;
  for (const auto& t : replicas_[0]) {
    i += 2 * t.dim();
  }
  at::TensorOptions options;
  options = options.dtype(at::kLong);
  auto metadata = at::empty({static_cast<long>(i)}, options);

  // Technically, process 0 is the broadcast source, so only process 0 needs
  // to populate metadata.  But no harm keeping work aligned across processes.
  auto metadata_accessor = metadata.accessor<int64_t, 1>();
  i = 0;
  for (const auto& t : replicas_[0]) {
    for (const auto& sz : t.sizes()) {
      metadata_accessor[i++] = sz;
    }
    for (const auto& str : t.strides()) {
      metadata_accessor[i++] = str;
    }
  }

  auto metadata_dev = metadata.clone().to(replicas_[0][0].device());
  std::vector<at::Tensor> vec{metadata_dev};
  process_group_->broadcast(vec)->wait();

  // Technically, process 0 doesn't need to double-check metadata, because it
  // was the source.  But no harm keeping work aligned.
  auto control = at::empty({static_cast<long>(i)}, options);
  control.copy_(metadata_dev, /*non_blocking=*/false);
  auto control_accessor = control.accessor<int64_t, 1>();
  i = 0;
  for (size_t p = 0; p < replicas_[0].size(); p++) {
    const auto& t = replicas_[0][p];
    // I'd like to include which process we are in the message,
    // but ProcessGroup::getRank is not public!
    for (const auto& sz : t.sizes()) {
      TORCH_CHECK(
          sz == control_accessor[i++],
          ""replicas[0]["",
          p,
          ""] in this process""
          "" with sizes "",
          t.sizes(),
          "" appears not to match sizes of the same param in process 0."");
    }
    for (const auto& str : t.strides()) {
      TORCH_CHECK(
          str == control_accessor[i++],
          ""replicas[0]["",
          p,
          ""] in this process""
          "" with strides "",
          t.strides(),
          "" appears not to match strides of the same param in process 0."");
    }
  }
}

void Reducer::check_grad_layout(
const at::Tensor& grad,
const at::Tensor& bucket_view) {
","}
}
void Reducer::check_grad_layout(
const at::Tensor& grad,
const at::Tensor& bucket_view) {
"
330,"// Parameters may have been unused in forward pass, or not all outputs
// were used in producing loss.
kBaseErrorMsg += ""You can enable unused parameter detection by passing the ""
      ""keyword argument `find_unused_parameters=True` to ""
      ""`torch.nn.parallel.DistributedDataParallel`, and by \n"";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
} else {
","// Parameters may have been unused in forward pass, or not all outputs
// were used in producing loss.
kBaseErrorMsg += ""You can enable unused parameter detection by passing the ""
          ""keyword argument `find_unused_parameters=True` to ""
          ""`torch.nn.parallel.DistributedDataParallel`, and by \n"";
kBaseErrorMsg += kOutputsNotUsedInLossErrorMsg;
kBaseErrorMsg += kDDPBugErrorMsg;
} else {
"
331,"const TensorOptions& options) {
const auto steps_ = steps.value_or(100);
TORCH_CHECK(steps_ >= 0, ""number of steps must be non-negative"");
  Tensor result = at::empty({steps_}, options);
return at::linspace_out(result, start, end, steps);
}
","const TensorOptions& options) {
const auto steps_ = steps.value_or(100);
TORCH_CHECK(steps_ >= 0, ""number of steps must be non-negative"");
  auto result_options = linspace_logspace_infer_options(start, end, options);
  Tensor result = at::empty({steps_}, result_options);
return at::linspace_out(result, start, end, steps);
}
"
332,"} break;
case WARN: {
drop(stack, 1);
        TORCH_WARN(pop(stack).toStringRef());
} break;
default:
","} break;
case WARN: {
drop(stack, 1);
        // Note: Please don't move the pop(stack) code below into the TORCH_WARN
        // macro since TORCH_WARN fails to evaluate its arguments when
        // STRIP_ERROR_MESSAGES is defined (which happens for production
        // mobile builds). This will cause the stack to be in an inconsistent
        // state. It has previously resulted in a SEV (S22350).
        auto sref = pop(stack).toStringRef();
        TORCH_WARN(sref);
+pc;
} break;
default:
"
333,"<< clientPipe.pipe_->getRemoteName() << "": ""
<< error.what();
}
          std::shared_ptr<AtomicJitFuture> futureResponseMessage;
          {
            std::lock_guard<std::mutex> lock(clientPipe.mutex_);
            auto pendingFutIt =
                clientPipe.pendingResponseMessage_.find(messageId);
            TORCH_INTERNAL_ASSERT(
                pendingFutIt != clientPipe.pendingResponseMessage_.end(),
                ""message ID "",
                messageId,
                "" is not recognized"");
            futureResponseMessage = std::move(pendingFutIt->second);
            clientPipe.pendingResponseMessage_.erase(pendingFutIt);
          }
          markFutureWithError(futureResponseMessage, error.what());
          removeFromTimeoutMap(messageId);
return;
}
","<< clientPipe.pipe_->getRemoteName() << "": ""
<< error.what();
}
          handleClientError(clientPipe, error);
return;
}
"
334,"name_base = name.substr(0, last_dot_pos);
}
}
std::string replacement_name;
do {
std::stringstream ss;
ss << name_base << ""."" << suffix++;
replacement_name = ss.str();
} while (names.count(replacement_name) > 0);
old_owner_of_name->second->setDebugName(replacement_name);
}
","name_base = name.substr(0, last_dot_pos);
}
}

    auto& names_suffixes = node()->owningGraph()->name_base_suffix_;
    auto it = names_suffixes.find(name_base);
    if (it != names_suffixes.end()) {
      suffix = std::max(suffix, it->second + 1);
    }

    // Verify that new name is not used and find next usable name in case
    // suffix is used.
std::string replacement_name;
do {
std::stringstream ss;
ss << name_base << ""."" << suffix++;
replacement_name = ss.str();
} while (names.count(replacement_name) > 0);

    names_suffixes[name_base] = suffix;

old_owner_of_name->second->setDebugName(replacement_name);
}
"
335,"def div_0_3(self: Tensor, other: number) -> Tensor:
if (self.is_floating_point() or isinstance(other, float)):
return self.true_divide(other)
  return self.floor_divide(other)
)SCRIPT"";
// Scalar x Scalar
","def div_0_3(self: Tensor, other: number) -> Tensor:
if (self.is_floating_point() or isinstance(other, float)):
return self.true_divide(other)
  return self.divide(other, rounding_mode='trunc')
)SCRIPT"";
// Scalar x Scalar
"
336,"int64_t istrideT,
int64_t istrideH,
int64_t istrideW) {
  int64_t d = 0;
  at::internal::lazy_init_num_threads();
#pragma omp parallel for private(d)
  for (d = 0; d < sizeD; d++) {
    /* loop over output */
    int64_t ot, oh, ow;
    for (ot = 0; ot < osizeT; ot++) {
      int istartT = start_index(ot, osizeT, isizeT);
      int iendT = end_index(ot, osizeT, isizeT);
      int kT = iendT - istartT;

      for (oh = 0; oh < osizeH; oh++) {
        int istartH = start_index(oh, osizeH, isizeH);
        int iendH = end_index(oh, osizeH, isizeH);
        int kH = iendH - istartH;

        for (ow = 0; ow < osizeW; ow++) {
          int istartW = start_index(ow, osizeW, isizeW);
          int iendW = end_index(ow, osizeW, isizeW);
          int kW = iendW - istartW;

          /* local pointers */
          scalar_t* ip = input_p + d * istrideD + istartT * istrideT +
              istartH * istrideH + istartW * istrideW;
          scalar_t* op = output_p + d * osizeT * osizeH * osizeW +
              ot * osizeH * osizeW + oh * osizeW + ow;

          /* compute local average: */
          scalar_t sum = 0;
          int it, ih, iw;
          for (it = 0; it < kT; it++) {
            for (ih = 0; ih < kH; ih++) {
              for (iw = 0; iw < kW; iw++) {
                scalar_t val =
                    *(ip + it * istrideT + ih * istrideH + iw * istrideW);
                sum += val;
}
}
          }
          /* set output to local average */
          *op = sum / kT / kH / kW;
}
}
}
  }
}
void adaptive_avg_pool3d_out_cpu_template(
","int64_t istrideT,
int64_t istrideH,
int64_t istrideW) {
  at::parallel_for(0, sizeD, 1, [&](int64_t start, int64_t end) {
    for (int64_t d = start; d < end; d++) {
      /* loop over output */
      for (int64_t ot = 0; ot < osizeT; ot++) {
        int istartT = start_index(ot, osizeT, isizeT);
        int iendT = end_index(ot, osizeT, isizeT);
        int kT = iendT - istartT;

        for (int64_t oh = 0; oh < osizeH; oh++) {
          int istartH = start_index(oh, osizeH, isizeH);
          int iendH = end_index(oh, osizeH, isizeH);
          int kH = iendH - istartH;

          for (int64_t ow = 0; ow < osizeW; ow++) {
            int istartW = start_index(ow, osizeW, isizeW);
            int iendW = end_index(ow, osizeW, isizeW);
            int kW = iendW - istartW;

            /* local pointers */
            scalar_t* ip = input_p + d * istrideD + istartT * istrideT +
                istartH * istrideH + istartW * istrideW;
            scalar_t* op = output_p + d * osizeT * osizeH * osizeW +
                ot * osizeH * osizeW + oh * osizeW + ow;

            /* compute local average: */
            scalar_t sum = 0;
            for (int it = 0; it < kT; it++) {
              for (int ih = 0; ih < kH; ih++) {
                for (int iw = 0; iw < kW; iw++) {
                  scalar_t val =
                      *(ip + it * istrideT + ih * istrideH + iw * istrideW);
                  sum += val;
                }
}
}
            /* set output to local average */
            *op = sum / kT / kH / kW;
          }
}
}
}
  });
}
void adaptive_avg_pool3d_out_cpu_template(
"
337,"}
}
catch (const std::exception& e) {
    LOG(WARNING)
        << ""Vulkan: Command pool destructor raised an exception!  Error: ""
        << e.what();
}
catch (...) {
    LOG(WARNING)
        << ""Vulkan: Command pool destructor raised an unknown exception!"";
}
}
","}
}
catch (const std::exception& e) {
    TORCH_WARN(
        ""Vulkan: Command pool destructor raised an exception! Error: "",
        e.what());
}
catch (...) {
    TORCH_WARN(
        ""Vulkan: Command pool destructor raised an exception! ""
        ""Error: Unknown"");
}
}
"
338,"}
Context* context() {
  Context* const context = initialize();
  TORCH_CHECK(context, ""Vulkan: Backend not available on this platform!"");
  return context;
}
Descriptor::Set dispatch_prologue(
","}
Context* context() {
  static const std::unique_ptr<Context> context([]() -> Context* {
    try {
      const Adapter adapter = runtime()->select([](const Adapter& adapter) {
        // Select the first adapter.
        return true;
      });

      return new Context(adapter);
    }
    catch (const std::exception& e) {
      TORCH_WARN(""Vulkan: Failed to initialize context! Error: "", e.what());
    }
    catch (...) {
      TORCH_WARN(""Vulkan: Failed to initialize context! Error: Unknown"");
    }

    return nullptr;
  }());

  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(
      context,
      ""Invalid Vulkan context!"");
  return context.get();
}
Descriptor::Set dispatch_prologue(
"
339,"function->set_register_size(register_size);
mcu.register_function(std::move(function));
}
}
// The deserializer class which loads the bytecode package from bc files.
class BytecodeDeserializer final {
 public:
  explicit BytecodeDeserializer(std::unique_ptr<PyTorchStreamReader> reader);
  mobile::Module deserialize(c10::optional<at::Device> device);
  mobile::Module deserialize(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);
  std::unordered_map<std::string, std::string> deserializeMetadata(
      c10::optional<at::Device> device);
  void deserialize_only_extra(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);

 private:
  c10::IValue readArchive(
      const std::string& archive_name,
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::unordered_map<std::string, std::string> readMobileMetadata(
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::shared_ptr<CompilationUnit> compilation_unit_;
  std::unordered_set<std::string> imported_libs_;
  std::unique_ptr<PyTorchStreamReader> reader_;
  c10::optional<at::Device> device_;
};

BytecodeDeserializer::BytecodeDeserializer(
    std::unique_ptr<PyTorchStreamReader> reader)
    : compilation_unit_(std::make_shared<CompilationUnit>()),
      reader_(std::move(reader)) {}

std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
","function->set_register_size(register_size);
    // function schema
    if (schemaTable) { // (schema is optional for back compat)
      auto parseArgList = [this](const std::vector<IValue>& argTables) {
        std::vector<c10::Argument> args;
        for (auto&& argTable : argTables) {
          auto name =
              expect_field(argTable, ""name"", BYTECODE_INDEX_ARGUMENT_NAME)
                  .toStringRef();
          const auto& type = resolveTypeName(
              (expect_field(argTable, ""type"", BYTECODE_INDEX_ARGUMENT_TYPE))
                  .toStringRef());
          auto default_value = expect_field(
                                   argTable,
                                   ""default_value"",
                                   BYTECODE_INDEX_ARGUMENT_DEFAULT_VALUE)
                                   .toIValue();
          auto arg =
              c10::Argument(name, type, c10::nullopt /*N*/, default_value);
          args.emplace_back(std::move(arg));
        }
        return args;
      };
      const auto& arg_list =
          expect_field(
              *schemaTable, ""arguments"", BYTECODE_INDEX_SCHEMA_ARGUMENTS)
              .toTuple()
              ->elements();
      const auto& ret_list =
          expect_field(*schemaTable, ""returns"", BYTECODE_INDEX_SCHEMA_RETURNS)
              .toTuple()
              ->elements();
      c10::FunctionSchema schema(
          function_name,
          """" /*overload_name*/,
          parseArgList(arg_list),
          parseArgList(ret_list),
          false /*is_varargs*/,
          false /*is_varret*/);
      function->setSchema(std::move(schema));
    }

mcu.register_function(std::move(function));
}
}
std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
"
340,"return !SRViewOperatorRegistry()->Has(op_name);
}
bool canReuseInputs(Node* n) {
auto op_name = std::string(n->kind().toQualString());
if (SROperatorRegistry()->Has(op_name)) {
","return !SRViewOperatorRegistry()->Has(op_name);
}
bool isViewOp(Node* n) {
  auto op_name = std::string(n->kind().toQualString());
  return SRViewOperatorRegistry()->Has(op_name);
}

bool canReuseInputs(Node* n) {
auto op_name = std::string(n->kind().toQualString());
if (SROperatorRegistry()->Has(op_name)) {
"
341,"write_opt(SS, onesidedOpt) << "", length=""; \
write_opt(SS, lengthOpt) << "", return_complex="" << return_complex << "") ""
// default_init hop_length and win_length
const auto hop_length = hop_lengthOpt.value_or(n_fft >> 2);
const auto win_length = win_lengthOpt.value_or(n_fft);
","write_opt(SS, onesidedOpt) << "", length=""; \
write_opt(SS, lengthOpt) << "", return_complex="" << return_complex << "") ""
  TORCH_CHECK(!window.defined() || window.device() == self.device(),
              ""istft input and window must be on the same device but got self on "",
              self.device(), "" and window on "", window.device())

// default_init hop_length and win_length
const auto hop_length = hop_lengthOpt.value_or(n_fft >> 2);
const auto win_length = win_lengthOpt.value_or(n_fft);
"
342,"const bool canAvoidTensorAccessor = mat1_strides[0] == mat1_sizes[1] * mat1_sizes[2] &&
mat2_strides[0] == mat2_sizes[1] * mat2_sizes[2];
  scalar_t* const res_data = static_cast<scalar_t*>(res.data_ptr());
if (batch_size == 1) {
const scalar_t* A;
const scalar_t* B;
if (canAvoidTensorAccessor) {
      scalar_t* mat1_data = static_cast<scalar_t*>(mat1.data_ptr());
      scalar_t* mat2_data = static_cast<scalar_t*>(mat2.data_ptr());
A = mat1_data;
B = mat2_data;
} else {
","const bool canAvoidTensorAccessor = mat1_strides[0] == mat1_sizes[1] * mat1_sizes[2] &&
mat2_strides[0] == mat2_sizes[1] * mat2_sizes[2];
  scalar_t* const res_data = res.data_ptr<scalar_t>();
if (batch_size == 1) {
const scalar_t* A;
const scalar_t* B;
if (canAvoidTensorAccessor) {
      scalar_t* mat1_data = mat1.data_ptr<scalar_t>();
      scalar_t* mat2_data = mat2.data_ptr<scalar_t>();
A = mat1_data;
B = mat2_data;
} else {
"
343,"#include <ATen/core/Vitals.h>
#include <cstdlib>
TorchVitalAttr& TorchVital::create(const std::string& attr) {
if (!torchVitalEnabled()) {
static TorchVitalAttr disabled;
","#include <ATen/core/Vitals.h>
#include <cstdlib>
namespace at {
namespace vitals {

TorchVitalAttr& TorchVital::create(const std::string& attr) {
if (!torchVitalEnabled()) {
static TorchVitalAttr disabled;
"
344,"return deserializer.deserialize(device, extra_files);
}
Module import_ir_module(
std::shared_ptr<CompilationUnit> cu,
const std::string& filename,
","return deserializer.deserialize(device, extra_files);
}
Module import_ir_module(
    std::shared_ptr<CompilationUnit> cu,
    const std::string& filename,
    c10::optional<at::Device> device) {
  ExtraFilesMap extra_files;
  return import_ir_module(std::move(cu), filename, device, extra_files);
}

Module import_ir_module(
std::shared_ptr<CompilationUnit> cu,
const std::string& filename,
"
345,"using at::IntArrayRef;
using at::TensorList;
bool isDefined(const c10::optional<Tensor>& t) {
return t.has_value() && t->defined();
}
","using at::IntArrayRef;
using at::TensorList;
const char* kCudnnDoubleBackwardMsg = ""Double backwards is not supported for CuDNN RNNs due to limitations in the CuDNN API. To run double backwards, please disable the CuDNN backend temporarily while running the forward pass of your RNN. For example: \nwith torch.backends.cudnn.flags(enabled=False):\n    output = model(inputs)"";


bool isDefined(const c10::optional<Tensor>& t) {
return t.has_value() && t->defined();
}
"
346,"odesc.desc(), grad_output->data_ptr(),
nullptr, nullptr,
idesc.desc(), grad_input_t.data_ptr(),
    wdesc.desc(), weight->data_ptr(),
nullptr,
grad_weight_t.data_ptr(),
grad_bias_t.data_ptr(),
","odesc.desc(), grad_output->data_ptr(),
nullptr, nullptr,
idesc.desc(), grad_input_t.data_ptr(),
    wdesc.desc(), weight->data_ptr(),
nullptr,
grad_weight_t.data_ptr(),
grad_bias_t.data_ptr(),
"
347,"#include <torch/csrc/jit/passes/fold_conv_bn.h>
#include <torch/csrc/jit/passes/freeze_module.h>
#include <torch/csrc/jit/passes/frozen_conv_folding.h>
#include <torch/csrc/jit/passes/fuse_linear.h>
#include <torch/csrc/jit/passes/fuse_relu.h>
#include <torch/csrc/jit/passes/graph_fuser.h>
","#include <torch/csrc/jit/passes/fold_conv_bn.h>
#include <torch/csrc/jit/passes/freeze_module.h>
#include <torch/csrc/jit/passes/frozen_conv_folding.h>
#include <torch/csrc/jit/passes/frozen_graph_optimizations.h>
#include <torch/csrc/jit/passes/fuse_linear.h>
#include <torch/csrc/jit/passes/fuse_relu.h>
#include <torch/csrc/jit/passes/graph_fuser.h>
"
348,"#include <torch/csrc/jit/codegen/cuda/index_compute.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
","#include <torch/csrc/jit/codegen/cuda/index_compute.h>

#include <c10/util/Exception.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
"
349,"#include <torch/csrc/jit/codegen/cuda/lower_utils.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/ir_iostream.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
","#include <torch/csrc/jit/codegen/cuda/lower_utils.h>

#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/ir_iostream.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
"
350,"#include <torch/csrc/jit/codegen/cuda/transform_replay.h>
#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/fusion.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
","#include <torch/csrc/jit/codegen/cuda/transform_replay.h>

#include <torch/csrc/jit/codegen/cuda/arith.h>
#include <torch/csrc/jit/codegen/cuda/fusion.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
"
351,"#include <torch/csrc/jit/frontend/script_type_parser.h>
#include <torch/csrc/jit/frontend/parser.h>
#include <torch/csrc/jit/ir/ir.h>
#include <torch/custom_class.h>
","#include <torch/csrc/jit/frontend/script_type_parser.h>

#include <torch/csrc/jit/frontend/parser.h>
#include <torch/csrc/jit/ir/ir.h>
#include <torch/custom_class.h>
"
352,"#include <torch/csrc/jit/passes/bailout_graph.h>
#include <ATen/core/function.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir_views.h>
","#include <torch/csrc/jit/passes/bailout_graph.h>

#include <ATen/core/function.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir_views.h>
"
353,"std::vector<int64_t> wt_sizes = unpacked_weight.sizes().vec();
if (unpacked_weight.ndimension() == 4) {
unpacked_weight.permute({0, 2, 3, 1});
      wt_sizes = {unpacked_weight.size(0),
                  unpacked_weight.size(2),
                  unpacked_weight.size(3),
                  unpacked_weight.size(1)};
}
// Remove packed_params
","std::vector<int64_t> wt_sizes = unpacked_weight.sizes().vec();
if (unpacked_weight.ndimension() == 4) {
unpacked_weight.permute({0, 2, 3, 1});
      wt_sizes = {
          unpacked_weight.size(0),
          unpacked_weight.size(2),
          unpacked_weight.size(3),
          unpacked_weight.size(1)};
}
// Remove packed_params
"
354,"std::tuple<c10::QScheme, QParamVector> _per_tensor_asym_qparam =
std::make_tuple(
c10::kPerTensorAffine,
        QParamVector({std::make_pair("".scale"", IValue(_asym_scale)),
                      std::make_pair("".zero_point"", IValue(_asym_zero_point)),
                      std::make_pair("".scalar_type"", IValue(c10::kQUInt8))}));
// quantization parrameters for ops with range -1 to 1
// for example: aten/src/ATen/native/quantized/cpu/qtanh.cpp
std::tuple<c10::QScheme, QParamVector> _per_tensor_sym_qparam = std::make_tuple(
c10::kPerTensorAffine,
    QParamVector({std::make_pair("".scale"", IValue(_sym_scale)),
                  std::make_pair("".zero_point"", IValue(_sym_zero_point)),
                  std::make_pair("".scalar_type"", IValue(c10::kQUInt8))}));
// Map from aten op symbol to the quantization parameters
// for the ops with fixed quantization parameters
","std::tuple<c10::QScheme, QParamVector> _per_tensor_asym_qparam =
std::make_tuple(
c10::kPerTensorAffine,
        QParamVector(
            {std::make_pair("".scale"", IValue(_asym_scale)),
             std::make_pair("".zero_point"", IValue(_asym_zero_point)),
             std::make_pair("".scalar_type"", IValue(c10::kQUInt8))}));
// quantization parrameters for ops with range -1 to 1
// for example: aten/src/ATen/native/quantized/cpu/qtanh.cpp
std::tuple<c10::QScheme, QParamVector> _per_tensor_sym_qparam = std::make_tuple(
c10::kPerTensorAffine,
    QParamVector(
        {std::make_pair("".scale"", IValue(_sym_scale)),
         std::make_pair("".zero_point"", IValue(_sym_zero_point)),
         std::make_pair("".scalar_type"", IValue(c10::kQUInt8))}));
// Map from aten op symbol to the quantization parameters
// for the ops with fixed quantization parameters
"
355,"#include <torch/csrc/jit/runtime/symbolic_script.h>
#include <torch/csrc/jit/frontend/ir_emitter.h>
#include <torch/csrc/jit/runtime/operator.h>
","#include <torch/csrc/jit/runtime/symbolic_script.h>

#include <torch/csrc/jit/frontend/ir_emitter.h>
#include <torch/csrc/jit/runtime/operator.h>
"
356,"#include <ATen/ATen.h>
#include <ATen/NativeFunctions.h>
namespace at {
namespace native {
","#include <ATen/ATen.h>
#include <ATen/NativeFunctions.h>
#include <ATen/ScalarOps.h>
namespace at {
namespace native {
"
357,"function->set_register_size(register_size);
    // function schema
    if (schemaTable) { // (schema is optional for back compat)
      auto parseArgList = [this](const std::vector<IValue>& argTables) {
        std::vector<c10::Argument> args;
        for (auto&& argTable : argTables) {
          auto name =
              expect_field(argTable, ""name"", BYTECODE_INDEX_ARGUMENT_NAME)
                  .toStringRef();
          const auto& type = resolveTypeName(
              (expect_field(argTable, ""type"", BYTECODE_INDEX_ARGUMENT_TYPE))
                  .toStringRef());
          auto default_value = expect_field(
                                   argTable,
                                   ""default_value"",
                                   BYTECODE_INDEX_ARGUMENT_DEFAULT_VALUE)
                                   .toIValue();
          auto arg =
              c10::Argument(name, type, c10::nullopt /*N*/, default_value);
          args.emplace_back(std::move(arg));
        }
        return args;
      };
      const auto& arg_list =
          expect_field(
              *schemaTable, ""arguments"", BYTECODE_INDEX_SCHEMA_ARGUMENTS)
              .toTuple()
              ->elements();
      const auto& ret_list =
          expect_field(*schemaTable, ""returns"", BYTECODE_INDEX_SCHEMA_RETURNS)
              .toTuple()
              ->elements();
      c10::FunctionSchema schema(
          function_name,
          """" /*overload_name*/,
          parseArgList(arg_list),
          parseArgList(ret_list),
          false /*is_varargs*/,
          false /*is_varret*/);
      function->setSchema(std::move(schema));
    }

mcu.register_function(std::move(function));
}
}
std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
","function->set_register_size(register_size);
mcu.register_function(std::move(function));
}
}
// The deserializer class which loads the bytecode package from bc files.
class BytecodeDeserializer final {
 public:
  explicit BytecodeDeserializer(std::unique_ptr<PyTorchStreamReader> reader);
  mobile::Module deserialize(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);
  std::unordered_map<std::string, std::string> deserializeMetadata(
      c10::optional<at::Device> device);

 private:
  c10::IValue readArchive(
      const std::string& archive_name,
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::unordered_map<std::string, std::string> readMobileMetadata(
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::shared_ptr<CompilationUnit> compilation_unit_;
  std::unordered_set<std::string> imported_libs_;
  std::unique_ptr<PyTorchStreamReader> reader_;
  c10::optional<at::Device> device_;
};

BytecodeDeserializer::BytecodeDeserializer(
    std::unique_ptr<PyTorchStreamReader> reader)
    : compilation_unit_(std::make_shared<CompilationUnit>()),
      reader_(std::move(reader)) {}

std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
"
358,"}
}
c10::IValue Method::operator()(std::vector<IValue> stack) const {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
","}
}
c10::IValue Method::operator()(std::vector<IValue> stack) {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
"
359,"// register size
auto register_size = static_cast<int>(code.register_size());
  auto codeTable = Table({{""instructions"", Tup(instructions)},
                          {""operators"", Tup(operators)},
                          {""constants"", Tup(constants)},
                          {""types"", Tup(types)},
                          {""register_size"", register_size}});

  // schema
  const auto& schema = func.getSchema();
  TORCH_CHECK(
      schema.overload_name().empty(), // @TODO: is this check correct?
      ""Overloads are not supported in mobile modules."");
  TORCH_CHECK(
      !schema.is_vararg(), ""Python *args are not supported in mobile modules."");
  TORCH_CHECK(
      !schema.is_varret(),
      ""A variable number of return values is not supported in mobile modules."");
  auto makeArgTuple = [](const std::vector<Argument>& args) {
    std::vector<IValue> argTables;
    for (auto&& arg : args) {
      TORCH_CHECK(
          !arg.N(),
          ""Arguments with known list lengths are not supported in mobile modules."");
      TORCH_CHECK(
          !arg.kwarg_only(),
          ""Keyword-only arguments are not supported in mobile modules."");
      argTables.emplace_back(Table({
          {""name"", arg.name()},
          {""type"", arg.type()->annotation_str()},
          {""default_value"", arg.default_value()},
      }));
    }
    return Tup(argTables);
  };
  auto schemaTable = Table({
      {""arguments"", makeArgTuple(schema.arguments())},
      {""returns"", makeArgTuple(schema.returns())},
  });

  // function tuple
  auto bytecode_vals =
      Tup({func.qualname().qualifiedName(), codeTable, schemaTable});
c10::optional<IValue> debug_info_vals;
if (save_mobile_debug_info) {
","// register size
auto register_size = static_cast<int>(code.register_size());
  auto table = Table({{""instructions"", Tup(instructions)},
                      {""operators"", Tup(operators)},
                      {""constants"", Tup(constants)},
                      {""types"", Tup(types)},
                      {""register_size"", register_size}});
  auto bytecode_vals = Tup({func.qualname().qualifiedName(), table});
c10::optional<IValue> debug_info_vals;
if (save_mobile_debug_info) {
"
360,"auto win_length = win_lengthOpt.value_or(n_fft);
const bool return_complex = return_complexOpt.value_or(
self.is_complex() || (window.defined() && window.is_complex()));
  if (!return_complexOpt && !return_complex) {
    TORCH_WARN_ONCE(""stft will require the return_complex parameter be explicitly ""
                    "" specified in a future PyTorch release. Use return_complex=False ""
                    "" to preserve the current behavior or return_complex=True to return ""
                    "" a complex output."");
}
if (!at::isFloatingType(self.scalar_type()) && !at::isComplexType(self.scalar_type())) {
","auto win_length = win_lengthOpt.value_or(n_fft);
const bool return_complex = return_complexOpt.value_or(
self.is_complex() || (window.defined() && window.is_complex()));
  if (!return_complex) {
    TORCH_CHECK(return_complexOpt.has_value(),
        ""stft requires the return_complex parameter be given for real inputs.""
        ""You should pass return_complex=True to opt-in to complex dtype returns ""
        ""(which will be required in a future pytorch release). ""
      );

    TORCH_WARN_ONCE(
        ""stft with return_complex=False is deprecated. In a future pytorch ""
        ""release, stft will return complex tensors for all inputs, and ""
        ""return_complex=False will raise an error.\n""
        ""Note: you can still call torch.view_as_real on the complex output to ""
        ""recover the old return format."");
}
if (!at::isFloatingType(self.scalar_type()) && !at::isComplexType(self.scalar_type())) {
"
361,"}
const auto prop = at::cuda::getCurrentDeviceProperties();
  int nvrtc_major, nvrtc_minor;
  AT_CUDA_NVRTC_CHECK(
      at::globalContext().getNVRTC().nvrtcVersion(&nvrtc_major, &nvrtc_minor));

  // Short-circuits if NVRTC version too low
  TORCH_INTERNAL_ASSERT(nvrtc_major >= 6);
  // Major and minor is determined by device properties and
  // possibly ""downcompiled"" to a lower (compatible) compute architecture
  // based on the NVRTC version
  const int major = prop->major;
  const int minor = prop->minor;
  nvrtcProgram program;
{
FUSER_PERF_SCOPE(""nvrtcCreateProgram"");
","}
const auto prop = at::cuda::getCurrentDeviceProperties();

  int major = 0, minor = 0;
  getMajorMinor(prop, major, minor);

  nvrtcProgram program; // NOLINT(cppcoreguidelines-init-variables)
{
FUSER_PERF_SCOPE(""nvrtcCreateProgram"");
"
362,"OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  TORCH_CHECK(op.operatorIterator_->def_count == 0, ""Tried to register an operator ("", schema, "") with the same name and overload name multiple times."",
                                                    "" Each overload's schema should only be registered with a single call to def()."",
                                                    "" Duplicate registration: "", debug, "". Original registration: "", op.operatorIterator_->op.debug());
op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
listeners_->callOnOperatorRegistered(op);
// NB: do not increment the counts until AFTER error checking
","OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  if (op.operatorIterator_->def_count == 0) {
    // NB: registerSchema is not idempotent! Only do it once!
op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
listeners_->callOnOperatorRegistered(op);
  } else {
    checkSchemaCompatibility(op, schema, debug);
  }
// NB: do not increment the counts until AFTER error checking
+op.operatorIterator_->def_count;
"
363,"});
}
void Dispatcher::checkSchemaCompatibility(const OperatorHandle& op, const FunctionSchema& schema, const std::string& debug) {
  TORCH_CHECK(op.schema() == schema, ""Tried to register multiple operators with the same name and the same overload name but different schemas: "", schema, "" ("", debug, "") vs "", op.schema(), "" ("", op.debug(), "")"");
  if (schema.isDefaultAliasAnalysisKind()) {
    // [BACKWARDS COMPAT] If the *new* schema is the default alias analysis
    // kind, for BC, we will accept it.  If we don't accept it, most extensions
    // that override existing operators will stop working (as they generally did
    // not specify alias information).
  } else if (op.schema().isDefaultAliasAnalysisKind()) {
    // [BACKWARDS COMPAT] If you POST-FACTO specify a non-default alias analysis
    // kind after we already have a schema for a function, bong it in for BC
    // reasons.
    op.operatorIterator_->op.updateSchemaAliasAnalysis(schema.aliasAnalysis());
  } else {
    TORCH_CHECK(op.schema().aliasAnalysis() == schema.aliasAnalysis(),
      ""Tried to define the schema for "", toString(op.operator_name()), "" with different alias analysis kinds: "",
      toString(op.schema().aliasAnalysis()), "" ("", op.debug(), "") vs "", toString(schema.aliasAnalysis()), "" ("", debug, "")"");
  }
}

void Dispatcher::deregisterDef_(const OperatorHandle& op, const OperatorName& op_name) {
// we need a lock to avoid concurrent writes
std::lock_guard<std::mutex> lock(mutex_);
","});
}
void Dispatcher::deregisterDef_(const OperatorHandle& op, const OperatorName& op_name) {
// we need a lock to avoid concurrent writes
std::lock_guard<std::mutex> lock(mutex_);
"
364,"the workers using the store.
.. warning::
    The ``num_keys`` API is only supported by the :class:`~torch.distributed.TCPStore`. Using this API
    with the :class:`~torch.distributed.FileStore` or :class:`~torch.distributed.HashStore` will result in an exception.
Returns:
The number of keys present in the store.
Example::
>>> import torch.distributed as dist
>>> store = dist.TCPStore(""127.0.0.1"", 0, true, timedelta(seconds=30))
>>> store.set(""first_key"", ""first_value"")
>>> # This should return 2
","the workers using the store.
.. warning::
    The ``num_keys`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API
    with the :class:`~torch.distributed.FileStore` will result in an exception.
Returns:
The number of keys present in the store.
Example::
>>> import torch.distributed as dist
    >>> # Using TCPStore as an example, HashStore can also be used
>>> store = dist.TCPStore(""127.0.0.1"", 0, true, timedelta(seconds=30))
>>> store.set(""first_key"", ""first_value"")
>>> # This should return 2
"
365,"// that would also invalidate the old TypedOperatorHandles.
if (cpp_signature.has_value()) {
if (cpp_signature_.has_value()) {
      TORCH_INTERNAL_ASSERT(*cpp_signature == cpp_signature_->signature,
        ""Tried to register a kernel ("", debug, "") for operator "", name_,"" ("",
        (this->schema_.has_value() ? this->schema_->debug : ""no debug info""),
        "") for dispatch key "", toString(dispatch_key), "", but the C++ function signature "",
        cpp_signature->name(), "" mismatched with a previous kernel ("", cpp_signature_->debug,
        "") that had the signature "", cpp_signature_->signature.name()
);
} else {
      cpp_signature_ = CppSignatureWithDebug { *cpp_signature, debug };
}
}
","// that would also invalidate the old TypedOperatorHandles.
if (cpp_signature.has_value()) {
if (cpp_signature_.has_value()) {
      TORCH_CHECK(*cpp_signature == cpp_signature_->signature,
        ""\nMismatch in kernel C++ signatures\n"",
        ""  operator: "", (this->schema_.has_value() ? toString(this->schema_->schema) : toString(name_)), ""\n"",
        ""    "", (this->schema_.has_value() ? this->schema_->debug : ""no debug info""), ""\n"",
        ""  kernel 1: "", cpp_signature_->signature.name(), ""\n"",
        ""    dispatch key: "", toString(cpp_signature_->dispatch_key), ""\n"",
        ""    "", cpp_signature_->debug, ""\n"",
        ""  kernel 2: "", cpp_signature->name(), ""\n"",
        ""    dispatch key: "", toString(dispatch_key), ""\n"",
        ""    "", debug, ""\n""
);
} else {
      cpp_signature_ = CppSignatureWithDebug { *cpp_signature, debug, dispatch_key };
}
}
"
366,"}
void logical_and_kernel(TensorIterator& iter) {
  // We use if-else here specifically for bool instead of using iter.common_dtype() like the CUDA implementation because
  // common_dtype() is unavailable for bfloat16.
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""logical_and_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return a && b;
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""logical_and_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(a && b);
","}
void logical_and_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""logical_and_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return a && b;
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), ""logical_and_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(a && b);
"
367,"cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return a >> b;
      });
});
}
}
void lt_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""lt_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a < b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), ""lt_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return a >> b;
        });
});
}
}
void lt_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""lt_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a < b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""lt_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
368,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.lt(b);
});
    });
}
}
void le_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""le_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a <= b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""le_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.lt(b);
});
      });
}
}
void le_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""le_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a <= b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), ""le_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
369,"return trivial_return.value();
}
  // NOTE: CPU performance significantly regressed when attempting to port to ATen,
  //   so this dispatches differently based on device type.
//   See https://github.com/pytorch/pytorch/pull/43858.
if (self.device().type() == kCPU) {
return at::_var(self, unbiased);
","return trivial_return.value();
}
  // NOTE: CPU performance significantly regressed when attempting to port to ATen,
  //   so this dispatches differently based on device type.
//   See https://github.com/pytorch/pytorch/pull/43858.
if (self.device().type() == kCPU) {
return at::_var(self, unbiased);
"
370,"auto device2 = x2.device().type();
TORCH_CHECK(device2 == kCPU || device2 == kCUDA, ""_cdist_backward only supports CPU and CUDA devices, X2 got: "", device2);
IntArrayRef batch_tensor1(x1.sizes().data(), std::max<int64_t>(x1.dim() - 2, 0));
  int batch_product = std::accumulate(batch_tensor1.begin(), batch_tensor1.end(), 1, std::multiplies<int64_t>());
  Tensor grad_x1 = at::empty_like(x1, x1.options(), LEGACY_CONTIGUOUS_MEMORY_FORMAT).view({batch_product, n, m});
cdist_backward_stub(device1, grad_x1, grad, x1, x2, p, cdist);
return grad_x1;
}
","auto device2 = x2.device().type();
TORCH_CHECK(device2 == kCPU || device2 == kCUDA, ""_cdist_backward only supports CPU and CUDA devices, X2 got: "", device2);
IntArrayRef batch_tensor1(x1.sizes().data(), std::max<int64_t>(x1.dim() - 2, 0));
  const int64_t batch_product = prod_intlist(batch_tensor1);
  Tensor grad_x1 =
      at::empty_like(x1, x1.options(), LEGACY_CONTIGUOUS_MEMORY_FORMAT)
          .view({batch_product, n, m});
cdist_backward_stub(device1, grad_x1, grad, x1, x2, p, cdist);
return grad_x1;
}
"
371,"VkDeviceSize buffer_size_for_sizes(std::vector<int64_t> sizes) const {
const auto d = sizes.size();
    const auto numel = std::accumulate(
        std::begin(sizes), std::end(sizes), 1, std::multiplies<int64_t>());
VkDeviceSize bufferSize{sizeof(float) * numel};
// alignment to be able to copy between image and buffer
if (d == 4) {
","VkDeviceSize buffer_size_for_sizes(std::vector<int64_t> sizes) const {
const auto d = sizes.size();
    const auto numel = prod_intlist(sizes);
VkDeviceSize bufferSize{sizeof(float) * numel};
// alignment to be able to copy between image and buffer
if (d == 4) {
"
372,"#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <ATen/InferSize.h>
#include <ATen/native/vulkan/Vulkan.h>
#include <ATen/native/vulkan/VulkanCommon.h>
","#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <ATen/InferSize.h>
#include <ATen/Utils.h>
#include <ATen/native/vulkan/Vulkan.h>
#include <ATen/native/vulkan/VulkanCommon.h>
"
373,"#define TH_GENERIC_FILE ""TH/generic/THLapack.cpp""
#else

TH_EXTERNC void dgels_(char *trans, int *m, int *n, int *nrhs, double *a, int *lda, double *b, int *ldb, double *work, int *lwork, int *info);
TH_EXTERNC void sgels_(char *trans, int *m, int *n, int *nrhs, float *a, int *lda, float *b, int *ldb, float *work, int *lwork, int *info);
TH_EXTERNC void dgeev_(char *jobvl, char *jobvr, int *n, double *a, int *lda, double *wr, double *wi, double* vl, int *ldvl, double *vr, int *ldvr, double *work, int *lwork, int *info);
","#define TH_GENERIC_FILE ""TH/generic/THLapack.cpp""
#else
TH_EXTERNC void dgels_(char *trans, int *m, int *n, int *nrhs, double *a, int *lda, double *b, int *ldb, double *work, int *lwork, int *info);
TH_EXTERNC void sgels_(char *trans, int *m, int *n, int *nrhs, float *a, int *lda, float *b, int *ldb, float *work, int *lwork, int *info);
TH_EXTERNC void dgeev_(char *jobvl, char *jobvr, int *n, double *a, int *lda, double *wr, double *wi, double* vl, int *ldvl, double *vr, int *ldvr, double *work, int *lwork, int *info);
"
374,"m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_unpack(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_stride(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_padding(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_dilation(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_groups(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int""));
// conv_tranpsose
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose1d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor""));
","m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_unpack(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
  m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_output_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_stride(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_padding(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
  m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_output_padding(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_dilation(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_groups(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int""));
  m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv3d_transpose(__torch__.torch.classes.quantized.Conv3dPackedParamsBase packed_weights) -> int""));
// conv_tranpsose
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose1d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d(Tensor qx, __torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weight, float output_scale, int output_zero_point) -> Tensor""));
"
375,"m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_unpack(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
","m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_unpack(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> (Tensor unpacked_weights, Tensor? B_origin)""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_stride(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
  m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_output_padding(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_dilation(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int[]""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_groups(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
m.def(TORCH_SELECTIVE_SCHEMA(""quantized::conv_transpose2d_transpose(__torch__.torch.classes.quantized.Conv2dPackedParamsBase packed_weights) -> int""));
"
376,"//
void OperatorEntry::updateDispatchTableFull_(const c10::Dispatcher& dispatcher) {
// Note [Undefined in dispatchTable_]
// (1) it gives people place to specify functionality that should run when there are no dispatch keys,
  //     e.g., an empty TensorList argument
// (2) it would let us remove the explicit error checking code in the dispatch hotpath, and so when
//     no dispatch keys are available we just slide into the undefined handler which would then raise
  //     the error message./
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
updateDispatchTable_(dispatcher, static_cast<DispatchKey>(iter));
}
","//
void OperatorEntry::updateDispatchTableFull_(const c10::Dispatcher& dispatcher) {
// Note [Undefined in dispatchTable_]
  // DispatchKey Undefined is used in runtime:
// (1) it gives people place to specify functionality that should run when there are no dispatch keys,
  //     e.g., an op without Tensor inputs or empty TensorList arguments
// (2) it would let us remove the explicit error checking code in the dispatch hotpath, and so when
//     no dispatch keys are available we just slide into the undefined handler which would then raise
  //     the error message.
  // In the old world of catchAll, the only way to ""register"" a kernel to Undefined is by registering it to
  // catchAll. After catchAllKernel_ is removed, Undefined now can get a kernel from either DefaultBackend
  // or Math alias key so that we don't break the support. Ideally isIncludedInAlias(Undefined, Math)
  // should return true, it returns false because Undefined cannot be represented in a DispatchKeySet.
for (uint8_t iter = 0; iter != static_cast<uint8_t>(DispatchKey::NumDispatchKeys); ++iter) {
updateDispatchTable_(dispatcher, static_cast<DispatchKey>(iter));
}
"
377,"int64_t HxW,
int64_t group,
T eps,
    Tensor* Y,
    Tensor* mean,
    Tensor* rstd) {
TORCH_CHECK(X.numel() == N * C * HxW);
TORCH_CHECK(!gamma.defined() || gamma.numel() == C);
TORCH_CHECK(!beta.defined() || beta.numel() == C);
","int64_t HxW,
int64_t group,
T eps,
    Tensor& Y,
    Tensor& mean,
    Tensor& rstd) {
TORCH_CHECK(X.numel() == N * C * HxW);
TORCH_CHECK(!gamma.defined() || gamma.numel() == C);
TORCH_CHECK(!beta.defined() || beta.numel() == C);
"
378,"// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
// TODO: we can remove (2.4) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.

// 1. Operator registration
if (auto direct_registration = getKernelForDispatchKey(dispatch_key)) {
return {*direct_registration.value(), ""kernel""};
","// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
// TODO: we can remove (2.4) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.

// 1. Operator registration
if (auto direct_registration = getKernelForDispatchKey(dispatch_key)) {
return {*direct_registration.value(), ""kernel""};
"
379,"// non backend keys (e.g AutogradXXX, Batched etc) due to (2.1).
bool has_backend_kernel =
hasKernelForAnyDispatchKey(getBackendKeySetFromAutograd(dispatch_key).add(DispatchKey::DefaultBackend));

// 2.2. Use Math kernel if available. For autograd keys, we only use kernel from Math
//      when there's no direct registration to its corresponding backend key or DefaultBackend.
//      For AutogradOther, we return ambiguousAutogradOtherKernel_ if there's registration
","// non backend keys (e.g AutogradXXX, Batched etc) due to (2.1).
bool has_backend_kernel =
hasKernelForAnyDispatchKey(getBackendKeySetFromAutograd(dispatch_key).add(DispatchKey::DefaultBackend));

// 2.2. Use Math kernel if available. For autograd keys, we only use kernel from Math
//      when there's no direct registration to its corresponding backend key or DefaultBackend.
//      For AutogradOther, we return ambiguousAutogradOtherKernel_ if there's registration
"
380,"""failed to create QNNPACK TanH operator"");
qy = at::_empty_affine_quantized(
input_contig.sizes(),
    input.options(),
output_scale,
    output_zero_point);
const pytorch_qnnp_status setupStatus = pytorch_qnnp_setup_tanh_nc_q8(
tanh_op,
","""failed to create QNNPACK TanH operator"");
qy = at::_empty_affine_quantized(
input_contig.sizes(),
    at::device(kCPU).dtype(input_contig.dtype()),
output_scale,
    output_zero_point,
    input_contig.suggest_memory_format());
const pytorch_qnnp_status setupStatus = pytorch_qnnp_setup_tanh_nc_q8(
tanh_op,
"
381,"ncclComm->ncclCommAbort();
const auto& storeKey = getNcclAbortedCommStoreKey(
buildNcclUniqueIdStr(ncclComm->getNcclId()));
          store_->set(storeKey, {});
          LOG(INFO) << ""Wrote aborted communicator id to store: "" << storeKey;
}
throw std::runtime_error(""Operation timed out!"");
}
// Check for errors and throw appropriate exception.
","ncclComm->ncclCommAbort();
const auto& storeKey = getNcclAbortedCommStoreKey(
buildNcclUniqueIdStr(ncclComm->getNcclId()));
          auto rankStr = std::to_string(rank_);
          store_->set(
              storeKey,
              std::vector<uint8_t>(
                  reinterpret_cast<const uint8_t*>(rankStr.data()),
                  reinterpret_cast<const uint8_t*>(rankStr.data()) +
                      rankStr.size()));
          LOG(INFO) << ""[Rank "" << rank_
                    << ""] Wrote aborted communicator id to store: "" << storeKey;
}
        LOG(INFO) << ""[Rank "" << rank_
                  << ""] Caught collective operation timeout for work: ""
                  << (*this);
throw std::runtime_error(""Operation timed out!"");
}
// Check for errors and throw appropriate exception.
"
382,"if (asyncErrorHandling_) {
workCleanupThread_ = std::thread(&ProcessGroupNCCL::workCleanupLoop, this);
}
}
ProcessGroupNCCL::~ProcessGroupNCCL() {
","if (asyncErrorHandling_) {
workCleanupThread_ = std::thread(&ProcessGroupNCCL::workCleanupLoop, this);
}
  LOG(INFO) << ""[Rank "" << rank_
            << ""] ProcessGroupNCCL initialized with following options:""
            << ""\nNCCL_ASYNC_ERROR_HANDLING: "" << asyncErrorHandling_
            << ""\nNCCL_BLOCKING_WAIT: "" << blockingWait_
            << ""\nTIMEOUT(ms): "" << opTimeout_.count()
            << ""\nUSE_HIGH_PRIORITY_STREAM: "" << isHighPriorityStream_;
}
ProcessGroupNCCL::~ProcessGroupNCCL() {
"
383,"store_->wait(
{storeKey},
std::chrono::milliseconds(kWaitForAbortCommStoreKey));
            LOG(INFO) << ""Found key in store: "" << storeKey
<< "", aborting appropriate communicators"";
// Now abort the appropriate communicators.
","store_->wait(
{storeKey},
std::chrono::milliseconds(kWaitForAbortCommStoreKey));
            auto val = store_->get(storeKey);
            std::string rank(reinterpret_cast<char*>(val.data()), val.size());
            LOG(INFO) << ""[Rank "" << rank_
                      << ""] Found key in store: "" << storeKey
                      << "", from rank: "" << rank
<< "", aborting appropriate communicators"";
// Now abort the appropriate communicators.
"
384,"return ncclSuccess;
},
dstRank,
      NCCLCommType::SEND);
return ret;
}
","return ncclSuccess;
},
dstRank,
      OpType::SEND);
return ret;
}
"
385,"std::unordered_set<torch::autograd::Node*> seen;
std::vector<torch::autograd::Node*> queue;
  // Check that any prior reduction has finished.
  // The variable `require_finalize_` is true until all gradients
  // have been computed and reduction of all buckets has been kicked off.
  if (require_finalize_) {
    TORCH_CHECK(
        false,
        ""Expected to have finished reduction in the prior iteration before "",
        ""starting a new one. "",
        """",
        ""This error indicates that your module has parameters that were "",
        ""not used in producing loss. "",
        """",
        ""You can enable unused parameter detection by (1) passing the keyword ""
        ""argument `find_unused_parameters=True` to "",
        ""`torch.nn.parallel.DistributedDataParallel`; (2) making sure all "",
        ""`forward` function outputs participate in calculating loss. ""
        """",
        ""If you already have done the above two steps, then the distributed "",
        ""data parallel module wasn't able to locate the output tensors in the "",
        ""return value of your module's `forward` function. "",
        ""Please include the loss function and the structure of the return "",
        ""value of `forward` of your module when reporting this issue (e.g. "",
        ""list, dict, iterable)."");
  }

// Reset accounting.
expect_autograd_hooks_ = true;
next_bucket_ = 0;
","std::unordered_set<torch::autograd::Node*> seen;
std::vector<torch::autograd::Node*> queue;
// Reset accounting.
expect_autograd_hooks_ = true;
next_bucket_ = 0;
"
386,"}
ProcessGroupNCCL::WorkNCCL::WorkNCCL(const WorkNCCL& w)
    : Work(w.rank_, w.opType_),
      std::enable_shared_from_this<WorkNCCL>(w),
devices_(w.devices_),
cudaEvents_(w.cudaEvents_),
ncclComms_(w.ncclComms_),
","}
ProcessGroupNCCL::WorkNCCL::WorkNCCL(const WorkNCCL& w)
    : std::enable_shared_from_this<WorkNCCL>(w),
devices_(w.devices_),
cudaEvents_(w.cudaEvents_),
ncclComms_(w.ncclComms_),
"
387,"ncclComm->ncclCommAbort();
}
abortedComms_.emplace(commId);
            LOG(INFO) << ""[Rank "" << rank_
                      << ""] Aborted communicators for key in store: ""
                      << storeKey;
} catch (std::exception& e) {
VLOG(1) << ""Did not find key in store: "" << storeKey
<< "", error: "" << e.what();
","ncclComm->ncclCommAbort();
}
abortedComms_.emplace(commId);
            LOG(INFO) << ""Aborted communicators for key in store: "" << storeKey;
} catch (std::exception& e) {
VLOG(1) << ""Did not find key in store: "" << storeKey
<< "", error: "" << e.what();
"
388,"} // namespace
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork(
    std::vector<at::Device> devices,
    int rank,
    OpType opType) {
  return std::make_shared<ProcessGroupNCCL::WorkNCCL>(devices, rank, opType);
}
std::vector<at::Tensor> ProcessGroupNCCL::WorkNCCL::result() {
","} // namespace
std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork(
    std::vector<at::Device> devices) {
  return std::make_shared<ProcessGroupNCCL::WorkNCCL>(devices);
}
std::vector<at::Tensor> ProcessGroupNCCL::WorkNCCL::result() {
"
389,"root,
comm,
stream.stream());
      },
      OpType::BROADCAST);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
","root,
comm,
stream.stream());
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
"
390,"const int64_t ProcessGroupNCCL::kProcessGroupNCCLOpTimeoutMillis = 10 * 1000;
thread_local uint64_t ProcessGroupNCCL::ncclActiveGroupCounter_ = 0;
ProcessGroupNCCL::WorkNCCL::WorkNCCL(const std::vector<at::Device>& devices)
    : devices_(devices), workStartTime_(std::chrono::steady_clock::now()) {
// Creates the CUDA event wrappers
// Note: The actual events are lazily created when first recorded to with
// DEFAULT_FLAGS = cudaEventDisableTiming.
","const int64_t ProcessGroupNCCL::kProcessGroupNCCLOpTimeoutMillis = 10 * 1000;
thread_local uint64_t ProcessGroupNCCL::ncclActiveGroupCounter_ = 0;
std::ostream& operator<<(
    std::ostream& output,
    const ProcessGroupNCCL::WorkNCCL& workNCCL) {
  return output << ""WorkNCCL(""
                << ""OpType="" << opTypeToString(workNCCL.opType_)
                << "", TensorShape="" << (*workNCCL.outputs_)[0].sizes()
                << "", Timeout(ms)="" << workNCCL.opTimeout_.count() << "")"";
}

ProcessGroupNCCL::WorkNCCL::WorkNCCL(
    const std::vector<at::Device>& devices,
    int rank,
    OpType opType)
    : Work(rank, opType),
      devices_(devices),
      workStartTime_(std::chrono::steady_clock::now()) {
// Creates the CUDA event wrappers
// Note: The actual events are lazily created when first recorded to with
// DEFAULT_FLAGS = cudaEventDisableTiming.
"
391,"ncclComm->ncclCommAbort();
const auto& storeKey = getNcclAbortedCommStoreKey(
buildNcclUniqueIdStr(ncclComm->getNcclId()));
          store_->set(storeKey, {});
          LOG(INFO) << ""Wrote aborted communicator id to store: "" << storeKey;
}
throw std::runtime_error(""Operation timed out!"");
}
// Check for errors and throw appropriate exception.
","ncclComm->ncclCommAbort();
const auto& storeKey = getNcclAbortedCommStoreKey(
buildNcclUniqueIdStr(ncclComm->getNcclId()));
          auto rankStr = std::to_string(rank_);
          store_->set(
              storeKey,
              std::vector<uint8_t>(
                  reinterpret_cast<const uint8_t*>(rankStr.data()),
                  reinterpret_cast<const uint8_t*>(rankStr.data()) +
                      rankStr.size()));
          LOG(INFO) << ""[Rank "" << rank_
                    << ""] Wrote aborted communicator id to store: "" << storeKey;
}
        LOG(INFO) << ""[Rank "" << rank_
                  << ""] Caught collective operation timeout for work: ""
                  << (*this);
throw std::runtime_error(""Operation timed out!"");
}
// Check for errors and throw appropriate exception.
"
392,"for (const auto& abortedCommId : abortedCommIds) {
abortedComms_.emplace(abortedCommId);
const auto& storeKey = getNcclAbortedCommStoreKey(abortedCommId);
        store_->set(storeKey, {});
        LOG(INFO) << ""Watchdog wrote aborted communicator id to store: ""
<< storeKey;
}
","for (const auto& abortedCommId : abortedCommIds) {
abortedComms_.emplace(abortedCommId);
const auto& storeKey = getNcclAbortedCommStoreKey(abortedCommId);
        auto rankStr = std::to_string(rank_);
        store_->set(
            storeKey,
            std::vector<uint8_t>(
                reinterpret_cast<const uint8_t*>(rankStr.data()),
                reinterpret_cast<const uint8_t*>(rankStr.data()) +
                    rankStr.size()));
        LOG(INFO) << ""[Rank "" << rank_
                  << ""] Watchdog wrote aborted communicator id to store: ""
<< storeKey;
}
"
393,"ncclComm->ncclCommAbort();
}
abortedComms_.emplace(commId);
            LOG(INFO) << ""Aborted communicators for key in store: "" << storeKey;
} catch (std::exception& e) {
VLOG(1) << ""Did not find key in store: "" << storeKey
<< "", error: "" << e.what();
","ncclComm->ncclCommAbort();
}
abortedComms_.emplace(commId);
            LOG(INFO) << ""[Rank "" << rank_
                      << ""] Aborted communicators for key in store: ""
                      << storeKey;
} catch (std::exception& e) {
VLOG(1) << ""Did not find key in store: "" << storeKey
<< "", error: "" << e.what();
"
394,"return list.release();
}
PyObject *THPModule_isEnabledXNNPACK(PyObject * /* unused */)
{
if (at::globalContext().isXNNPACKAvailable()) Py_RETURN_TRUE;
else Py_RETURN_FALSE;
","return list.release();
}
PyObject *THPModule_isEnabledXNNPACK(PyObject *_unused, PyObject *noargs)
{
if (at::globalContext().isXNNPACKAvailable()) Py_RETURN_TRUE;
else Py_RETURN_FALSE;
"
395,"}
Tensor smooth_l1_loss(const Tensor& input, const Tensor& target, const int64_t reduction, double beta) {
  TORCH_CHECK(beta >= 0, ""smooth_l1_loss does not support negative values for beta."")
  if (beta == 0) {
return at::native::l1_loss(input, target, reduction);
  }
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
smooth_l1_stub(iter.device_type(), iter, beta);
","}
Tensor smooth_l1_loss(const Tensor& input, const Tensor& target, const int64_t reduction, double beta) {
  if (beta <= 0)
return at::native::l1_loss(input, target, reduction);
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
smooth_l1_stub(iter.device_type(), iter, beta);
"
396,"THCCachingAllocator caching_allocator;
// NB: I decided not to fold this into THCCachingAllocator, because the latter
// has a lot more methods and it wasn't altogether clear that they should
// actually be publicly exposed
","THCCachingAllocator caching_allocator;
// Returns whether to force all allocations to bypass the caching allocator and
// go straight to cudaMalloc.  This setting is useful when debugging GPU memory
// errors, since the caching allocator foils cuda-memcheck.
bool forceUncachedAllocator() {
  static bool force_uncached =
      getenv(""PYTORCH_NO_CUDA_MEMORY_CACHING"") != nullptr;
  return force_uncached;
}

static void uncached_delete(void* ptr) {
  C10_CUDA_CHECK(cudaFree(ptr));
}

// NB: I decided not to fold this into THCCachingAllocator, because the latter
// has a lot more methods and it wasn't altogether clear that they should
// actually be publicly exposed
"
397,"if (nested_if_then_else_ > 0) {
return IRMutator::mutate(v);
}
if (thread_local_bufs_.count(v->base_handle()) > 0) {
return IRMutator::mutate(v);
}
","if (nested_if_then_else_ > 0) {
return IRMutator::mutate(v);
}
    if (nested_let_) {
      return IRMutator::mutate(v);
    }
if (thread_local_bufs_.count(v->base_handle()) > 0) {
return IRMutator::mutate(v);
}
"
398,"auto& thread_extents = cuda_analysis_->gpu_thread_extents();
for (size_t i = 0; i < gpu_thread_vars_.size(); ++i) {
if (!exprEquals(current_thread_reach_[i], thread_extents[i])) {
// Mask it against the current dimensions.
inner = new Cond(
new CompareSelect(
","auto& thread_extents = cuda_analysis_->gpu_thread_extents();
for (size_t i = 0; i < gpu_thread_vars_.size(); ++i) {
if (!exprEquals(current_thread_reach_[i], thread_extents[i])) {
        need_sync = true;
// Mask it against the current dimensions.
inner = new Cond(
new CompareSelect(
"
399,"}
}
} else {
    if (beta != scalar_t(1)) scal<scalar_t>(m, beta, y, incy);
for (int64_t j = 0; j < n; j++) {
scalar_t *column_ = a + lda * j;
scalar_t z = alpha * x[j * incx];
for (int64_t i = 0; i < m; i++) {
y[i * incy] += z * column_[i];
}
}
","}
}
} else {
    if (beta != scalar_t(1) && beta != scalar_t(0)) scal<scalar_t>(m, beta, y, incy);
for (int64_t j = 0; j < n; j++) {
scalar_t *column_ = a + lda * j;
scalar_t z = alpha * x[j * incx];
for (int64_t i = 0; i < m; i++) {
        //output values are ignored if beta is 0, and set to 0, nans and infs are not propagated
        if (j==0 && beta==scalar_t(0)) {
         y[i * incy] = scalar_t(0);
        }
y[i * incy] += z * column_[i];
}
}
"
400,"bool found_needs_ids = false;
auto init_handles = [
scope, &found_active_cb, &found_needs_inputs, &found_needs_ids](
          CallbackHandles& handles, RecordFunctionCallbacks& cbs) {
handles.clear();
for (const auto& cb : cbs) {
if (cb.first.shouldRun(scope)) {
handles.push_back(cb.second);
found_active_cb = true;
if (cb.first.needsInputs()) {
found_needs_inputs = true;
","bool found_needs_ids = false;
auto init_handles = [
scope, &found_active_cb, &found_needs_inputs, &found_needs_ids](
          CallbackHandles& handles, RecordFunctionCallbacks& cbs, ObserverContextList& ctx_list) {
handles.clear();

      size_t num_callbacks = 0;
for (const auto& cb : cbs) {
if (cb.first.shouldRun(scope)) {
handles.push_back(cb.second);
          ++num_callbacks;
found_active_cb = true;
if (cb.first.needsInputs()) {
found_needs_inputs = true;
"
401,"os() << *v->base_handle();
return;
}
  if (v->dtype().scalar_type() == ScalarType::Bool) {
    // There's no __ldg overload for bool.
os() << *v->base_handle() << ""["" << *v->flat_index() << ""]"";
return;
}
","os() << *v->base_handle();
return;
}
  if (v->dtype().scalar_type() == ScalarType::Bool ||
      v->dtype().scalar_type() == ScalarType::Half) {
    // There's no __ldg overload for bool or half.
os() << *v->base_handle() << ""["" << *v->flat_index() << ""]"";
return;
}
"
402,"Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_fill_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_fill_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
403,"Tensor & _th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_scatter_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Bool: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_masked_scatter_bool_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
404,"Tensor & _th_fmod_(Tensor & self, Scalar other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_fmod_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_fmod_(Tensor & self, Scalar other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_fmod_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
405,"Tensor & _th_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_copy_ignoring_overlaps_"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_th_copy_ignoring_overlaps_"", false, DeviceType::CUDA, dispatch_scalar_type);
"
406,"return at::native::argmin_out(result, self, dim, keepdims);
}
static Tensor &std_var_out(Tensor &result, const Tensor &self, IntArrayRef dim, bool unbiased, bool keepdim, bool take_sqrt) {
TORCH_CHECK(self.device().type() == DeviceType::CPU || self.device().type() == DeviceType::CUDA,
""std and var only supports CPU AND CUDA device type, got: "", self.device().type());
TORCH_CHECK(self.layout() == Layout::Strided,
","return at::native::argmin_out(result, self, dim, keepdims);
}
static Tensor& std_var_out(Tensor& result, const Tensor& self, IntArrayRef dim, bool unbiased, bool keepdim, bool take_sqrt) {
TORCH_CHECK(self.device().type() == DeviceType::CPU || self.device().type() == DeviceType::CUDA,
""std and var only supports CPU AND CUDA device type, got: "", self.device().type());
TORCH_CHECK(self.layout() == Layout::Strided,
"
407,"reduce_args);
}
Tensor* Reduce(
    const std::string& func_name,
    const std::vector<DimArg>& dim_args,
    const Reducer& reducer,
    Tensor* tensor,
    const std::vector<DimArg>& reduce_args) {
  return Reduce(
      func_name,
      dim_args,
      reducer,
      [&](ParameterList& p) { return tensor->call(p); },
      reduce_args);
}

} // namespace tensorexpr
} // namespace jit
} // namespace torch
","reduce_args);
}
} // namespace tensorexpr
} // namespace jit
} // namespace torch
"
408,"// For every output tensor we've created a flattened 1D tensor - let's
// mark the original output tensor with computeInline
      l.computeInline(tensorOutputs_[i]->buf());
int loopLevels = getTECudaPointwiseLoopLevels();
const int kDefaultLoopLevels = 2;
","// For every output tensor we've created a flattened 1D tensor - let's
// mark the original output tensor with computeInline
      l.computeInline(l.getLoopBodyFor(tensorOutputs_[i]));
int loopLevels = getTECudaPointwiseLoopLevels();
const int kDefaultLoopLevels = 2;
"
409,".def(""_jit_texpr_fuser_enabled"", &tensorExprFuserEnabled)
.def(""_jit_texpr_fallback_allowed"", &tensorexpr::fallbackAllowed)
.def(""_jit_texpr_set_fallback_allowed"", &tensorexpr::setFallbackAllowed)
.def(
""_jit_set_te_generate_block_code"",
[](bool gen_block_code) {
",".def(""_jit_texpr_fuser_enabled"", &tensorExprFuserEnabled)
.def(""_jit_texpr_fallback_allowed"", &tensorexpr::fallbackAllowed)
.def(""_jit_texpr_set_fallback_allowed"", &tensorexpr::setFallbackAllowed)
      .def(""_jit_set_texpr_reductions_enabled"", &setTexprReductionsEnabled)
      .def(""_jit_texpr_reductions_enabled"", &texprReductionsEnabled)
.def(
""_jit_set_te_generate_block_code"",
[](bool gen_block_code) {
"
410,"const Tensor& input, const Tensor& target, int64_t reduction) {
auto norm = reduction == Reduction::Mean ? 2. / input.numel() : 2.;
auto iter = at::TensorIteratorConfig()
    .set_check_mem_overlap(true)
.add_output(grad_input)
.add_input(input)
.add_input(target)
","const Tensor& input, const Tensor& target, int64_t reduction) {
auto norm = reduction == Reduction::Mean ? 2. / input.numel() : 2.;
auto iter = at::TensorIteratorConfig()
.add_output(grad_input)
.add_input(input)
.add_input(target)
"
411,"Tensor& polar_out(Tensor& result, const Tensor& abs, const Tensor& angle) {
complex_check_dtype(result, abs, angle);
auto iter = TensorIteratorConfig()
      .set_check_mem_overlap(true)
.add_output(result)
.add_input(abs)
.add_input(angle)
","Tensor& polar_out(Tensor& result, const Tensor& abs, const Tensor& angle) {
complex_check_dtype(result, abs, angle);
auto iter = TensorIteratorConfig()
.add_output(result)
.add_input(abs)
.add_input(angle)
"
412,"""unsupported operation: the input tensors cannot refer to any of the ""
""output memory locations. Found overlap in input tensor "", i);
}
auto should_skip = [](const Tensor& t) { return t.numel() == 0 && t.dim() == 1; };
for (auto const &tensor : tensors) {
","""unsupported operation: the input tensors cannot refer to any of the ""
""output memory locations. Found overlap in input tensor "", i);
}
  at::assert_no_internal_overlap(result);
auto should_skip = [](const Tensor& t) { return t.numel() == 0 && t.dim() == 1; };
for (auto const &tensor : tensors) {
"
413,"}
void runNooptPassPipeline(std::shared_ptr<Graph>& graph) {
  GRAPH_DUMP(""Before LowerGradOf (beginning of runNooptPassPipeline)"", graph);
LowerGradOf(*graph);
  GRAPH_DUMP(""After LowerGradOf, before RemoveExpands"", graph);
RemoveExpands(graph);
  GRAPH_DUMP(""After RemoveExpands, before CanonicalizeOps"", graph);
CanonicalizeOps(graph);
  GRAPH_DUMP(""After CanonicalizeOps, before EliminateDeadCode"", graph);
EliminateDeadCode(graph);
  GRAPH_DUMP(""After EliminateDeadCode (end of runNooptPassPipeline)"", graph);
}
void runPreAutodiffPassPipeline(std::shared_ptr<Graph>& graph) {
  GRAPH_DUMP(
      ""Before InsertGuards (beginning of runPreAutodiffPassPipeline)"", graph);
if (tensorExprFuserEnabled()) {
// With TE fuser we don't generate bailouts
LowerGradOf(*graph);
    GRAPH_DUMP(""After LowerGradOf, before specializeAutogradZero"", graph);
} else {
InsertGuards(graph);
    GRAPH_DUMP(""After InsertGuards, before LowerGradOf"", graph);
LowerGradOf(*graph);
    GRAPH_DUMP(""After LowerGradOf, before EliminateRedundantGuards"", graph);
EliminateRedundantGuards(graph);
    GRAPH_DUMP(""After EliminateRedundantGuards, before InsertBailOuts"", graph);
InsertBailOuts(graph);
    GRAPH_DUMP(""After InsertBailOuts, before specializeAutogradZero"", graph);
}
specializeAutogradZero(graph);
  GRAPH_DUMP(""After specializeAutogradZero"", graph);
// runRequiredPasses
{
RemoveExpands(graph);
    GRAPH_DUMP(""After RemoveExpands, before CanonicalizeOps"", graph);
CanonicalizeOps(graph);
    GRAPH_DUMP(""After CanonicalizeOps, before EliminateDeadCode"", graph);
EliminateDeadCode(graph);
    GRAPH_DUMP(""After EliminateDeadCode"", graph);
}
PeepholeOptimize(graph);
  GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
// runOptimization:
{
EliminateDeadCode(graph);
    GRAPH_DUMP(
        ""After EliminateDeadCode, before EliminateCommonSubexpression"", graph);
EliminateCommonSubexpression(graph);
    GRAPH_DUMP(
        ""After EliminateCommonSubexpression, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
    GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
    GRAPH_DUMP(""After ConstantPropagation, before ConstantPooling"", graph);
ConstantPooling(graph);
    GRAPH_DUMP(""After ConstantPooling, before UnrollLoops"", graph);
UnrollLoops(graph);
    GRAPH_DUMP(""After UnrollLoops, before RemoveListMutation"", graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DUMP(""After RemoveListMutation, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
    GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
    GRAPH_DUMP(
        ""After ConstantPropagation, before EliminateCommonSubexpression"",
        graph);
EliminateCommonSubexpression(graph);
    GRAPH_DUMP(
        ""After EliminateCommonSubexpression, before CheckInplace"", graph);
CheckInplace(graph);
}
  GRAPH_DUMP(""After CheckInplace (end of runPreAutodiffPassPipeline)"", graph);
}
void runDiffGraphPasses(std::shared_ptr<Graph>& graph) {
  GRAPH_DUMP(
      ""Before EliminateDeadCode (beginning of runDiffGraphPasses)"", graph);
// runOptimization:
{
// Basic graph preprocessing to eliminate noise.
EliminateDeadCode(graph);
    GRAPH_DUMP(
        ""After EliminateDeadCode, before EliminateCommonSubexpression"", graph);
EliminateCommonSubexpression(graph);
    GRAPH_DUMP(
        ""After EliminateCommonSubexpression, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
    GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
    GRAPH_DUMP(""After ConstantPropagation, before ConstantPooling"", graph);
ConstantPooling(graph);
    GRAPH_DUMP(""After ConstantPooling, before UnrollLoops"", graph);
UnrollLoops(graph);
    GRAPH_DUMP(""After UnrollLoops, before RemoveListMutation"", graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DUMP(""After RemoveListMutation, before PeepholeOptimize"", graph);
PeepholeOptimize(graph);
    GRAPH_DUMP(""After PeepholeOptimize, before ConstantPropagation"", graph);
ConstantPropagation(graph);
    GRAPH_DUMP(
        ""After ConstantPropagation, before EliminateCommonSubexpression"",
        graph);
EliminateCommonSubexpression(graph);
    GRAPH_DUMP(
        ""After EliminateCommonSubexpression, before CheckInplace"", graph);
CheckInplace(graph);
}
  GRAPH_DUMP(""After CheckInplace, before customPrePasses"", graph);
// runNondiffOptimization
{
","}
void runNooptPassPipeline(std::shared_ptr<Graph>& graph) {
  GRAPH_DEBUG(
      ""Before LowerGradOf (beginning of runNooptPassPipeline)\n"", *graph);
LowerGradOf(*graph);
  GRAPH_DEBUG(""After LowerGradOf, before RemoveExpands\n"", *graph);
RemoveExpands(graph);
  GRAPH_DEBUG(""After RemoveExpands, before CanonicalizeOps\n"", *graph);
CanonicalizeOps(graph);
  GRAPH_DEBUG(""After CanonicalizeOps, before EliminateDeadCode\n"", *graph);
EliminateDeadCode(graph);
  GRAPH_DEBUG(
      ""After EliminateDeadCode (end of runNooptPassPipeline)\n"", *graph);
}
void runPreAutodiffPassPipeline(std::shared_ptr<Graph>& graph) {
  GRAPH_DEBUG(
      ""Before InsertGuards (beginning of runPreAutodiffPassPipeline)\n"",
      *graph);
if (tensorExprFuserEnabled()) {
// With TE fuser we don't generate bailouts
LowerGradOf(*graph);
    GRAPH_DEBUG(""After LowerGradOf, before specializeAutogradZero\n"", *graph);
} else {
InsertGuards(graph);
    GRAPH_DEBUG(""After InsertGuards, before LowerGradOf\n"", *graph);
LowerGradOf(*graph);
    GRAPH_DEBUG(""After LowerGradOf, before EliminateRedundantGuards\n"", *graph);
EliminateRedundantGuards(graph);
    GRAPH_DEBUG(
        ""After EliminateRedundantGuards, before InsertBailOuts\n"", *graph);
InsertBailOuts(graph);
    GRAPH_DEBUG(
        ""After InsertBailOuts, before specializeAutogradZero\n"", *graph);
}
specializeAutogradZero(graph);
  GRAPH_DEBUG(""After specializeAutogradZero\n"", *graph);
// runRequiredPasses
{
RemoveExpands(graph);
    GRAPH_DEBUG(""After RemoveExpands, before CanonicalizeOps\n"", *graph);
CanonicalizeOps(graph);
    GRAPH_DEBUG(""After CanonicalizeOps, before EliminateDeadCode\n"", *graph);
EliminateDeadCode(graph);
    GRAPH_DEBUG(""After EliminateDeadCode"", *graph);
}
PeepholeOptimize(graph);
  GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
// runOptimization:
{
EliminateDeadCode(graph);
    GRAPH_DEBUG(
        ""After EliminateDeadCode, before EliminateCommonSubexpression\n"",
        *graph);
EliminateCommonSubexpression(graph);
    GRAPH_DEBUG(
        ""After EliminateCommonSubexpression, before PeepholeOptimize\n"",
        *graph);
PeepholeOptimize(graph);
    GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
    GRAPH_DEBUG(""After ConstantPropagation, before ConstantPooling\n"", *graph);
ConstantPooling(graph);
    GRAPH_DEBUG(""After ConstantPooling, before UnrollLoops\n"", *graph);
UnrollLoops(graph);
    GRAPH_DEBUG(""After UnrollLoops, before RemoveListMutation\n"", *graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DEBUG(""After RemoveListMutation, before PeepholeOptimize\n"", *graph);
PeepholeOptimize(graph);
    GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
    GRAPH_DEBUG(
        ""After ConstantPropagation, before EliminateCommonSubexpression\n"",
        *graph);
EliminateCommonSubexpression(graph);
    GRAPH_DEBUG(
        ""After EliminateCommonSubexpression, before CheckInplace\n"", *graph);
CheckInplace(graph);
}
  GRAPH_DEBUG(
      ""After CheckInplace (end of runPreAutodiffPassPipeline)\n"", *graph);
}
void runDiffGraphPasses(std::shared_ptr<Graph>& graph) {
  GRAPH_DEBUG(
      ""Before EliminateDeadCode (beginning of runDiffGraphPasses)\n"", *graph);
// runOptimization:
{
// Basic graph preprocessing to eliminate noise.
EliminateDeadCode(graph);
    GRAPH_DEBUG(
        ""After EliminateDeadCode, before EliminateCommonSubexpression\n"",
        *graph);
EliminateCommonSubexpression(graph);
    GRAPH_DEBUG(
        ""After EliminateCommonSubexpression, before PeepholeOptimize\n"",
        *graph);
PeepholeOptimize(graph);
    GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
    GRAPH_DEBUG(""After ConstantPropagation, before ConstantPooling\n"", *graph);
ConstantPooling(graph);
    GRAPH_DEBUG(""After ConstantPooling, before UnrollLoops\n"", *graph);
UnrollLoops(graph);
    GRAPH_DEBUG(""After UnrollLoops, before RemoveListMutation\n"", *graph);
// run again with unrolled loops
RemoveListMutation(graph);
    GRAPH_DEBUG(""After RemoveListMutation, before PeepholeOptimize\n"", *graph);
PeepholeOptimize(graph);
    GRAPH_DEBUG(""After PeepholeOptimize, before ConstantPropagation\n"", *graph);
ConstantPropagation(graph);
    GRAPH_DEBUG(
        ""After ConstantPropagation, before EliminateCommonSubexpression\n"",
        *graph);
EliminateCommonSubexpression(graph);
    GRAPH_DEBUG(
        ""After EliminateCommonSubexpression, before CheckInplace\n"", *graph);
CheckInplace(graph);
}
  GRAPH_DEBUG(""After CheckInplace, before customPrePasses\n"", *graph);
// runNondiffOptimization
{
"
414,"if (*remaining_bailout_depth_ == 0) {
auto copy = graph->copy();
runProfilingInsensitiveOptimizations(copy);
    GRAPH_DUMP(""Optimized SimpleExecutor Graph : "", copy);
optimized_plan_ = ExecutionPlan(copy, function_name_);
return *optimized_plan_;
}
","if (*remaining_bailout_depth_ == 0) {
auto copy = graph->copy();
runProfilingInsensitiveOptimizations(copy);
    GRAPH_DUMP(""Optimized SimpleExecutor Graph: "", copy);
optimized_plan_ = ExecutionPlan(copy, function_name_);
return *optimized_plan_;
}
"
415,"formatStackTrace(ss);
ss << ""RuntimeError: "" << msg << ""\n"";
if (future_) {
      future_->setError(Future::FutureError(ss.str()));
} else if (is_jit_exception) {
throw JITException(ss.str());
} else {
","formatStackTrace(ss);
ss << ""RuntimeError: "" << msg << ""\n"";
if (future_) {
      future_->setError(std::make_exception_ptr(Future::FutureError(ss.str())));
} else if (is_jit_exception) {
throw JITException(ss.str());
} else {
"
416,"const Tensor& self,
const Tensor& weight,
const Tensor& bias) {
  AT_ERROR(""mkldnn_linear: ATen not compiled with MKLDNN support"");
}
} // namespace native
","const Tensor& self,
const Tensor& weight,
const Tensor& bias) {
  TORCH_CHECK(false, ""mkldnn_linear: ATen not compiled with MKLDNN support"");
}
} // namespace native
"
417,"IntArrayRef padding,
IntArrayRef dilation,
bool ceil_mode) {
return _mkldnn_pooling(
input,
kernel_size,
","IntArrayRef padding,
IntArrayRef dilation,
bool ceil_mode) {
  TORCH_CHECK(std::all_of(dilation.cbegin(), dilation.cend(), [](int64_t i) { return 1 == i; }),
      ""mkldnn_max_pool2d does not support dilation case"");
return _mkldnn_pooling(
input,
kernel_size,
"
418,"aten::vtensor_from_vulkan(weight_prepacked_vulkan);
VulkanTensor voutput =
VulkanTensor{{params.N, params.OC, params.OH, params.OW}};
  voutput.allocate_storage();
const bool hasBias = bias.has_value() && bias->defined();
  const bool vulkanBias = (*bias).is_vulkan();
  if (hasBias && vulkanBias) {
const VulkanTensor& vbias = aten::vtensor_from_vulkan(*bias);
vulkan::detail::conv2d(
voutput, vinput, vweight, vbias, params, output_min, output_max);
","aten::vtensor_from_vulkan(weight_prepacked_vulkan);
VulkanTensor voutput =
VulkanTensor{{params.N, params.OC, params.OH, params.OW}};
const bool hasBias = bias.has_value() && bias->defined();
  if (hasBias && bias->is_vulkan()) {
const VulkanTensor& vbias = aten::vtensor_from_vulkan(*bias);
vulkan::detail::conv2d(
voutput, vinput, vweight, vbias, params, output_min, output_max);
"
419,"paramsDict.insert(nameTensorParamPair.second);
}
}
} // namespace jit
} // namespace torch
","paramsDict.insert(nameTensorParamPair.second);
}
}

Node* addNodeToBlock(Block* block, Value* input, Symbol kind) {
  auto new_node = block->appendNode(block->owningGraph()->create(kind));
  auto new_input = new_node->addInput(input);
  for (size_t i = 0; i < new_node->outputs().size(); i++) {
    auto output = new_node->outputs()[i];
    block->registerOutput(output);
  }
  return new_node;
}
} // namespace jit
} // namespace torch
"
420,"#include <torch/csrc/python_headers.h>
#include <torch/csrc/utils/pybind.h>
#include <torch/csrc/utils/python_strings.h>

#include <iostream>
#include <sstream>
","#include <torch/csrc/python_headers.h>
#include <torch/csrc/utils/pybind.h>
#include <torch/csrc/utils/python_strings.h>
#include <iostream>
#include <sstream>
"
421,"""torch._C.Generator"",                   /* tp_name */
sizeof(THPGenerator),                        /* tp_basicsize */
0,                                           /* tp_itemsize */
  (destructor)THPGenerator_dealloc,            /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","""torch._C.Generator"",                   /* tp_name */
sizeof(THPGenerator),                        /* tp_basicsize */
0,                                           /* tp_itemsize */
  THPGenerator_dealloc,                        /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
422,"}
}
Tensor masked_select_cpu(const Tensor & self, const Tensor & mask) {
  namedinference::compute_broadcast_outnames(self, mask);

  Tensor b_self, b_mask;
  std::tie(b_self, b_mask) = expand_outplace(self, mask, ""masked_select"");
  if (b_mask.dtype() == at::ScalarType::Byte) {
    TORCH_WARN(""masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,"" \
            ""please use a mask with dtype torch.bool instead."");
    return legacy::cpu::_th_masked_select(b_self, b_mask);
  } else {
    return legacy::cpu::_th_masked_select_bool(b_self, b_mask);
  }
}

Tensor & masked_select_out_cpu(Tensor & result, const Tensor & self, const Tensor & mask) {
  namedinference::compute_broadcast_outnames(self, mask);

  Tensor b_self, b_mask;
  std::tie(b_self, b_mask) = expand_outplace(self, mask, ""masked_select_out"");
  if (b_mask.dtype() == at::ScalarType::Bool) {
    return legacy::cpu::_th_masked_select_bool_out(result, b_self, b_mask);
  } else {
    return legacy::cpu::_th_masked_select_out(result, b_self, b_mask);
  }
}

Tensor argsort(const Tensor & self, int64_t dim, bool descending) {
return std::get<1>(at::sort(self, dim, descending));
}
","}
}
Tensor argsort(const Tensor & self, int64_t dim, bool descending) {
return std::get<1>(at::sort(self, dim, descending));
}
"
423,"}
Tensor frobenius_norm(const Tensor& self) {
return at::norm(self);
}
Tensor frobenius_norm(const Tensor& self, IntArrayRef dim, bool keepdim) {
TORCH_CHECK(
dim.size() <= 2,
""Expected at most 2 dimensions, but got "",
dim.size(),
"" dimensions instead."");
  if (dim.size() == 1) {
    return at::norm(self, 2, dim, keepdim, self.scalar_type());
}
if (self.is_complex()){
return at::sqrt(at::sum(at::real(self.conj() * self), dim, keepdim));
","}
Tensor frobenius_norm(const Tensor& self) {
  TORCH_CHECK(!self.is_complex(), ""frobenius norm not supported for complex tensors"");
return at::norm(self);
}
Tensor frobenius_norm(const Tensor& self, IntArrayRef dim, bool keepdim) {
  TORCH_CHECK(!self.is_complex(), ""frobenius norm not supported for complex tensors"");
TORCH_CHECK(
dim.size() <= 2,
""Expected at most 2 dimensions, but got "",
dim.size(),
"" dimensions instead."");
  if (dim.size() == 1 || dim.size() == 0) {
    return at::norm(self, 2, dim, keepdim);
}
if (self.is_complex()){
return at::sqrt(at::sum(at::real(self.conj() * self), dim, keepdim));
"
424,"// Since we error out on svd_backward when we don't compute U and V, the backward pass for nuclear_norm
// would end up throwing an error as a result if U and V aren't computed.
// Due to this, we have to compute U and V conditionally.
  return at::sum(std::get<1>(at::svd(self, /*some=*/true,
/*compute_uv=*/at::GradMode::is_enabled() && self.requires_grad())), 0, keepdim);
}
Tensor &nuclear_norm_out(Tensor& result, const Tensor& self, bool keepdim) {
","// Since we error out on svd_backward when we don't compute U and V, the backward pass for nuclear_norm
// would end up throwing an error as a result if U and V aren't computed.
// Due to this, we have to compute U and V conditionally.
  Tensor result = at::sum(std::get<1>(at::svd(self, /*some=*/true,
/*compute_uv=*/at::GradMode::is_enabled() && self.requires_grad())), 0, keepdim);
  if (keepdim) {
    result.unsqueeze_(0);
  }
  return result;
}
Tensor &nuclear_norm_out(Tensor& result, const Tensor& self, bool keepdim) {
"
425,"Stmt* stmt = l.root_stmt();
// Arithmetic Simplification.
stmt = IRSimplifier::simplify(stmt);
return stmt;
}
","Stmt* stmt = l.root_stmt();
// Arithmetic Simplification.
stmt = IRSimplifier::simplify(stmt);
  GRAPH_DEBUG(""Final Stmt:\n"", std::to_string(stmt), ""\n"");
return stmt;
}
"
426,"queue_ = {};
vkGetDeviceQueue(device_, queueFamilyIndex_, 0, &queue_);
  VkPhysicalDeviceProperties physicalDeviceProperties;
vkGetPhysicalDeviceProperties(physicalDevice_, &physicalDeviceProperties);
VkCommandPoolCreateInfo commandPoolCreateInfo{};
","queue_ = {};
vkGetDeviceQueue(device_, queueFamilyIndex_, 0, &queue_);
  VkPhysicalDeviceProperties physicalDeviceProperties{};
vkGetPhysicalDeviceProperties(physicalDevice_, &physicalDeviceProperties);
VkCommandPoolCreateInfo commandPoolCreateInfo{};
"
427,"}
void VImage::addImageMemoryBarrierToGeneral(
    VkCommandBuffer commandBuffer) const {
addImageMemoryBarrier(commandBuffer, VK_IMAGE_LAYOUT_GENERAL);
}
void VImage::addImageMemoryBarrierToShaderRead(
    VkCommandBuffer commandBuffer) const {
addImageMemoryBarrier(
commandBuffer, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);
}
VkDescriptorSetLayoutBinding descriptorSetLayoutBinding(
    uint32_t binding,
    VkDescriptorType descriptorType) {
return {binding, descriptorType, 1, VK_SHADER_STAGE_COMPUTE_BIT, nullptr};
}
void createDescriptorSetLayout(
    VkDevice device,
    const VkDescriptorSetLayoutBinding* bindings,
    uint32_t bindingCount,
    VkDescriptorSetLayout* setLayout) {
VkDescriptorSetLayoutCreateInfo createInfo{};
createInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
createInfo.pNext = nullptr;
","}
void VImage::addImageMemoryBarrierToGeneral(
    const VkCommandBuffer commandBuffer) const {
addImageMemoryBarrier(commandBuffer, VK_IMAGE_LAYOUT_GENERAL);
}
void VImage::addImageMemoryBarrierToShaderRead(
    const VkCommandBuffer commandBuffer) const {
addImageMemoryBarrier(
commandBuffer, VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);
}
VkDescriptorSetLayoutBinding descriptorSetLayoutBinding(
    const uint32_t binding,
    const VkDescriptorType descriptorType) {
return {binding, descriptorType, 1, VK_SHADER_STAGE_COMPUTE_BIT, nullptr};
}
void createDescriptorSetLayout(
    const VkDevice device,
    const VkDescriptorSetLayoutBinding* const bindings,
    const uint32_t bindingCount,
    VkDescriptorSetLayout* const setLayout) {
VkDescriptorSetLayoutCreateInfo createInfo{};
createInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
createInfo.pNext = nullptr;
"
428,"}
void createDescriptorSetLayoutSinglePool(
    VkDevice device,
    std::vector<VkDescriptorType> descrTypes,
    VkDescriptorSetLayout* descrSetLayout,
    VkDescriptorPool* descrPool,
    VkDescriptorSet* descrSet) {
  auto size = descrTypes.size();
std::vector<VkDescriptorSetLayoutBinding> bindings;
std::vector<VkDescriptorPoolSize> poolSizes;
uint32_t i = 0;
","}
void createDescriptorSetLayoutSinglePool(
    const VkDevice device,
    const std::vector<VkDescriptorType>& descrTypes,
    VkDescriptorSetLayout* const descrSetLayout,
    VkDescriptorPool* const descrPool,
    VkDescriptorSet* const descrSet) {
  const auto size = descrTypes.size();
std::vector<VkDescriptorSetLayoutBinding> bindings;
std::vector<VkDescriptorPoolSize> poolSizes;
uint32_t i = 0;
"
429,"submitInfo.commandBufferCount = 1;
submitInfo.pCommandBuffers = &commandBuffer_;
  VkFence fence;
VkFenceCreateInfo fenceCreateInfo{};
fenceCreateInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
fenceCreateInfo.flags = 0;
","submitInfo.commandBufferCount = 1;
submitInfo.pCommandBuffers = &commandBuffer_;
  VkFence fence{};
VkFenceCreateInfo fenceCreateInfo{};
fenceCreateInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
fenceCreateInfo.flags = 0;
"
430,"#ifdef USE_VULKAN_SHADERC_RUNTIME
ComputeUnit& ComputeUnitFactory::get(
    const char* key,
    const char* glslSrc,
    const VkDescriptorSetLayout& descrSetLayout,
    WorkGroupSize workGroupSize) {
return get(
getCacheKey(key, workGroupSize),
[glslSrc,
","#ifdef USE_VULKAN_SHADERC_RUNTIME
ComputeUnit& ComputeUnitFactory::get(
    const char* const key,
    const char* const glslSrc,
    const VkDescriptorSetLayout descrSetLayout,
    const WorkGroupSize workGroupSize) {
return get(
getCacheKey(key, workGroupSize),
[glslSrc,
"
431,"}
#else
ComputeUnit& ComputeUnitFactory::get(
    const char* key,
    const uint32_t* code,
const uint32_t codeSize,
    const VkDescriptorSetLayout& descrSetLayout,
    WorkGroupSize& workGroupSize) {
return get(
getCacheKey(key, workGroupSize),
[code,
","}
#else
ComputeUnit& ComputeUnitFactory::get(
    const char* const key,
    const uint32_t* const code,
const uint32_t codeSize,
    const VkDescriptorSetLayout descrSetLayout,
    const WorkGroupSize workGroupSize) {
return get(
getCacheKey(key, workGroupSize),
[code,
"
432,"device, bindings, 3 /* bindingsCount */, &descrSetLayout);
VkDescriptorPool descrPool{};
  VkDescriptorPoolSize poolSizes[] = {
{VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 1},
{VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1},
{VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 1}};
","device, bindings, 3 /* bindingsCount */, &descrSetLayout);
VkDescriptorPool descrPool{};
  const VkDescriptorPoolSize poolSizes[] = {
{VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 1},
{VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1},
{VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 1}};
"
433,"Tensor& _clamp__vulkan(
Tensor& self,
    c10::optional<Scalar> min,
    c10::optional<Scalar> max) {
auto y = vulkan_clamp(self, min, max);
self.copy_(y);
return self;
}
Tensor vulkan_hardtanh(const Tensor& self, Scalar min, Scalar max) {
return vulkan_clamp(self, min, max);
}
Tensor& vulkan_hardtanh_(Tensor& self, Scalar min, Scalar max) {
return _clamp__vulkan(self, min, max);
}
Tensor mean_vulkan(
const Tensor& self,
    IntArrayRef dim,
    bool keepdim,
    optional<ScalarType> dtype) {
TORCH_INTERNAL_ASSERT(
self.is_vulkan(), ""mean_vulkan expects Vulkan tensor input"");
TORCH_INTERNAL_ASSERT(
self.dim() == 4 && dim.size() == 2 && dim[0] == 2 && dim[1] == 3);
VulkanTensor& x = vtensor_from_vulkan(self);
  auto sizes = self.sizes();
  std::vector<int64_t> outputSizes{sizes[0], sizes[1]};
  VulkanTensor output = VulkanTensor{outputSizes};
output.allocate_storage();
vulkan::detail::mean(output, x);
return new_with_vtensor_vulkan(std::move(output), self.options());
","Tensor& _clamp__vulkan(
Tensor& self,
    const c10::optional<Scalar> min,
    const c10::optional<Scalar> max) {
auto y = vulkan_clamp(self, min, max);
self.copy_(y);
return self;
}
Tensor vulkan_hardtanh(const Tensor& self, const Scalar min, const Scalar max) {
return vulkan_clamp(self, min, max);
}
Tensor& vulkan_hardtanh_(Tensor& self, const Scalar min, const Scalar max) {
return _clamp__vulkan(self, min, max);
}
Tensor mean_vulkan(
const Tensor& self,
    const IntArrayRef dim,
    const bool keepdim,
    const optional<ScalarType> dtype) {
TORCH_INTERNAL_ASSERT(
self.is_vulkan(), ""mean_vulkan expects Vulkan tensor input"");
TORCH_INTERNAL_ASSERT(
self.dim() == 4 && dim.size() == 2 && dim[0] == 2 && dim[1] == 3);
VulkanTensor& x = vtensor_from_vulkan(self);
  const auto sizes = self.sizes();
  VulkanTensor output = VulkanTensor{std::vector<int64_t>{sizes[0], sizes[1]}};
output.allocate_storage();
vulkan::detail::mean(output, x);
return new_with_vtensor_vulkan(std::move(output), self.options());
"
434,"std::vector<int64_t>&& padding,
std::vector<int64_t>&& stride,
std::vector<int64_t>&& dilation,
    int64_t groups,
const c10::optional<Scalar> output_min,
const c10::optional<Scalar> output_max) {
auto op_context = vulkan::detail::convolution2d::create(
","std::vector<int64_t>&& padding,
std::vector<int64_t>&& stride,
std::vector<int64_t>&& dilation,
    const int64_t groups,
const c10::optional<Scalar> output_min,
const c10::optional<Scalar> output_max) {
auto op_context = vulkan::detail::convolution2d::create(
"
435,"out << l.delim;
}
printValueRef(out, n);
    out << "" : "";
    out << *n->type();
}
return out;
}
","out << l.delim;
}
printValueRef(out, n);
    if (c10::type_verbosity() >= c10::TypeVerbosity::Type) {
      out << "" : "";
      out << *n->type();
    }
}
return out;
}
"
436,"// APIs are almost always in the same function.
static void scanConnectedNodes(
Value* src,
      const VALUE_SET& blocked,
const std::function<void(Value*)>& CB, VALUE_MAP* debugPath) {
std::deque<Value*> worklist;
SmallPtrSet<Value*, 16> visited;
","// APIs are almost always in the same function.
static void scanConnectedNodes(
Value* src,
      VALUE_SET blocked,
const std::function<void(Value*)>& CB, VALUE_MAP* debugPath) {
std::deque<Value*> worklist;
SmallPtrSet<Value*, 16> visited;
"
437,"}
};
auto expandUsers = [&](Value* V) -> void {
// If the value is not constant, then the user of the value might pass
// other value into it, e.g.:
","}
};
    auto blockSiblingOperands = [&](User* U, Value* V) -> void {
      // This is to handle a special case only appears in LLVM 9 (not in 5 - 8
      // and 10), where it can falsely associate unrelated PyTorch op
      // registrations.
      //
      // If the value `V` is used by a PHI-node `U`, then we should stop
      // crawling `U`'s operands, i.e. `V`'s siblings in `U`. E.g.:
      //
      //   114:                                            ; preds = %111, %109
      //     %115 = phi i32 [ %110, %109 ], [ %112, %111 ]
      //
      // `%115` might take the value of `%110` or `%112`, depending on from
      // which label it comes. Assuming `V` is `%110` and `U` is `%115`, we can
      // continue to scan `%115` but should not crawl `%112` as it does not
      // directly pass data from `%110` to `%112` (and vice versa).
      //
      // NB: we probably should do the same for other LLVM instructions with
      // this kind of selective semantics. But for the purpose of analyzing
      // PyTorch registrations it seems to be sufficent for now.
      if (isa<PHINode>(U)) {
        for (auto& S : U->operands()) {
          blocked.insert(S);
        }
      }
    };

auto expandUsers = [&](Value* V) -> void {
// If the value is not constant, then the user of the value might pass
// other value into it, e.g.:
"
438,"}
for (auto U : V->users()) {
insert(U, V);
}
};
","}
for (auto U : V->users()) {
insert(U, V);
        blockSiblingOperands(U, V);
}
};
"
439,"std::cerr << op << "" "";
}
std::cerr << "") in a invocation call in function: ""
                  << demangle(caller) << std::endl;
}
for (const auto& op : visitedOps) {
opSchemaStrs->insert(op);
(*functionToSchemaStrs)[caller].insert(op);
if (Verbose) {
          std::cerr << ""[DEBUG][OP_CALL] "" << demangle(caller) << "" => ""
<< op << std::endl;
}
}
","std::cerr << op << "" "";
}
std::cerr << "") in a invocation call in function: ""
                  << _demangle(caller) << std::endl;
}
for (const auto& op : visitedOps) {
opSchemaStrs->insert(op);
(*functionToSchemaStrs)[caller].insert(op);
if (Verbose) {
          std::cerr << ""[DEBUG][OP_CALL] "" << _demangle(caller) << "" => ""
<< op << std::endl;
}
}
"
440,"if (!PySequence_Check(data)) {
throw TypeError(""new(): data must be a sequence (got %s)"", Py_TYPE(data)->tp_name);
}
  return internal_new_from_data(dispatch_key, scalar_type, std::move(device), data, false, false, false);
}
// ""base"" here refers to the Tensor type on which the function was invoked, e.g.:
","if (!PySequence_Check(data)) {
throw TypeError(""new(): data must be a sequence (got %s)"", Py_TYPE(data)->tp_name);
}
  return internal_new_from_data(dispatch_key, scalar_type, std::move(device), data,
                                /*copy_variables=*/false, /*copy_numpy=*/false,
                                /*type_inference=*/false);
}
// ""base"" here refers to the Tensor type on which the function was invoked, e.g.:
"
441,"PyWarningHandler::~PyWarningHandler() noexcept(false) {
c10::Warning::set_warning_handler(prev_handler_);
  if(warning_buffer_.size() > 0) {
    if(in_exception_) {
      // An error happened after the warning
      // Simply handle with the previous handler
      for(const auto& warning: warning_buffer_) {
        auto source_location = warning.source_location_;
        const auto& msg = processErrorMsg(warning.msg_);
        c10::Warning::warn(source_location, msg, warning.verbatim_);
}
      warning_buffer_.clear();
    } else {
      pybind11::gil_scoped_acquire gil;
      auto result = 0;
      for (const auto& warning: warning_buffer_) {
        auto source_location = warning.source_location_;
        const auto& msg = processErrorMsg(warning.msg_);
        if (source_location.file == nullptr) {
          result = PyErr_WarnEx(PyExc_RuntimeWarning, msg.c_str(), 1);
        } else if (warning.verbatim_) {
          // Sets the source location from the warning
          // Note: PyErr_WarnExplicit will disregard Python's warning filter
          // and always appear. This is in contrast to PyErr_WarnEx,
          // which respects the warning filter.
          result = PyErr_WarnExplicit(
              /*category=*/PyExc_UserWarning,
              /*message=*/msg.c_str(),
              /*filename=*/source_location.file,
              /*lineno=*/source_location.line,
              /*module=*/nullptr,
              /*registry=*/nullptr);
} else {
          // Lets Python set the source location and puts the C++ warning
          // location into the message.
          std::ostringstream os;
          os << msg << "" (Triggered internally at  "" << source_location.file;
          os << "":"" << source_location.line << "".)"";
          result = PyErr_WarnEx(PyExc_UserWarning, os.str().c_str(), 1);
        }
        if (result < 0) {
break;
}
}
      warning_buffer_.clear();
      if (result < 0) {
        /// A warning raised an error, we need to force the parent
        /// function to return an error code.
        throw python_error();
      }
}
}
}
","PyWarningHandler::~PyWarningHandler() noexcept(false) {
c10::Warning::set_warning_handler(prev_handler_);
  if (warning_buffer_.size() > 0) {
    PyObject *type, *value, *traceback;
    pybind11::gil_scoped_acquire gil;
    auto result = 0;
    if (in_exception_) {
      // This (combined with PyErr_Restore below) also works when no python
      // error has been set yet
      PyErr_Fetch(&type, &value, &traceback);
    }
    for (const auto& warning : warning_buffer_) {
      auto source_location = warning.source_location_;
      const auto& msg = processErrorMsg(warning.msg_);
      if (source_location.file == nullptr) {
        result = PyErr_WarnEx(PyExc_RuntimeWarning, msg.c_str(), 1);
      } else if (warning.verbatim_) {
        // Sets the source location from the warning
        // Note: PyErr_WarnExplicit will disregard Python's warning filter
        // and always appear. This is in contrast to PyErr_WarnEx,
        // which respects the warning filter.
        result = PyErr_WarnExplicit(
            /*category=*/PyExc_UserWarning,
            /*message=*/msg.c_str(),
            /*filename=*/source_location.file,
            /*lineno=*/source_location.line,
            /*module=*/nullptr,
            /*registry=*/nullptr);
      } else {
        // Lets Python set the source location and puts the C++ warning
        // location into the message.
        std::ostringstream os;
        os << msg << "" (Triggered internally at  "" << source_location.file;
        os << "":"" << source_location.line << "".)"";
        result = PyErr_WarnEx(PyExc_UserWarning, os.str().c_str(), 1);
}
      if (result < 0) {
        if (in_exception_) {
          // PyErr_Print prints the traceback to sys.stderr and
          // clears the error indicator
          PyErr_Print();
} else {
break;
}
}
    }
    warning_buffer_.clear();
    if ((result < 0) && (!in_exception_)) {
      /// A warning raised an error, we need to force the parent
      /// function to return an error code.
      throw python_error();
    }
    if (in_exception_) {
      PyErr_Restore(type, value, traceback);
}
}
}
"
442,"// some ops may have mixed tensor/primitive outputs
// for primitives, we don't need to change the type because it is already
// its most constrained form.
      if (stack[i].isTensor())
        node->outputs()[i]->inferTypeFrom(stack[i].toTensor());
}
return true;
}
","// some ops may have mixed tensor/primitive outputs
// for primitives, we don't need to change the type because it is already
// its most constrained form.
      auto tensor_type = node->outputs()[i]->type()->cast<TensorType>();
      if (stack[i].isTensor() && tensor_type) {
        // gradient information isn't always available or part of represenative
        // inputs, maintain original grad property
        auto tensor_grad = tensor_type->requiresGrad();
        node->outputs()[i]->setType(TensorType::create(stack[i].toTensor())
                                        ->withRequiresGrad(tensor_grad));
      }
}
return true;
}
"
443,"if (auto type =
node->namedInput(attr::self)->type()->cast<TensorType>()) {
if (type->dim()) {
              return factory_with_ndim(node, *type->dim());
}
}
return {};
","if (auto type =
node->namedInput(attr::self)->type()->cast<TensorType>()) {
if (type->dim()) {
              return factory_like_with_ndim(node, *type->dim());
}
}
return {};
"
444,"return is_func_node;
}
bool isSingleInputGeneralValueAtenFunction(Node* n) {
  return isAtenFunc(n, _single_input_general_value_aten_funcs);
}
bool isSingleInputGeneralCallFunction(Node* n) {
","return is_func_node;
}
bool isSingleInputGeneralShapeAtenFunction(Node* n) {
  return isAtenFunc(n, _single_input_general_shape_aten_funcs);
}

bool isSingleInputGeneralValueAtenFunction(Node* n) {
  return isAtenFunc(n, _single_input_general_value_aten_funcs) ||
      isBinaryOpWithScalarInput(n);
}
bool isSingleInputGeneralCallFunction(Node* n) {
"
445,"}
local_used_work_ = process_group_->allreduce(local_used_maps_dev_);
torch::autograd::Engine::get_default_engine().queue_callback([=] {
std::unique_lock<std::mutex> lock(this->mutex_);
this->finalize_backward();
// Rebuild bucket if this is the first time to rebuild
if (!rebuilt_params_.empty()) {
","}
local_used_work_ = process_group_->allreduce(local_used_maps_dev_);
    // The autograd engine uses the default stream when running callbacks, so we
    // pass in the current CUDA stream in case it is not the default.
    c10::DeviceType deviceType = replica.contents.device().type();
    const c10::impl::VirtualGuardImpl guard =
        c10::impl::VirtualGuardImpl{deviceType};
    const c10::Stream currentStream =
        guard.getStream(replica.contents.device());
torch::autograd::Engine::get_default_engine().queue_callback([=] {
std::unique_lock<std::mutex> lock(this->mutex_);
      // Run callback with the current stream
      c10::OptionalStreamGuard currentStreamGuard{currentStream};
this->finalize_backward();
// Rebuild bucket if this is the first time to rebuild
if (!rebuilt_params_.empty()) {
"
446,"auto r = parser.parse(args, kwargs, parsed_args);
PyObject* cls = r.pyobject(0);
if (!PyType_Check(cls)) {
    throw TypeError(""cls must be a type (got %s)"", Py_TYPE(cls)->tp_name);
}
auto data = r.tensor(1).detach();
// We set `data`'s `allow_tensor_metadata_change` to true here, because we want to
","auto r = parser.parse(args, kwargs, parsed_args);
PyObject* cls = r.pyobject(0);
if (!PyType_Check(cls)) {
    throw torch::TypeError(""cls must be a type (got %s)"", Py_TYPE(cls)->tp_name);
}
auto data = r.tensor(1).detach();
// We set `data`'s `allow_tensor_metadata_change` to true here, because we want to
"
447,"static inline
at::Tensor extract_tensor(PyObject* obj) {
if (!THPVariable_Check(obj)) {
    throw TypeError(""expected Tensor (got %s)"", Py_TYPE(obj)->tp_name);
}
return ((THPVariable*)obj)->cdata;
}
","static inline
at::Tensor extract_tensor(PyObject* obj) {
if (!THPVariable_Check(obj)) {
    throw torch::TypeError(""expected Tensor (got %s)"", Py_TYPE(obj)->tp_name);
}
return ((THPVariable*)obj)->cdata;
}
"
448,"}
Tensor new_with_tensor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, const Tensor& other) {
  if (legacyExtractDispatchKey(other.key_set()) != dispatch_key) {
    // In temporary expression lifetime we trust
    throw TypeError(""expected %s (got %s)"", toString(dispatch_key), toString(other.key_set()).c_str());
  }
  if (other.scalar_type() != scalar_type) {
    throw TypeError(""expected %s (got %s)"", toString(scalar_type), toString(other.scalar_type()));
  }
return other.slice();
}
","}
Tensor new_with_tensor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, const Tensor& other) {
  TORCH_CHECK_TYPE(legacyExtractDispatchKey(other.key_set()) == dispatch_key, ""expected "",
                   toString(dispatch_key), "" (got "", toString(legacyExtractDispatchKey(other.key_set())), "")"");
  TORCH_CHECK_TYPE(other.scalar_type() == scalar_type, ""expected "",
                   toString(scalar_type), "" (got "", toString(other.scalar_type()), "")"");
return other.slice();
}
"
449,"torch::autograd::profiler::ProfilerConfig cfg =
torch::autograd::profiler::ProfilerConfig::fromIValue(tupleElements[1]);
// Create new message type and build wrapped RPC
rpc::Message wrappedMessage(
std::move(payload), std::move(tensors), wrappedMsgType, msgId);
","torch::autograd::profiler::ProfilerConfig cfg =
torch::autograd::profiler::ProfilerConfig::fromIValue(tupleElements[1]);
  rpc::ProfilingId profilerId = rpc::ProfilingId::fromIValue(tupleElements[2]);

// Create new message type and build wrapped RPC
rpc::Message wrappedMessage(
std::move(payload), std::move(tensors), wrappedMsgType, msgId);
"
450,"std::move(wrappedRpc),
wrappedMsgType,
std::move(wrappedMessage.tensors()),
      std::move(cfg));
}
} // namespace autograd
} // namespace distributed
","std::move(wrappedRpc),
wrappedMsgType,
std::move(wrappedMessage.tensors()),
      std::move(cfg),
      profilerId);
}
} // namespace autograd
} // namespace distributed
"
451,"module.def(""_set_profiler_node_id"", &at::RecordFunction::setDefaultNodeId);
Py_RETURN_TRUE;
}
","module.def(""_set_profiler_node_id"", &at::RecordFunction::setDefaultNodeId);
  py::class_<
      RemoteProfilerManager,
      std::unique_ptr<RemoteProfilerManager, py::nodelete>>(
      module, ""RemoteProfilerManager"")
      .def(""set_current_profiling_key"", [](const std::string& key) {
        auto& inst = RemoteProfilerManager::getInstance();
        inst.setCurrentKey(key);
      });

Py_RETURN_TRUE;
}
"
452,"Module& input_module,
const std::string& method_name,
bool inplace,
QuantType quant_type) {
Module module = input_module.clone(inplace);
InsertQuantDeQuantHelper h;
","Module& input_module,
const std::string& method_name,
bool inplace,
    bool debug,
QuantType quant_type) {
Module module = input_module.clone(inplace);
InsertQuantDeQuantHelper h;
"
453,"RegisterOperators reg_rpc_ops(
{Operator(
         ""aten::to_here(RRef(t) self) -> t"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
IValue res;
","RegisterOperators reg_rpc_ops(
{Operator(
         ""aten::to_here(RRef(t) self) -> t(*)"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
IValue res;
"
454,"{0,
2,
Symbol::fromQualString(""upgraders::_test_serialization_subcmul_0_2"")}},
});
Symbol get_symbol_for_version(const Symbol name, const uint64_t version) {
","{0,
2,
Symbol::fromQualString(""upgraders::_test_serialization_subcmul_0_2"")}},
    {Symbol::fromQualString(""aten::div""),
     {0, 3, Symbol::fromQualString(""upgraders::div_0_3"")}},
    {Symbol::fromQualString(""aten::div_""),
     {0, 3, Symbol::fromQualString(""upgraders::div__0_3"")}},
});
Symbol get_symbol_for_version(const Symbol name, const uint64_t version) {
"
455,"}
TORCH_CHECK(
gradient_edge.function,
        ""element "", i, "" of tensors does not require grad and does not have a grad_fn"",
        i);
roots.push_back(std::move(gradient_edge));
}
","}
TORCH_CHECK(
gradient_edge.function,
        ""element "", i, "" of tensors does not require grad and does not have a grad_fn"");
roots.push_back(std::move(gradient_edge));
}
"
456,"}
if (uprc.isAsyncExecution()) {
        processAsyncExecution(
            uprc.pythonUdf(),
            messageId,
            responseFuture,
            [ownerRRef, rrefId, forkId](
                const py::object& result,
                const int64_t messageId,
                PythonRpcHandler& pythonRpcHandler,
                const std::shared_ptr<FutureMessage>& responseFuture) {
              IValue py_ivalue = jit::toIValue(result, PyObjectType::get());

              py::gil_scoped_release release;
              ownerRRef->setValue(std::move(py_ivalue));
              auto m = RemoteRet(rrefId, forkId).toMessage();
              m.setId(messageId);
              responseFuture->markCompleted(std::move(m));
            });
} else {
IValue py_ivalue;
try {
","}
if (uprc.isAsyncExecution()) {
        try {
          processAsyncExecution(
              uprc.pythonUdf(),
              messageId,
              responseFuture,
              [ownerRRef, rrefId, forkId](
                  const py::object& result,
                  const int64_t messageId,
                  PythonRpcHandler& /* unused */,
                  const std::shared_ptr<FutureMessage>& responseFuture) {
                IValue py_ivalue = jit::toIValue(result, PyObjectType::get());

                py::gil_scoped_release release;
                ownerRRef->setValue(std::move(py_ivalue));
                auto m = RemoteRet(rrefId, forkId).toMessage();
                m.setId(messageId);
                responseFuture->markCompleted(std::move(m));
              });
        } catch (std::exception& e) {
          ownerRRef->setError(e.what());
          auto m = RemoteRet(rrefId, forkId).toMessage();
          m.setId(messageId);
          responseFuture->markCompleted(std::move(m));
        }
} else {
IValue py_ivalue;
try {
"
457,"""leaky_relu_"",
};
const float _asym_scale = 1.0f / 256.0f;
const int _asym_zero_point = 0;
const float _sym_scale = 2.0f / 256.0f;
","""leaky_relu_"",
};
std::vector<std::string> _clamp_funcs = {
    ""hardtanh"",
    ""hardtanh_"",
    ""clamp"",
    // ""clamp_"",  // Enable when quantized `clamp_` is ready
};

const float _asym_scale = 1.0f / 256.0f;
const int _asym_zero_point = 0;
const float _sym_scale = 2.0f / 256.0f;
"
458,"rewriter.runOnGraph(graph, filter);
}
void checkCalculateQParamsResult(const IValue& qparams) {
TORCH_CHECK(
qparams.isTuple(),
","rewriter.runOnGraph(graph, filter);
}
void ReplicateClampScalarArgs(std::shared_ptr<Graph>& graph) {
  std::stack<Block*> blocks_to_visit;
  std::unordered_set<Node*> scalar_nodes_to_rewrite;
  ;
  blocks_to_visit.push(graph->block());
  while (!blocks_to_visit.empty()) {
    Block* b = blocks_to_visit.top();
    blocks_to_visit.pop();
    for (Node* n : b->nodes()) {
      for (Value* output : n->outputs()) {
        if (getClampScalarInputUse(output) && output->uses().size() > 1) {
          scalar_nodes_to_rewrite.insert(n);
        }
      }
      for (Block* subblock : n->blocks()) {
        blocks_to_visit.push(subblock);
      }
    }
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    const std::vector<Use> uses = n->output()->uses();
    for (const auto& use : uses) {
      Node* user = use.user;
      WithInsertPoint ins(user);
      Node* cloned_node = graph->createClone(n, [](Value* v) { return v; });
      graph->insertNode(cloned_node);
      user->replaceInput(use.offset, cloned_node->output());
    }
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    n->removeAllInputs();
  }

  for (Node* n : scalar_nodes_to_rewrite) {
    n->destroy();
  }
}

void checkCalculateQParamsResult(const IValue& qparams) {
TORCH_CHECK(
qparams.isTuple(),
"
459,"insertion_point->append_stmt(
new Store(second_buf, second_indices, second_reduce, new IntImm(1)));
} else {
    For* new_for = new For(
        target_for->var(),
        target_for->start(),
        target_for->stop(),
        new Store(second_buf, second_indices, second_reduce, new IntImm(1)),
        target_for->loop_options());
if (insertion_point) {
      insertion_point->append_stmt(new_for);
} else {
      parent_block->append_stmt(new_for);
}
}
","insertion_point->append_stmt(
new Store(second_buf, second_indices, second_reduce, new IntImm(1)));
} else {
    Stmt* body_stmt =
        new Store(second_buf, second_indices, second_reduce, new IntImm(1));

    for (auto* il : output_loops) {
      body_stmt = il->cloneWithNewBody(body_stmt);
    }
if (insertion_point) {
      insertion_point->append_stmt(body_stmt);
} else {
      parent_block->insert_stmt_after(body_stmt, new_root_for);
}
}
"
460,"continue;
}
std::vector<uint8_t> workerNameVector =
        addressStore_->get(""names/"" + c10::to_string(workerId));
std::string workerName(
(char*)workerNameVector.data(), workerNameVector.size());
","continue;
}
std::vector<uint8_t> workerNameVector =
        rankToNameStore_.get(c10::to_string(workerId));
std::string workerName(
(char*)workerNameVector.data(), workerNameVector.size());
"
461,"}
TensorPipeAgent::TensorPipeAgent(
    std::shared_ptr<::c10d::Store> addressStore,
std::string selfName,
worker_id_t selfId,
int worldSize,
","}
TensorPipeAgent::TensorPipeAgent(
    const std::shared_ptr<::c10d::Store>& store,
std::string selfName,
worker_id_t selfId,
int worldSize,
"
462,"[this, &clientPipe, futureResponseMessage](
const tensorpipe::Error& error) mutable {
if (error) {
          LOG(WARNING) << ""client write error: "" << error.what();
markFutureWithError(std::move(futureResponseMessage), error.what());
return;
}
","[this, &clientPipe, futureResponseMessage](
const tensorpipe::Error& error) mutable {
if (error) {
          if (error.isOfType<tensorpipe::PipeClosedError>() &&
              !rpcAgentRunning_.load()) {
            // This is expected.
          } else {
            LOG(WARNING) << ""RPC agent for "" << workerInfo_.name_
                         << "" encountered error when writing outgoing request: ""
                         << error.what();
          }
markFutureWithError(std::move(futureResponseMessage), error.what());
return;
}
"
463,"//    std::cout << std::endl;
switch (inst.op) {
case OP: {
        if (auto debug_info = c10::ThreadLocalDebugInfo::get(
c10::DebugInfoKind::MOBILE_RUNTIME_INFO)) {
          if (auto* mobile_debug_info =
                  dynamic_cast<MobileDebugInfo*>(debug_info.get())) {
            mobile_debug_info->setOpIdx(pc);
}
}
// TODO(iliacher): remove the workaround after RecordFunction is in
// Dispatcher
bool prev_value = isRecordFunctionEnabled();
","//    std::cout << std::endl;
switch (inst.op) {
case OP: {
        if (at::hasGlobalCallbacks()) {
          if (auto debug_info = c10::ThreadLocalDebugInfo::get(
c10::DebugInfoKind::MOBILE_RUNTIME_INFO)) {
            if (auto* mobile_debug_info =
                dynamic_cast<MobileDebugInfo*>(debug_info.get())) {
              mobile_debug_info->setOpIdx(pc);
            }
}
}

// TODO(iliacher): remove the workaround after RecordFunction is in
// Dispatcher
bool prev_value = isRecordFunctionEnabled();
"
464,"// Don't need to hold lock while calling tensorpipe API.
lock.unlock();
  futureResponseMessage->futMsg.addCallback([this]() {
    // Decrease the callcount through a callback so it is only decremented once
    // per future.
    --clientActiveCalls_;
  });

pipeWrite(
clientPipe.pipe_,
std::move(requestMessage),
","// Don't need to hold lock while calling tensorpipe API.
lock.unlock();
pipeWrite(
clientPipe.pipe_,
std::move(requestMessage),
"
465,"auto it = connectedPipes_.find(toWorkerInfo.id_);
if (it == connectedPipes_.end()) {
std::tie(it, std::ignore) = connectedPipes_.emplace(
        toWorkerInfo.id_, ClientPipe(context_->connect(url)));
}
ClientPipe& clientPipe = it->second;
auto& pendingResponseMessage = clientPipe.pendingResponseMessage_;
","auto it = connectedPipes_.find(toWorkerInfo.id_);
if (it == connectedPipes_.end()) {
std::tie(it, std::ignore) = connectedPipes_.emplace(
        toWorkerInfo.id_,
        ClientPipe(context_->connect(
            url, tensorpipe::PipeOptions().name(toWorkerInfo.name_))));
}
ClientPipe& clientPipe = it->second;
auto& pendingResponseMessage = clientPipe.pendingResponseMessage_;
"
466,"auto param = *it;
auto def = param.defaultValue();
if (def.present()) {
default_types.emplace_back(param.type().get());
default_exprs.emplace_back(def.get());
}
","auto param = *it;
auto def = param.defaultValue();
if (def.present()) {
      if (!param.type().present()) {
        // We require explicit type-hints for default expressions.
        // If param doesn't have a type, we could default to ""Tensor"",
        // just like what happens in the Python frontend.
        // However here things are a bit more complicated, because
        // default expressions are evaluated using a custom-built
        // graph, and error messages coming out of that in case
        // the type doesn't match the value are quite obscure.
        throw ErrorReport(param.range())
            << ""Keyword arguments with defaults need to be type-hinted (TorchScript C++ frontend)"";
      }
default_types.emplace_back(param.type().get());
default_exprs.emplace_back(def.get());
}
"
467,"class SwapReduce : public IRMutator {
public:
  SwapReduce(ReduceOp* new_reduce) : new_reduce_(new_reduce) {}
Stmt* mutate(const Store* v) override {
    if (dynamic_cast<const ReduceOp*>(v->value())) {
      auto buf = new_reduce_->accumulator();
      return new Store(
          buf, new_reduce_->output_args(), new_reduce_, new IntImm(1));
}
return IRMutator::mutate(v);
}
private:
ReduceOp* new_reduce_;
};
","class SwapReduce : public IRMutator {
public:
  SwapReduce(const ReduceOp* old_reduce, ReduceOp* new_reduce)
      : old_reduce_(old_reduce), new_reduce_(new_reduce) {}
Stmt* mutate(const Store* v) override {
    if (const ReduceOp* op = dynamic_cast<const ReduceOp*>(v->value())) {
      if (op == old_reduce_) {
        auto buf = new_reduce_->accumulator();
        return new Store(
            buf, new_reduce_->output_args(), new_reduce_, new IntImm(1));
      }
}
return IRMutator::mutate(v);
}
private:
  const ReduceOp* old_reduce_;
ReduceOp* new_reduce_;
};
"
468,"""can't convert %s layout tensor to numpy.""
""convert the tensor to a strided layout first."", c10::str(tensor.layout()).c_str());
}
  if (tensor.requires_grad()) {
throw std::runtime_error(
        ""Can't call numpy() on Variable that requires grad. ""
        ""Use var.detach().numpy() instead."");
}
auto dtype = aten_to_numpy_dtype(tensor.scalar_type());
auto sizes = to_numpy_shape(tensor.sizes());
","""can't convert %s layout tensor to numpy.""
""convert the tensor to a strided layout first."", c10::str(tensor.layout()).c_str());
}
  if (at::GradMode::is_enabled() && tensor.requires_grad()) {
throw std::runtime_error(
        ""Can't call numpy() on Tensor that requires grad. ""
        ""Use tensor.detach().numpy() instead."");
}
auto dtype = aten_to_numpy_dtype(tensor.scalar_type());
auto sizes = to_numpy_shape(tensor.sizes());
"
469,"AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), ""pow"", [&]() {
cpu_kernel(iter,
[=](scalar_t base, scalar_t exp) -> scalar_t {
          return std::pow(base, exp);
}
);
});
","AT_DISPATCH_INTEGRAL_TYPES(iter.dtype(), ""pow"", [&]() {
cpu_kernel(iter,
[=](scalar_t base, scalar_t exp) -> scalar_t {
          return native::powi(base, exp);
}
);
});
"
470,"#endif
break;
default:
      invalidArgument(""unsupported device type"");
}
std::shared_ptr<AsyncBroadcastWork> work;
","#endif
break;
default:
      invalidArgument(c10::str(""unsupported device type "", device.type()));
}
std::shared_ptr<AsyncBroadcastWork> work;
"
471,"#endif
break;
default:
      invalidArgument(""unsupported device type"");
}
const auto& layout = inputs[0].layout();
","#endif
break;
default:
      invalidArgument(c10::str(""unsupported device type "", device.type()));
}
const auto& layout = inputs[0].layout();
"
472,"#endif
break;
default:
      invalidArgument(""unsupported device type"");
}
std::shared_ptr<AsyncAllgatherWork> work;
","#endif
break;
default:
      invalidArgument(c10::str(""unsupported device type "", device.type()));
}
std::shared_ptr<AsyncAllgatherWork> work;
"
473,"if (task.fn_ && !local_graph_task->has_error_.load()) {
AutoGradMode grad_mode(local_graph_task->grad_mode_);
try {
          engine_.evaluate_function(local_graph_task, task.fn_.get(), task.inputs_, cpu_ready_queue);
} catch (std::exception& e) {
engine_.thread_on_exception(local_graph_task, task.fn_, e);
// break the loop in error so that we immediately stop the execution
","if (task.fn_ && !local_graph_task->has_error_.load()) {
AutoGradMode grad_mode(local_graph_task->grad_mode_);
try {
          engine_.evaluate_function(
              local_graph_task, task.fn_.get(), task.inputs_, cpu_ready_queue);
} catch (std::exception& e) {
engine_.thread_on_exception(local_graph_task, task.fn_, e);
// break the loop in error so that we immediately stop the execution
"
474,"}
};
class ReductionExpander : public IRMutator {
 public:
  Stmt* expand(Stmt* s) {
    return s->accept_mutator(this);
  }

  Stmt* mutate(const For* v) override {
    Stmt* body_new = v->body()->accept_mutator(this);
    if (body_new == v->body()) {
      body_new = Stmt::clone(v->body());
    }

    Stmt* ret = v->cloneWithNewBody(body_new);

    for (size_t i = 0; i < initializers_.size();) {
      InitializerInfo& info = initializers_[i];

      auto end = std::remove(info.vars.begin(), info.vars.end(), v->var());
      if (end == info.vars.end()) {
        info.skipped_loops.push_back(v);
        i++;
        continue;
      }

      info.vars.erase(end);
      if (info.vars.empty()) {
        const ReduceOp* op = info.op;
        std::vector<const Expr*> indices(
            op->output_args().begin(), op->output_args().end());

        Stmt* init = new Store(
            op->accumulator(), indices, op->initializer(), new IntImm(1));

        for (auto it = info.skipped_loops.rbegin();
             it != info.skipped_loops.rend();
             it++) {
          const For* old_for = *it;
          init = old_for->cloneWithNewBody(init);
        }
        info.skipped_loops.clear();

        if (Block* b = dynamic_cast<Block*>(ret)) {
          b->prepend_stmt(init);
        } else {
          ret = new Block({init, ret});
        }
        initializers_.erase(initializers_.begin() + i);
        continue;
      }

      i++;
    }
    return ret;
  }

  const Expr* mutate(const ReduceOp* v) override {
    const std::vector<const Var*>& reduce_vars(v->reduce_args());
    initializers_.emplace_back(InitializerInfo(v, reduce_vars));
    return v->complete().node();
  }

 private:
  struct InitializerInfo {
    InitializerInfo(const ReduceOp* o, std::vector<const Var*> v)
        : op(o), vars(std::move(v)) {}
    const ReduceOp* op;
    std::vector<const Var*> vars;
    std::vector<const For*> skipped_loops;
  };

  std::vector<InitializerInfo> initializers_;
};

class Vectorizer : public IRMutator {
public:
Stmt* vectorize(const For* v) {
","}
};
class Vectorizer : public IRMutator {
public:
Stmt* vectorize(const For* v) {
"
475,"TORCH_API Dtype kHandle(ScalarType::Handle, 1);
TORCH_API Dtype kUninitialized(ScalarType::Uninitialized, 1);
Dtype ToDtype(ScalarType type) {
switch (type) {
","TORCH_API Dtype kHandle(ScalarType::Handle, 1);
TORCH_API Dtype kUninitialized(ScalarType::Uninitialized, 1);
TORCH_API Dtype kVoid(ScalarType::None, 1);
Dtype ToDtype(ScalarType type) {
switch (type) {
"
476,"return kHandle;
case ScalarType::Uninitialized:
return kUninitialized;
default:
throw unsupported_dtype();
}
","return kHandle;
case ScalarType::Uninitialized:
return kUninitialized;
    case ScalarType::None:
      return kVoid;
default:
throw unsupported_dtype();
}
"
477,"""_max_pool3d"",
""dropout"",
""relu"",
    ""relu_"",
    ""sigmoid"",
    ""tanh"",
""hardsigmoid"",
    ""hardsigmoid_"",
};
// Similar to prim::CallFunctions, there are aten ops that doesn't
","""_max_pool3d"",
""dropout"",
""relu"",
""hardsigmoid"",
};
// Similar to prim::CallFunctions, there are aten ops that doesn't
"
478,"}
bool isSingleInputGeneralValueAtenFunction(Node* n) {
  return isFunctionNode(
      n,
      /* call_funcs = */ {},
      /* aten_funcs = */ _single_input_general_value_aten_funcs);
}
bool isSingleInputGeneralCallFunction(Node* n) {
  static std::vector<std::string> _single_input_general_call_funcs;
std::copy(
_single_input_general_shape_call_funcs.begin(),
_single_input_general_shape_call_funcs.end(),
      std::back_inserter(_single_input_general_call_funcs));
std::copy(
_single_input_general_value_call_funcs.begin(),
_single_input_general_value_call_funcs.end(),
      std::back_inserter(_single_input_general_call_funcs));
return isFunctionNode(
n,
      /* call_funcs = */ _single_input_general_call_funcs,
/* aten_funcs = */ {});
}
bool isSingleInputGeneralAtenFunction(Node* n) {
  static std::vector<std::string> _single_input_general_aten_funcs;
  std::copy(
      _single_input_general_shape_aten_funcs.begin(),
      _single_input_general_shape_aten_funcs.end(),
      std::back_inserter(_single_input_general_aten_funcs));
  std::copy(
      _single_input_general_value_aten_funcs.begin(),
      _single_input_general_value_aten_funcs.end(),
      std::back_inserter(_single_input_general_aten_funcs));
  return isFunctionNode(
      n,
      /* call_funcs = */ {},
      /* aten_funcs = */ _single_input_general_aten_funcs);
}
bool userDefinedCallFunction(Node* n) {
","}
bool isSingleInputGeneralValueAtenFunction(Node* n) {
  return isAtenFunc(n, _single_input_general_value_aten_funcs);
}
bool isSingleInputGeneralCallFunction(Node* n) {
  static std::vector<std::string> single_input_general_call_funcs;
std::copy(
_single_input_general_shape_call_funcs.begin(),
_single_input_general_shape_call_funcs.end(),
      std::back_inserter(single_input_general_call_funcs));
std::copy(
_single_input_general_value_call_funcs.begin(),
_single_input_general_value_call_funcs.end(),
      std::back_inserter(single_input_general_call_funcs));
return isFunctionNode(
n,
      /* call_funcs = */ single_input_general_call_funcs,
/* aten_funcs = */ {});
}
bool isSingleInputGeneralAtenFunction(Node* n) {
  static std::vector<NodeKind> fixed_qparams_aten_funcs;
  std::transform(
      _fixed_qparams_map.begin(),
      _fixed_qparams_map.end(),
      std::back_inserter(fixed_qparams_aten_funcs),
      [](auto pair) { return pair.first; });

  return isAtenFunc(n, _single_input_general_shape_aten_funcs) ||
      isAtenFunc(n, _single_input_general_value_aten_funcs) ||
      isAtenFunc(n, fixed_qparams_aten_funcs);
}

c10::optional<std::tuple<c10::QScheme, QParamVector>> getFixedQParams(Node* n) {
  static std::vector<NodeKind> fixed_qparam_funcs;
  std::transform(
      _fixed_qparams_map.begin(),
      _fixed_qparams_map.end(),
      std::back_inserter(fixed_qparam_funcs),
      [](const auto& pair) { return pair.first; });
  if (isAtenFunc(n, fixed_qparam_funcs)) {
    return _fixed_qparams_map.at(n->kind());
  }
  return c10::nullopt;
}
bool userDefinedCallFunction(Node* n) {
"
479,"#endif
#ifdef TP_ENABLE_SHM
#include <tensorpipe/transport/shm/context.h>
#include <unistd.h>
#endif
#include <tensorpipe/transport/uv/context.h>
namespace torch {
namespace distributed {
namespace rpc {
","#endif
#ifdef TP_ENABLE_SHM
#include <tensorpipe/transport/shm/context.h>
#endif
#include <tensorpipe/transport/uv/context.h>
#include <arpa/inet.h>
#include <ifaddrs.h>
#include <netdb.h>
#include <netinet/in.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <unistd.h>

namespace torch {
namespace distributed {
namespace rpc {
"
480,"{ParallelType::TIDy, ""blockDim.y""},
{ParallelType::TIDx, ""blockDim.x""}};
static std::unordered_set<BinaryOpType> logical_binary_ops{BinaryOpType::And,
                                                           BinaryOpType::Eq,
                                                           BinaryOpType::GE,
                                                           BinaryOpType::GT,
                                                           BinaryOpType::LE,
                                                           BinaryOpType::LT,
                                                           BinaryOpType::NE};
bool is_logical_op(const BinaryOpType& bot) {
return logical_binary_ops.count(bot) > 0;
","{ParallelType::TIDy, ""blockDim.y""},
{ParallelType::TIDx, ""blockDim.x""}};
static std::unordered_set<BinaryOpType, _enum_class_hash<BinaryOpType>>
    logical_binary_ops{BinaryOpType::And,
                       BinaryOpType::Eq,
                       BinaryOpType::GE,
                       BinaryOpType::GT,
                       BinaryOpType::LE,
                       BinaryOpType::LT,
                       BinaryOpType::NE};
bool is_logical_op(const BinaryOpType& bot) {
return logical_binary_ops.count(bot) > 0;
"
481,"template <class Result, class... Args>
inline Result callOpUnboxed(const c10::OperatorHandle& op, Args... args) {
at::AutoNonVariableTypeMode non_var_type_mode(true);
return c10::Dispatcher::singleton().template callUnboxed<Result, Args...>(
op, std::forward<Args>(args)...);
}
","template <class Result, class... Args>
inline Result callOpUnboxed(const c10::OperatorHandle& op, Args... args) {
at::AutoNonVariableTypeMode non_var_type_mode(true);
  // Temporary hack: when the `Profiler` dispatch key is inserted, this call
  // will fail since the `unpack()` ops return multiple values, however the
  // boxing code currently does not support this. Instead, exclude the Profiler
  // dispatch key and go through unboxed dispatch, avoiding boxing altogether
  c10::impl::ExcludeDispatchKeyGuard key_guard(c10::DispatchKey::Profiler);
return c10::Dispatcher::singleton().template callUnboxed<Result, Args...>(
op, std::forward<Args>(args)...);
}
"
482,"std::move(loader),
version)) {}
TypePtr SourceImporter::loadNamedType(const QualifiedName& name) const {
  TypePtr t = pImpl->findNamedType(name);
  TORCH_INTERNAL_ASSERT(t != nullptr);
return t;
}
","std::move(loader),
version)) {}
TypePtr SourceImporter::loadType(const QualifiedName& name) const {
  ScriptTypeParser type_parser(pImpl);
  TypePtr t = type_parser.parseType(name.qualifiedName());
return t;
}
"
483,"// this node is safe to inline, so assign the output value
// to that expression directly
assignValue(node->output(), ss);
}
}
}
","// this node is safe to inline, so assign the output value
// to that expression directly
assignValue(node->output(), ss);
          if (isLongLine(ss->str())) {
            splitLongInlines(node->output());
          }
}
}
}
"
484,"// assigning a temporary variable
std::unordered_set<Node*> output_inline_;
// what valid identifiers are in use for the current function
std::unordered_set<std::string> used_names_;
","// assigning a temporary variable
std::unordered_set<Node*> output_inline_;
  // see [reordering of inlines]
  // used to track parts of an inline statement we already scanned
  // for splitting long lines, so that we do not revisit them causing n^2
  // behavior. stores the maximum offset into inputs that has already been
  // scanned for the node.
  std::unordered_map<Node*, int64_t> visited_split_inline_uses_;

// what valid identifiers are in use for the current function
std::unordered_set<std::string> used_names_;
"
485,"// Make node outputs a single tuple;
std::vector<TypePtr> types;
for (size_t i = 0; i < n->outputs().size(); ++i) {
          types.push_back(n->output(0)->type());
}
Value* tup_output = n->addOutput()->setType(TupleType::create(types));
Node* tup_unpack = g->createTupleUnpack(tup_output)->insertAfter(n);
","// Make node outputs a single tuple;
std::vector<TypePtr> types;
for (size_t i = 0; i < n->outputs().size(); ++i) {
          types.push_back(n->output(i)->type());
}
Value* tup_output = n->addOutput()->setType(TupleType::create(types));
Node* tup_unpack = g->createTupleUnpack(tup_output)->insertAfter(n);
"
486,"auto wrap_dim = maybe_wrap_dim(dim, self.dim());
int64_t self_dim_size = ensure_nonempty_size(self, wrap_dim);
  AT_DISPATCH_ALL_TYPES(self.scalar_type(), ""cumprod_out_cpu"", [&] {
cpu_cum_base_kernel<scalar_t>(result, self, wrap_dim, [&] (
scalar_t* result_data, auto result_dim_stride,
const scalar_t* self_data, auto self_dim_stride, scalar_t init_val) {
","auto wrap_dim = maybe_wrap_dim(dim, self.dim());
int64_t self_dim_size = ensure_nonempty_size(self, wrap_dim);
  AT_DISPATCH_ALL_TYPES_AND_COMPLEX(self.scalar_type(), ""cumprod_out_cpu"", [&] {
cpu_cum_base_kernel<scalar_t>(result, self, wrap_dim, [&] (
scalar_t* result_data, auto result_dim_stride,
const scalar_t* self_data, auto self_dim_stride, scalar_t init_val) {
"
487,"//   _0 = x.add_(b)
//   _1 = some_long + expression
//   r = foo(_0, _1)
  void splitLongInlines(at::ArrayRef<Value*> inputs) {
    size_t long_inline_slice = 0;
    // find the last input that is too long
    for (size_t i = 0; i < inputs.size(); ++i) {
      if (isLongInline(inputs[i]->node())) {
        long_inline_slice = i + 1;
      }
}
    // un-inline everything through the last long line
    // constants are ignored since long constants are never inlined in the
    // first place
    for (size_t i = 0; i < long_inline_slice; ++i) {
      if (isNonConstantInline(inputs[i])) {
        printOutputDefinition(inputs[i]->node(), *useOf(inputs[i]));
}
}
}
template <typename T>
","//   _0 = x.add_(b)
//   _1 = some_long + expression
//   r = foo(_0, _1)

  void splitLongInlines(Value* v) {
    std::vector<Value*> to_split_reversed;
    Use u = v->uses().at(0);
    scanLongInlines(u.user, u.offset, to_split_reversed);
    for (auto it = to_split_reversed.rbegin(), end = to_split_reversed.rend();
         it != end;
         ++it) {
      printOutputDefinition((*it)->node(), *useOf(*it));
}
  }

  void scanLongInlines(
      Node* user,
      int64_t offset,
      std::vector<Value*>& to_split_reversed) {
    auto it = visited_split_inline_uses_.find(user);
    bool present = it != visited_split_inline_uses_.end();
    for (int64_t i = offset; i >= (present ? it->second + 1 : 0); --i) {
      Value* prev_arg = user->input(i);
      if (isNonConstantInline(prev_arg)) {
        to_split_reversed.push_back(prev_arg);
}
}
    visited_split_inline_uses_[user] = offset;
    if (!present && output_inline_.count(user)) {
      Use u = user->output()->uses().at(0);
      scanLongInlines(u.user, int64_t(u.offset) - 1, to_split_reversed);
      // -1 because the actual use is still being
      // emitted so it cannot be split
    }
}
template <typename T>
"
488,"auto sizes = compute_sizes(data);
checkListInputType(elem_type, sizes.size() == 1 && sizes[0] == 0);
at::ScalarType initial_scalar_type = scalarTypeFromJitType(elem_type);
auto tensor =
at::empty(sizes, at::initialTensorOptions().dtype(initial_scalar_type));
","auto sizes = compute_sizes(data);
checkListInputType(elem_type, sizes.size() == 1 && sizes[0] == 0);
at::ScalarType initial_scalar_type = scalarTypeFromJitType(elem_type);
  if (initial_scalar_type == at::ScalarType::Double) {
    initial_scalar_type = typeMetaToScalarType(c10::get_default_dtype());
  }
auto tensor =
at::empty(sizes, at::initialTensorOptions().dtype(initial_scalar_type));
"
489,"AT_CUDA_CHECK(cudaGetDevice(&device));
if (!myPoolWindow)
    myPoolWindow.reset(pool.newPoolWindow());
auto handle = myPoolWindow->reserve(device);
cusparseSetStream(handle, c10::cuda::getCurrentCUDAStream());
","AT_CUDA_CHECK(cudaGetDevice(&device));
if (!myPoolWindow)
    myPoolWindow.reset(pool->newPoolWindow());
auto handle = myPoolWindow->reserve(device);
cusparseSetStream(handle, c10::cuda::getCurrentCUDAStream());
"
490,"namespace {

RegisterOperators reg(
    {
      Operator(
         ""prim::TupleUnpack(Any tup) -> ..."",
         [](Stack& stack) {
           tupleUnpack(stack);
           return 0;
         },
         aliasAnalysisSpecialCase()),
     Operator(
         ""prim::unchecked_cast(t x) -> t"",
         noop,
         aliasAnalysisSpecialCase()),
Operator(
""aten::format(str self, ...) -> str"",
[](Stack& stack) {
","namespace {
RegisterOperators reg({
    Operator(
        ""prim::TupleUnpack(Any tup) -> ..."",
        [](Stack& stack) {
          tupleUnpack(stack);
          return 0;
        },
        aliasAnalysisSpecialCase()),
    Operator(
        ""prim::unchecked_cast(t x) -> t"",
        noop,
        aliasAnalysisSpecialCase()),
Operator(
""aten::format(str self, ...) -> str"",
[](Stack& stack) {
"
491,"return unshapedType(type);
case TypeKind::OptionalType:
return getMutableType(type->cast<OptionalType>()->getElementType());
case TypeKind::FutureType: {
if (auto elem =
getMutableType(type->cast<FutureType>()->getElementType())) {
","return unshapedType(type);
case TypeKind::OptionalType:
return getMutableType(type->cast<OptionalType>()->getElementType());
      case TypeKind::AnyType:
        return type;
case TypeKind::FutureType: {
if (auto elem =
getMutableType(type->cast<FutureType>()->getElementType())) {
"
492,"llvm::FunctionType* fntype = llvm::FunctionType::get(retTy, params, false);
fn_ = llvm::Function::Create(
fntype, llvm::Function::PrivateLinkage, ""pytorch"", module_.get());
  for (int i = 0; i < args.size(); i++) {
if (!args[i].isVar()) {
fn_->addParamAttr(i, llvm::Attribute::NoAlias);
}
","llvm::FunctionType* fntype = llvm::FunctionType::get(retTy, params, false);
fn_ = llvm::Function::Create(
fntype, llvm::Function::PrivateLinkage, ""pytorch"", module_.get());
  for (size_t i = 0; i < args.size(); i++) {
if (!args[i].isVar()) {
fn_->addParamAttr(i, llvm::Attribute::NoAlias);
}
"
493,"std::vector<int64_t> padding,
std::vector<int64_t> dilation,
int64_t groups,
        c10::optional<double> output_min,
        c10::optional<double> output_max) {
return xnnpack::XNNPackConv2dOpContext::create_context(
std::move(weight),
std::move(bias),
","std::vector<int64_t> padding,
std::vector<int64_t> dilation,
int64_t groups,
        c10::optional<Scalar> output_min,
        c10::optional<Scalar> output_max) {
return xnnpack::XNNPackConv2dOpContext::create_context(
std::move(weight),
std::move(bias),
"
494,"},
c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
});
} // namespace jit
} // namespace torch
","},
c10::AliasAnalysisKind::INTERNAL_SPECIAL_CASE),
});
}
} // namespace jit
} // namespace torch
"
495,"for (auto n : nodes_to_delete_) {
n->destroy();
}
}
bool SubgraphRewriter::overlapsWithPreviousMatches(const Match* match) {
","for (auto n : nodes_to_delete_) {
n->destroy();
}
  nodes_to_delete_.clear();
}
bool SubgraphRewriter::overlapsWithPreviousMatches(const Match* match) {
"
496,"std::move(work)));
}
int ProcessGroupAgent::handleRecv(RecvWork& work) {
torch::Tensor& payload = work.payload_;
auto data = wireDeserialize(payload.storage().data(), payload.numel());
Message message(
","std::move(work)));
}
bool ProcessGroupAgent::handleRecv(RecvWork& work) {
torch::Tensor& payload = work.payload_;
auto data = wireDeserialize(payload.storage().data(), payload.numel());
Message message(
"
497,"std::lock_guard<std::mutex> lock{futureMutex_};
const auto& futureInfo = futures_.find(id);
if (futureInfo == futures_.end()) {
        // Received a completion for a timed out future, drop the recv.
        // RecvCounts will not be incremented here, it will be
        // incremented by the sender who has determined the future has
        // timed out.
        return 0;
}
// Use futureInfo before destructing it.
fm = futureInfo->second.future_;
","std::lock_guard<std::mutex> lock{futureMutex_};
const auto& futureInfo = futures_.find(id);
if (futureInfo == futures_.end()) {
        // Received a completion for an already-processed future (such as one
        // that timed out), drop the recv. By returning false, recvCounts will
        // not be incremented, it will be incremented by the thread that
        // determined that the future timed out.
        return false;
}
// Use futureInfo before destructing it.
fm = futureInfo->second.future_;
"
498,"return ""BackendSelect"";
case DispatchKey::TESTING_ONLY_GenericModeTensorId:
return ""TESTING_ONLY_GenericModeTensorId"";
    case DispatchKey::AutocastTensorId:
      return ""AutocastTensorId"";
case DispatchKey::TESTING_ONLY_GenericWrapperTensorId:
return ""TESTING_ONLY_GenericWrapperTensorId"";
default:
","return ""BackendSelect"";
case DispatchKey::TESTING_ONLY_GenericModeTensorId:
return ""TESTING_ONLY_GenericModeTensorId"";
case DispatchKey::TESTING_ONLY_GenericWrapperTensorId:
return ""TESTING_ONLY_GenericWrapperTensorId"";
default:
"
499,"namespace torch { namespace autograd {
static PyObject * set_autocast_enabled(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  if (!PyBool_Check(arg)) {
    throw TypeError(""enabled must be a bool (got %s)"", Py_TYPE(arg)->tp_name);
  }
  at::autocast::set_enabled(arg == Py_True);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

static PyObject * is_autocast_enabled(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  if (at::autocast::is_enabled()) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

static PyObject * clear_autocast_cache(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  at::autocast::clear_cache();
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

static PyObject * autocast_increment_nesting(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  return THPUtils_packInt64(at::autocast::increment_nesting());
  END_HANDLE_TH_ERRORS
}

static PyObject * autocast_decrement_nesting(PyObject* _unused, PyObject *arg) {
  HANDLE_TH_ERRORS
  return THPUtils_packInt64(at::autocast::decrement_nesting());
  END_HANDLE_TH_ERRORS
}

static PyObject * set_grad_enabled(PyObject* _unused, PyObject *arg) {
HANDLE_TH_ERRORS
if (!PyBool_Check(arg)) {
","namespace torch { namespace autograd {
static PyObject * set_grad_enabled(PyObject* _unused, PyObject *arg) {
HANDLE_TH_ERRORS
if (!PyBool_Check(arg)) {
"
500,"""Could not get buffer for asset '%s'"",
assetName->toStdString().c_str());
}
module_ = torch::jit::load(torch::make_unique<MemoryReadAdapter>(
assetBuffer, AAsset_getLength(asset)));
AAsset_close(asset);
","""Could not get buffer for asset '%s'"",
assetName->toStdString().c_str());
}
    JITCallGuard guard;
module_ = torch::jit::load(torch::make_unique<MemoryReadAdapter>(
assetBuffer, AAsset_getLength(asset)));
AAsset_close(asset);
"
501,"}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
module_ = torch::jit::_load_for_mobile(std::move(modelPath->toStdString()));
}
","}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
    LiteJITCallGuard guard;
module_ = torch::jit::_load_for_mobile(std::move(modelPath->toStdString()));
}
"
502,"}
auto output = [&]() {
      torch::autograd::AutoGradMode guard(false);
      at::AutoNonVariableTypeMode non_var_type_mode(true);
return module_.forward(inputs);
}();
return JIValue::newJIValueFromAtIValue(output);
","}
auto output = [&]() {
      LiteJITCallGuard guard;
return module_.forward(inputs);
}();
return JIValue::newJIValueFromAtIValue(output);
"
503,"// Map from Graph to observer node, we can use observer node to
// get the information of original value that's been observed and
// the quantization parameters
  std::unordered_map<Graph*, std::vector<Node*>> observer_nodes_;
// Record qscheme for every graph, this is for checking
// each graph is only quantized with one type of QScheme
std::unordered_map<Graph*, c10::QScheme> qscheme_for_graph_;
","// Map from Graph to observer node, we can use observer node to
// get the information of original value that's been observed and
// the quantization parameters
  std::unordered_map<Graph*, std::vector<Node*>> observer_nodes_for_graph_;
  // A map from qparam name (e.g. _scale) to the attribute name in
  // the module(e.g. weight_scale_0)
  std::unordered_map<Node*, std::unordered_map<std::string, std::string>> qparam_name_map_for_node_;
// Record qscheme for every graph, this is for checking
// each graph is only quantized with one type of QScheme
std::unordered_map<Graph*, c10::QScheme> qscheme_for_graph_;
"
504,"get_gradients(context_id: int) -> Dict[Tensor, Tensor]
Retrieves a map from Tensor to the appropriate gradient for that Tensor
accumulated in the provided ``context_id`` as part of the distributed autograd
backward pass.
Arguments:
context_id(int): The autograd context id for which we should retrieve the
","get_gradients(context_id: int) -> Dict[Tensor, Tensor]
Retrieves a map from Tensor to the appropriate gradient for that Tensor
accumulated in the provided context corresponding to the given ``context_id``
as part of the distributed autograd backward pass.
Arguments:
context_id(int): The autograd context id for which we should retrieve the
"
505,"}
};
// NOTE [ Convolution design ]
//
// cuDNN convolutions does not handle bias. Bias is handled outside.
","}
};
inline Tensor allocate_workspace(size_t size, const Tensor &other) {
  // Sometimes cuDNN returns a workspace size > 2^63, this could makes the allocation of
  // workspace fail with some 64bit indexing error instead of an OOM error. In such case,
  // we manually fail with OOM.
  TORCH_CHECK_WITH(CUDAOutOfMemoryError, size < 1_TiB, ""Not enough memory for workspace!"");
  return at::empty({static_cast<int64_t>(size)}, other.options().dtype(kByte));
}

// NOTE [ Convolution design ]
//
// cuDNN convolutions does not handle bias. Bias is handled outside.
"
506,"} else {
tasks.no_id();
}
      } catch (const std::exception&) {
}
// Update status of empty, maybe
","} else {
tasks.no_id();
}
      } catch (const std::exception& e) {
        LOG(ERROR) << ""Exception in thread pool task: "" << e.what();
      } catch (...) {
        LOG(ERROR) << ""Exception in thread pool task: unknown"";
}
// Update status of empty, maybe
"
507,"Tensor new_with_tensor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, const Tensor& other) {
if (legacyExtractDispatchKey(other.key_set()) != dispatch_key) {
// In temporary expression lifetime we trust
    throw TypeError(""expected %s (got %s)"", dispatch_key, toString(other.key_set()).c_str());
}
if (other.scalar_type() != scalar_type) {
throw TypeError(""expected %s (got %s)"", toString(scalar_type), toString(other.scalar_type()));
","Tensor new_with_tensor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, const Tensor& other) {
if (legacyExtractDispatchKey(other.key_set()) != dispatch_key) {
// In temporary expression lifetime we trust
    throw TypeError(""expected %s (got %s)"", toString(dispatch_key), toString(other.key_set()).c_str());
}
if (other.scalar_type() != scalar_type) {
throw TypeError(""expected %s (got %s)"", toString(scalar_type), toString(other.scalar_type()));
"
508,"${th_headers}
${extra_cuda_headers}
namespace at {
namespace native {
namespace legacy {
","${th_headers}
${extra_cuda_headers}
namespace {
static const char* named_tensors_unsupported_error =
  "" is not yet supported with named tensors. Please drop names via ""
  ""`tensor = tensor.rename(None)`, call the op with an unnamed tensor, ""
  ""and set names on the result of the operation."";
}

namespace at {
namespace native {
namespace legacy {
"
509,"return;
}
  if (!mutableType(from)) {
    TORCH_INTERNAL_ASSERT(!mutableType(to));
return;
}
  if (from == to) {
return;
}
  // Special case: if `from` is an optional, `to` could be a None. Don't
  // create a pointer in that case
  if (from->type()->kind() == TypeKind::OptionalType &&
      to->type()->kind() == TypeKind::NoneType) {
return;
}
  // At this point, we should be dealing with two mutable types.
  TORCH_INTERNAL_ASSERT(mutableType(from) && mutableType(to));

auto fromEl = getOrCreateElement(from);
auto toEl = getOrCreateElement(to);
","return;
}
  // covariant type containers can be point to types which are not
  // also mutable/immutable because we unify the contained types
  if (mutableType(from) != mutableType(to)) {
    auto from_kind = from->type()->kind();
    TORCH_INTERNAL_ASSERT(
        from_kind == TypeKind::OptionalType ||
        from_kind == TypeKind::FutureType || from_kind == TypeKind::TupleType);
return;
}
  // both immutable
  if (!mutableType(from)) {
return;
}
  if (from == to) {
return;
}
  // At this point, we are dealing with two mutable types.
auto fromEl = getOrCreateElement(from);
auto toEl = getOrCreateElement(to);
"
510,"namespace torch { namespace autograd {
// Threads spawned by the engine are assigned a constant 'worker_device'
// specifying what device they process work for.  This variable is initialized
// at thread creation time and is constant afterwards.  This is used when
","namespace torch { namespace autograd {
namespace {
static bool in_bad_autograd_fork =
    false; // True for children forked after engine's thread pool init

// Called in the forked child if engine's thread pool has already been
// initialized
static void forked_autograd_child() { in_bad_autograd_fork = true; }

// Should be called before unsafe for forks (thread pool) calls
static void track_bad_autograd_forks() {
#ifndef WIN32
  static std::once_flag flag;
  std::call_once(
      flag, [&] { pthread_atfork(nullptr, nullptr, forked_autograd_child); });
#endif
}
}

// Threads spawned by the engine are assigned a constant 'worker_device'
// specifying what device they process work for.  This variable is initialized
// at thread creation time and is constant afterwards.  This is used when
"
511,"free(z_standard);
free(z_optimized);
free(x);
}
void test_THFloatVector_cdiv_VSX()
","free(z_standard);
free(z_optimized);
free(x);
    free(y);
}
void test_THFloatVector_cdiv_VSX()
"
512,"};
// the simple vm that scans instructions_ has a limited stack depth,
// this prevents going deeper than that.
  if (depth >= DEPTH_LIMIT) {
instructions_.emplace_back(SKIP);
}
if (typ->isSubtypeOf(TensorType::get())) {
","};
// the simple vm that scans instructions_ has a limited stack depth,
// this prevents going deeper than that.
  if (depth >= ARG_SPEC_DEPTH_LIMIT) {
instructions_.emplace_back(SKIP);
}
if (typ->isSubtypeOf(TensorType::get())) {
"
513,"// Only retry when the file doesn't exist, since we are waiting for the
// file to be created in this case to address the following issue:
// https://github.com/pytorch/pytorch/issues/13750
      if (fd_ >= 0 || (fd_ < 0 && errno != ENOENT)) {
break;
}
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
","// Only retry when the file doesn't exist, since we are waiting for the
// file to be created in this case to address the following issue:
// https://github.com/pytorch/pytorch/issues/13750
      if (fd_ >= 0 || errno != ENOENT) {
break;
}
const auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(
"
514,"whenValueSet->addCallback(
[responseFuture, messageId, rref](
const rpc::Message& /* unused */,
              const c10::optional<utils::FutureError>& /* unused */) {
            Message m = ScriptRRefFetchRet({rref->getValue()}).toMessage();
            m.setId(messageId);
            responseFuture->markCompleted(m);
});
return responseFuture;
}
","whenValueSet->addCallback(
[responseFuture, messageId, rref](
const rpc::Message& /* unused */,
              const c10::optional<utils::FutureError>& error) {
            if (!error) {
              Message m = ScriptRRefFetchRet({rref->getValue()}).toMessage();
              m.setId(messageId);
              responseFuture->markCompleted(std::move(m));
            } else {
              responseFuture->setError(error->what());
            }
});
return responseFuture;
}
"
515,"bool keep_graph,
bool create_graph,
const edge_list& outputs) {
try {
return Engine::execute(roots, inputs, keep_graph, create_graph, outputs);
} catch (python_error& e) {
","bool keep_graph,
bool create_graph,
const edge_list& outputs) {
  TORCH_CHECK(!PyGILState_Check(), ""The autograd engine was called while holding the GIL. If you are using the C++ ""
                                   ""API, the autograd engine is an expensive operation that does not require the ""
                                   ""GIL to be held so you should release it with 'pybind11::gil_scoped_release no_gil;'""
                                   "". If you are not using the C++ API, please report a bug to the pytorch team."")
try {
return Engine::execute(roots, inputs, keep_graph, create_graph, outputs);
} catch (python_error& e) {
"
516,"return self_working_copy;
}
// Supports arbitrary batch dimensions for self and LU_data (implicity LU_pivots also)
Tensor lu_solve(const Tensor& self, const Tensor& LU_data, const Tensor& LU_pivots) {
TORCH_CHECK(self.dim() >= 2,
""b should have at least 2 dimensions, but has "", self.dim(), "" dimensions instead"");
","return self_working_copy;
}
// Supports arbitrary batch dimensions for self and LU_data (implicitly LU_pivots also)
Tensor lu_solve(const Tensor& self, const Tensor& LU_data, const Tensor& LU_pivots) {
TORCH_CHECK(self.dim() >= 2,
""b should have at least 2 dimensions, but has "", self.dim(), "" dimensions instead"");
"
517,"}
return result;
#else // USE_FBGEMM
  TORCH_INTERNAL_ASSERT(false, ""Tried to use quantized RNN wihtout FBGEMM!"")
#endif // USE_FBGEMM
}
","}
return result;
#else // USE_FBGEMM
  TORCH_INTERNAL_ASSERT(false, ""Tried to use quantized RNN without FBGEMM!"")
#endif // USE_FBGEMM
}
"
518,"self - sparse tensor, its shape is sizes = sparse_shape + dense_shape
indices - 2-D tensor of indices, shape is (sparse_dims, nnz)
values - (1+len(dense_shape))-D tensor of values, shape is (nnz,) + dense_shape
    index_select(dim, index) returns a sparse tensor with the follwing data
new_sizes = sizes[:dim] + (n,) + sizes[dim+1:]
new_indices - shape is (sparse_dims, new_nnz)
new_values - shape is (new_nnz,) + dense_shape
","self - sparse tensor, its shape is sizes = sparse_shape + dense_shape
indices - 2-D tensor of indices, shape is (sparse_dims, nnz)
values - (1+len(dense_shape))-D tensor of values, shape is (nnz,) + dense_shape
    index_select(dim, index) returns a sparse tensor with the following data
new_sizes = sizes[:dim] + (n,) + sizes[dim+1:]
new_indices - shape is (sparse_dims, new_nnz)
new_values - shape is (new_nnz,) + dense_shape
"
519,"*      `apply_fn` will be called multiple times, and together cover the entire
*      output spatial space.
*
 *  Now you should be able tp understand everything about the implementaion of
*  2D forward kernel shown at the beginning of this note.
*
**/
","*      `apply_fn` will be called multiple times, and together cover the entire
*      output spatial space.
*
 *  Now you should be able tp understand everything about the implementation of
*  2D forward kernel shown at the beginning of this note.
*
**/
"
520,"//   1. if this dim idx becomes 1, will need to add (size - 1) * stride
//   2. otherwise, will need to subtract stride
if (from_slice_indices[d] == 0) {
          // Substract. Carries over to previous dimension
from_slice_data -= output.stride(d);
} else if (from_slice_indices[d] == 1) {
// Dimension index becomes 1
","//   1. if this dim idx becomes 1, will need to add (size - 1) * stride
//   2. otherwise, will need to subtract stride
if (from_slice_indices[d] == 0) {
          // Subtract. Carries over to previous dimension
from_slice_data -= output.stride(d);
} else if (from_slice_indices[d] == 1) {
// Dimension index becomes 1
"
521,"* in_len: the dimension_size of input matrix
* Basically, in_len / out_len gives the number of
* elements in each average computation.
   * This functin computes the start index on input matrix.
*/
return (int)std::floor((float)(out_idx * in_len) / out_len);
}
","* in_len: the dimension_size of input matrix
* Basically, in_len / out_len gives the number of
* elements in each average computation.
   * This function computes the start index on input matrix.
*/
return (int)std::floor((float)(out_idx * in_len) / out_len);
}
"
522,"// functions.
{
std::lock_guard<std::mutex> guard(initializedContextIdsLock_);
    // Context should not have been intialized already.
TORCH_INTERNAL_ASSERT(
initializedContextIds_.find(autogradContext->contextId()) ==
initializedContextIds_.end());
","// functions.
{
std::lock_guard<std::mutex> guard(initializedContextIdsLock_);
    // Context should not have been initialized already.
TORCH_INTERNAL_ASSERT(
initializedContextIds_.find(autogradContext->contextId()) ==
initializedContextIds_.end());
"
523,"// against torch.cuda.FloatTensor, this will immediately initialize CUDA.
// I originally thought that it would not be possible for aten_type_ to
// be nullptr if you had a tensor of some type, in which case you can
    // skip initializign aten_type(), but TestAutograd.test_type_conversions
// seems to violate this property (for whatever reason.)
//
// TODO: Stop using legacyExtractDispatchKey here (probably need to build
","// against torch.cuda.FloatTensor, this will immediately initialize CUDA.
// I originally thought that it would not be possible for aten_type_ to
// be nullptr if you had a tensor of some type, in which case you can
    // skip initializing aten_type(), but TestAutograd.test_type_conversions
// seems to violate this property (for whatever reason.)
//
// TODO: Stop using legacyExtractDispatchKey here (probably need to build
"
