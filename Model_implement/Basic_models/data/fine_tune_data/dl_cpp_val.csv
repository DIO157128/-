,source,target
0,"at::empty_like(input, c10::TensorOptions().dtype(result_type))
);
}
  at::Tensor input_reshaped = input.view({1, M, -1});
// Unlike Batch Normalization, which applies scalar scale and bias for each
// entire channel/plane with the affine option, Layer Normalization applies
// per-element scale and bias. E.g. For input {N, C, H, W}, weight for
","at::empty_like(input, c10::TensorOptions().dtype(result_type))
);
}
  at::Tensor input_reshaped = input.reshape({1, M, -1});
// Unlike Batch Normalization, which applies scalar scale and bias for each
// entire channel/plane with the affine option, Layer Normalization applies
// per-element scale and bias. E.g. For input {N, C, H, W}, weight for
"
1,"m.impl(""stack"", stack_batching_rule);
// still legacy b/c needs special inplace rules
m.impl(""squeeze_.dim"", squeeze_dim__batching_rule);
m.impl(""unsqueeze_"", unsqueeze__batching_rule);
m.impl(""transpose_"", transpose__batching_rule);
","m.impl(""stack"", stack_batching_rule);
// still legacy b/c needs special inplace rules
  m.impl(""squeeze_"", squeeze__batching_rule);
m.impl(""squeeze_.dim"", squeeze_dim__batching_rule);
m.impl(""unsqueeze_"", unsqueeze__batching_rule);
m.impl(""transpose_"", transpose__batching_rule);
"
2,"int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Helper to convert 1d tensors or scalar tensor to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1), strides(input_ndim, 0);
  auto as_nd = [&](const Tensor& t) {
    TORCH_INTERNAL_ASSERT(t.defined() && (t.dim() == 1 || t.dim() == 0));
    sizes[1] = t.dim() == 1 ? t.sizes()[0] : 1;
    strides[1] = t.dim() == 1 ? t.strides()[0] : 0;
    return t.as_strided(sizes, strides);
  };

  auto qw_nd = as_nd(qw);
auto iter = TensorIteratorConfig()
.add_output(out)
","int64_t input_ndim = qx.dim();
TORCH_CHECK(input_ndim > 0, ""qprelu: zero-dim input tensor is not allowed."");
  // Weight should be a 1d or scalar tensor
  // Reshape it to an nd tensor that broadcasts with input
// All elements go into the channel dimension
  DimVector sizes(input_ndim, 1);
  if (input_ndim > 1) {
    sizes[1] = qw.numel();
  }
  auto qw_nd = qw.reshape(sizes);
auto iter = TensorIteratorConfig()
.add_output(out)
"
3,"OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
","OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
  OP_DECOMPOSE(inverse);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
"
4,"OP_DECOMPOSE2(less, Tensor );
OP_DECOMPOSE(linalg_cond);
OP_DECOMPOSE(linalg_det);
OP_DECOMPOSE(linalg_matmul);
OP_DECOMPOSE(linalg_multi_dot);
OP_DECOMPOSE(linalg_svd);
OP_DECOMPOSE(linalg_svdvals);
OP_DECOMPOSE(matmul);
OP_DECOMPOSE(matrix_H);
OP_DECOMPOSE2(max, other );
","OP_DECOMPOSE2(less, Tensor );
OP_DECOMPOSE(linalg_cond);
OP_DECOMPOSE(linalg_det);
  OP_DECOMPOSE(linalg_inv);
OP_DECOMPOSE(linalg_matmul);
OP_DECOMPOSE(linalg_multi_dot);
OP_DECOMPOSE(linalg_svd);
OP_DECOMPOSE(linalg_svdvals);
  OP_DECOMPOSE(_lu_with_info);
OP_DECOMPOSE(matmul);
OP_DECOMPOSE(matrix_H);
OP_DECOMPOSE2(max, other );
"
5,"OP_DECOMPOSE2(less, Tensor );
OP_DECOMPOSE(linalg_cond);
OP_DECOMPOSE(linalg_det);
  OP_DECOMPOSE(linalg_inv);
OP_DECOMPOSE(linalg_matmul);
OP_DECOMPOSE(linalg_multi_dot);
OP_DECOMPOSE(linalg_svd);
OP_DECOMPOSE(linalg_svdvals);
  OP_DECOMPOSE(_lu_with_info);
OP_DECOMPOSE(matmul);
OP_DECOMPOSE(matrix_H);
OP_DECOMPOSE2(max, other );
","OP_DECOMPOSE2(less, Tensor );
OP_DECOMPOSE(linalg_cond);
OP_DECOMPOSE(linalg_det);
OP_DECOMPOSE(linalg_matmul);
OP_DECOMPOSE(linalg_multi_dot);
OP_DECOMPOSE(linalg_svd);
OP_DECOMPOSE(linalg_svdvals);
OP_DECOMPOSE(matmul);
OP_DECOMPOSE(matrix_H);
OP_DECOMPOSE2(max, other );
"
6,"namespace at { namespace functorch {
typedef std::tuple<Tensor, optional<int64_t>> oneOutput;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>> twoOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> threeOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> fourOutputs;

// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
","namespace at { namespace functorch {
// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
"
7,"OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
","OP_DECOMPOSE(index_select_backward);
OP_DECOMPOSE(inner);
OP_DECOMPOSE(instance_norm);
  OP_DECOMPOSE(inverse);
OP_DECOMPOSE(kron);
OP_DECOMPOSE(l1_loss);
OP_DECOMPOSE(layer_norm);
"
8,"namespace at { namespace functorch {
// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
","namespace at { namespace functorch {
typedef std::tuple<Tensor, optional<int64_t>> oneOutput;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>> twoOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> threeOutputs;
typedef std::tuple<Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>, Tensor, optional<int64_t>> fourOutputs;

// Note [Batching rules for matmul-like operators]
// at::matmul doesn't ""de-expand"" arguments to get better performance (maybe
// it should). In the batching rules for matmul-like operators (dot, mv, mm),
"
9,"c10::optional<Layout> layout,
c10::optional<Device> device,
c10::optional<bool> pin_memory) {
    return zeros(asIntArrayRefSlow(size), dtype, layout, device, pin_memory);
}
Tensor _efficientzerotensor(IntArrayRef size,
","c10::optional<Layout> layout,
c10::optional<Device> device,
c10::optional<bool> pin_memory) {
    return at::zeros(asIntArrayRefSlow(size), dtype, layout, device, pin_memory);
}
Tensor _efficientzerotensor(IntArrayRef size,
"
10,"}
}
bool isFunctionalTensor(const c10::List<Tensor>& t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
  auto nonfunctional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].defined()) continue;
if (isFunctionalTensor(t_list[i])) {
    } else {
      ++nonfunctional_count;
}
}
  TORCH_INTERNAL_ASSERT(
       functional_count == 0 || nonfunctional_count == 0,
      ""Functionalization encountered a list of tensors where some are functional"",
      ""and some are not, which is not currently unsupported."");
return functional_count > 0;
}
bool isFunctionalTensor(const c10::List<c10::optional<Tensor>>& t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
  auto nonfunctional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].has_value() || !t_list[i]->defined()) continue;
if (isFunctionalTensor(t_list[i])) {
    } else {
      ++nonfunctional_count;
}
}
  TORCH_INTERNAL_ASSERT(
       functional_count == 0 || nonfunctional_count == 0,
      ""Functionalization encountered a list of tensors where some are functional"",
      ""and some are not, which is not currently unsupported."");
return functional_count > 0;
}
bool isFunctionalTensor(const c10::ArrayRef<Tensor> t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
  auto nonfunctional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].defined()) continue;
if (isFunctionalTensor(t_list[i])) {
    } else {
      ++nonfunctional_count;
}
}
  TORCH_INTERNAL_ASSERT(
       functional_count == 0 || nonfunctional_count == 0,
      ""Functionalization encountered a list of tensors where some are functional"",
      ""and some are not, which is not currently unsupported."");
return functional_count > 0;
}
","}
}
// For lists that have a mix of functional and nonfunctional tensors,
// functionalization machinery should just unwrap the functional wrappers
// and leave the ordinary tensors alone.
bool isFunctionalTensor(const c10::List<Tensor>& t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].defined()) continue;
if (isFunctionalTensor(t_list[i])) {
+functional_count;
}
}
return functional_count > 0;
}
bool isFunctionalTensor(const c10::List<c10::optional<Tensor>>& t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].has_value() || !t_list[i]->defined()) continue;
if (isFunctionalTensor(t_list[i])) {
+functional_count;
}
}
return functional_count > 0;
}
bool isFunctionalTensor(const c10::ArrayRef<Tensor> t_list) {
if (t_list.size() == 0) return false;
auto functional_count = 0;
for (const auto i : c10::irange(t_list.size())) {
if (!t_list[i].defined()) continue;
if (isFunctionalTensor(t_list[i])) {
+functional_count;
}
}
return functional_count > 0;
}
"
11,"""Vulkan binary elementwise ops require channel to be a multiple of 4 to broadcast along batch dimension!"")
}
  const uint32_t input1_h = height_size(input1);
  const uint32_t input1_w = width_size(input1);
  const uint32_t input2_h = height_size(input2);
  const uint32_t input2_w = width_size(input2);

const std::string broadcast_error_msg =
""Incompatible input dimensions for broadcasting for Vulkan binary elementwise op!"";
  if (input1_h != input2_h) {
    if (input1_h > input2_h) {
      TORCH_CHECK(input2_h == 1, broadcast_error_msg);
      TORCH_CHECK(input2_w == input1_w || input2_w == 1, broadcast_error_msg);
    } else if (input2_h > input1_h) {
      TORCH_CHECK(input1_h == 1, broadcast_error_msg);
      TORCH_CHECK(input1_w == input2_w || input1_w == 1, broadcast_error_msg);
}
  } else if (input1_w != input2_w) {
    if (input1_w > input2_w) {
      TORCH_CHECK(input2_w == 1, broadcast_error_msg);
    } else if (input2_w > input1_w) {
      TORCH_CHECK(input1_h == 1, broadcast_error_msg);
}
}
}
bool broadcast_first_input(const vTensor& input1, const vTensor& input2) {
  return (
      (input2.extents().data[1u] > 1 && input1.extents().data[1u] == 1) ||
      (input2.extents().data[2u] > 1 && input1.extents().data[2u] == 1) ||
      input2.extents().data[0u] > input1.extents().data[0u]);
}
} // namespace
using namespace api::utils;
","""Vulkan binary elementwise ops require channel to be a multiple of 4 to broadcast along batch dimension!"")
}
const std::string broadcast_error_msg =
""Incompatible input dimensions for broadcasting for Vulkan binary elementwise op!"";

  TORCH_CHECK(broadcast_input(input1, input2), broadcast_error_msg);
}

std::vector<int64_t> broadcast_size(
    const Tensor& input1,
    const Tensor& input2) {
  std::vector<int64_t> out = {};
  int input1_size = input1.sizes().size();
  int input2_size = input2.sizes().size();
  if (input1_size > input2_size) {
    for (int i = 0; i < input1_size; i++) {
      out.push_back(input1.sizes()[i]);
}
  } else {
    for (int i = 0; i < input2_size; i++) {
      out.push_back(input2.sizes()[i]);
    }
  }

  if (width_size(input1) > 1 && width_size(input2) == 1) {
    out[out.size() - 1] = width_size(input1);
  } else if (width_size(input2) > 1 && width_size(input1) == 1) {
    out[out.size() - 1] = width_size(input2);
  }

  if (out.size() > 1) {
    if (height_size(input1) > 1 && height_size(input2) == 1) {
      out[out.size() - 2] = height_size(input1);
    } else if (height_size(input2) > 1 && height_size(input1) == 1) {
      out[out.size() - 2] = height_size(input2);
}
}
  return out;
}
} // namespace
using namespace api::utils;
"
12,"TORCH_SELECTIVE_SCHEMA(""aten::sym_size(Tensor self) -> SymInt[]""),
sym_size,
aliasAnalysisFromSchema()),
OperatorGeneratorArgs(
TORCH_SELECTIVE_SCHEMA(""prim::EnumName(AnyEnumType enum) -> str""),
[](Stack& stack) {
","TORCH_SELECTIVE_SCHEMA(""aten::sym_size(Tensor self) -> SymInt[]""),
sym_size,
aliasAnalysisFromSchema()),
    OperatorGeneratorArgs(
        TORCH_SELECTIVE_SCHEMA(""aten::stride(Tensor self) -> int[]""),
        [](Stack& stack) {
          at::Tensor arg = pop(stack).toTensor();
          push(stack, arg.strides());
        },
        aliasAnalysisFromSchema()),
OperatorGeneratorArgs(
TORCH_SELECTIVE_SCHEMA(""prim::EnumName(AnyEnumType enum) -> str""),
[](Stack& stack) {
"
13,"std::unordered_map<std::string, IValue> value_map;
for (const auto& key_pair : values) {
IValue key = toTypeInferredIValue(key_pair.first);
IValue value = toTypeInferredIValue(key_pair.second);
TORCH_INTERNAL_ASSERT(
key.isString(),
","std::unordered_map<std::string, IValue> value_map;
for (const auto& key_pair : values) {
IValue key = toTypeInferredIValue(key_pair.first);
          if (isEmptyContainer(key_pair.second)) {
            continue;
          }
IValue value = toTypeInferredIValue(key_pair.second);
TORCH_INTERNAL_ASSERT(
key.isString(),
"
14,"at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
} // namespace utils
} // namespace torch
#else
","at::Tensor tensor_from_cuda_array_interface(PyObject* obj) {
throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}

void warn_numpy_not_writeable() {
  throw std::runtime_error(""PyTorch was compiled without NumPy support"");
}
} // namespace utils
} // namespace torch
#else
"
15,"optional<int64_t> grad_normalized_input_bdim;
std::tie(grad_normalized_input_value, grad_normalized_input_bdim) =
unwrapTensorAtLevel(grad_normalized_input, cur_level);
    auto grad_out_ = moveBatchDimToFront(grad_normalized_input_value, grad_normalized_input_bdim);
    auto input_ = moveBatchDimToFront(input_value, input_bdim);
    auto mean_ = moveBatchDimToFront(mean_value, mean_bdim);
    auto rstd_ = moveBatchDimToFront(rstd_value, rstd_bdim);

    const auto bdim_size = get_bdim_size3(grad_out_, grad_out_bdim, input_, input_bdim, weight, weight_bdim);
    grad_out_ = ensure_has_bdim(grad_out_, grad_out_bdim.has_value(), bdim_size);
    input_ = ensure_has_bdim(input_, input_bdim.has_value(), bdim_size);
    mean_ = ensure_has_bdim(mean_, mean_bdim.has_value(), bdim_size);
    rstd_ = ensure_has_bdim(rstd_, rstd_bdim.has_value(), bdim_size);

    grad_out_ = reshape_dim_into(0, 0, grad_out_); // [B0 * N, C, *]
    input_ = reshape_dim_into(0, 0, input_);       // [B0 * N, C, *]
    mean_ = reshape_dim_into(0, 0, mean_);         // [B0 * N, G]
    rstd_ = reshape_dim_into(0, 0, rstd_);         // [B0 * N, G]
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    const auto result = native_group_norm_backward(
        grad_out_,
        input_,
        mean_,
        rstd_,
        nullopt, N * bdim_size, C, HxW, group, {true, false, false});
    auto result0 = std::get<0>(result);
    result0 = reshape_dim_outof(0, bdim_size, result0);
    grad_input = makeBatched(result0, 0, cur_level);
}
return std::make_tuple(grad_input, grad_weight, grad_bias);
}
","optional<int64_t> grad_normalized_input_bdim;
std::tie(grad_normalized_input_value, grad_normalized_input_bdim) =
unwrapTensorAtLevel(grad_normalized_input, cur_level);
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    const auto res = group_norm_backward_no_weight_bias_batch_rule(
        grad_normalized_input_value, grad_normalized_input_bdim,
        input_value, input_bdim,
        mean_value, mean_bdim,
        rstd_value, rstd_bdim,
        N, C, HxW, group
    );
    grad_input = makeBatched(std::get<0>(res), std::get<1>(res), cur_level);
}
return std::make_tuple(grad_input, grad_weight, grad_bias);
}
"
16,"auto running_mean = *running_mean_maybe_owned;
c10::MaybeOwned<Tensor> running_var_maybe_owned = at::borrow_from_optional_tensor(running_var_opt);
auto running_var = *running_var_maybe_owned;

  if (input_bdim && ((running_mean.defined() && !running_mean_bdim) || (running_var.defined() && !running_var_bdim))) {
    throw std::runtime_error(""Batch norm got a batched tensor as input while the running_mean or running_var, which will be updated in place, were not batched."");
  }
c10::optional<int64_t> bdim_size;
Tensor result0;
Tensor mean;
","auto running_mean = *running_mean_maybe_owned;
c10::MaybeOwned<Tensor> running_var_maybe_owned = at::borrow_from_optional_tensor(running_var_opt);
auto running_var = *running_var_maybe_owned;
  TORCH_CHECK(!input_bdim || ((!running_mean.defined() || running_mean_bdim) && (!running_var.defined() || running_var_bdim)),
      ""Batch norm got a batched tensor as input while the running_mean or running_var, which will be updated in place, "",
      ""were not batched.\nIf you are using a module and do not need eval mode, please set `track_running_stats` to be False."",
      ""If you are using a prebuilt module and do not need eval mode, please see the functorch website for resources on "",
      ""how to patch your module to work with vmap"");
c10::optional<int64_t> bdim_size;
Tensor result0;
Tensor mean;
"
17,"auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor grad_out_value;
optional<int64_t> grad_out_bdim;
std::tie(grad_out_value, grad_out_bdim) = unwrapTensorAtLevel(grad_out, cur_level);
","auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();

Tensor grad_out_value;
optional<int64_t> grad_out_bdim;
std::tie(grad_out_value, grad_out_bdim) = unwrapTensorAtLevel(grad_out, cur_level);
"
18,"TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
Tensor grad_out_value;
optional<int64_t> grad_out_bdim;
std::tie(grad_out_value, grad_out_bdim) = unwrapTensorAtLevel(grad_out, cur_level);
","TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
  if (!areAnyBatchedAtLevel({grad_out, input, mean, rstd, weight_opt}, cur_level)) {
    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
    return at::native_group_norm_backward(grad_out, input, mean, rstd, weight_opt, N, C, HxW, group, output_mask);
  }

Tensor grad_out_value;
optional<int64_t> grad_out_bdim;
std::tie(grad_out_value, grad_out_bdim) = unwrapTensorAtLevel(grad_out, cur_level);
"
19,"bool training, double momentum, double eps) {
auto reserve = at::empty({0}, input.options().dtype(kByte));  // in experiments, reserve was never set to anything other than empty by cuda
auto res = batch_norm_batch_rule<F, Func>(
        input, input_bdim, weight_opt, weight_bdim, bias_opt, bias_bdim,
running_mean_opt, running_mean_bdim, running_var_opt, running_var_bdim, training, momentum, eps);
return std::tuple_cat(res, std::make_tuple(reserve, nullopt));
}
","bool training, double momentum, double eps) {
auto reserve = at::empty({0}, input.options().dtype(kByte));  // in experiments, reserve was never set to anything other than empty by cuda
auto res = batch_norm_batch_rule<F, Func>(
        input, input_bdim, weight_opt, weight_bdim, bias_opt, bias_bdim,
running_mean_opt, running_mean_bdim, running_var_opt, running_var_bdim, training, momentum, eps);
return std::tuple_cat(res, std::make_tuple(reserve, nullopt));
}
"
20,"bool training,
double eps,
std::array<bool,3> output_mask) {
return batch_norm_backward_plumbing<F, Func>(
grad_out, input, weight_opt, running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt, training, eps, output_mask);
}
","bool training,
double eps,
std::array<bool,3> output_mask) {

    auto maybe_layer = maybeCurrentDynamicLayer();
    TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
    int64_t cur_level = maybe_layer->layerId();

    if (!areAnyBatchedAtLevel({grad_out, input, weight_opt, running_mean_opt,
          running_var_opt, save_mean_opt, save_rstd_opt}, cur_level)) {
      c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
      return at::native_batch_norm_backward(grad_out, input, weight_opt,
          running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt,
          training, eps, output_mask);
    }

return batch_norm_backward_plumbing<F, Func>(
grad_out, input, weight_opt, running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt, training, eps, output_mask);
}
"
21,"const c10::optional<at::Tensor> & save_rstd_opt,
double eps,
const at::Tensor & reserve) {
return batch_norm_backward_plumbing<F, Func>(
grad_out, input, weight, running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt, true, eps, {true, true, true});
}
","const c10::optional<at::Tensor> & save_rstd_opt,
double eps,
const at::Tensor & reserve) {

    auto maybe_layer = maybeCurrentDynamicLayer();
    TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
    int64_t cur_level = maybe_layer->layerId();

    if (!areAnyBatchedAtLevel({input, grad_out, weight, running_mean_opt,
          running_var_opt, save_mean_opt, save_rstd_opt, reserve}, cur_level)) {
      c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
      return at::cudnn_batch_norm_backward(input, grad_out, weight,
          running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt, eps, reserve);
    }

return batch_norm_backward_plumbing<F, Func>(
grad_out, input, weight, running_mean_opt, running_var_opt, save_mean_opt, save_rstd_opt, true, eps, {true, true, true});
}
"
22,"return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level));
}
// ['rename']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_7_t)(const Tensor &, c10::optional<int64_t>, c10::optional<DimnameList>);
template <>
Tensor lowerToNextLayer<batch_rule_7_t,Tensor,const Tensor &, c10::optional<DimnameList>>(
batch_rule_7_t batch_rule,
const Tensor & self, c10::optional<DimnameList> names
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level));
}
// ['_has_same_storage_numel', 'is_same_size', '_has_compatible_shallow_copy_type', 'is_set_to', 'equal']
typedef std::tuple<bool> (*batch_rule_7_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>);
template <>
bool lowerToNextLayer<batch_rule_7_t,bool,const Tensor &, const Tensor &>(
batch_rule_7_t batch_rule,
  const Tensor & self, const Tensor & other
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor other_value;
  optional<int64_t> other_bdim;
  std::tie(other_value, other_bdim) = unwrapTensorAtLevel(other, cur_level);
  auto results = batch_rule(self_value, self_bdim, other_value, other_bdim);
  return std::get<0>(results);
}

// ['rename']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_8_t)(const Tensor &, c10::optional<int64_t>, c10::optional<DimnameList>);
template <>
Tensor lowerToNextLayer<batch_rule_8_t,Tensor,const Tensor &, c10::optional<DimnameList>>(
  batch_rule_8_t batch_rule,
const Tensor & self, c10::optional<DimnameList> names
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
23,"}
// ['add.Tensor', '_add_relu.Tensor', 'hardshrink_backward', 'threshold_backward', 'where.ScalarOther', 'sub.Tensor', 'subtract.Tensor', 'rsub.Tensor', 'masked_fill.Scalar', 'dist', 'lerp.Scalar', 'softshrink_backward', '_test_serialization_subcmul']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_21_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_21_t,Tensor,const Tensor &, const Tensor &, const Scalar &>(
  batch_rule_21_t batch_rule,
const Tensor & self, const Tensor & other, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['add.Tensor', '_add_relu.Tensor', 'hardshrink_backward', 'threshold_backward', 'where.ScalarOther', 'sub.Tensor', 'subtract.Tensor', 'rsub.Tensor', 'masked_fill.Scalar', 'dist', 'lerp.Scalar', 'softshrink_backward', '_test_serialization_subcmul']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_22_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &);
template <>
Tensor lowerToNextLayer<batch_rule_22_t,Tensor,const Tensor &, const Tensor &, const Scalar &>(
  batch_rule_22_t batch_rule,
const Tensor & self, const Tensor & other, const Scalar & alpha
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
24,"}
// ['bernoulli.p', 'normal.Tensor_float']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_35_t)(const Tensor &, c10::optional<int64_t>, double, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_35_t,Tensor,const Tensor &, double, c10::optional<Generator>>(
  batch_rule_35_t batch_rule,
const Tensor & self, double p, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['bernoulli.p', 'normal.Tensor_float']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_36_t)(const Tensor &, c10::optional<int64_t>, double, c10::optional<Generator>);
template <>
Tensor lowerToNextLayer<batch_rule_36_t,Tensor,const Tensor &, double, c10::optional<Generator>>(
  batch_rule_36_t batch_rule,
const Tensor & self, double p, c10::optional<Generator> generator
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
25,"}
// ['conv_tbc_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_59_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_59_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_59_t batch_rule,
const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['conv_tbc_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_60_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_60_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t>(
  batch_rule_60_t batch_rule,
const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
26,"}
// ['count_nonzero', 'repeat_interleave.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_63_t)(const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_63_t,Tensor,const Tensor &, c10::optional<int64_t>>(
  batch_rule_63_t batch_rule,
const Tensor & self, c10::optional<int64_t> dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['count_nonzero', 'repeat_interleave.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_64_t)(const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_64_t,Tensor,const Tensor &, c10::optional<int64_t>>(
  batch_rule_64_t batch_rule,
const Tensor & self, c10::optional<int64_t> dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
27,"}
// ['cudnn_affine_grid_generator', 'cudnn_affine_grid_generator_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_65_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_65_t,Tensor,const Tensor &, int64_t, int64_t, int64_t, int64_t>(
  batch_rule_65_t batch_rule,
const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['cudnn_affine_grid_generator', 'cudnn_affine_grid_generator_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_66_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_66_t,Tensor,const Tensor &, int64_t, int64_t, int64_t, int64_t>(
  batch_rule_66_t batch_rule,
const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
28,"return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['cudnn_convolution.deprecated', 'miopen_convolution', 'miopen_depthwise_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_68_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_68_t,Tensor,const Tensor &, const Tensor &, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
  batch_rule_68_t batch_rule,
  const Tensor & self, const Tensor & weight, const c10::optional<Tensor> & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, bias_value, bias_bdim, padding, stride, dilation, groups, benchmark, deterministic);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['cudnn_convolution.deprecated2', 'miopen_convolution_transpose_backward_input']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_69_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_69_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool>(
batch_rule_69_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
","return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['cudnn_convolution']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_69_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_69_t,Tensor,const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t, bool, bool, bool>(
batch_rule_69_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
auto maybe_layer = maybeCurrentDynamicLayer();
"
29,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['diag_embed', 'diagonal', 'narrow_copy', 'narrow', 'unfold', '_remove_batch_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_88_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_88_t,Tensor,const Tensor &, int64_t, int64_t, int64_t>(
  batch_rule_88_t batch_rule,
const Tensor & self, int64_t offset, int64_t dim1, int64_t dim2
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['diag_embed', 'diagonal', 'linalg_diagonal', 'narrow_copy', 'narrow', 'unfold', '_remove_batch_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_82_t)(const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_82_t,Tensor,const Tensor &, int64_t, int64_t, int64_t>(
  batch_rule_82_t batch_rule,
const Tensor & self, int64_t offset, int64_t dim1, int64_t dim2
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
30,"}
// ['gradient.scalarint']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_92_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_92_t,::std::vector<Tensor>,const Tensor &, const c10::optional<Scalar> &, c10::optional<int64_t>, int64_t>(
  batch_rule_92_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['gradient.scalarint']
typedef std::tuple<::std::vector<Tensor>,c10::optional<int64_t>> (*batch_rule_86_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Scalar> &, c10::optional<int64_t>, int64_t);
template <>
::std::vector<Tensor> lowerToNextLayer<batch_rule_86_t,::std::vector<Tensor>,const Tensor &, const c10::optional<Scalar> &, c10::optional<int64_t>, int64_t>(
  batch_rule_86_t batch_rule,
const Tensor & self, const c10::optional<Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
31,"}
// ['div.Tensor_mode', 'divide.Tensor_mode']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_96_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_96_t,Tensor,const Tensor &, const Tensor &, c10::optional<c10::string_view>>(
  batch_rule_96_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<c10::string_view> rounding_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['div.Tensor_mode', 'divide.Tensor_mode']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_90_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<c10::string_view>);
template <>
Tensor lowerToNextLayer<batch_rule_90_t,Tensor,const Tensor &, const Tensor &, c10::optional<c10::string_view>>(
  batch_rule_90_t batch_rule,
const Tensor & self, const Tensor & other, c10::optional<c10::string_view> rounding_mode
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
32,"}
// ['grid_sampler_2d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_121_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, bool, ::std::array<bool,2>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_121_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, bool, ::std::array<bool,2>>(
  batch_rule_121_t batch_rule,
const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['grid_sampler_2d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_115_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, bool, ::std::array<bool,2>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_115_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, bool, ::std::array<bool,2>>(
  batch_rule_115_t batch_rule,
const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
33,"}
// ['native_group_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_126_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t, int64_t, int64_t, int64_t, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_126_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t, int64_t, int64_t, int64_t, ::std::array<bool,3>>(
  batch_rule_126_t batch_rule,
const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['native_group_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_120_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t, int64_t, int64_t, int64_t, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_120_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor> &, int64_t, int64_t, int64_t, int64_t, ::std::array<bool,3>>(
  batch_rule_120_t batch_rule,
const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
34,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['isin.Tensor_Tensor', 'bucketize.Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_134_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_134_t,Tensor,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_134_t batch_rule,
const Tensor & elements, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['isin.Tensor_Tensor', 'bucketize.Tensor', '_convert_indices_from_csr_to_coo']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_128_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_128_t,Tensor,const Tensor &, const Tensor &, bool, bool>(
  batch_rule_128_t batch_rule,
const Tensor & elements, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
35,"}
// ['isin.Scalar_Tensor', 'bucketize.Scalar']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_136_t)(const Scalar &, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_136_t,Tensor,const Scalar &, const Tensor &, bool, bool>(
  batch_rule_136_t batch_rule,
const Scalar & element, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['isin.Scalar_Tensor', 'bucketize.Scalar']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_130_t)(const Scalar &, const Tensor &, c10::optional<int64_t>, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_130_t,Tensor,const Scalar &, const Tensor &, bool, bool>(
  batch_rule_130_t batch_rule,
const Scalar & element, const Tensor & test_elements, bool assume_unique, bool invert
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
36,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['is_same_size', '_has_compatible_shallow_copy_type', 'is_set_to', 'equal']
typedef std::tuple<bool> (*batch_rule_137_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>);
template <>
bool lowerToNextLayer<batch_rule_137_t,bool,const Tensor &, const Tensor &>(
  batch_rule_137_t batch_rule,
  const Tensor & self, const Tensor & other
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor other_value;
  optional<int64_t> other_bdim;
  std::tie(other_value, other_bdim) = unwrapTensorAtLevel(other, cur_level);
  auto results = batch_rule(self_value, self_bdim, other_value, other_bdim);
  return std::get<0>(results);
}

// ['kl_div']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_138_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_138_t,Tensor,const Tensor &, const Tensor &, int64_t, bool>(
  batch_rule_138_t batch_rule,
const Tensor & self, const Tensor & target, int64_t reduction, bool log_target
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['kl_div']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_131_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, bool);
template <>
Tensor lowerToNextLayer<batch_rule_131_t,Tensor,const Tensor &, const Tensor &, int64_t, bool>(
  batch_rule_131_t batch_rule,
const Tensor & self, const Tensor & target, int64_t reduction, bool log_target
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
37,"return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['native_layer_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_144_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_144_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, IntArrayRef, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, ::std::array<bool,3>>(
  batch_rule_144_t batch_rule,
const Tensor & grad_out, const Tensor & input, IntArrayRef normalized_shape, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}
// ['_native_multi_head_self_attention']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_137_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, const c10::optional<Tensor> &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_137_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, const c10::optional<Tensor> &>(
  batch_rule_137_t batch_rule,
  const Tensor & query, const Tensor & qkv_weight, const Tensor & qkv_bias, const Tensor & proj_weight, const Tensor & proj_bias, int64_t num_head, const c10::optional<Tensor> & mask
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor query_value;
  optional<int64_t> query_bdim;
  std::tie(query_value, query_bdim) = unwrapTensorAtLevel(query, cur_level);
  Tensor qkv_weight_value;
  optional<int64_t> qkv_weight_bdim;
  std::tie(qkv_weight_value, qkv_weight_bdim) = unwrapTensorAtLevel(qkv_weight, cur_level);
  Tensor qkv_bias_value;
  optional<int64_t> qkv_bias_bdim;
  std::tie(qkv_bias_value, qkv_bias_bdim) = unwrapTensorAtLevel(qkv_bias, cur_level);
  Tensor proj_weight_value;
  optional<int64_t> proj_weight_bdim;
  std::tie(proj_weight_value, proj_weight_bdim) = unwrapTensorAtLevel(proj_weight, cur_level);
  Tensor proj_bias_value;
  optional<int64_t> proj_bias_bdim;
  std::tie(proj_bias_value, proj_bias_bdim) = unwrapTensorAtLevel(proj_bias, cur_level);
  optional<Tensor> mask_value;
  optional<int64_t> mask_bdim;
  if (mask) {
      std::tie(mask_value, mask_bdim) = unwrapTensorAtLevel(mask.value(), cur_level);
  }
  auto results = batch_rule(query_value, query_bdim, qkv_weight_value, qkv_weight_bdim, qkv_bias_value, qkv_bias_bdim, proj_weight_value, proj_weight_bdim, proj_bias_value, proj_bias_bdim, num_head, mask_value, mask_bdim);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['native_layer_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_138_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_138_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, IntArrayRef, const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, ::std::array<bool,3>>(
  batch_rule_138_t batch_rule,
const Tensor & grad_out, const Tensor & input, IntArrayRef normalized_shape, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
38,"}
// ['fbgemm_linear_int8_weight_fp32_activation', 'fbgemm_linear_int8_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_150_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_150_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, const Tensor &>(
  batch_rule_150_t batch_rule,
const Tensor & input, const Tensor & weight, const Tensor & packed, const Tensor & col_offsets, const Scalar & weight_scale, const Scalar & weight_zero_point, const Tensor & bias
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['fbgemm_linear_int8_weight_fp32_activation', 'fbgemm_linear_int8_weight']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_144_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_144_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, const Tensor &>(
  batch_rule_144_t batch_rule,
const Tensor & input, const Tensor & weight, const Tensor & packed, const Tensor & col_offsets, const Scalar & weight_scale, const Scalar & weight_zero_point, const Tensor & bias
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
39,"}
// ['native_batch_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_177_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, double, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_177_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool, double, double>(
  batch_rule_177_t batch_rule,
const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, bool training, double momentum, double eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['native_batch_norm']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_167_t)(const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, double, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_167_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool, double, double>(
  batch_rule_167_t batch_rule,
const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & bias, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, bool training, double momentum, double eps
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
40,"}
// ['native_batch_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_182_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, double, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_182_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool, double, ::std::array<bool,3>>(
  batch_rule_182_t batch_rule,
const Tensor & grad_out, const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, const c10::optional<Tensor> & save_mean, const c10::optional<Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['native_batch_norm_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_172_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, bool, double, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_172_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, const c10::optional<Tensor> &, bool, double, ::std::array<bool,3>>(
  batch_rule_172_t batch_rule,
const Tensor & grad_out, const Tensor & input, const c10::optional<Tensor> & weight, const c10::optional<Tensor> & running_mean, const c10::optional<Tensor> & running_var, const c10::optional<Tensor> & save_mean, const c10::optional<Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
41,"}
// ['repeat_interleave.self_Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_201_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_201_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_201_t batch_rule,
const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['repeat_interleave.self_Tensor']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_188_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_188_t,Tensor,const Tensor &, const Tensor &, c10::optional<int64_t>, c10::optional<int64_t>>(
  batch_rule_188_t batch_rule,
const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
42,"}
// ['size.int', 'stride.int']
typedef std::tuple<int64_t> (*batch_rule_206_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
int64_t lowerToNextLayer<batch_rule_206_t,int64_t,const Tensor &, int64_t>(
  batch_rule_206_t batch_rule,
const Tensor & self, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['size.int', 'stride.int']
typedef std::tuple<int64_t> (*batch_rule_195_t)(const Tensor &, c10::optional<int64_t>, int64_t);
template <>
int64_t lowerToNextLayer<batch_rule_195_t,int64_t,const Tensor &, int64_t>(
  batch_rule_195_t batch_rule,
const Tensor & self, int64_t dim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
43,"}
// ['std.correction', 'var.correction']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_218_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_218_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, c10::optional<int64_t>, bool>(
  batch_rule_218_t batch_rule,
const Tensor & self, c10::optional<IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['std.correction', 'var.correction']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_207_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_207_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, c10::optional<int64_t>, bool>(
  batch_rule_207_t batch_rule,
const Tensor & self, c10::optional<IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
44,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['std_mean', 'var_mean', 'eig', 'qr', 'linalg_inv_ex']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_219_t)(const Tensor &, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_219_t,std::tuple<Tensor,Tensor>,const Tensor &, bool>(
  batch_rule_219_t batch_rule,
const Tensor & self, bool unbiased
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['std_mean', 'var_mean', 'eig', 'qr', 'linalg_lu_factor', 'linalg_inv_ex']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_208_t)(const Tensor &, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_208_t,std::tuple<Tensor,Tensor>,const Tensor &, bool>(
  batch_rule_208_t batch_rule,
const Tensor & self, bool unbiased
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
45,"}
// ['std_mean.names_dim', 'var_mean.names_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_222_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_222_t,std::tuple<Tensor,Tensor>,const Tensor &, DimnameList, bool, bool>(
  batch_rule_222_t batch_rule,
const Tensor & self, DimnameList dim, bool unbiased, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['std_mean.names_dim', 'var_mean.names_dim']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_211_t)(const Tensor &, c10::optional<int64_t>, DimnameList, bool, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_211_t,std::tuple<Tensor,Tensor>,const Tensor &, DimnameList, bool, bool>(
  batch_rule_211_t batch_rule,
const Tensor & self, DimnameList dim, bool unbiased, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
46,"}
// ['std_mean.correction_names', 'var_mean.correction_names']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_223_t)(const Tensor &, c10::optional<int64_t>, DimnameList, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_223_t,std::tuple<Tensor,Tensor>,const Tensor &, DimnameList, c10::optional<int64_t>, bool>(
  batch_rule_223_t batch_rule,
const Tensor & self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['std_mean.correction_names', 'var_mean.correction_names']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_212_t)(const Tensor &, c10::optional<int64_t>, DimnameList, c10::optional<int64_t>, bool);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_212_t,std::tuple<Tensor,Tensor>,const Tensor &, DimnameList, c10::optional<int64_t>, bool>(
  batch_rule_212_t batch_rule,
const Tensor & self, DimnameList dim, c10::optional<int64_t> correction, bool keepdim
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
47,"}
// ['where.ScalarSelf']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_239_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_239_t,Tensor,const Tensor &, const Scalar &, const Tensor &>(
  batch_rule_239_t batch_rule,
const Tensor & condition, const Scalar & self, const Tensor & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['where.ScalarSelf']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_228_t)(const Tensor &, c10::optional<int64_t>, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_228_t,Tensor,const Tensor &, const Scalar &, const Tensor &>(
  batch_rule_228_t batch_rule,
const Tensor & condition, const Scalar & self, const Tensor & other
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
48,"}
// ['clone']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_253_t)(const Tensor &, c10::optional<int64_t>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_253_t,Tensor,const Tensor &, c10::optional<MemoryFormat>>(
  batch_rule_253_t batch_rule,
const Tensor & self, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['clone']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_242_t)(const Tensor &, c10::optional<int64_t>, c10::optional<MemoryFormat>);
template <>
Tensor lowerToNextLayer<batch_rule_242_t,Tensor,const Tensor &, c10::optional<MemoryFormat>>(
  batch_rule_242_t batch_rule,
const Tensor & self, c10::optional<MemoryFormat> memory_format
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
49,"}
// ['sparse_csr_tensor.crow_col_value']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_255_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_255_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_255_t batch_rule,
const Tensor & crow_indices, const Tensor & col_indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['sparse_csr_tensor.crow_col_value']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_244_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>);
template <>
Tensor lowerToNextLayer<batch_rule_244_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>>(
  batch_rule_244_t batch_rule,
const Tensor & crow_indices, const Tensor & col_indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
50,"}
// ['_fake_quantize_learnable_per_channel_affine_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_276_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_276_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, double>(
  batch_rule_276_t batch_rule,
const Tensor & grad, const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_fake_quantize_learnable_per_channel_affine_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_265_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, int64_t, int64_t, int64_t, double);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_265_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, double>(
  batch_rule_265_t batch_rule,
const Tensor & grad, const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
51,"}
// ['index_select.dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_312_t)(const Tensor &, c10::optional<int64_t>, Dimname, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_312_t,Tensor,const Tensor &, Dimname, const Tensor &>(
  batch_rule_312_t batch_rule,
const Tensor & self, Dimname dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['index_select.dimname']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_301_t)(const Tensor &, c10::optional<int64_t>, Dimname, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_301_t,Tensor,const Tensor &, Dimname, const Tensor &>(
  batch_rule_301_t batch_rule,
const Tensor & self, Dimname dim, const Tensor & index
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
52,"}
// ['gather']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_314_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_314_t,Tensor,const Tensor &, int64_t, const Tensor &, bool>(
  batch_rule_314_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['gather']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_303_t)(const Tensor &, c10::optional<int64_t>, int64_t, const Tensor &, c10::optional<int64_t>, bool);
template <>
Tensor lowerToNextLayer<batch_rule_303_t,Tensor,const Tensor &, int64_t, const Tensor &, bool>(
  batch_rule_303_t batch_rule,
const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
53,"}
// ['polygamma', 'special_polygamma']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_326_t)(int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_326_t,Tensor,int64_t, const Tensor &>(
  batch_rule_326_t batch_rule,
int64_t n, const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['polygamma', 'special_polygamma']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_315_t)(int64_t, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_315_t,Tensor,int64_t, const Tensor &>(
  batch_rule_315_t batch_rule,
int64_t n, const Tensor & self
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
54,"}
// ['rrelu_with_noise_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_355_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_355_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, bool, bool>(
  batch_rule_355_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & noise, const Scalar & lower, const Scalar & upper, bool training, bool self_is_result
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['rrelu_with_noise_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_342_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, bool, bool);
template <>
Tensor lowerToNextLayer<batch_rule_342_t,Tensor,const Tensor &, const Tensor &, const Tensor &, const Scalar &, const Scalar &, bool, bool>(
  batch_rule_342_t batch_rule,
const Tensor & grad_output, const Tensor & self, const Tensor & noise, const Scalar & lower, const Scalar & upper, bool training, bool self_is_result
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
55,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['softplus_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_356_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Scalar &, const Scalar &, const Tensor &, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_356_t,Tensor,const Tensor &, const Tensor &, const Scalar &, const Scalar &, const Tensor &>(
  batch_rule_356_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Scalar & beta, const Scalar & threshold, const Tensor & output
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor output_value;
  optional<int64_t> output_bdim;
  std::tie(output_value, output_bdim) = unwrapTensorAtLevel(output, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, beta, threshold, output_value, output_bdim);
  return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}

// ['avg_pool2d', 'avg_pool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_357_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_357_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>>(
  batch_rule_357_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['avg_pool2d', 'avg_pool3d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_343_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>);
template <>
Tensor lowerToNextLayer<batch_rule_343_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>>(
  batch_rule_343_t batch_rule,
const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
56,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['upsample_linear1d_backward.vec', 'upsample_bilinear2d_backward.vec', '_upsample_bilinear2d_aa_backward.vec', 'upsample_trilinear3d_backward.vec', 'upsample_bicubic2d_backward.vec']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_365_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, IntArrayRef, bool, c10::optional<ArrayRef<double>>);
template <>
Tensor lowerToNextLayer<batch_rule_365_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, IntArrayRef, bool, c10::optional<ArrayRef<double>>>(
  batch_rule_365_t batch_rule,
const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['upsample_linear1d_backward.vec', 'upsample_bilinear2d_backward.vec', '_upsample_bilinear2d_aa_backward.vec', 'upsample_trilinear3d_backward.vec', 'upsample_bicubic2d_backward.vec', '_upsample_bicubic2d_aa_backward.vec']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_352_t)(const Tensor &, c10::optional<int64_t>, c10::optional<IntArrayRef>, IntArrayRef, bool, c10::optional<ArrayRef<double>>);
template <>
Tensor lowerToNextLayer<batch_rule_352_t,Tensor,const Tensor &, c10::optional<IntArrayRef>, IntArrayRef, bool, c10::optional<ArrayRef<double>>>(
  batch_rule_352_t batch_rule,
const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
57,"}
// ['upsample_linear1d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_368_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, bool, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_368_t,Tensor,const Tensor &, IntArrayRef, bool, c10::optional<double>>(
  batch_rule_368_t batch_rule,
const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_linear1d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_355_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, bool, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_355_t,Tensor,const Tensor &, IntArrayRef, bool, c10::optional<double>>(
  batch_rule_355_t batch_rule,
const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
58,"}
// ['upsample_nearest1d', '_upsample_nearest_exact1d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_374_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_374_t,Tensor,const Tensor &, IntArrayRef, c10::optional<double>>(
  batch_rule_374_t batch_rule,
const Tensor & self, IntArrayRef output_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['upsample_nearest1d', '_upsample_nearest_exact1d']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_361_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, c10::optional<double>);
template <>
Tensor lowerToNextLayer<batch_rule_361_t,Tensor,const Tensor &, IntArrayRef, c10::optional<double>>(
  batch_rule_361_t batch_rule,
const Tensor & self, IntArrayRef output_size, c10::optional<double> scales
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
59,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['_conv_depthwise2d_backward.output_mask']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_387_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,2>);
template <>
std::tuple<Tensor,Tensor> lowerToNextLayer<batch_rule_387_t,std::tuple<Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,2>>(
  batch_rule_387_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, ::std::array<bool,2> output_mask
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, weight_value, weight_bdim, kernel_size, stride, padding, dilation, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level));
}

// ['conv_depthwise3d_backward.output_mask', 'slow_conv_dilated2d_backward', 'slow_conv_dilated3d_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_388_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_388_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, ::std::array<bool,3>>(
  batch_rule_388_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, ::std::array<bool,3> output_mask
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, weight_value, weight_bdim, kernel_size, stride, padding, dilation, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}

// ['slow_conv3d_forward']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_389_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, const c10::optional<Tensor> &, c10::optional<int64_t>, IntArrayRef, IntArrayRef);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_389_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, IntArrayRef, const c10::optional<Tensor> &, IntArrayRef, IntArrayRef>(
  batch_rule_389_t batch_rule,
  const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const c10::optional<Tensor> & bias, IntArrayRef stride, IntArrayRef padding
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  optional<Tensor> bias_value;
  optional<int64_t> bias_bdim;
  if (bias) {
      std::tie(bias_value, bias_bdim) = unwrapTensorAtLevel(bias.value(), cur_level);
  }
  auto results = batch_rule(self_value, self_bdim, weight_value, weight_bdim, kernel_size, bias_value, bias_bdim, stride, padding);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}

// ['slow_conv3d_backward.output_mask']
typedef std::tuple<Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>,Tensor,c10::optional<int64_t>> (*batch_rule_390_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, ::std::array<bool,3>);
template <>
std::tuple<Tensor,Tensor,Tensor> lowerToNextLayer<batch_rule_390_t,std::tuple<Tensor,Tensor,Tensor>,const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, const Tensor &, const Tensor &, ::std::array<bool,3>>(
  batch_rule_390_t batch_rule,
  const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, ::std::array<bool,3> output_mask
) {
  c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
  auto maybe_layer = maybeCurrentDynamicLayer();
  TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
  int64_t cur_level = maybe_layer->layerId();
  Tensor grad_output_value;
  optional<int64_t> grad_output_bdim;
  std::tie(grad_output_value, grad_output_bdim) = unwrapTensorAtLevel(grad_output, cur_level);
  Tensor self_value;
  optional<int64_t> self_bdim;
  std::tie(self_value, self_bdim) = unwrapTensorAtLevel(self, cur_level);
  Tensor weight_value;
  optional<int64_t> weight_bdim;
  std::tie(weight_value, weight_bdim) = unwrapTensorAtLevel(weight, cur_level);
  Tensor finput_value;
  optional<int64_t> finput_bdim;
  std::tie(finput_value, finput_bdim) = unwrapTensorAtLevel(finput, cur_level);
  Tensor fgrad_input_value;
  optional<int64_t> fgrad_input_bdim;
  std::tie(fgrad_input_value, fgrad_input_bdim) = unwrapTensorAtLevel(fgrad_input, cur_level);
  auto results = batch_rule(grad_output_value, grad_output_bdim, self_value, self_bdim, weight_value, weight_bdim, kernel_size, stride, padding, finput_value, finput_bdim, fgrad_input_value, fgrad_input_bdim, output_mask);
  return std::make_tuple(makeBatched(std::get<0>(results), std::get<1>(results), cur_level), makeBatched(std::get<2>(results), std::get<3>(results), cur_level), makeBatched(std::get<4>(results), std::get<5>(results), cur_level));
}

// ['col2im', 'im2col_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_391_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_391_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_391_t batch_rule,
const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
// ['col2im', 'im2col_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_372_t)(const Tensor &, c10::optional<int64_t>, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef);
template <>
Tensor lowerToNextLayer<batch_rule_372_t,Tensor,const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef>(
  batch_rule_372_t batch_rule,
const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
60,"}
// ['_test_ambiguous_defaults.b']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_414_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_414_t,Tensor,const Tensor &, int64_t, c10::string_view>(
  batch_rule_414_t batch_rule,
const Tensor & dummy, int64_t a, c10::string_view b
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_test_ambiguous_defaults.b']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_394_t)(const Tensor &, c10::optional<int64_t>, int64_t, c10::string_view);
template <>
Tensor lowerToNextLayer<batch_rule_394_t,Tensor,const Tensor &, int64_t, c10::string_view>(
  batch_rule_394_t batch_rule,
const Tensor & dummy, int64_t a, c10::string_view b
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
61,"}
// ['segment_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_415_t)(const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t, bool, const c10::optional<Scalar> &);
template <>
Tensor lowerToNextLayer<batch_rule_415_t,Tensor,const Tensor &, c10::string_view, const c10::optional<Tensor> &, const c10::optional<Tensor> &, int64_t, bool, const c10::optional<Scalar> &>(
  batch_rule_415_t batch_rule,
const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, const c10::optional<Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<Scalar> & initial
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['segment_reduce']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_395_t)(const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t, bool, const c10::optional<Scalar> &);
template <>
Tensor lowerToNextLayer<batch_rule_395_t,Tensor,const Tensor &, c10::string_view, const c10::optional<Tensor> &, const c10::optional<Tensor> &, int64_t, bool, const c10::optional<Scalar> &>(
  batch_rule_395_t batch_rule,
const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, const c10::optional<Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<Scalar> & initial
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
62,"}
// ['_segment_reduce_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_416_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_416_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::string_view, const c10::optional<Tensor> &, int64_t>(
  batch_rule_416_t batch_rule,
const Tensor & grad, const Tensor & output, const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, int64_t axis
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
","}
// ['_segment_reduce_backward']
typedef std::tuple<Tensor,c10::optional<int64_t>> (*batch_rule_396_t)(const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, const Tensor &, c10::optional<int64_t>, c10::string_view, const c10::optional<Tensor> &, c10::optional<int64_t>, int64_t);
template <>
Tensor lowerToNextLayer<batch_rule_396_t,Tensor,const Tensor &, const Tensor &, const Tensor &, c10::string_view, const c10::optional<Tensor> &, int64_t>(
  batch_rule_396_t batch_rule,
const Tensor & grad, const Tensor & output, const Tensor & data, c10::string_view reduce, const c10::optional<Tensor> & lengths, int64_t axis
) {
c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);
"
63,"return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
void index_put__batch_rule(
Tensor& self,
optional<int64_t> self_bdim,
","return makeBatched(std::get<0>(results), std::get<1>(results), cur_level);
}
namespace {
  // Code is mostly duplicated from
  // https://github.com/pytorch/pytorch/blob/fb0e27d38a8fdab4e1c14d6378c9e41cb30fd6a3
  // /aten/src/ATen/native/TensorAdvancedIndexing.cpp#L294-L312
  VmapDimVector compute_indexed_shape(const Tensor &src, TensorList indices_list)
  {
    int64_t dims_before = 0, dims_after = 0, dims_indexed = 0;
    IntArrayRef replacement_shape;
    for (const auto dim : c10::irange(indices_list.size())) {
      if (!indices_list[dim].defined()) {
        if (dims_indexed == 0) {
          dims_before++;
        } else {
          dims_after++;
        }
      } else {
        dims_indexed++;
        replacement_shape = indices_list[dim].sizes();
      }
    }

    // Replace indexed dimensions in src with stride 0 and the size of the result tensor.
    // The offset in these dimensions is computed by the kernel using the index tensor's
    // values and the stride of src. The new shape is not meaningful. It's used to make
    // the shape compatible with the result tensor.
    auto shape = VmapDimVector(src.sizes());
    int64_t end = dims_before + dims_indexed;
    shape.erase(shape.begin() + dims_before, shape.begin() + end);
    shape.insert(shape.begin() + dims_before, replacement_shape.begin(), replacement_shape.end());
    return shape;
  }

  // Code is mostly duplicated from
  // https://github.com/pytorch/pytorch/blob/fb0e27d38a8fdab4e1c14d6378c9e41cb30fd6a3
  // /aten/src/ATen/native/TensorAdvancedIndexing.cpp#L379-L405
  VmapDimVector get_indexed_shape(Tensor self, const torch::List<c10::optional<at::Tensor>> &orig)
  {
    at::native::checkIndexTensorTypes(orig);
    // first expand BoolTensor (masks) or ByteTensor (masks) into 1 or more LongTensors
    auto indices = at::native::expandTensors(self, orig);
    // next broadcast all index tensors together
    try {
      indices = at::expand_outplace(indices);
    } catch (std::exception &e) {
      TORCH_CHECK_INDEX(false, ""shape mismatch: indexing tensors could not be broadcast together""
                               "" with shapes "");
    }
    // add missing null Tensors so that it matches self.dim()
    while (indices.size() < static_cast<size_t>(self.dim())) {
      indices.emplace_back();
    }
    // if the non-null indices are not all adjacent, transpose self and indices
    // together so that they're adjacent at the front
    if (!at::native::hasContiguousSubspace(indices)) {
      std::tie(self, indices) = at::native::transposeToFront(self, indices);
    }
    return compute_indexed_shape(self, indices);
  }

  std::tuple<Tensor, std::vector<optional<Tensor>>, Tensor>
  index_put_batch_rule_helper(const Tensor &self,
                              optional<int64_t> self_bdim,
                              ArrayRef<optional<Tensor>> indices,
                              ArrayRef<optional<int64_t>> indices_bdims,
                              const Tensor &values,
                              optional<int64_t> values_bdim) {

    auto self_logical_rank = rankWithoutBatchDim(self, self_bdim);
    auto values_logical_rank = rankWithoutBatchDim(values, values_bdim);
    Tensor self_ = moveBatchDimToFront(self, self_bdim);
    Tensor values_ = moveBatchDimToFront(values, values_bdim);
    const auto batch_size = self_.size(0);
    values_ = ensure_has_bdim(values_, values_bdim.has_value(), batch_size);
    TORCH_INTERNAL_ASSERT(indices.size() == indices_bdims.size());
    std::vector<optional<Tensor>> indices_ = batchIndices(indices, indices_bdims, self_.size(0), self_bdim, values_bdim);

    auto indexed_shape = get_indexed_shape(self_, List<optional<Tensor>>(indices_));

    // handle broadcasting support for values
    // Eg. Given `indexed_shape.size()` is 5 and
    // shape of `values` is (N, 2, 3), then following block
    // will reshape `values` to (N, 1, 1, 2, 3).
    if (indexed_shape.size() > values_.dim()) {
      auto values_sizes = values_.sizes();

      // number of unit dims (for broadcasting value to indexed_shape)
      auto n_unit_dims = indexed_shape.size() - values_sizes.size();
      VmapDimVector new_values_shape(values_sizes.size() + n_unit_dims);

      // add the batch-dim
      new_values_shape[0] = batch_size;

      // insert the unit dims for broadcasting.
      for (const auto idx : c10::irange(n_unit_dims)) {
        // since batch-dim is already be filled.
        new_values_shape[idx + 1] = 1;
      }
      for (const auto idx: c10::irange(1, values_sizes.size())) {
        // since batch and unit dims are already be filled.
        new_values_shape[idx + n_unit_dims] = values_sizes[idx];
      }
      values_ = values_.view(new_values_shape);
    }

    return std::make_tuple(self_, indices_, values_);
  }
}  // namespace

void index_put__batch_rule(
Tensor& self,
optional<int64_t> self_bdim,
"
64,"if (!self_bdim.has_value()) {
vmapIncompatibleInplaceError(""_index_put_impl_"");
}
  auto self_ = moveBatchDimToFront(self, self_bdim);
  auto values_ = moveBatchDimToFront(values, values_bdim);
  TORCH_INTERNAL_ASSERT(indices.size() == indices_bdims.size());
  std::vector<optional<Tensor>> indices_ = batchIndices(indices, indices_bdims, self_.size(0), self_bdim, values_bdim);
at::_index_put_impl_(self_, List<optional<Tensor>>(indices_), values_, accumulate, unsafe);
}
","if (!self_bdim.has_value()) {
vmapIncompatibleInplaceError(""_index_put_impl_"");
}
  Tensor self_, values_;
  std::vector<optional<Tensor>> indices_;
  std::tie(self_, indices_, values_) = index_put_batch_rule_helper(
      self, self_bdim, indices, indices_bdims, values, values_bdim);
at::_index_put_impl_(self_, List<optional<Tensor>>(indices_), values_, accumulate, unsafe);
}
"
65,"const py::object &compileFn, PyObject *args) {
std::vector<at::Tensor> tensorArgs = parsePythonArgs(numArgs, args);
LocalState state;
    std::vector<int> cacheKey= computeCacheKey(tensorArgs, numArgs, hasherType, id);
cache_.emplace(cacheKey, compileFn);
}
","const py::object &compileFn, PyObject *args) {
std::vector<at::Tensor> tensorArgs = parsePythonArgs(numArgs, args);
LocalState state;
    hash_key_t cacheKey= computeCacheKey(tensorArgs, numArgs, hasherType, id);
cache_.emplace(cacheKey, compileFn);
}
"
66,"VMAP_SUPPORT(""slice_backward"", slice_backward_batch_rule);
VMAP_SUPPORT(""view"", view_batching_rule);
VMAP_SUPPORT(""expand"", expand_batch_rule);
}
}}
","VMAP_SUPPORT(""slice_backward"", slice_backward_batch_rule);
VMAP_SUPPORT(""view"", view_batching_rule);
VMAP_SUPPORT(""expand"", expand_batch_rule);
  VMAP_SUPPORT(""unfold"", unfold_batch_rule);
}
}}
"
67,"const Tensor& a, bool a_has_bdim,
const Tensor& b, bool b_has_bdim,
const Tensor& c, bool c_has_bdim) {
  int64_t bdim_size = -1;
Tensor flagpole;
if (a_has_bdim) {
    bdim_size = a.size(0);
flagpole = a;
} else if (b_has_bdim) {
    bdim_size = b.size(0);
flagpole = b;
} else if (c_has_bdim) {
    bdim_size = c.size(0);
flagpole = c;
} else {
TORCH_INTERNAL_ASSERT(false);
","const Tensor& a, bool a_has_bdim,
const Tensor& b, bool b_has_bdim,
const Tensor& c, bool c_has_bdim) {
Tensor flagpole;
if (a_has_bdim) {
flagpole = a;
} else if (b_has_bdim) {
flagpole = b;
} else if (c_has_bdim) {
flagpole = c;
} else {
TORCH_INTERNAL_ASSERT(false);
"
68,"return slow_fallback<Tensor,Tensor>(op, { self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask });
}
static Tensor expand_bdim(const Tensor& a, bool a_has_bdim, int64_t bdim_size) {
  if (a_has_bdim) {
    return a;
  }
  DimVector expanded_shape(a.sizes().begin(), a.sizes().end());
  expanded_shape.insert(expanded_shape.begin(), bdim_size);
  return a.expand(expanded_shape);
}

static std::tuple<Tensor, Tensor, Tensor> expand_bdims(
    const Tensor& a, bool a_has_bdim,
    const Tensor& b, bool b_has_bdim,
    const Tensor& c, bool c_has_bdim) {
  int64_t bdim_size = -1;
  if (a_has_bdim) {
    bdim_size = a.size(0);
  } else if (b_has_bdim) {
    bdim_size = b.size(0);
  } else if (c_has_bdim) {
    bdim_size = c.size(0);
  } else {
    TORCH_INTERNAL_ASSERT(false);
  }
  auto a_ = expand_bdim(a, a_has_bdim, bdim_size);
  auto b_ = expand_bdim(b, b_has_bdim, bdim_size);
  auto c_ = expand_bdim(c, b_has_bdim, bdim_size);
  return std::make_tuple(a_, b_, c_);
}

std::tuple<Tensor,optional<int64_t>> _softmax_backward_batch_rule(
    const Tensor& grad_output, optional<int64_t> grad_output_bdim,
    const Tensor& output, optional<int64_t> output_bdim,
    int64_t dim,
    const Tensor& self, optional<int64_t> self_bdim) {
  // NB: self only gets used for its dtype. We handle it anyways just in case.
  // softmax_backward's decomposition is y * gy - y * (y * gy).sum(dim, keepdim=True)
  // NB: the CUDA kernel handles strides so we can just expand
  // all of the tensors and call it a day. The CPU kernel is not as good but
  // idk if the perf on that really matters
  auto grad_output_ = moveBatchDimToFront(grad_output, grad_output_bdim);
  auto output_ = moveBatchDimToFront(output, output_bdim);
  auto self_ = moveBatchDimToFront(self, self_bdim);

  // Expand out that extra dimension for everyone
  std::tie(grad_output_, output_, self_) = expand_bdims(
      grad_output_, grad_output_bdim.has_value(),
      output_, output_bdim.has_value(),
      self_, self_bdim.has_value());

  // Scalar tensor case. softmax turns into the identity when this happens.
  // I don't know why the output is zeros, though, but that's what softmax tells me...
  if (self_.dim() == 1 && (dim == 0 || dim == -1)) {
    return std::make_tuple(at::zeros_like(grad_output_), 0);
  }

  dim = getPhysicalDim(self_, true, dim);

  return std::make_tuple(at::_softmax_backward_data(grad_output_, output_, dim, self_), 0);
}

TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""convolution"", convolution_batching_rule);
m.impl(""conv1d"", convNd_decomp);
","return slow_fallback<Tensor,Tensor>(op, { self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask });
}
TORCH_LIBRARY_IMPL(aten, FT_BATCHED_KEY, m) {
VMAP_SUPPORT(""convolution"", convolution_batching_rule);
m.impl(""conv1d"", convNd_decomp);
"
69,"return std_dim_batch_rule(self, self_bdim, range(0, self.dim() - 1), unbiased, false);
}
// Skipping frobenius/nuclear/all/any since they don't have opinfo tests right now :P
template<typename F, F Func>
","return std_dim_batch_rule(self, self_bdim, range(0, self.dim() - 1), unbiased, false);
}

// Skipping frobenius/nuclear/all/any since they don't have opinfo tests right now :P
template<typename F, F Func>
"
70,"// 2. Unwrap all the args in the copy set
// 3. Call the operator
// 4. Wrap the output
  // 5. (!) refreshSizesAndStrides for all the args in the original set
// 6. (!) Pop those args off.
// Step 1 & 2
","// 2. Unwrap all the args in the copy set
// 3. Call the operator
// 4. Wrap the output
  // 5. (!) refreshMetadata for all the args in the original set
// 6. (!) Pop those args off.
// Step 1 & 2
"
71,"if (tensor.is_cuda()) {
key_set = key_set.add(DispatchKey::CUDA);
}
return at::detail::make_tensor<BatchedTensorImpl>(key_set, tensor, std::move(bdims));
}
","if (tensor.is_cuda()) {
key_set = key_set.add(DispatchKey::CUDA);
}
  auto* batched = maybeGetBatchedImpl(tensor);
  if (batched) {
    auto requested_level = bdims.back().level();
    auto batched_level = batched->bdims().back().level();
    TORCH_INTERNAL_ASSERT(requested_level > batched_level);
  }
return at::detail::make_tensor<BatchedTensorImpl>(key_set, tensor, std::move(bdims));
}
"
72,"auto A_data = A.data_ptr<scalar_t>();
auto b_data = b.data_ptr<scalar_t>();
auto A_mat_stride = matrixStride(A);
auto b_mat_stride = matrixStride(b);
auto batch_size = batchCount(A);
","auto A_data = A.data_ptr<scalar_t>();
auto b_data = b.data_ptr<scalar_t>();
  auto infos_data = infos.data_ptr<int>();
auto A_mat_stride = matrixStride(A);
auto b_mat_stride = matrixStride(b);
auto batch_size = batchCount(A);
"
73,"// call lapackEig once to get the optimal size for work data
scalar_t wkopt;
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
    int info;
lapackEig<scalar_t, value_t>('N', jobvr, n, self_data, n, wr,
      nullptr, 1, vecs_data, ldvr, &wkopt, -1, rwork_data, &info);
int lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));
// call again to do the actual work
Tensor work = at::empty({lwork}, self.dtype());
lapackEig<scalar_t, value_t>('N', jobvr, n, self_data, n, wr,
      nullptr, 1, vecs_data, ldvr, work.data_ptr<scalar_t>(), lwork, rwork_data, &info);
    *info_ptr = info;
}
#endif
}
","// call lapackEig once to get the optimal size for work data
scalar_t wkopt;
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
lapackEig<scalar_t, value_t>('N', jobvr, n, self_data, n, wr,
      nullptr, 1, vecs_data, ldvr, &wkopt, -1, rwork_data, info_ptr);
int lwork = std::max<int>(1, real_impl<scalar_t, value_t>(wkopt));
// call again to do the actual work
Tensor work = at::empty({lwork}, self.dtype());
lapackEig<scalar_t, value_t>('N', jobvr, n, self_data, n, wr,
      nullptr, 1, vecs_data, ldvr, work.data_ptr<scalar_t>(), lwork, rwork_data, info_ptr);
}
#endif
}
"
74,": Tensor();
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  int64_t info;
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""eig_cpu"", [&]{
    apply_eig<scalar_t>(self_, eigenvectors, vals_, vecs_, &info);
});
// NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
  singleCheckErrors(info, ""eig_cpu"");
return std::tuple<Tensor, Tensor>(vals_, vecs_);
}
",": Tensor();
// NOLINTNEXTLINE(cppcoreguidelines-init-variables)
  auto infos = at::zeros({}, self.options().dtype(kInt));
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES(self.scalar_type(), ""eig_cpu"", [&]{
    apply_eig<scalar_t>(self_, eigenvectors, vals_, vecs_, infos.data_ptr<int>());
});
// NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
  at::_linalg_check_errors(infos, ""eig"", /*is_matrix*/true);
return std::tuple<Tensor, Tensor>(vals_, vecs_);
}
"
75,"apply_magma_eigh<scalar_t>(eigvals_working_copy, self_working_copy, infos, upper, eigenvectors);
});
  if (self.dim() > 2) {
    batchCheckErrors(infos, ""symeig_cuda"");
  } else {
    singleCheckErrors(infos.item().toInt(), ""symeig_cuda"");
  }
if (eigenvectors) {
return std::tuple<Tensor, Tensor>(eigvals_working_copy.to(self.device()), self_working_copy);
} else {
","apply_magma_eigh<scalar_t>(eigvals_working_copy, self_working_copy, infos, upper, eigenvectors);
});
  at::_linalg_check_errors(infos, ""symeig"", self.dim() == 2);

if (eigenvectors) {
return std::tuple<Tensor, Tensor>(eigvals_working_copy.to(self.device()), self_working_copy);
} else {
"
76,"const Tensor& grad_output, const Tensor& input, const Tensor& weight,
IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation,
bool transposed, IntArrayRef output_padding, int64_t groups, std::array<bool, 3> output_mask) {
  AT_ERROR(""You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function "");
return std::tuple<Tensor, Tensor, Tensor>(
at::empty_like(input, LEGACY_CONTIGUOUS_MEMORY_FORMAT),
at::empty_like(weight, LEGACY_CONTIGUOUS_MEMORY_FORMAT),
","const Tensor& grad_output, const Tensor& input, const Tensor& weight,
IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation,
bool transposed, IntArrayRef output_padding, int64_t groups, std::array<bool, 3> output_mask) {
   TORCH_CHECK_NOT_IMPLEMENTED(false, ""convolution_backward_overrideable: You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function "");
return std::tuple<Tensor, Tensor, Tensor>(
at::empty_like(input, LEGACY_CONTIGUOUS_MEMORY_FORMAT),
at::empty_like(weight, LEGACY_CONTIGUOUS_MEMORY_FORMAT),
"
77,"//! 2) MergeAdjacentSingletonAxes class merges or reduces any
//!    adjacent singleton dimensions.
class MergeAxesInterface {
   public:
    virtual ~MergeAxesInterface() = default;

protected:
// See addMergeTransform for ""is_index_merge_rhs"" and
// ""is_last_axis_rfactor"" descriptions
","//! 2) MergeAdjacentSingletonAxes class merges or reduces any
//!    adjacent singleton dimensions.
class MergeAxesInterface {
protected:
// See addMergeTransform for ""is_index_merge_rhs"" and
// ""is_last_axis_rfactor"" descriptions
"
78,"return at::functionalization::impl::to_functional_tensor(out);
}
TORCH_LIBRARY_IMPL(_, Functionalize, m) {
m.fallback(torch::CppFunction::makeFromBoxedFunction<&functionalizeFallback>());
}
","return at::functionalization::impl::to_functional_tensor(out);
}

// Why is _unsafe_view special-cased here?
// Basically just to satisfy autograd's debug asserts.
// The situation:
// - _unsafe_view's autograd kernel has debug asserts to confirm
//   that the input and output alias storage.
// - _unsafe_view's schema in native_functions.yaml
//   does not contain alias annotations, so it advertises as non-aliasing.
// - functionalization will then treat _unsafe_view like a non-aliasing op.
//   Specifically, autograd will redispatch to functionalization's
//   boxed fallback kernel, which creates a new FunctionalTensorWrapper output
//   that does **not** alias storage with the input, tripping the assert.
// The kernel written here just manually re-ifies the aliasing relationship.
//
// Another way to handle this would be to fix unsafe_view's alias annotations
// in native_functions.yaml, but I think this would be a pessimization.
// The idea with _unsafe_view is that you're guaranteed that the input
// is a temporary, and don't actually have to worry about propagating
// mutations between the input and output.
at::Tensor _unsafe_view_functionalize(const at::Tensor & self, at::IntArrayRef size) {
  if (!at::functionalization::impl::isFunctionalTensor(self)) {
    at::AutoDispatchSkipFunctionalize guard;
    return at::_unsafe_view(self, size);
  }

  auto self_ = at::functionalization::impl::from_functional_tensor(self);
  at::Tensor tmp_output;
  {
    at::AutoDispatchSkipFunctionalize guard;
    tmp_output = at::_unsafe_view(self_, size);
  }

  at::functionalization::ViewMeta view_meta = at::functionalization::ViewMeta(
    [size = size.vec()](const at::Tensor & base, int64_t mutated_view_idx) -> at::Tensor {
      return at::_unsafe_view(base, size);
    },
    [size = size.vec()](const at::Tensor & base, const at::Tensor & mutated_view, int64_t mutated_view_idx) -> at::Tensor {
      return at::_unsafe_view(mutated_view, base.sizes());
    }
  );

  auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
  // See  Note [Propagating strides in the functionalization pass]
  // (for _unsafe_view, I'm just manually doing the shape inference rule here instead of calling the meta function for unsafe_view)
  auto inferred_size = at::infer_size_dv(size, self.numel());
  auto stride = at::detail::computeStride(self.sizes(), self.strides(), inferred_size);
  TORCH_INTERNAL_ASSERT(stride.has_value());
  out.unsafeGetTensorImpl()->set_sizes_and_strides(size, stride.value());
  return out;
}

TORCH_LIBRARY_IMPL(_, Functionalize, m) {
m.fallback(torch::CppFunction::makeFromBoxedFunction<&functionalizeFallback>());
}
"
79,"VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
},
          VK_KERNEL(clamp),
v_output.extents(),
context->gpu().adapter->local_work_group_size(),
// Write-only access bypasses synchronization but inserts appropriate
","VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
},
          shader_descriptor,
v_output.extents(),
context->gpu().adapter->local_work_group_size(),
// Write-only access bypasses synchronization but inserts appropriate
"
80,"VK_DESCRIPTOR_TYPE_STORAGE_IMAGE,
VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
},
          VK_KERNEL(clamp_),
v_self.extents(),
context->gpu().adapter->local_work_group_size(),
// Read-Write access triggers an async synchronization if necessory
","VK_DESCRIPTOR_TYPE_STORAGE_IMAGE,
VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
},
          shader_descriptor,
v_self.extents(),
context->gpu().adapter->local_work_group_size(),
// Read-Write access triggers an async synchronization if necessory
"
81,"std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::Backend get_backend(bool is_cuda, bool is_sparse) {
  if (is_cuda) {
    if (is_sparse){
      return at::Backend::SparseCUDA;
    } else {
      return at::Backend::CUDA;
    }
  } else {
    if (is_sparse){
      return at::Backend::SparseCPU;
    } else {
      return at::Backend::CPU;
    }
  }
}

at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
","std::array<THPLayout*, static_cast<int>(at::Layout::NumOptions)> layout_registry = {};
at::DeprecatedTypeProperties* get_type_properties(at::DeviceType device_type, at::ScalarType scalarType) {
at::Backend backend;
if (device_type == at::kCPU) {
"
82,"return ss.str();
}
std::string refType(DataType dt) {
  std::stringstream ss;
  ss << dt << ""&"";
  return ss.str();
}

//! Utility class to build an argument list
class ArgumentBuilder {
public:
","return ss.str();
}
//! Utility class to build an argument list
class ArgumentBuilder {
public:
"
83,"}
}
std::unordered_set<const FunctionSchema*> getInterfaceCalls(Graph& graph) {
  std::unordered_set<const FunctionSchema*> ret;
  auto nodes = findAllNodes(graph, c10::prim::CallMethod, true);
  for (Node* node : nodes) {
    if (auto iface = node->input(0)->type()->castRaw<InterfaceType>()) {
      ret.insert(iface->getMethod(node->s(attr::name)));
    }
  }
  return ret;
}

struct ModuleMethod {
ModuleMethod(const Module& m, const GraphFunction& f, c10::QualifiedName n)
: module(m), function(f), exportName(std::move(n)) {}
","}
}
struct ModuleMethod {
ModuleMethod(const Module& m, const GraphFunction& f, c10::QualifiedName n)
: module(m), function(f), exportName(std::move(n)) {}
"
84,"throw std::runtime_error(""new(): invalid arguments"");
}
Tensor legacy_sparse_tensor_ctor(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
  return legacy_sparse_tensor_generic_ctor_new(dispatch_key, scalar_type, args, kwargs, CtorOrNew::CTOR);
}

Tensor legacy_sparse_tensor_new(c10::DispatchKey dispatch_key, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs) {
  return legacy_sparse_tensor_generic_ctor_new(dispatch_key, scalar_type, args, kwargs, CtorOrNew::NEW);
}

// NB: device_idx here is NOT a DeviceIndex, but index into PythonArgs
c10::TensorOptions typeIdWithDefault(PythonArgs& r, int64_t device_idx, c10::DispatchKey dispatch_key) {
auto options = dispatchKeyToTensorOptions(dispatch_key);
","throw std::runtime_error(""new(): invalid arguments"");
}
// NB: device_idx here is NOT a DeviceIndex, but index into PythonArgs
c10::TensorOptions typeIdWithDefault(PythonArgs& r, int64_t device_idx, c10::DispatchKey dispatch_key) {
auto options = dispatchKeyToTensorOptions(dispatch_key);
"
85,"return at::native::nansum_out(self, dim, keepdim, dtype, result);
}
// NOTE: this could be implemented via diag and sum, but this has perf problems,
// see https://github.com/pytorch/pytorch/pull/47305,
Tensor trace_cpu(const Tensor& self) {
","return at::native::nansum_out(self, dim, keepdim, dtype, result);
}
static Tensor& prod_out_impl(Tensor& result, const Tensor& self, IntArrayRef dim,
                        bool keepdim, c10::optional<ScalarType> opt_dtype) {
  ScalarType dtype = get_dtype_from_result(result, opt_dtype);
  auto iter = make_reduction(""prod"", result, self, dim, keepdim, dtype);
  if (iter.numel() == 0) {
    result.fill_(1);
  } else {
    prod_stub(iter.device_type(), iter);
  }
  return result;
}

// NOTE: this could be implemented via diag and sum, but this has perf problems,
// see https://github.com/pytorch/pytorch/pull/47305,
Tensor trace_cpu(const Tensor& self) {
"
86,"c10::QualifiedName exportName;
};
bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
","c10::QualifiedName exportName;
};
std::vector<ModuleMethod> getModuleInterfaceExports(
    const Module& module,
    const std::unordered_set<const FunctionSchema*>& schemas) {
  if (schemas.size() == 0) {
    return {};
  }
  std::unordered_set<std::string> names;
  for (auto schema : schemas) {
    names.insert(schema->name());
  }
  std::vector<ModuleMethod> ret;
  for (const auto& submodule : module.modules()) {
    for (const auto& method : submodule.get_methods()) {
      const auto& f = toGraphFunction(method.function());
      if (names.find(f.qualname().name()) != names.end()) {
        ret.emplace_back(submodule, f, f.qualname());
      }
    }
  }
  return ret;
}

bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
"
87,"END_HANDLE_TH_ERRORS
}
static PyObject * THPStorage_dtype(THPStorage *self, void *unused)
{
  HANDLE_TH_ERRORS
  return torch::autograd::utils::wrap(
      torch::getTHPDtype(at::typeMetaToScalarType(
#ifdef THQUANTIZED
          caffe2::TypeMeta::Make<quantized_t>()
#else
          caffe2::TypeMeta::Make<uint8_t>()
#endif
              )));
  END_HANDLE_TH_ERRORS
}

typedef PyObject *(*getter)(PyObject *, void *);
// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)
","END_HANDLE_TH_ERRORS
}
typedef PyObject *(*getter)(PyObject *, void *);
// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays,cppcoreguidelines-avoid-non-const-global-variables)
"
88,"}
}
// Copy all content from reader to stringstream
void get_model_stream(PyTorchStreamReader& reader, std::stringstream& out) {
  auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
    out.write(static_cast<const char*>(buf), nbytes);
    return !out ? 0 : nbytes;
  };
  PyTorchStreamWriter writer(writer_func);

  selective_copy(
      reader,
      writer,
      std::unordered_set<std::string>(),
      std::unordered_set<std::string>());
}

// The write_archive_current function is used for bytecode from version v5 to
// v7 (the latest bytecode version). pre-v5 we serialized things differently.
// This write archive function may change in export_module.cpp, however we don't
","}
}
// The write_archive_current function is used for bytecode from version v5 to
// v7 (the latest bytecode version). pre-v5 we serialized things differently.
// This write archive function may change in export_module.cpp, however we don't
"
89,"}
}
// this function checks wheather the blocks of If node have the same return
// type.
bool IsBlockReturnTypeSame(Node* n) {
  TORCH_INTERNAL_ASSERT(n->kind() == ::c10::onnx::If);
  auto then_block = n->blocks()[0];
  auto else_block = n->blocks()[1];
  for (const auto i : c10::irange(n->outputs().size())) {
    // check the type
    auto then_block_type = then_block->outputs()[i]->type();
    auto else_block_type = else_block->outputs()[i]->type();
    if (then_block_type->cast<TensorType>() &&
        else_block_type->cast<TensorType>()) {
      if (then_block_type->castRaw<TensorType>()->scalarType() !=
          else_block_type->castRaw<TensorType>()->scalarType()) {
        return false;
      }
    }
  }
  return true;
}

c10::optional<at::Tensor> ComputeConstantFolding(Node* n, int opset_version) {
if (n->inputs().size() == 0) {
return c10::nullopt;
","}
}
c10::optional<at::Tensor> ComputeConstantFolding(Node* n, int opset_version) {
if (n->inputs().size() == 0) {
return c10::nullopt;
"
90,"c10::QualifiedName exportName;
};
std::vector<ModuleMethod> getModuleInterfaceExports(
    const Module& module,
    const std::unordered_set<const FunctionSchema*>& schemas) {
  if (schemas.size() == 0) {
    return {};
  }
  std::unordered_set<std::string> names;
  for (auto schema : schemas) {
    names.insert(schema->name());
  }
  std::vector<ModuleMethod> ret;
  for (const auto& submodule : module.modules()) {
    for (const auto& method : submodule.get_methods()) {
      const auto& f = toGraphFunction(method.function());
      if (names.find(f.qualname().name()) != names.end()) {
        ret.emplace_back(submodule, f, f.qualname());
      }
    }
  }
  return ret;
}

bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
","c10::QualifiedName exportName;
};
bool isLoweredModule(const Module& m) {
c10::QualifiedName type_name;
if (m.type()->name()) {
"
91,"cpu_kernel_vec(
iter,
          [=](scalar_t a) -> scalar_t { return (0 < a) - (a < 0); },
[=](Vectorized<scalar_t> self_vec){
// Comparison operators returns bitmask.
","cpu_kernel_vec(
iter,
          [=](scalar_t a) -> scalar_t { return (0 < a) - c10::is_negative(a); },
[=](Vectorized<scalar_t> self_vec){
// Comparison operators returns bitmask.
"
92,"push<PickleOpCode>(PickleOpCode::EMPTY_DICT);
  if (dict.size() >= 0) {
    push<PickleOpCode>(PickleOpCode::MARK);

    // Sort the dict for deterministic keys
    for (const auto& entry : dict) {
      pushIValue(entry.key());
      pushIValue(entry.value());
    }
    push<PickleOpCode>(PickleOpCode::SETITEMS);
}
endTypeTag(ivalue);
}
","push<PickleOpCode>(PickleOpCode::EMPTY_DICT);
  static_assert(
      std::is_unsigned<decltype(dict.size())>::value,
      ""Expected size to be non-negative."");
  push<PickleOpCode>(PickleOpCode::MARK);
  // Sort the dict for deterministic keys
  for (const auto& entry : dict) {
    pushIValue(entry.key());
    pushIValue(entry.value());
}
  push<PickleOpCode>(PickleOpCode::SETITEMS);

endTypeTag(ivalue);
}
"
93,"}
std::tuple<Tensor, Tensor> qmax(const Tensor& self, int64_t dim, bool keepdim) {
Tensor max_indices = at::empty({0}, self.options().dtype(kLong));
Tensor max = at::empty({0}, self.options().dtype(toUnderlying(self.scalar_type())));
at::max_outf(self.int_repr(), dim, keepdim, max, max_indices);
","}
std::tuple<Tensor, Tensor> qmax(const Tensor& self, int64_t dim, bool keepdim) {
  TORCH_CHECK(self.qscheme() == at::kPerTensorAffine, ""Max operator for quantized tensors only works for per tensor quantized tensors. ""
  ""Please open an issue on https://github.com/pytorch/pytorch/issues if you need per channel quantized tensor support."");
Tensor max_indices = at::empty({0}, self.options().dtype(kLong));
Tensor max = at::empty({0}, self.options().dtype(toUnderlying(self.scalar_type())));
at::max_outf(self.int_repr(), dim, keepdim, max, max_indices);
"
94,"return torch.log2(self), backward
        def rand_like(self, *, memory_format: Optional[int]):
            def backward(grad_output):
                return None
            return torch.rand_like(self, memory_format=memory_format), backward
def reciprocal(self):
result = torch.reciprocal(self)
","return torch.log2(self), backward
        # TODO: Fix rand_like to match expected format
        # def rand_like(self, *, memory_format: Optional[int]):
        #    def backward(grad_output):
        #        return None
        #    return torch.rand_like(self, memory_format=memory_format), backward
def reciprocal(self):
result = torch.reciprocal(self)
"
95,"size_t size,
ExtraFilesMap& extra_files,
c10::optional<at::Device> device) {
auto* flatbuffer_module = mobile::serialization::GetMutableModule(data.get());
FlatbufferLoader loader;
mobile::Module mobilem = loader.parseModule(flatbuffer_module);
","size_t size,
ExtraFilesMap& extra_files,
c10::optional<at::Device> device) {
#if ENABLE_UPGRADERS
  populate_upgraders_graph_map();
#endif
auto* flatbuffer_module = mobile::serialization::GetMutableModule(data.get());
FlatbufferLoader loader;
mobile::Module mobilem = loader.parseModule(flatbuffer_module);
"
96,"// out= ops are allowed to resize the output tensors, mutating both the data and metadata of the tensor.
// We need to propagate that metadata mutation to the wrapper (new size).
set_sizes_and_strides(value_.sizes(), value_.strides());
}
","// out= ops are allowed to resize the output tensors, mutating both the data and metadata of the tensor.
// We need to propagate that metadata mutation to the wrapper (new size).
set_sizes_and_strides(value_.sizes(), value_.strides());
  set_storage_offset(value_.storage_offset());
  if (dtype() != value_.unsafeGetTensorImpl()->dtype() || layout() != value_.unsafeGetTensorImpl()->layout()) {
    value_ = value_.to(c10::TensorOptions().dtype(dtype()).layout(layout()));
  }
}
"
97,"return opGraph;
}
const auto get_generator_sources(const cudnnBackendDescriptorType_t& desc, const Tensor& x, const bool deterministic, const bool allow_tf32, const cudnnBackendHeurMode_t heur_mode) {
// Method for engine config generator based on heuristics
auto heurgen_method = [/*&desc,*/ &x, deterministic, allow_tf32, heur_mode](cudnn_frontend::OperationGraph &opGraph) -> cudnn_frontend::EngineConfigList {
auto heuristics = cudnn_frontend::EngineHeuristicsBuilder()
","return opGraph;
}
auto get_generator_sources(const cudnnBackendDescriptorType_t& desc, const Tensor& x, const bool deterministic, const bool allow_tf32, const cudnnBackendHeurMode_t heur_mode) {
// Method for engine config generator based on heuristics
auto heurgen_method = [/*&desc,*/ &x, deterministic, allow_tf32, heur_mode](cudnn_frontend::OperationGraph &opGraph) -> cudnn_frontend::EngineConfigList {
auto heuristics = cudnn_frontend::EngineHeuristicsBuilder()
"
98,"if (getCachedFuserEnabledEnvVar().has_value()) {
return *getCachedFuserEnabledEnvVar();
}
    // 3. default value
#ifdef FBCODE_CAFFE2
return false;
#else
    return nvfuserCanBeEnabled();
#endif
}
public:
","if (getCachedFuserEnabledEnvVar().has_value()) {
return *getCachedFuserEnabledEnvVar();
}
    // 3. default value (if you switch this to true, make sure
    //    to check nvfuserCanBeEnabled())
return false;
}
public:
"
99,"auto schemaAwareToPyObject = [&](int64_t idx) -> py::object {
const auto& arg = schema.arguments()[idx];
    if (arg.real_type()->kind() == c10::ScalarTypeType::Kind) {
auto* obj = getTHPDtype(static_cast<c10::ScalarType>(arguments[idx].toInt()));
return py::reinterpret_borrow<py::object>(reinterpret_cast<PyObject*>(obj));
    } else if (arg.real_type()->kind() == c10::LayoutType::Kind) {
auto* obj = getTHPLayout(static_cast<c10::Layout>(arguments[idx].toInt()));
return py::reinterpret_borrow<py::object>(reinterpret_cast<PyObject*>(obj));
    } else if (arg.real_type()->kind() == c10::MemoryFormatType::Kind) {
      // TODO: https://github.com/pytorch/pytorch/issues/77135
      auto* obj = THPMemoryFormat_New(static_cast<c10::MemoryFormat>(arguments[idx].toInt()), ""unused"");
      return py::reinterpret_steal<py::object>(reinterpret_cast<PyObject*>(obj));
} else {
return torch::jit::toPyObject(arguments[idx]);
}
","auto schemaAwareToPyObject = [&](int64_t idx) -> py::object {
const auto& arg = schema.arguments()[idx];
    auto match = [&](c10::TypeKind kind) {
      const auto& t = arg.real_type();
      if (t->kind() == kind) return true;
      if (auto opt_t = t->cast<c10::OptionalType>()) {
        if (opt_t->getElementType()->kind() == kind) return true;
      }
      return false;
    };
    if (arguments[idx].isNone()) {
      return py::none();
    } else if (match(c10::ScalarTypeType::Kind)) {
auto* obj = getTHPDtype(static_cast<c10::ScalarType>(arguments[idx].toInt()));
return py::reinterpret_borrow<py::object>(reinterpret_cast<PyObject*>(obj));
    } else if (match(c10::LayoutType::Kind)) {
auto* obj = getTHPLayout(static_cast<c10::Layout>(arguments[idx].toInt()));
return py::reinterpret_borrow<py::object>(reinterpret_cast<PyObject*>(obj));
    } else if (match(c10::MemoryFormatType::Kind)) {
      return torch::utils::getTHPMemoryFormat(static_cast<c10::MemoryFormat>(arguments[idx].toInt()));
} else {
return torch::jit::toPyObject(arguments[idx]);
}
"
100,"}
}


TORCH_META_FUNC2(norm, ScalarOpt_dim)
(const Tensor& self, const OptionalScalarRef p, IntArrayRef dim, bool keepdim) {
  check_floating_or_complex_dtype(""norm"", self.scalar_type());
auto out_dtype = get_result_or_self_value_dtype(self, maybe_get_output(), c10::nullopt);
resize_reduction(*this, self, dim, keepdim, out_dtype);
}
","}
}
TORCH_META_FUNC2(norm, ScalarOpt_dim)
(const Tensor& self, const OptionalScalarRef p, IntArrayRef dim, bool keepdim) {
  TORCH_CHECK(
      at::isFloatingType(self.scalar_type()) || at::isComplexType(self.scalar_type()),
      ""norm(): input dtype should be either floating point or complex. ""
      ""Got "", self.scalar_type(), "" instead."");

auto out_dtype = get_result_or_self_value_dtype(self, maybe_get_output(), c10::nullopt);
resize_reduction(*this, self, dim, keepdim, out_dtype);
}
"
101,"IntArrayRef stride,
ScalarType dtype,
c10::optional<Device> device_opt) {
  auto device = device_or_default(device_opt);
  TORCH_INTERNAL_ASSERT(device.is_mps());
  const DeviceGuard device_guard(device);
  auto* allocator = at::mps::GetMPSAllocator();
  constexpr c10::DispatchKeySet mps_dks(c10::DispatchKey::MPS);
  return at::detail::empty_strided_generic(
      size, stride, allocator, mps_dks, dtype);
}
TensorBase empty_strided_mps(
","IntArrayRef stride,
ScalarType dtype,
c10::optional<Device> device_opt) {
#if defined(__APPLE__)
#if __is_target_os(macOS)
  if (__builtin_available(macOS 12.3, *)) {
    auto device = device_or_default(device_opt);
    TORCH_INTERNAL_ASSERT(device.is_mps());
    const DeviceGuard device_guard(device);
    auto* allocator = at::mps::GetMPSAllocator();
    constexpr c10::DispatchKeySet mps_dks(c10::DispatchKey::MPS);
    return at::detail::empty_strided_generic(
        size, stride, allocator, mps_dks, dtype);
  } else {
    TORCH_CHECK(false, MPS_ERROR_RUNTIME_TOO_LOW)
  }
#else
  TORCH_CHECK(false, MPS_ERROR_NOT_COMPILED)
#endif
#else
  TORCH_CHECK(false, MPS_ERROR_NOT_COMPILED)
#endif
}
TensorBase empty_strided_mps(
"
102,"python_class_name_(std::move(python_class_name)),
original_msg_(std::move(original_msg)) {}
  const std::string& JITException::getCaughtOriginalMsg() {
    return caughtOriginalMsg;
  }
  const std::string& JITException::getCaughtPythonClassName() {
    return caughtPythonClassName;
  }
  void JITException::setCaughtOriginalMsg(const std::string& msg) {
    caughtOriginalMsg = msg;
  }
  void JITException::setCaughtPythonClassName(
      const std::string& pythonClassName) {
    caughtPythonClassName = pythonClassName;
  }
} // namespace jit
} // namespace torch
","python_class_name_(std::move(python_class_name)),
original_msg_(std::move(original_msg)) {}
const std::string& JITException::getCaughtOriginalMsg() {
  return caughtOriginalMsg;
}
const std::string& JITException::getCaughtPythonClassName() {
  return caughtPythonClassName;
}
void JITException::setCaughtOriginalMsg(const std::string& msg) {
  caughtOriginalMsg = msg;
}
void JITException::setCaughtPythonClassName(
    const std::string& pythonClassName) {
  caughtPythonClassName = pythonClassName;
}
} // namespace jit
} // namespace torch
"
103,"// For now I'm retroactively setting this in functorch,
// but once Open Multiple Dispatch lands we should be able to calculate this in core.
level_ = -1;
  // shallow_copy_from overwrites the storage and dispatch keyset...
  auto functional_storage = storage_;
  shallow_copy_from(value_.getIntrusivePtr());
  storage_ = functional_storage;
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
// All of the keys corresponding to functorch transforms should not be copied over.
","// For now I'm retroactively setting this in functorch,
// but once Open Multiple Dispatch lands we should be able to calculate this in core.
level_ = -1;
  // mirror all of the generic tensor metadata onto the wrapper
  copy_generic_tensor_metadata(value_.getIntrusivePtr().get(), this);
  refresh_numel();
  refresh_contiguous();
storage_access_should_throw_ = false;
key_set_ = c10::DispatchKeySet(c10::DispatchKey::Functionalize) | value_.key_set();
// All of the keys corresponding to functorch transforms should not be copied over.
"
104,"#include <torch/library.h>
#include <c10/util/irange.h>
namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
","#include <torch/library.h>
#include <c10/util/irange.h>
#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/NativeFunctions.h>
#else
#include <ATen/ops/to_native.h>
#endif

namespace {
void functionalizeFallback(const c10::OperatorHandle& op, c10::DispatchKeySet dispatchKeySet, torch::jit::Stack* stack) {
const auto& schema = op.schema();
"
105,"TORCH_IMPL_FUNC(clamp_max_Tensor_out)
(const Tensor& self, const Tensor& max, const Tensor& result) {
  clamp_max_stub(device_type(), *this);
}
TORCH_IMPL_FUNC(clamp_min_out)
","TORCH_IMPL_FUNC(clamp_max_Tensor_out)
(const Tensor& self, const Tensor& max, const Tensor& result) {
  minimum_stub(device_type(), *this);
}
TORCH_IMPL_FUNC(clamp_min_out)
"
106,"REGISTER_DISPATCH(isneginf_stub, &isneginf_kernel_impl);
REGISTER_DISPATCH(mode_stub, &mode_kernel_impl);
REGISTER_DISPATCH(clamp_stub, &clamp_kernel_impl);
REGISTER_DISPATCH(clamp_min_stub, &clamp_min_kernel_impl);
REGISTER_DISPATCH(clamp_max_stub, &clamp_max_kernel_impl);
REGISTER_DISPATCH(clamp_scalar_stub, &clamp_scalar_kernel_impl);
REGISTER_DISPATCH(clamp_min_scalar_stub, &clamp_min_scalar_kernel_impl);
REGISTER_DISPATCH(clamp_max_scalar_stub, &clamp_max_scalar_kernel_impl);
","REGISTER_DISPATCH(isneginf_stub, &isneginf_kernel_impl);
REGISTER_DISPATCH(mode_stub, &mode_kernel_impl);
REGISTER_DISPATCH(clamp_stub, &clamp_kernel_impl);
REGISTER_DISPATCH(clamp_scalar_stub, &clamp_scalar_kernel_impl);
REGISTER_DISPATCH(clamp_min_scalar_stub, &clamp_min_scalar_kernel_impl);
REGISTER_DISPATCH(clamp_max_scalar_stub, &clamp_max_scalar_kernel_impl);
"
107,"// This exists to prevent us from tracing the call to empty().  The actual
// autograd code doesn't really matter, because requires_grad is always false
// here.
Tensor tensor;
{
    at::AutoDispatchBelowADInplaceOrView guard;  // TODO: remove
    at::tracer::impl::NoTracerDispatchMode tracer_guard;
c10::impl::ExcludeDispatchKeyGuard torchdispatchmode_guard(c10::DispatchKey::Python);
c10::impl::ExcludeDispatchKeyGuard torchdispatchmode_snapshot_guard(c10::DispatchKey::PythonTLSSnapshot);
// functorch uses FuncTorchDynamicLayerBackMode as a mode key to wrap all
// tensors returned from operators in special TensorWrapper tensor extension
    // The problem with this is that TensorWrapper does not have storage so
    // accessing the data_ptr (for recursive_store) internal asserts.
    // As a quick hack, the guard here prevents functorch from wrapping the empty
    // tensor in a TensorWrapper and instead when `tensor.to` is called later,
    // the tensor gets wrapped. A more long-term solution is to think about
    // what the extensibility mechanism for this function (internal_new_from_data)
    // looks like for mode-based dispatch keys and C++ tensor extensions.
c10::impl::ExcludeDispatchKeyGuard functorch_guard(c10::DispatchKey::FuncTorchDynamicLayerBackMode);
// We disable DeferredInit handler for similar reasons as functorch.
c10::impl::ExcludeDispatchKeyGuard deferred_init_guard(c10::DispatchKey::DeferredInit);
    if (isStorage(data)) {
      ScalarType storage_scalar_type;
      bool is_typed_storage = false;
      Storage storage = createStorageGetType(data, storage_scalar_type, is_typed_storage);
      TORCH_CHECK(!is_typed_storage || storage_scalar_type == scalar_type,
          ""Expected a Storage of type "", scalar_type,
          "" or an _UntypedStorage, but got "", storage_scalar_type);
      tensor = at::empty(sizes, at::initialTensorOptions().dtype(is_typed_storage ? storage_scalar_type : inferred_scalar_type).pinned_memory(pin_memory).device(storage.device()));
      tensor.set_(storage);

    } else {
      TensorOptions opts = at::initialTensorOptions().dtype(inferred_scalar_type);

      // If the device is Meta, take the shortcut. We don't want to allocate an
      // empty CPU tensor which would break our contract for meta tensors.
      if (device == at::kMeta) {
        return at::empty(sizes, opts.device(device));
      }
      tensor = at::empty(sizes, opts.pinned_memory(pin_memory));
      if (c10::multiply_integers(tensor.sizes()) != 0) {
        recursive_store(
            (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,
            inferred_scalar_type, tensor.dtype().itemsize(), data);
}
}
}
  pybind11::gil_scoped_release no_gil;
  maybe_initialize_cuda(device);
  // However, it is VERY important that we trace the to() call here (even
  // though the reason this is important is a hack).  Without *some* factory
  // function call that is traced at construction time, we will consider
  // a tensor constant as originating from ""outside"" the trace, and if you
  // try to return it directly we will fail with the error saying no
  // ""no observable data dependence"".  In an ideal world, we wouldn't trace
  // a to() call but I need to think harder about what exactly we should trace
  // in this case.
  return tensor.to(device, inferred_scalar_type, /*non_blocking=*/false, /*copy=*/false);
}
Tensor new_from_data_copy(
","// This exists to prevent us from tracing the call to empty().  The actual
// autograd code doesn't really matter, because requires_grad is always false
// here.
  // What are the semantics of tensor_new()?
  // We manually construct a tensor and place on it on the correct device with empty() and to().
  // We then have to ""lift"" the newly constructed tensor in some cases, like when we're performing
  // a functorch transform or running functionalization.
  // The exclude guards are all to ensure that extra logic doesn't run when we're constructing the raw tensor.
Tensor tensor;
{
    at::AutoDispatchBelowADInplaceOrView guard;
c10::impl::ExcludeDispatchKeyGuard torchdispatchmode_guard(c10::DispatchKey::Python);
c10::impl::ExcludeDispatchKeyGuard torchdispatchmode_snapshot_guard(c10::DispatchKey::PythonTLSSnapshot);
// functorch uses FuncTorchDynamicLayerBackMode as a mode key to wrap all
// tensors returned from operators in special TensorWrapper tensor extension
c10::impl::ExcludeDispatchKeyGuard functorch_guard(c10::DispatchKey::FuncTorchDynamicLayerBackMode);
// We disable DeferredInit handler for similar reasons as functorch.
c10::impl::ExcludeDispatchKeyGuard deferred_init_guard(c10::DispatchKey::DeferredInit);
    // Note [Functionalization <> torch.Tensor constructor]
    // Functionalization ""lifts"" the newly constructed tensor into a wrapper using aten::lift().
    c10::impl::ExcludeDispatchKeyGuard functionalize_guard(c10::DispatchKey::Functionalize);
    {
      // Tracing should probably also use the ""lift"" operator to add the tensor to a trace,
      // but it's technically BC-breaking to do that, since we currently trace .to() calls.
      at::tracer::impl::NoTracerDispatchMode tracer_guard;

      if (isStorage(data)) {
        ScalarType storage_scalar_type;
        bool is_typed_storage = false;
        Storage storage = createStorageGetType(data, storage_scalar_type, is_typed_storage);
        TORCH_CHECK(!is_typed_storage || storage_scalar_type == scalar_type,
            ""Expected a Storage of type "", scalar_type,
            "" or an _UntypedStorage, but got "", storage_scalar_type);
        tensor = at::empty(sizes, at::initialTensorOptions().dtype(is_typed_storage ? storage_scalar_type : inferred_scalar_type).pinned_memory(pin_memory).device(storage.device()));
        tensor.set_(storage);
      } else {
        TensorOptions opts = at::initialTensorOptions().dtype(inferred_scalar_type);

        // If the device is Meta, take the shortcut. We don't want to allocate an
        // empty CPU tensor which would break our contract for meta tensors.
        if (device == at::kMeta) {
          return at::empty(sizes, opts.device(device));
        }
        tensor = at::empty(sizes, opts.pinned_memory(pin_memory));
        if (c10::multiply_integers(tensor.sizes()) != 0) {
          recursive_store(
              (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,
              inferred_scalar_type, tensor.dtype().itemsize(), data);
        }
}
}
    pybind11::gil_scoped_release no_gil;
    maybe_initialize_cuda(device);
    // However, it is VERY important that we trace the to() call here (even
    // though the reason this is important is a hack).  Without *some* factory
    // function call that is traced at construction time, we will consider
    // a tensor constant as originating from ""outside"" the trace, and if you
    // try to return it directly we will fail with the error saying no
    // ""no observable data dependence"".  In an ideal world, we wouldn't trace
    // a to() call but I need to think harder about what exactly we should trace
    // in this case.
    tensor = tensor.to(device, inferred_scalar_type, /*non_blocking=*/false, /*copy=*/false);
}

  return tensor.lift();
}
Tensor new_from_data_copy(
"
108,"if (batch_mode) {
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
        kHalf, input.scalar_type(), ""replication_pad3d_cpu"", [&] {
auto input_data = input.data_ptr<scalar_t>();
auto output_data = output.data_ptr<scalar_t>();
auto nbatch = input.size(0);
","if (batch_mode) {
AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(
        kHalf, input.scalar_type(), ""reflection_pad3d_cpu"", [&] {
auto input_data = input.data_ptr<scalar_t>();
auto output_data = output.data_ptr<scalar_t>();
auto nbatch = input.size(0);
"
109,"bool cpu_fuser_enabled = false;
#endif
bool gpu_fuser_enabled = true;
} // namespace detail
","bool cpu_fuser_enabled = false;
#endif
// note: this doesn't necessarily enable NNC because NVFuser might override it
bool gpu_fuser_enabled = true;
} // namespace detail
"
110,"#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
","#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/NumericUtils.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
"
111,"public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = std::min(*self_data, *src_data);
}
};
static ReduceMinimum reduce_minimum;
","public:
template <typename scalar_t>
constexpr void operator() (scalar_t * self_data, scalar_t * src_data) const {
    *self_data = at::_isnan<scalar_t>(*src_data) ? *src_data : std::min(*self_data, *src_data);
}
};
static ReduceMinimum reduce_minimum;
"
112,"grad_self = grad;
grad_src = grad.gather(dim, index);
} else if (reduce == ""prod"") {
    grad_self = (grad * result) / self;
    grad_self.masked_fill_(self == 0, 0);
    grad_src = (grad * result).gather(dim, index) / src;
    grad_src.masked_fill_(src == 0, 0);
} else if (reduce == ""mean"") {
Tensor N = include_self ? ones_like(grad) : zeros_like(grad);
N = N.scatter_add(dim, index, ones_like(src));
","grad_self = grad;
grad_src = grad.gather(dim, index);
} else if (reduce == ""prod"") {
    // Explicitly compute exclusive prod for elements in self/src that are 0
    Tensor masked_self = self.masked_fill(self == 0, 1);
    Tensor masked_self_result = masked_self.scatter_reduce(dim, index, src, reduce, include_self);
    grad_self = grad * masked_self_result / masked_self;
    Tensor src_zero = src == 0;
    Tensor src_num_zeros = zeros_like(self).scatter_add(dim, index, src_zero.to(self.dtype())).gather(dim, index);
    Tensor src_single_zero = bitwise_and(src_zero, src_num_zeros == 1);
    // For src positions with src_single_zero, grad * result.gather(dim,index) / src.masked_fill(src_zero, 1)
    // would incorrectly propagate zeros as the gradient
    Tensor masked_src = src.masked_fill(src_single_zero, 1);
    Tensor masked_src_result = self.scatter_reduce(dim, index, masked_src, reduce, include_self);
    Tensor grad_src1 = where(src_single_zero,
                             (grad * masked_src_result).gather(dim, index),
                             (grad * result).gather(dim, index) / src.masked_fill(src_zero, 1));
    if ((src_num_zeros > 1).any().item<bool>()) {
      auto node = std::make_shared<DelayedError>(
        ""scatter_reduce(): Double backward is unsupported for src when >1 zeros in src are scattered to the same position in self"",
        /* num inputs */ 1);
      auto result = node->apply({ grad_src1 });
      grad_src = result[0];
    } else {
      grad_src = grad_src1;
    }
} else if (reduce == ""mean"") {
Tensor N = include_self ? ones_like(grad) : zeros_like(grad);
N = N.scatter_add(dim, index, ones_like(src));
"
113,"CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());
at::cuda::sparse::bsrsm2_solve(
handle,
block_layout,
","CUSPARSE_SOLVE_POLICY_NO_LEVEL,
work_data.get());
        if (!unitriangular) {
          int first_zero_diag_idx = -1;
          cusparseStatus_t status = cusparseXbsrsm2_zeroPivot(handle, info.descriptor(), &first_zero_diag_idx);
          if (status == CUSPARSE_STATUS_ZERO_PIVOT) {
            X_->fill_(NAN);
            return;
          }
        }

at::cuda::sparse::bsrsm2_solve(
handle,
block_layout,
"
114,"#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/NumericUtils.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
","#include <ATen/native/TensorAdvancedIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/Dispatch.h>
#include <ATen/Parallel.h>
#include <c10/util/irange.h>
"
115,"init_val = (scalar_t)1;
break;
case SCATTER_GATHER_OP::REDUCE_MAXIMUM:
        init_val = std::numeric_limits<scalar_t>::lowest();
break;
case SCATTER_GATHER_OP::REDUCE_MINIMUM:
        init_val = std::numeric_limits<scalar_t>::max();
break;
case SCATTER_GATHER_OP::REDUCE_MEAN:
init_val = (scalar_t)0;
","init_val = (scalar_t)1;
break;
case SCATTER_GATHER_OP::REDUCE_MAXIMUM:
        init_val = std::numeric_limits<scalar_t>::has_infinity ? -std::numeric_limits<scalar_t>::infinity()
                   : std::numeric_limits<scalar_t>::lowest();
break;
case SCATTER_GATHER_OP::REDUCE_MINIMUM:
        init_val = std::numeric_limits<scalar_t>::has_infinity ? std::numeric_limits<scalar_t>::infinity()
                   : std::numeric_limits<scalar_t>::max();
break;
case SCATTER_GATHER_OP::REDUCE_MEAN:
init_val = (scalar_t)0;
"
116,"cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
bool deterministic{true};
bool allow_tf32{false};
setLinearParams(&key.params, input, orig_weight, deterministic, allow_tf32);
","cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are
  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two
  // CacheKey objects have the same user defined parameters, but
  // different padded values, resulting in different hash outputs.
  memset(&key, 0, sizeof(key));
bool deterministic{true};
bool allow_tf32{false};
setLinearParams(&key.params, input, orig_weight, deterministic, allow_tf32);
"
117,"cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
bool deterministic{true};
bool allow_tf32{false};
auto padding_vec = padding_.vec();
","cudnnHandle_t handle = at::native::getCudnnHandle();
CacheKey key;
  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are
  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two
  // CacheKey objects have the same user defined parameters, but
  // different padded values, resulting in different hash outputs.
  memset(&key, 0, sizeof(key));
bool deterministic{true};
bool allow_tf32{false};
auto padding_vec = padding_.vec();
"
118,"if (!obj) throw python_error();
PyList_SET_ITEM(list.get(), i, obj);
auto advance_data_ptr = strides[dim] * elementSize;
    TORCH_INTERNAL_ASSERT(data || advance_data_ptr == 0);
data += advance_data_ptr;
}
return list.release();
","if (!obj) throw python_error();
PyList_SET_ITEM(list.get(), i, obj);
auto advance_data_ptr = strides[dim] * elementSize;
    TORCH_INTERNAL_ASSERT(data || (advance_data_ptr == 0));
data += advance_data_ptr;
}
return list.release();
"
119,"}
return outputs;
}
TensorList to_functional_tensor(const TensorList& t_list) {
std::vector<Tensor> outputs(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
outputs[i] = to_functional_tensor(t_list[i]);
","}
return outputs;
}
std::vector<Tensor> to_functional_tensor(const TensorList& t_list) {
std::vector<Tensor> outputs(t_list.size());
for (const auto i : c10::irange(t_list.size())) {
outputs[i] = to_functional_tensor(t_list[i]);
"
120,"// expression and base type names.
if (resolver_) {
if (auto typePtr =
            resolver_->resolveType(expr.range().text(), expr.range())) {
return typePtr;
}
}
","// expression and base type names.
if (resolver_) {
if (auto typePtr =
            resolver_->resolveType(expr.range().text().str(), expr.range())) {
return typePtr;
}
}
"
121,"at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalues =
          std::move(*jit::unpickle(
                         reinterpret_cast<const char*>(debug_data.get()),
                         debug_size,
                         nullptr,
                         {},
                         c10::parseType)
                         .toTuple())
              .elements();
      SourceRangeDeserializer deserializer;
      for (auto& val : ivalues) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
","at::DataPtr debug_data;
size_t debug_size{0};
std::tie(debug_data, debug_size) = reader->getRecord(record_name);
      auto ivalueTuple = jit::unpickle(
          reinterpret_cast<const char*>(debug_data.get()),
          debug_size,
          nullptr,
          {},
          c10::parseType);
      const auto& ivalues = ivalueTuple.toTuple()->elements();
      IValue lines;
      std::unique_ptr<SourceRangeDeserializer> deserializer;
      if (ivalues.size() == 3 && ivalues[0].isString() &&
          kFormatWithStringTable == ivalues[0].toStringRef()) {
        // new format
        deserializer = std::make_unique<SourceRangeDeserializer>(ivalues[1]);
        lines = ivalues[2];
      } else {
        deserializer = std::make_unique<SourceRangeDeserializer>();
        lines = ivalueTuple;
      }

      for (auto& val : lines.toTuple()->elements()) {
auto tup_elems = std::move(*std::move(val).toTuple()).elements();
// For BC we decode only tuples with 3 elements
// assuming it contains
"
122,"const c10::intrusive_ptr<ConvPackedParamsBase<kSpatialDim>>& packed_weight,
double output_scale,
int64_t output_zero_point) {
    act = act.contiguous(c10::MemoryFormat::ChannelsLast);
// TODO: check all zero_points are zero/all tensors are symmetrically quantized
if (kReluFused) {
return packed_weight->apply_relu(act, output_scale, output_zero_point);
","const c10::intrusive_ptr<ConvPackedParamsBase<kSpatialDim>>& packed_weight,
double output_scale,
int64_t output_zero_point) {
    act = act.to(c10::MemoryFormat::ChannelsLast);
// TODO: check all zero_points are zero/all tensors are symmetrically quantized
if (kReluFused) {
return packed_weight->apply_relu(act, output_scale, output_zero_point);
"
123,"GraphExecutor executor;
CaptureList captures_;
UnpackInstructions input_instructions_;
};
// an optimized way of executing the subgraph computed directly on
","GraphExecutor executor;
CaptureList captures_;
UnpackInstructions input_instructions_;
  // we need to track input lists to fwd graph
  // since in backward graphs these will become
  // an undefined tensors if gradients are zeros
  // we will need to convert an undefined tensor
  // back to a list
  // TODO: switch to using UnpackInstructions
  size_t index_ = 0;
  std::map<size_t, size_t> input_tensor_lists_;
};
// an optimized way of executing the subgraph computed directly on
"
124,"}
bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {
  return t1.dtype() == t2.dtype() && t1.equal(t2);
}
void DeduplicateInitializers(
","}
bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {
  if (t1.dtype() != t2.dtype() || !t1.sizes().equals(t2.sizes()) ||
      !t1.strides().equals(t2.strides())) {
    return false;
  }

  if (t1.device() != t2.device()) {
    return t1.to(""cpu"").equal(t2.to(""cpu""));
  }

  return t1.equal(t2);
}
void DeduplicateInitializers(
"
125,"// make sure buffer size is multiple of alignment
size_t buffer_size =
(size / FLATBUFFERS_MAX_ALIGNMENT + 1) * FLATBUFFERS_MAX_ALIGNMENT;
#if defined(__ANDROID__)
std::shared_ptr<char> data(
      static_cast<char*>(memalign(FLATBUFFERS_MAX_ALIGNMENT, buffer_size)),
      free);
#elif defined(_WIN32)
  std::shared_ptr<char> data(
      static_cast<char*>(
          _aligned_malloc(buffer_size, FLATBUFFERS_MAX_ALIGNMENT)),
      _aligned_free); // NOLINT
#else
  std::shared_ptr<char> data(
      static_cast<char*>(aligned_alloc(FLATBUFFERS_MAX_ALIGNMENT, buffer_size)),
      free); // NOLINT
#endif
fread(data.get(), size, 1, f);
fclose(f);
#endif
return std::make_tuple(data, size);
}
void FlatbufferLoader::extractJitSourceAndConstants(
ExtraFilesMap* jit_sources,
std::vector<IValue>* constants) {
","// make sure buffer size is multiple of alignment
size_t buffer_size =
(size / FLATBUFFERS_MAX_ALIGNMENT + 1) * FLATBUFFERS_MAX_ALIGNMENT;
std::shared_ptr<char> data(
      static_cast<char*>(c10::alloc_cpu(buffer_size)), c10::free_cpu);
fread(data.get(), size, 1, f);
fclose(f);
#endif
return std::make_tuple(data, size);
}
std::tuple<std::shared_ptr<char>, size_t> get_stream_content(std::istream& in) {
  // get size of the stream and reset to orig
  std::streampos orig_pos = in.tellg();
  in.seekg(orig_pos, std::ios::end);
  const long size = in.tellg();
  in.seekg(orig_pos, in.beg);

  // read stream
  // NOLINT make sure buffer size is multiple of alignment
  size_t buffer_size =
      (size / FLATBUFFERS_MAX_ALIGNMENT + 1) * FLATBUFFERS_MAX_ALIGNMENT;
  std::shared_ptr<char> data(
      static_cast<char*>(c10::alloc_cpu(buffer_size)), c10::free_cpu);
  in.read(data.get(), size);

  // reset stream to original position
  in.seekg(orig_pos, in.beg);
  return std::make_tuple(data, size);
}

void FlatbufferLoader::extractJitSourceAndConstants(
ExtraFilesMap* jit_sources,
std::vector<IValue>* constants) {
"
126,"[](Module& m,
const std::string& filename,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false) {
            m._save_for_mobile(filename, _extra_files, _save_mobile_debug_info);
},
py::arg(""filename""),
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false)
.def(
""_save_to_buffer_for_mobile"",
[](Module& m,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false) {
std::ostringstream buf;
            m._save_for_mobile(buf, _extra_files, _save_mobile_debug_info);
return py::bytes(buf.str());
},
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false)
.def(""_set_optimized"", &Module::set_optimized)
.def(
""dump"",
","[](Module& m,
const std::string& filename,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false,
             bool _use_flatbuffer = false) {
            m._save_for_mobile(
                filename,
                _extra_files,
                _save_mobile_debug_info,
                _use_flatbuffer);
},
py::arg(""filename""),
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false,
          py::arg(""_use_flatbuffer"") = false)
.def(
""_save_to_buffer_for_mobile"",
[](Module& m,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false,
             bool _use_flatbuffer = false) {
std::ostringstream buf;
            m._save_for_mobile(
                buf, _extra_files, _save_mobile_debug_info, _use_flatbuffer);
return py::bytes(buf.str());
},
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false,
          py::arg(""_use_flatbuffer"") = false)
.def(""_set_optimized"", &Module::set_optimized)
.def(
""dump"",
"
127,"#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/callstack_debug_info_serialization.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_functions.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
","#include <torch/csrc/jit/runtime/instruction.h>
#include <torch/csrc/jit/serialization/callstack_debug_info_serialization.h>
#include <torch/csrc/jit/serialization/export_bytecode.h>
#if defined(ENABLE_FLATBUFFER)
#include <torch/csrc/jit/serialization/flatbuffer_serializer.h>
#endif
#include <torch/csrc/jit/serialization/import_export_constants.h>
#include <torch/csrc/jit/serialization/import_export_functions.h>
#include <torch/csrc/jit/serialization/import_export_helpers.h>
"
128,"[](Module& m,
const std::string& filename,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false,
             bool _use_flatbuffer = false) {
            m._save_for_mobile(
                filename,
                _extra_files,
                _save_mobile_debug_info,
                _use_flatbuffer);
},
py::arg(""filename""),
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false,
          py::arg(""_use_flatbuffer"") = false)
.def(
""_save_to_buffer_for_mobile"",
[](Module& m,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false,
             bool _use_flatbuffer = false) {
std::ostringstream buf;
            m._save_for_mobile(
                buf, _extra_files, _save_mobile_debug_info, _use_flatbuffer);
return py::bytes(buf.str());
},
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false,
          py::arg(""_use_flatbuffer"") = false)
.def(""_set_optimized"", &Module::set_optimized)
.def(
""dump"",
","[](Module& m,
const std::string& filename,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false) {
            m._save_for_mobile(filename, _extra_files, _save_mobile_debug_info);
},
py::arg(""filename""),
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false)
.def(
""_save_to_buffer_for_mobile"",
[](Module& m,
const ExtraFilesMap& _extra_files = ExtraFilesMap(),
             bool _save_mobile_debug_info = false) {
std::ostringstream buf;
            m._save_for_mobile(buf, _extra_files, _save_mobile_debug_info);
return py::bytes(buf.str());
},
py::arg(""_extra_files"") = ExtraFilesMap(),
          py::arg(""_save_mobile_debug_info"") = false)
.def(""_set_optimized"", &Module::set_optimized)
.def(
""dump"",
"
129,"const std::string& filename,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info) {
  caffe2::serialize::PyTorchStreamWriter writer(filename);
  ScriptModuleSerializer serializer(writer);
  serializer.serialize(
      module, extra_files, bytecode_format, save_mobile_debug_info);
}
void ExportModule(
","const std::string& filename,
const ExtraFilesMap& extra_files,
bool bytecode_format,
    bool save_mobile_debug_info,
    bool use_flatbuffer) {
  if (use_flatbuffer) {
#if defined(ENABLE_FLATBUFFER)
    auto writer_func = [&](const void* buf, size_t nbytes) -> size_t {
      std::fstream ofile(filename, std::ios::binary | std::ios::out);
      ofile.write(static_cast<const char*>(buf), nbytes);
      ofile.close();
      return !ofile ? 0 : nbytes;
    };
    save_mobile_module_to(
        module, extra_files, save_mobile_debug_info, writer_func);
#else
    TORCH_CHECK(
        false,
        ""Trying to export as flatbuffer file but the build hasn't enabled flatbuffer"");
#endif
  } else {
    caffe2::serialize::PyTorchStreamWriter writer(filename);
    ScriptModuleSerializer serializer(writer);
    serializer.serialize(
        module, extra_files, bytecode_format, save_mobile_debug_info);
  }
}
void ExportModule(
"
130,"#include <dlfcn.h>
#include <fmt/format.h>
#include <sys/stat.h>
#include <torch/csrc/deploy/elf_file.h>
#include <torch/csrc/deploy/unity/xar_environment.h>
","#include <dlfcn.h>
#include <fmt/format.h>
#include <sys/stat.h>
#include <torch/csrc/deploy/Exception.h>
#include <torch/csrc/deploy/elf_file.h>
#include <torch/csrc/deploy/unity/xar_environment.h>
"
131,"}
void XarEnvironment::setupPythonApp() {
  TORCH_CHECK(
!alreadySetupPythonApp_,
""Already setup the python application. It should only been done once!"");
","}
void XarEnvironment::setupPythonApp() {
  MULTIPY_CHECK(
!alreadySetupPythonApp_,
""Already setup the python application. It should only been done once!"");
"
132,"vec_conv.emplace_back(FusionBehavior::DYNAMIC, pair.second);
} else {
TORCH_INTERNAL_ASSERT(
""FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: "",
pair.first);
}
","vec_conv.emplace_back(FusionBehavior::DYNAMIC, pair.second);
} else {
TORCH_INTERNAL_ASSERT(
                    false,
""FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: "",
pair.first);
}
"
133,"return quantized_linear_dynamic_fp16_impl<true>(n);
});
REGISTER_OPERATOR_FUNCTOR(aten::full, aten_full, [](Node* n) -> SROperator {
if (!n->matches(torch::schema(
""aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor""))) {
","return quantized_linear_dynamic_fp16_impl<true>(n);
});
// device & pin_memory matter only when CUDA is enabled.
static bool hasTensorWithOptions(
    const IValue& ivalue,
    c10::optional<c10::ScalarType> dtype,
    c10::optional<c10::Layout> layout) {
  if (!ivalue.isTensor()) {
    return false;
  }
  const auto& tensor = ivalue.toTensor();
  if (dtype == tensor.dtype().toScalarType() &&
      layout == tensor.options().layout_opt()) {
    return true;
  }
  VLOG(1) << ""tensor exists, but tensor options were different"";
  return false;
}

static bool hasTensorWithOptions(
    const IValue& ivalue,
    c10::optional<c10::ScalarType> dtype,
    c10::optional<c10::Layout> layout,
    c10::optional<c10::MemoryFormat> memory_format) {
  return hasTensorWithOptions(ivalue, dtype, layout) &&
      (memory_format == ivalue.toTensor().options().memory_format_opt());
}

REGISTER_OPERATOR_FUNCTOR(aten::full, aten_full, [](Node* n) -> SROperator {
if (!n->matches(torch::schema(
""aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor""))) {
"
134,"namespace torch {
static thread_local bool enable_torch_function = true;
PyObject* disabled_torch_function = nullptr;
bool torch_function_enabled() {
return enable_torch_function;
","namespace torch {
static thread_local bool enable_torch_function = true;
PyObject* disabled_torch_function = nullptr;
  PyObject* disabled_torch_dispatch = nullptr;
bool torch_function_enabled() {
return enable_torch_function;
"
135,"std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.push_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.push_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
","std::vector<std::string> dtype_strs;
std::vector<std::string> device_type_strs;
for (const auto& tensor_dtype : collective_fingerprint.tensor_dtypes_) {
      dtype_strs.emplace_back(
c10::toString(static_cast<at::ScalarType>(tensor_dtype)));
}
for (const auto& tensor_device_type :
collective_fingerprint.tensor_device_types_) {
      device_type_strs.emplace_back(
c10::toString(static_cast<at::DeviceType>(tensor_device_type)));
}
"
136,"TORCH_INTERNAL_ASSERT(
running_mean.has_value() && running_var.has_value(),
""Expect running_mean and running_var to have value when train=false"");
mean_p = running_mean.value().view(view_size);
invstd_p = (1 / at::sqrt(running_var.value() + at::Scalar(eps))).view(view_size);
result_t = input_t * invstd_p;
","TORCH_INTERNAL_ASSERT(
running_mean.has_value() && running_var.has_value(),
""Expect running_mean and running_var to have value when train=false"");
    TORCH_CHECK(
        !running_mean.value()._fw_grad(/*level=*/0).defined() && !running_var.value()._fw_grad(/*level=*/0).defined(),
        ""batch_norm is not differentiable wrt running_mean and running_var, they cannot have forward grad defined"");
mean_p = running_mean.value().view(view_size);
invstd_p = (1 / at::sqrt(running_var.value() + at::Scalar(eps))).view(view_size);
result_t = input_t * invstd_p;
"
137,"namespace torch {
namespace jit {
size_t SourceRangeHasher::operator()(const torch::jit::SourceRange& key) const {
return (
std::hash<uintptr_t>()(reinterpret_cast<uintptr_t>(key.source().get())) ^
std::hash<size_t>()(key.start()) ^ std::hash<size_t>()(key.end()));
}
c10::optional<SourceRange> SourceView::findSourceRangeThatGenerated(
const SourceRange& range) {
if (!gen_ranges_) {
return c10::nullopt;
","namespace torch {
namespace jit {

// A stringlike class backed by a vector of string_view
// the string represented are logically the concatenation of  the string_views
// This has advantage of not needing continues memory.
StringCordView::StringCordView() {
  accumulated_sizes_.push_back(0);
}

StringCordView::StringCordView(
    std::vector<c10::string_view> inputs,
    std::vector<std::shared_ptr<std::string>> ownerships)
    : pieces_(std::move(inputs)), owned_strings_(std::move(ownerships)) {
  accumulated_sizes_.push_back(0);
  size_t running_sum = 0;
  for (auto& s : pieces_) {
    if (s.size() > 0) {
      running_sum += s.size();
      accumulated_sizes_.push_back(running_sum);
    }
  }
}

size_t StringCordView::find(const std::string& tok, size_t start) const {
  if (tok.size() == 0) {
    return 0;
  }

  if ((size() - start) < tok.size()) {
    return std::string::npos;
  }

  Iterator begin = iter_for_pos(start);
  Iterator end_iter = end();
  size_t offset = start;
  for (; begin != end_iter; ++begin, ++offset) {
    if (*begin == tok[0]) {
      auto mis = std::mismatch(begin, end_iter, tok.begin(), tok.end());
      if (mis.second == tok.end()) {
        // no mismatch, and second string (tok) is exhausted.
        return offset;
      }
      if (mis.first == end_iter) {
        // this str is exhausted but tok is not
        return std::string::npos;
      }
    }
  }
  return std::string::npos;
}

StringCordView StringCordView::substr(size_t start, size_t size) const {
  std::vector<c10::string_view> pieces;
  std::vector<std::shared_ptr<std::string>> ownerships;
  if (start >= this->size()) {
    // out of bounds
    return StringCordView();
  }
  if (start + size >= this->size()) {
    size = this->size() - start;
  }
  Iterator begin = iter_for_pos(start);
  Iterator end = iter_for_pos(start + size);

  if (begin.line_ == end.line_) {
    // same line
    pieces.push_back(pieces_[begin.line_].substr(begin.pos_, size));
  } else {
    pieces.push_back(pieces_[begin.line_].substr(begin.pos_));

    size_t last_line = pieces_.size();
    if (end != this->end() && end.line_ < last_line) {
      // end is within the string
      last_line = end.line_;
    }
    for (size_t i = begin.line_ + 1; i < last_line; i++) {
      pieces.push_back(pieces_[i]);
    }
    if (end != this->end()) {
      pieces.push_back(pieces_[end.line_].substr(0, end.pos_));
    }
  }

  // share ownership
  std::copy(
      owned_strings_.begin(),
      owned_strings_.end(),
      std::back_inserter(ownerships));

  return StringCordView(std::move(pieces), std::move(ownerships));
}

bool StringCordView::operator==(const std::string& rhs) {
  if (size() != rhs.size()) {
    return false;
  }
  auto res = std::mismatch(begin(), end(), rhs.begin(), rhs.end());
  // both need to exhaust
  return res.first == end() && res.second == rhs.end();
}

bool StringCordView::operator==(const StringCordView& rhs) {
  if (size() != rhs.size()) {
    return false;
  }
  auto res = std::mismatch(begin(), end(), rhs.begin(), rhs.end());
  // both need to exhaust
  return res.first == end() && res.second == rhs.end();
}

StringCordView::Iterator StringCordView::iter_for_pos(size_t pos) const {
  if (pos == 0) {
    return begin();
  }
  if (pos >= size()) {
    return end();
  }
  auto upper = std::upper_bound(
      accumulated_sizes_.begin(), accumulated_sizes_.end(), pos);
  if (upper == accumulated_sizes_.end()) {
    return end();
  }
  size_t line = upper - accumulated_sizes_.begin() - 1;
  assert(accumulated_sizes_[line] <= pos);
  assert(accumulated_sizes_[line + 1] > pos);
  return Iterator(this, line, pos - accumulated_sizes_[line], size() - pos);
}

size_t SourceRangeHasher::operator()(const torch::jit::SourceRange& key) const {
return (
std::hash<uintptr_t>()(reinterpret_cast<uintptr_t>(key.source().get())) ^
std::hash<size_t>()(key.start()) ^ std::hash<size_t>()(key.end()));
}
c10::optional<SourceRange> Source::findSourceRangeThatGenerated(
const SourceRange& range) {
if (!gen_ranges_) {
return c10::nullopt;
"
138,"return;
}
loaded_sources_.insert(qualifier);
  std::shared_ptr<SourceView> src = source_loader_(qualifier);
// The importer, when looking for classes/functions doesn't know if 'foo'
// contains definitions or if it is a prefix of 'foo.bar', we only figure it
","return;
}
loaded_sources_.insert(qualifier);
  std::shared_ptr<Source> src = source_loader_(qualifier);
// The importer, when looking for classes/functions doesn't know if 'foo'
// contains definitions or if it is a prefix of 'foo.bar', we only figure it
"
139,"serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<SourceView>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
  if (s == nullptr) {
    serialized = c10::ivalue::Tuple::create({"""", """", 0});
} else {
    serialized = c10::ivalue::Tuple::create(
        {s->text(), s->filename(), (int64_t)s->starting_line_no()});
}
serialized_sources[s] = serialized;
return serialized;
","serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
int64_t SourceRangeSerializer::store_text_and_get_index(
    const std::string& text_view) {
  auto text_iter = text_to_idx_.find(text_view);
  if (text_iter == text_to_idx_.end()) {
    int64_t text_pos = static_cast<int64_t>(texts_.size());
    texts_.emplace_back(text_view);
    text_to_idx_[texts_.back().toStringView()] = text_pos;
    return text_pos;
  } else {
    return text_iter->second;
  }
}

c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<Source>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
  c10::List<int64_t> lines;
  if (should_use_format_with_string_table_) {
    if (s == nullptr) {
      serialized = c10::ivalue::Tuple::create({lines, 0, 0});
    } else {
      for (size_t lineno = 0; lineno < s->num_lines(); lineno++) {
        std::string line_content = s->get_line(lineno).str();
        int64_t text_pos = store_text_and_get_index(line_content);
        lines.push_back(text_pos);
      }

      int64_t fname_pos = 0;
      if (s->filename().has_value()) {
        fname_pos = store_text_and_get_index(*s->filename());
      }
      serialized = c10::ivalue::Tuple::create(
          {lines, fname_pos, (int64_t)s->starting_line_no()});
    }
} else {
    if (s == nullptr) {
      serialized = c10::ivalue::Tuple::create({"""", """", 0});
    } else {
      serialized = c10::ivalue::Tuple::create(
          {s->text_str().str(), s->filename(), (int64_t)s->starting_line_no()});
    }
}
serialized_sources[s] = serialized;
return serialized;
"
140,"#include <ATen/core/Reduction.h>
#include <ATen/core/type_factory.h>
#include <c10/util/Optional.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/frontend/lexer.h>
#include <torch/csrc/jit/frontend/parse_string_literal.h>
","#include <ATen/core/Reduction.h>
#include <ATen/core/type_factory.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/frontend/lexer.h>
#include <torch/csrc/jit/frontend/parse_string_literal.h>
"
141,"serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
int64_t SourceRangeSerializer::store_text_and_get_index(
    const std::string& text_view) {
  auto text_iter = text_to_idx_.find(text_view);
  if (text_iter == text_to_idx_.end()) {
    int64_t text_pos = static_cast<int64_t>(texts_.size());
    texts_.emplace_back(text_view);
    text_to_idx_[texts_.back().toStringView()] = text_pos;
    return text_pos;
  } else {
    return text_iter->second;
  }
}

c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<Source>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
  c10::List<int64_t> lines;
if (s == nullptr) {
    serialized = c10::ivalue::Tuple::create({lines, 0, 0});
} else {
    for (size_t lineno = 0; lineno < s->num_lines(); lineno++) {
      std::string line_content = s->get_line(lineno).str();
      int64_t text_pos = store_text_and_get_index(line_content);
      lines.push_back(text_pos);
    }

    int64_t fname_pos = 0;
    if (s->filename().has_value()) {
      fname_pos = store_text_and_get_index(*s->filename());
    }
serialized = c10::ivalue::Tuple::create(
        {lines, fname_pos, (int64_t)s->starting_line_no()});
}
serialized_sources[s] = serialized;
return serialized;
","serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());
}
c10::IValue SourceRangeSerializer::serialize_source(
    const std::shared_ptr<SourceView>& s) {
if (serialized_sources.count(s)) {
return serialized_sources.at(s);
}
c10::intrusive_ptr<c10::ivalue::Tuple> serialized;
if (s == nullptr) {
    serialized = c10::ivalue::Tuple::create({"""", """", 0});
} else {
serialized = c10::ivalue::Tuple::create(
        {s->text(), s->filename(), (int64_t)s->starting_line_no()});
}
serialized_sources[s] = serialized;
return serialized;
"
142,"if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer.deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
","if (tup_elems.size() == 3) {
int64_t debug_handle = tup_elems[kSourceRangeTagIndex].toInt();
auto source_range =
              deserializer->deserialize(tup_elems[kSourceRangeIndex]);
source_range_map.emplace(debug_handle, std::move(source_range));
}
}
"
143,"addr.ai_addr = addr_ptr;
addr.ai_addrlen = addr_len;
  C10D_INFO(""The server socket on {} has accepted a connection from {}."", *this, addr);
auto impl = std::make_unique<SocketImpl>(hnd);
","addr.ai_addr = addr_ptr;
addr.ai_addrlen = addr_len;
  C10D_DEBUG(""The server socket on {} has accepted a connection from {}."", *this, addr);
auto impl = std::make_unique<SocketImpl>(hnd);
"
144,"return ResultBuf;
}
BufHandle makeQBufHandleNHWC(
const std::string& name,
const std::vector<ExprHandle>& dims,
Dtype dtype,
const double qscale,
const int64_t qzero) {
  return makeQBufHandleNHWC(
name,
dims,
dtype,
","return ResultBuf;
}
BufHandle makeQBufHandleChannelsLast(
const std::string& name,
const std::vector<ExprHandle>& dims,
Dtype dtype,
const double qscale,
const int64_t qzero) {
  return makeQBufHandleChannelsLast(
name,
dims,
dtype,
"
145,"// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
double scale1 = immQScale(qa);
  auto ResultBuf = makeQBufHandleNCHW(
""quantized_mul_scalar"",
outputShape,
Dtype(out_qdtype),
","// Change to dtype based on outputType when dtype propagation implemented
const auto out_qdtype = immQDType(qa);
double scale1 = immQScale(qa);
  auto ResultBuf = makeQBufHandleContiguous(
""quantized_mul_scalar"",
outputShape,
Dtype(out_qdtype),
"
146,"q_data.store(&q_k_v_data
[0 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
k_data.store(&q_k_v_data
[1 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
v_data.store(&q_k_v_data
[2 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    num_head * T * dim_per_head +
t * dim_per_head + dh]);
}
}
","q_data.store(&q_k_v_data
[0 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
k_data.store(&q_k_v_data
[1 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
v_data.store(&q_k_v_data
[2 * B * num_head * T * dim_per_head +
b * num_head * T * dim_per_head +
                                    nh * T * dim_per_head +
t * dim_per_head + dh]);
}
}
"
147,"param_size /= elem_size;
if(linear_id == 0 || linear_id == num_linear_layers / 2) {
                const auto size = { static_cast<int64_t>(param_size * num_linear_layers / 2), 1L};
Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), offset, size);
params.emplace_back(std::move(param));
layer_params_count++;
","param_size /= elem_size;
if(linear_id == 0 || linear_id == num_linear_layers / 2) {
                std::initializer_list<int64_t> size = { static_cast<int64_t>(param_size * num_linear_layers / 2), 1L};
Tensor param = at::empty({0}, weight_buf.options()).set_(weight_buf.storage(), offset, size);
params.emplace_back(std::move(param));
layer_params_count++;
"
148,"#endif
}
#if defined(USE_DIRECT_NVRTC)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, at::cuda::load_nvrtc());
}
#elif !defined(USE_ROCM)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, &at::cuda::detail::lazyNVRTC);
}
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
#if defined(_WIN32)
","#endif
}
#if !defined(USE_ROCM)
#if defined(USE_DIRECT_NVRTC)
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, at::cuda::load_nvrtc());
}
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
return std::make_pair(nullptr, &at::cuda::detail::lazyNVRTC);
}
#endif
#else
static std::pair<std::unique_ptr<at::DynamicLibrary>, at::cuda::NVRTC*> load_nvrtc() {
#if defined(_WIN32)
"
149,"const Tensor& qkv_bias,
const Tensor& proj_weight,
const Tensor& proj_bias,
const c10::optional<Tensor>& mask) {
// query shape: [B, T, D]
// qkv_weight shape: [3 * D, D]
","const Tensor& qkv_bias,
const Tensor& proj_weight,
const Tensor& proj_bias,
    const int64_t num_head,
const c10::optional<Tensor>& mask) {
// query shape: [B, T, D]
// qkv_weight shape: [3 * D, D]
"
150,"return handle_torch_function(r, self, args, kwargs, THPVariableClass, ""torch.Tensor"");
}
auto requires_grad = r.toBool(0);
// should we throw if requires_grad is true?  var.requires_grad = True throws here
// but it's nice to let this be a no-op.
","return handle_torch_function(r, self, args, kwargs, THPVariableClass, ""torch.Tensor"");
}
  // temporary hack to improve functorch UX.
  const auto& functorch_tls = at::functorch::functorchTLSAccessor();
  if (functorch_tls) {
    functorch_tls->checkSupportsInplaceRequiresGrad();
  }

auto requires_grad = r.toBool(0);
// should we throw if requires_grad is true?  var.requires_grad = True throws here
// but it's nice to let this be a no-op.
"
151,"} // namespace native
} // namespace at
#endif // AT_MKLDNN_EBABLED
","} // namespace native
} // namespace at
#endif // AT_MKLDNN_ENABLED
"
152,"} // namespace native
} // namespace at
#endif // AT_MKLDNN_EBABLED
","} // namespace native
} // namespace at
#endif // AT_MKLDNN_ENABLED
"
153,"auto* src_data_bytes = data[SRC_ITER_STRIDE_IDX];
// we change the order of TensorIterator-dim loop
// vs dim-TensorIterator loop order depending on
          // whether dim is the last dimension and/or
          // whether `n` is smaller than `index_dim_size`
          if ((dim== self.dim() - 1) || (n < index_dim_size)) {
for (const auto nelem : c10::irange(n)) {
(void)nelem; //Suppress unused variable warning
// dim loop is a separate code block
","auto* src_data_bytes = data[SRC_ITER_STRIDE_IDX];
// we change the order of TensorIterator-dim loop
// vs dim-TensorIterator loop order depending on
          // whether dim is the last dimension
          if (dim== self.dim() - 1) {
for (const auto nelem : c10::irange(n)) {
(void)nelem; //Suppress unused variable warning
// dim loop is a separate code block
"
154,"}
auto numel = add_indices.numel();
int64_t ddim = src.sizes()[1];
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","}
auto numel = add_indices.numel();
int64_t ddim = src.sizes()[1];
  auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
155,"bag_size_data = bag_size.data_ptr<index_t>();
}
auto numel = add_indices.numel();
  int64_t ddim = src.sizes()[1];
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
","bag_size_data = bag_size.data_ptr<index_t>();
}
auto numel = add_indices.numel();
  int64_t ddim = src.size(1);
  auto vocab_size = src.size(0);
auto src_stride0 = src.strides()[0];
auto src_stride1 = src.strides()[1];
auto output_stride0 = output.strides()[0];
"
156,"Tensor& bag_size,
int64_t padding_idx) {
int64_t numIndices = indices.numel();
  int64_t featureSize = weight.sizes()[1];
AT_DISPATCH_INDEX_TYPES(indices.scalar_type(), ""embedding_bag_cpu_max_out"", [&] {
auto* indices_data = indices.data_ptr<index_t>();
auto* offset2bag_data = offset2bag.data_ptr<index_t>();
","Tensor& bag_size,
int64_t padding_idx) {
int64_t numIndices = indices.numel();
  int64_t featureSize = weight.size(1);
  int64_t vocab_size = weight.size(0);
AT_DISPATCH_INDEX_TYPES(indices.scalar_type(), ""embedding_bag_cpu_max_out"", [&] {
auto* indices_data = indices.data_ptr<index_t>();
auto* offset2bag_data = offset2bag.data_ptr<index_t>();
"
157,"// Allow only if the node has a shape function defined.
// ListConstruct node is an exception since that is needed to fuse
// aten::cat, though it does not have a shape function.
      REQ(node->kind() == prim::ListConstruct ||
(node->maybeSchema() && shapeComputeGraphForSchema(node->schema())));
}
","// Allow only if the node has a shape function defined.
// ListConstruct node is an exception since that is needed to fuse
// aten::cat, though it does not have a shape function.
      REQ(node->kind() == prim::ListConstruct || node->kind() == prim::TensorExprGroup ||
(node->maybeSchema() && shapeComputeGraphForSchema(node->schema())));
}
"
158,"{""aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor"", ""unary""},
{""aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"", ""broadcast""},
{""aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor"", ""broadcast""},
      {""aten::type_as(Tensor self, Tensor other) -> Tensor"", ""broadcast""},
{""aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"", ""broadcast""},
{""aten::mul.Tensor(Tensor self, Tensor other) -> Tensor"", ""broadcast""},
{""aten::div.Tensor(Tensor self, Tensor other) -> Tensor"", ""broadcast""},
","{""aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor"", ""unary""},
{""aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"", ""broadcast""},
{""aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor"", ""broadcast""},
      {""aten::type_as(Tensor self, Tensor other) -> Tensor"", ""unary""},
{""aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"", ""broadcast""},
{""aten::mul.Tensor(Tensor self, Tensor other) -> Tensor"", ""broadcast""},
{""aten::div.Tensor(Tensor self, Tensor other) -> Tensor"", ""broadcast""},
"
159,"std::vector<ExprHandle> newAxes(axes.begin(), axes.end());
ExprHandle load = promoteToDtype(
tensorOrConstant(nonEmptyInputs[0], newAxes), highType);
        auto offset = *intValue(nonEmptyInputs[0].node()->dim(dim));
        newAxes[dim] = newAxes[dim] - ExprHandle(immLike(newAxes[dim], offset));
for (size_t ii = 1; ii < nonEmptyInputs.size(); ++ii) {
auto input = nonEmptyInputs[ii];
","std::vector<ExprHandle> newAxes(axes.begin(), axes.end());
ExprHandle load = promoteToDtype(
tensorOrConstant(nonEmptyInputs[0], newAxes), highType);
        auto offset = ExprHandle(nonEmptyInputs[0].node()->dim(dim));
        newAxes[dim] = newAxes[dim] - offset;
for (size_t ii = 1; ii < nonEmptyInputs.size(); ++ii) {
auto input = nonEmptyInputs[ii];
"
160,"return upgraderBytecodeList;
}
} // namespace jit
} // namespace torch
","return upgraderBytecodeList;
}
// clang-format on

} // namespace jit
} // namespace torch
"
161,"#include <torch/csrc/utils/python_strings.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
#include <exception>
#include <functional>
","#include <torch/csrc/utils/python_strings.h>
#include <torch/csrc/DynamicTypes.h>
#include <torch/csrc/Exceptions.h>
#include <ATen/FuncTorchTLS.h>
#include <exception>
#include <functional>
"
162,"fbb,
fbb.CreateVector(keys),
fbb.CreateVector(values),
      fbb.CreateSharedString(ivalue.type()->annotation_str()));
}
flatbuffers::Offset<mobile::serialization::ObjectType> FlatbufferSerializer::
","fbb,
fbb.CreateVector(keys),
fbb.CreateVector(values),
      fbb.CreateSharedString(ivalue.type<c10::Type>()->annotation_str()));
}
flatbuffers::Offset<mobile::serialization::ObjectType> FlatbufferSerializer::
"
163,"bool keepdim,
const c10::string_view interpolation,
Tensor& out) {
  quantile_impl(
out,
self,
q,
","bool keepdim,
const c10::string_view interpolation,
Tensor& out) {
  quantile_out_impl(
out,
self,
q,
"
164,"return ret;
}
} // namespace jit
} // namespace torch
","return ret;
}
/* static */ ClassDef ClassDef::create(
    const SourceRange& range,
    const Ident& name,
    const Maybe<Expr>& superclass,
    const List<Stmt>& body,
    const List<Property>& properties,
    const List<Assign>& assigns) {
  return ClassDef(Compound::create(
      TK_CLASS_DEF,
      range,
      {name,
       superclass,
       body,
       Maybe<List<Property>>::create(range, properties),
       Maybe<List<Assign>>::create(range, assigns)}));
}

} // namespace jit
} // namespace torch
"
165,"#include <ATen/SavedTensorHooks.h>
#include <c10/util/Exception.h>
namespace at {
namespace {
// PyObject is defined in c10/util/python_stub.h
  // Reference counting is handled by the caller of `set_hooks`.
  thread_local PyObject* pack_hook_(nullptr);
  thread_local PyObject* unpack_hook_(nullptr);
// This flag is set to true the first time default hooks are registered
// and left at true for the rest of the execution.
","#include <ATen/SavedTensorHooks.h>
#include <c10/util/Exception.h>
#include <stack>
namespace at {
namespace {
// PyObject is defined in c10/util/python_stub.h
  thread_local std::stack<std::pair<PyObject*, PyObject*>> stack;
// This flag is set to true the first time default hooks are registered
// and left at true for the rest of the execution.
"
166,"is_enabled = true;
}
void SavedTensorDefaultHooks::set_hooks(PyObject* pack_hook, PyObject* unpack_hook) {
  if (!is_enabled) {
    TORCH_INTERNAL_ASSERT(pack_hook == nullptr && unpack_hook == nullptr);
    return;
  }
  pack_hook_ = pack_hook;
  unpack_hook_ = unpack_hook;
}
std::pair<PyObject*, PyObject*> SavedTensorDefaultHooks::get_hooks() {
  if (!is_enabled) {
return std::make_pair(nullptr, nullptr);
}
  return std::make_pair(pack_hook_, unpack_hook_);
}
}
","is_enabled = true;
}
void SavedTensorDefaultHooks::push_hooks(PyObject* pack_hook, PyObject* unpack_hook) {
  // Reference counting is handled by the caller of `push_hooks`
  TORCH_INTERNAL_ASSERT(is_enabled);
  TORCH_INTERNAL_ASSERT(pack_hook != nullptr && unpack_hook != nullptr);
  stack.push(std::make_pair(pack_hook, unpack_hook));
}

void SavedTensorDefaultHooks::pop_hooks() {
  // Reference counting is handled by the caller of `pop_hooks`
  TORCH_INTERNAL_ASSERT(is_enabled && !stack.empty());
  stack.pop();
}
std::pair<PyObject*, PyObject*> SavedTensorDefaultHooks::get_hooks() {
  if (!is_enabled || stack.empty()) {
return std::make_pair(nullptr, nullptr);
}
  return stack.top();
}

std::stack<std::pair<PyObject*, PyObject*>> SavedTensorDefaultHooks::get_stack() {
  return stack;
}

void SavedTensorDefaultHooks::set_stack(std::stack<std::pair<PyObject*, PyObject*>> stack_) {
  stack = stack_;
}
}
"
167,"#include <torch/csrc/THP.h>
PyObject *THPException_FatalError;
#define ASSERT_TRUE(cond) if (!(cond)) return false
bool THPException_init(PyObject *module)
{
ASSERT_TRUE(THPException_FatalError = PyErr_NewException(""torch.FatalError"", nullptr, nullptr));
ASSERT_TRUE(PyModule_AddObject(module, ""FatalError"", THPException_FatalError) == 0);
return true;
}
","#include <torch/csrc/THP.h>
PyObject *THPException_FatalError, *THPException_LinAlgError;
#define ASSERT_TRUE(cond) if (!(cond)) return false
bool THPException_init(PyObject *module)
{
ASSERT_TRUE(THPException_FatalError = PyErr_NewException(""torch.FatalError"", nullptr, nullptr));
ASSERT_TRUE(PyModule_AddObject(module, ""FatalError"", THPException_FatalError) == 0);

  // Set the doc string here since _add_docstr throws malloc errors if tp_doc is modified
  // for an error class.
  ASSERT_TRUE(THPException_LinAlgError = PyErr_NewExceptionWithDoc(""torch._C._LinAlgError"",
    ""Error raised by torch.linalg function when the cause of error is a numerical inconsistency in the data.\n \
For example, you can the torch.linalg.inv function will raise torch.linalg.LinAlgError when it finds that \
a matrix is not invertible.\n \
\n\
Example:\n \
>>> matrix = torch.eye(3, 3)\n \
>>> matrix[-1, -1] = 0\n \
>>> matrix\n \
    tensor([[1., 0., 0.],\n \
            [0., 1., 0.],\n \
            [0., 0., 0.]])\n \
>>> torch.linalg.inv(matrix)\n \
Traceback (most recent call last):\n \
File \""<stdin>\"", line 1, in <module>\n \
torch._C._LinAlgError: torch.linalg.inv: The diagonal element 3 is zero, the inversion\n \
could not be completed because the input matrix is singular."", PyExc_RuntimeError, nullptr));
  ASSERT_TRUE(PyModule_AddObject(module, ""_LinAlgError"", THPException_LinAlgError) == 0);

return true;
}
"
168,"return cat_sparse(tensors, dim);
}
  check_cat_no_zero_dim(tensors);
  dim = legacy_cat_wrap_dim(dim, tensors);
auto maybe_outnames = namedinference::compute_cat_outnames(tensors);
Tensor result;
{
","return cat_sparse(tensors, dim);
}
auto maybe_outnames = namedinference::compute_cat_outnames(tensors);
Tensor result;
{
"
169,"Tensor& stack_out(TensorList tensors, int64_t dim, Tensor& result) {
TORCH_CHECK(tensors.size() > 0,
""stack expects a non-empty TensorList"");
  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);
return at::cat_out(result, get_stack_inputs(tensors, dim), dim);
}
","Tensor& stack_out(TensorList tensors, int64_t dim, Tensor& result) {
TORCH_CHECK(tensors.size() > 0,
""stack expects a non-empty TensorList"");
return at::cat_out(result, get_stack_inputs(tensors, dim), dim);
}
"
170,"TORCH_CHECK(
all_inputs_sharing_qparams(qxs),
""All inputs should share the same quantization parameters."");

double _scale = qxs[0].q_scale();
int64_t _zero_point = qxs[0].q_zero_point();
return quantized_cat_impl<false>(c10::List<Tensor>(qxs), dim, _scale, _zero_point);
","TORCH_CHECK(
all_inputs_sharing_qparams(qxs),
""All inputs should share the same quantization parameters."");
  check_cat_no_zero_dim(qxs);
  dim = legacy_cat_wrap_dim(dim, qxs);
double _scale = qxs[0].q_scale();
int64_t _zero_point = qxs[0].q_zero_point();
return quantized_cat_impl<false>(c10::List<Tensor>(qxs), dim, _scale, _zero_point);
"
171,"}
void standardizeVectorForUnion(std::vector<TypePtr>& reference, std::vector<TypePtr>* to_fill) {
  for (auto type : reference) {
flattenUnion(type, to_fill);
}
filterDuplicateSubtypes(to_fill);
","}
void standardizeVectorForUnion(std::vector<TypePtr>& reference, std::vector<TypePtr>* to_fill) {
  for (const auto& type : reference) {
flattenUnion(type, to_fill);
}
filterDuplicateSubtypes(to_fill);
"
172,"const bool include_last_offset,
const bool requires_grad) {
if (requires_grad || mode == MODE_MEAN || mode == MODE_MAX) {
    auto num_bags = offsets.size(0) - (include_last_offset ? 1 : 0);
    bag_size_out = at::zeros({num_bags}, offsets.options());
// Compute this for MODE_MEAN and MODE_MAX (latter needed for backwards)
if (num_bags != 1) {
bag_size_out.slice(0, 0, bag_size_out.sizes()[0] - 1, 1) =
","const bool include_last_offset,
const bool requires_grad) {
if (requires_grad || mode == MODE_MEAN || mode == MODE_MAX) {
    auto num_bags = offsets.sizes()[0] - (include_last_offset ? 1 : 0);
    at::native::resize_(bag_size_out, {num_bags}, c10::nullopt);
// Compute this for MODE_MEAN and MODE_MAX (latter needed for backwards)
if (num_bags != 1) {
bag_size_out.slice(0, 0, bag_size_out.sizes()[0] - 1, 1) =
"
173,"float StaticRuntime::benchmark_model(
const std::vector<std::vector<c10::IValue>>& args_list,
    const std::vector<std::unordered_map<std::string, c10::IValue>>&
        kwargs_list,
const int warmup_runs,
const int main_runs) {
TORCH_CHECK(warmup_runs >= 0 && main_runs >= 1);
","float StaticRuntime::benchmark_model(
const std::vector<std::vector<c10::IValue>>& args_list,
    const std::vector<KeywordArgs>& kwargs_list,
const int warmup_runs,
const int main_runs) {
TORCH_CHECK(warmup_runs >= 0 && main_runs >= 1);
"
174,"double output_scale,
int64_t output_zero_point);
#endif // USE_FBGEMM
#ifdef USE_PYTORCH_QNNPACK
","double output_scale,
int64_t output_zero_point);
template at::Tensor PackedConvWeight<2>::apply_impl<false>(
    const at::Tensor& act,
    double output_scale,
    int64_t output_zero_point);

template at::Tensor PackedConvWeight<3>::apply_impl<false>(
  const at::Tensor& act,
  double output_scale,
  int64_t output_zero_point);

#endif // USE_FBGEMM
#ifdef USE_PYTORCH_QNNPACK
"
175,"// for all the values in the alias set,
// we set them ""alive""
for (auto* aliased_v : refined_aliases) {
      GRAPH_DEBUG(""aliased_v: %"", aliased_v->debugName());
add_live_value_fn(aliased_v);
}
};
","// for all the values in the alias set,
// we set them ""alive""
for (auto* aliased_v : refined_aliases) {
      GRAPH_DEBUG(
          ""aliased_v: %"",
          aliased_v->debugName(),
          "" (for %"",
          v->debugName(),
          "")"");
add_live_value_fn(aliased_v);
}
};
"
176,"// >>> x = torch.randn(B0)  # the per-examples are all scalars
// >>> vmap(partial(torch.sum, dim=0), x)
// then we replicate the behavior of sum(scalar_tensor, dim=0).
  if (/*logical*/self.dim() == 0 && dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])) {
return self.clone();
}
auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
","// >>> x = torch.randn(B0)  # the per-examples are all scalars
// >>> vmap(partial(torch.sum, dim=0), x)
// then we replicate the behavior of sum(scalar_tensor, dim=0).
  if (/*logical*/self.dim() == 0 && (dims.size() == 0 || (dims.size() == 1 && is_allowed_dim_on_scalar_tensor(dims[0])))) {
return self.clone();
}
auto self_physical = MultiBatchVmapTransform::logicalToPhysical(self);
"
177,"}),
py::arg(""pg""),
py::arg(""gloo_pg""),
              py::call_guard<py::gil_scoped_release>());
#endif
#ifdef USE_C10D_NCCL
","}),
py::arg(""pg""),
py::arg(""gloo_pg""),
              py::call_guard<py::gil_scoped_release>())
         .def_property_readonly(
              ""wrapped_pg"", &::c10d::ProcessGroupWrapper::getWrappedPg
         );
#endif
#ifdef USE_C10D_NCCL
"
178,"Node* node,
std::unique_ptr<const IValue*[]> inputs,
size_t inputsSize,
    bool enable_out_variant)
: node_(node),
inputs_(std::move(inputs)),
inputs_size_(inputsSize),
","Node* node,
std::unique_ptr<const IValue*[]> inputs,
size_t inputsSize,
    bool enable_out_variant,
    bool check_memory_overlap)
: node_(node),
inputs_(std::move(inputs)),
inputs_size_(inputsSize),
"
179,"}
DCHECK_EQ(managed_tensor_storage_impls_.size(), managed_tensors_.size());
// for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so
// that the objects pointed to by *iv may be reclaimed by reference counting
","}
DCHECK_EQ(managed_tensor_storage_impls_.size(), managed_tensors_.size());
  VLOG(1) << ""managed_bytes: "" << managed_bytes_;
// for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so
// that the objects pointed to by *iv may be reclaimed by reference counting
"
180,"cpu_dtype : int):
self_dtype = self.dtype
def backward(grad_output):
                return grad_output.to(self_dtype)
return torch._autocast_to_reduced_precision(self, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype), backward
","cpu_dtype : int):
self_dtype = self.dtype
def backward(grad_output):
                return grad_output.to(self_dtype), None, None, None, None
return torch._autocast_to_reduced_precision(self, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype), backward
"
181,"#include <benchmark/benchmark.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/tensorexpr/ir.h>
","#include <benchmark/benchmark.h>
#include ""ATen/Functions.h""
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/tensorexpr/ir.h>
"
182,"}
}
}
auto schema = node()->maybeSchema();
// skip memory overlap check for mutable ops with only one output
if (!schema || (schema->is_mutable() && outputs_size_ == 1)) {
","}
}
}
  return true;
}
bool ProcessedNode::verify_inputs_dont_overlap_outputs() const {
auto schema = node()->maybeSchema();
// skip memory overlap check for mutable ops with only one output
if (!schema || (schema->is_mutable() && outputs_size_ == 1)) {
"
183,"}
std::tuple<Tensor, Tensor> linalg_eigh(const Tensor& input, c10::string_view uplo) {
  squareCheckInputs(input);
checkUplo(uplo);
ScalarType real_dtype = toValueType(input.scalar_type());
Tensor values = at::empty({0}, input.options().dtype(real_dtype));
","}
std::tuple<Tensor, Tensor> linalg_eigh(const Tensor& input, c10::string_view uplo) {
  squareCheckInputs(input, ""linalg.eigh"");
checkUplo(uplo);
ScalarType real_dtype = toValueType(input.scalar_type());
Tensor values = at::empty({0}, input.options().dtype(real_dtype));
"
184,"// Mathematics 2019, 7, 1174.
//
Tensor linalg_matrix_exp(const Tensor& a) {
  squareCheckInputs(a);
  TORCH_CHECK((at::isFloatingType(a.scalar_type()) || at::isComplexType(a.scalar_type())),
              ""Expected a floating point or complex tensor as input. Got: "", a.scalar_type());
NoTF32Guard disable_tf32;
","// Mathematics 2019, 7, 1174.
//
Tensor linalg_matrix_exp(const Tensor& a) {
  squareCheckInputs(a, ""linalg.matrix_exp"");
  checkFloatingOrComplex(a, ""matrix_exp"");
NoTF32Guard disable_tf32;
"
185,"""but got "", opt_dtype.value());
}
ScalarType in_dtype = opt_dtype.value_or(self.scalar_type());
  TORCH_CHECK(
      at::isFloatingType(in_dtype) || at::isComplexType(in_dtype),
      ""linalg.vector_norm only supports floating point and complex dtypes, but got: "",
      toString(in_dtype));
IntArrayRef dim = opt_dim.value_or(IntArrayRef{});
","""but got "", opt_dtype.value());
}
  checkFloatingOrComplex(self, ""linalg.vector_norm"");
ScalarType in_dtype = opt_dtype.value_or(self.scalar_type());
IntArrayRef dim = opt_dim.value_or(IntArrayRef{});
"
186,"// math_dispatch_keyset contains all keys in backend_dispatch_keyset and
// autograd_dispatch_keyset Alias key DispatchKey::CompositeImplicitAutograd
// maps to math_dispatch_keyset.
constexpr DispatchKeySet math_dispatch_keyset = backend_dispatch_keyset |
    autograd_dispatch_keyset | DispatchKeySet({DispatchKey::FuncTorchBatched});
DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t) {
TORCH_INTERNAL_ASSERT(t != DispatchKey::Undefined);
","// math_dispatch_keyset contains all keys in backend_dispatch_keyset and
// autograd_dispatch_keyset Alias key DispatchKey::CompositeImplicitAutograd
// maps to math_dispatch_keyset.
constexpr DispatchKeySet math_dispatch_keyset =
    backend_dispatch_keyset | autograd_dispatch_keyset;
DispatchKeySet getRuntimeDispatchKeySet(DispatchKey t) {
TORCH_INTERNAL_ASSERT(t != DispatchKey::Undefined);
"
187,"return can_support;
}
namespace {
void OptimizeGraph(
","return can_support;
}
std::string dumpValueSet(
    const FastSet<const Value*>& value_set,
    const char* set_name) {
  std::ostringstream oss;
  oss << set_name << "": {"";
  for (const auto* val : value_set) {
    oss << ""%"" << val->debugName() << "", "";
  }
  oss << ""}"";
  return oss.str();
}

namespace {
void OptimizeGraph(
"
188,"case TypeKind::UnionType: {
AliasTypeSet mutable_types;
for (const TypePtr& inner :
             type->expect<UnionType>()->containedTypes()) {
if (auto maybe_inner_types = mapTypeToAliasTypeSet(inner)) {
mutable_types.insert(
mutable_types.end(),
","case TypeKind::UnionType: {
AliasTypeSet mutable_types;
for (const TypePtr& inner :
             type->expectRef<UnionType>().containedTypes()) {
if (auto maybe_inner_types = mapTypeToAliasTypeSet(inner)) {
mutable_types.insert(
mutable_types.end(),
"
189,"#include <ATen/WrapDimUtils.h>
#include <ATen/core/DimVector.h>
#include <c10/util/Exception.h>
#include <c10/core/ScalarType.h>
#include <c10/core/Scalar.h>
","#include <ATen/WrapDimUtils.h>
#include <ATen/core/DimVector.h>
#include <c10/util/Exception.h>
#include <c10/util/irange.h>
#include <c10/core/ScalarType.h>
#include <c10/core/Scalar.h>
"
190,"namespace c10 {
TypeVerbosity type_verbosity() {
static const char* c_verbosity = std::getenv(""PYTORCH_JIT_TYPE_VERBOSITY"");
static TypeVerbosity verbosity = c_verbosity ?
","namespace c10 {
namespace {
inline bool is_contiguous_strides(
    const IntArrayRef sizes,
    const IntArrayRef strides) {
  int n_dim = static_cast<int>(sizes.size());

  if (n_dim == 0 || strides[n_dim-1] != 1) {
    return false;
  }

  for (int i = n_dim - 2; i >= 0; i--) {
    if (strides[i] != strides[i+1] * sizes[i+1]) {
      return false;
    }
  }
  return true;
}

} // namespace

TypeVerbosity type_verbosity() {
static const char* c_verbosity = std::getenv(""PYTORCH_JIT_TYPE_VERBOSITY"");
static TypeVerbosity verbosity = c_verbosity ?
"
191,"if (j != 0 && inner_dim != -1) {
// we are not looking at dim-j, but dim-sorted_index, which
// is the j-th fastest dim;
        // TODO: merge this with above and put a long comment there
        if (t_strides[sorted_index] < t_strides[inner_dim]) {
return false;
}
}
","if (j != 0 && inner_dim != -1) {
// we are not looking at dim-j, but dim-sorted_index, which
// is the j-th fastest dim;
        // Note: we ignore 0-stride dimension, since eager logic on stride
        // indices is ambiguous
        if (t_strides[sorted_index] != 0 && t_strides[inner_dim] != 0 &&
            t_strides[sorted_index] < t_strides[inner_dim]) {
return false;
}
}
"
192,"// make bag_size output deterministic
at::native::zero_(bag_size);
}
    max_indices = bag_size;
} else { // MODE_MAX
AT_DISPATCH_FLOATING_TYPES_AND_HALF(
weight.scalar_type(), ""embedding_bag_cpu_max_out"", [&]() {
","// make bag_size output deterministic
at::native::zero_(bag_size);
}
     max_indices.copy_(bag_size);
} else { // MODE_MAX
AT_DISPATCH_FLOATING_TYPES_AND_HALF(
weight.scalar_type(), ""embedding_bag_cpu_max_out"", [&]() {
"
193,"if (((max_index_dim - min_index_dim + 1) != tensor_ind_count) &&
tensor_ind_count != 0) {
AT_ERROR(
        ""Only consecutive 1-d tensor indices are supported in exporting aten::index_put to ONNX."");
}
size_t tensor_ind_offset = tensor_ind_count == 0 ? 0 : tensor_ind_count - 1;
","if (((max_index_dim - min_index_dim + 1) != tensor_ind_count) &&
tensor_ind_count != 0) {
AT_ERROR(
        ""Only consecutive 1-d tensor indices are supported in exporting aten::index_put to ONNX."",
        ""Check https://pytorch.org/docs/stable/onnx.html#indexing for details"");
}
size_t tensor_ind_offset = tensor_ind_count == 0 ? 0 : tensor_ind_count - 1;
"
194,"n->destroy();
return c10::nullopt;
};
} else if (
(val.isGenericDict() && insertableIValue(val)) || (val.isEnum()) ||
(val.isObject() && !val.toObjectRef().type()->is_module())) {
","n->destroy();
return c10::nullopt;
};
    // TODO: bail on inserting object with owning compilation unit reference
    // see: [Constant Object Weak CompilationUnit Reference]
} else if (
(val.isGenericDict() && insertableIValue(val)) || (val.isEnum()) ||
(val.isObject() && !val.toObjectRef().type()->is_module())) {
"
195,"} else {
attr = overrideGradient(attr);
}
if (auto attrVal = tryInsertConstant(*graph, attr)) {
paramConst = *attrVal;
} else {
","} else {
attr = overrideGradient(attr);
}
            if (attr.isObject()) {
              if (object_memo_.count(attr.toObject())) {
                attr = object_memo_[attr.toObject()];
              } else {
                auto weak_class_obj = attr.toObject()->copy_to_weak_compilation_ref();
                object_memo_[attr.toObject()] = weak_class_obj;
                attr = weak_class_obj;
              }
            }
if (auto attrVal = tryInsertConstant(*graph, attr)) {
paramConst = *attrVal;
} else {
"
196,": nullptr,
/*output=*/output_data,
/*compressed_indices_table=*/compressed_indices_mapping_data);
    TORCH_CHECK(
        success,
        ""FBGEMM GenerateEmbeddingSpMDMNBitRowWiseSparse kernel failed for "",
        bit_width,
        ""-bit input"");
}
return output;
#else
",": nullptr,
/*output=*/output_data,
/*compressed_indices_table=*/compressed_indices_mapping_data);
    if (!success) {
      fbgemm_spmdm_report_error_(
          output_size,
          index_size,
          compressed_index_size,
          offsets_data,
          indices_data);
    }
}
return output;
#else
"
197,"return ouput_model_stream;
}
void writeArchiveV5(
    PyTorchStreamWriter& writer,
    const IValue& value,
    const std::string& archive_name,
    const std::string& archive_dir,
    const std::string& tensor_dir,
    bool use_storage_context,
    SerializationStorageContext& storage_context) {
  std::vector<char> data;
  // Vector to capture the run-time class types during pickling the IValues
  std::vector<c10::ClassTypePtr> memoizedClassTypes;
  std::vector<std::string> tensor_names;
  Pickler data_pickle(
      [&](const char* buf, size_t size) {
        data.insert(data.end(), buf, buf + size);
      },
      nullptr,
      nullptr,
      &memoizedClassTypes,
      [&](const at::Tensor& tensor) {
        // returns a string to use in picker.cpp as storage obj key
        if (use_storage_context) {
          std::string string_id =
              std::to_string(reinterpret_cast<std::intptr_t>(
                  tensor.storage().unsafeGetStorageImpl()));
          tensor_names.push_back(string_id + "".storage"");
          storage_context.getOrAddStorage(tensor.storage());
        } else {
          tensor_names.push_back(std::to_string(tensor_names.size()));
        }
        return tensor_names.back();
      });
  data_pickle.protocol();
  data_pickle.pushIValue(value);
  data_pickle.stop();
  // write out tensor data
  size_t i = 0;
  std::string prefix = archive_name + ""/"";

  TORCH_INTERNAL_ASSERT(tensor_names.size() == data_pickle.tensorData().size());
  const std::unordered_set<std::string>& pre_serialized_files =
      writer.getAllWrittenRecords();

  for (const auto& td : data_pickle.tensorData()) {
    WriteableTensorData writable_td = getWriteableTensorData(td);
    std::string fname = tensor_dir + tensor_names[i++];
    if (use_storage_context &&
        std::find(
            pre_serialized_files.begin(), pre_serialized_files.end(), fname) !=
            pre_serialized_files.end()) {
      // storage has been serialzed already, skip
      continue;
    }
    writer.writeRecord(fname, writable_td.data(), writable_td.sizeInBytes());
  }

  std::string fname = archive_dir + archive_name + "".pkl"";
  writer.writeRecord(fname, data.data(), data.size());
}

std::stringstream backport_v6_to_v5(std::stringstream& input_model_stream) {
std::shared_ptr<IStreamAdapter> rai =
std::make_shared<IStreamAdapter>(&input_model_stream);
auto reader = std::make_shared<PyTorchStreamReader>(rai);
  std::vector<IValue> constants_values =
      readArchive(kArchiveNameConstants, *reader.get()).toTuple()->elements();
// If there are debug info files in the original model file, it should also
// show up in the backported model
","return ouput_model_stream;
}
/*
Backport function bytecode v6 that introduced support for operators with default
arguments in mobile. Previously, in v5, there is no number of specified
arguments for operators in bytecode operator table. In v6, operators are aware
of the number of specified arguments being present in the schema.

The bump was needed because the v6 bytecode specifies number of specified
arguments for operators in the schema, since the runtime code is now able to
query the number of specified arguments and supports default arguments.

For example, aten::foo's schema in v5 is
foo(Tensor a, Tensor b) -> Tensor
and in v6, it's
foo(Tensor a, Tensor b, int groups=1) -> Tensor

Accordingly, the operator table in v5 is:
('operators', (('aten::foo', ''),))
and in v6, it's
('operators', (('aten::foo', '', 2),))

Thus, the backport is necessary such that the bytecode operator table contains
number of specified arguments.
*/
std::stringstream backport_v6_to_v5(std::stringstream& input_model_stream) {
std::shared_ptr<IStreamAdapter> rai =
std::make_shared<IStreamAdapter>(&input_model_stream);
auto reader = std::make_shared<PyTorchStreamReader>(rai);
// If there are debug info files in the original model file, it should also
// show up in the backported model
"
198,"BackportManager::BackportManager() {
registerBytecodeBackportFunction(kBytecodeVersionV5, backport_v5_to_v4);
registerBytecodeBackportFunction(kBytecodeVersionV6, backport_v6_to_v5);
}
std::unordered_map<
","BackportManager::BackportManager() {
registerBytecodeBackportFunction(kBytecodeVersionV5, backport_v5_to_v4);
registerBytecodeBackportFunction(kBytecodeVersionV6, backport_v6_to_v5);
  registerBytecodeBackportFunction(kBytecodeVersionV7, backport_v7_to_v6);
}
std::unordered_map<
"
199,"}                                                           \
Type* ptr##Name = static_cast<Type*>(ptr);                  \
for (const auto i : c10::irange(index.size())) {            \
ptr##Name[index[i]] = value[i];                           \
}                                                           \
} break;
","}                                                           \
Type* ptr##Name = static_cast<Type*>(ptr);                  \
for (const auto i : c10::irange(index.size())) {            \
      GRAPH_DEBUG(                                              \
          ""STORE: ptr="",                                        \
          ptr##Name,                                            \
          "", buf="",                                             \
          v->buf()->name_hint(),                                \
          "", idx="",                                             \
          index[i],                                             \
          "", val="",                                             \
          (int)value[i]);                                       \
ptr##Name[index[i]] = value[i];                           \
}                                                           \
} break;
"
200,"bool destUnsigned = v->dtype().scalar_type() == ScalarType::Byte ||
v->dtype().scalar_type() == ScalarType::Bool;
// Scalar casts
if (srcType->isFPOrFPVectorTy()) {
","bool destUnsigned = v->dtype().scalar_type() == ScalarType::Byte ||
v->dtype().scalar_type() == ScalarType::Bool;
  bool srcUnsigned =
      v->src_value()->dtype().scalar_type() == ScalarType::Byte ||
      v->src_value()->dtype().scalar_type() == ScalarType::Bool;
// Scalar casts
if (srcType->isFPOrFPVectorTy()) {
"
201,"throw unimplemented_lowering(v);
}
if (dstType->isFPOrFPVectorTy()) {
    if (destUnsigned) {
value_ = irb_.CreateUIToFP(value_, dstType);
} else {
value_ = irb_.CreateSIToFP(value_, dstType);
","throw unimplemented_lowering(v);
}
if (dstType->isFPOrFPVectorTy()) {
    if (srcUnsigned) {
value_ = irb_.CreateUIToFP(value_, dstType);
} else {
value_ = irb_.CreateSIToFP(value_, dstType);
"
202,"TORCH_CHECK(y.scalar_type() != kBool && x.scalar_type() != kBool, ""cumulative_trapezoid: received a bool input for `x` or `y`, but bool is not supported"")
Tensor x_viewed;
if (x.dim() == 1) {
TORCH_CHECK(x.size(0) == y.size(dim), ""cumulative_trapezoid: There must be one `x` value for each sample point"");
DimVector new_sizes(y.dim(), 1); // shape = [1] * y.
new_sizes[dim] = x.size(0); // shape[axis] = d.shape[0]
x_viewed = x.view(new_sizes);
} else if (x.dim() < y.dim()) {
DimVector new_sizes = add_padding_to_shape(x.sizes(), y.dim());
x_viewed = x.view(new_sizes);
} else {
","TORCH_CHECK(y.scalar_type() != kBool && x.scalar_type() != kBool, ""cumulative_trapezoid: received a bool input for `x` or `y`, but bool is not supported"")
Tensor x_viewed;
if (x.dim() == 1) {
        // See trapezoid for implementation notes
TORCH_CHECK(x.size(0) == y.size(dim), ""cumulative_trapezoid: There must be one `x` value for each sample point"");
DimVector new_sizes(y.dim(), 1); // shape = [1] * y.
new_sizes[dim] = x.size(0); // shape[axis] = d.shape[0]
x_viewed = x.view(new_sizes);
} else if (x.dim() < y.dim()) {
        // See trapezoid for implementation notes
DimVector new_sizes = add_padding_to_shape(x.sizes(), y.dim());
x_viewed = x.view(new_sizes);
} else {
"
203,"std::string linear_before_inline = R""(
graph(%linear, %input, %weight, %bias):
        %r = prim::CallFunction(%linear, %input, %weight, %bias)
        return (%r))"";
std::string prepacked_ops_pattern_before_inline = R""(
graph(%linear, %input, %weight, %bias):
%output_min_max : None = prim::Constant()
","std::string linear_before_inline = R""(
graph(%linear, %input, %weight, %bias):
        %res = prim::CallFunction(%linear, %input, %weight, %bias)
        return (%res))"";
std::string prepacked_ops_pattern_before_inline = R""(
graph(%linear, %input, %weight, %bias):
%output_min_max : None = prim::Constant()
"
204,"%packed_weight_bias : __torch__.torch.classes.xnnpack.Conv2dOpContext = prepacked::conv2d_clamp_prepack(
%weight, %bias, %stride, %padding, %dilation, %groups,
%output_min, %output_max)
        %r = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        return (%r) )"";
std::string linear_prepack_run_relu = R""(
graph(%input, %weight, %bias, %dummy_min_max):
","%packed_weight_bias : __torch__.torch.classes.xnnpack.Conv2dOpContext = prepacked::conv2d_clamp_prepack(
%weight, %bias, %stride, %padding, %dilation, %groups,
%output_min, %output_max)
        %res = prepacked::conv2d_clamp_run(%input, %packed_weight_bias)
        return (%res) )"";
std::string linear_prepack_run_relu = R""(
graph(%input, %weight, %bias, %dummy_min_max):
"
205,"return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(old_indices_.size() == v->indices().size());
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
","return IRMutator::mutate(v);
}
    TORCH_INTERNAL_ASSERT(
        old_indices_.size() == v->indices().size(),
        buildErrorMessage(
            ""Expected ranks to match in RfactorStoreRewriter in the fuser.""));
bool equal_indices = true;
for (size_t i = 0; i < v->indices().size(); ++i) {
"
206,"bool AccessInfo::overlaps(const std::shared_ptr<AccessInfo>& other) {
// All accesses to a buf must have the same dimensionality.
  TORCH_INTERNAL_ASSERT(indices_.size() == other->indices().size());
auto& other_indices = other->indices();
","bool AccessInfo::overlaps(const std::shared_ptr<AccessInfo>& other) {
// All accesses to a buf must have the same dimensionality.
  TORCH_INTERNAL_ASSERT(
      indices_.size() == other->indices().size(),
      buildErrorMessage(
          ""Expected ranks to match in registerizer in the fuser.""));
auto& other_indices = other->indices();
"
207,"caffe2::serialize::kMinSupportedBytecodeVersion <= model_version &&
// NOLINTNEXTLINE(clang-diagnostic-sign-compare)
model_version <= caffe2::serialize::kMaxSupportedBytecodeVersion,
      ""Lite Interpreter verson number does not match. "",
""The model version must be between "",
caffe2::serialize::kMinSupportedBytecodeVersion,
"" and "",
caffe2::serialize::kMaxSupportedBytecodeVersion,
      ""But the model version is "",
model_version);
bool has_debug_handles = debug_handles.has_value();
","caffe2::serialize::kMinSupportedBytecodeVersion <= model_version &&
// NOLINTNEXTLINE(clang-diagnostic-sign-compare)
model_version <= caffe2::serialize::kMaxSupportedBytecodeVersion,
      ""Lite Interpreter version number does not match. "",
""The model version must be between "",
caffe2::serialize::kMinSupportedBytecodeVersion,
"" and "",
caffe2::serialize::kMaxSupportedBytecodeVersion,
      "" but the model version is "",
model_version);
bool has_debug_handles = debug_handles.has_value();
"
208,"WithInsertPoint guard(m);
std::vector<IValue> parameterIValues = {};
for (auto it = block->nodes().begin(); it != block->nodes().end();) {
Node* n = *it;
it++; // node n can be destroyed
","WithInsertPoint guard(m);
std::vector<IValue> parameterIValues = {};
  std::unordered_set<Node*> nodesToDestroy;
for (auto it = block->nodes().begin(); it != block->nodes().end();) {
Node* n = *it;
it++; // node n can be destroyed
"
209,"}
for (auto input : b->inputs()) {
    for (auto use : input->uses()) {
      Node* node = use.user;
      if (!mr.inplaceOpVariant(node)) {
        continue;
      }
      auto it = std::find(node->inputs().begin(), node->inputs().end(), input);
      if (it != node->inputs().end()) {
        int index = std::distance(node->inputs().begin(), it);
        std::cerr << ""Warning: ONNX Preprocess - Removing mutation from node ""
                  << node->kind().toQualString() << "" on block input: '""
                  << (*it)->debugName() << ""'. This changes graph semantics.""
                  << std::endl;

        Node* newNode =
            addDummyClone(b->owningGraph(), input, false, b->return_node());
        TORCH_INTERNAL_ASSERT(nullptr != newNode);
        node->replaceInput(index, newNode->output());
        input->replaceAllUsesAfterNodeWith(node, newNode->output());
}
    }
}
}
","}
for (auto input : b->inputs()) {
    bool needsRestart = false;
    do {
      needsRestart = false;
      for (auto use : input->uses()) {
        Node* node = use.user;
        if (!mr.inplaceOpVariant(node)) {
          continue;
        }
        auto it =
            std::find(node->inputs().begin(), node->inputs().end(), input);
        if (it != node->inputs().end()) {
          int index = std::distance(node->inputs().begin(), it);
          std::cerr << ""Warning: ONNX Preprocess - Removing mutation from node ""
                    << node->kind().toQualString() << "" on block input: '""
                    << (*it)->debugName() << ""'. This changes graph semantics.""
                    << std::endl;

          Node* newNode =
              addDummyClone(b->owningGraph(), input, false, b->return_node());
          TORCH_INTERNAL_ASSERT(nullptr != newNode);
          node->replaceInput(index, newNode->output());
          input->replaceAllUsesAfterNodeWith(node, newNode->output());
          needsRestart = true;
          break;
        }
}
    } while (needsRestart);
}
}
"
210,"}
void ReturnRefCounter(const std::string& handle, uint64_t offset /* unused */) {
std::lock_guard<std::mutex> lock(
cuda_ipc_global_entities.ref_counters_mutex_);
auto& map = cuda_ipc_global_entities.ref_counters_files_;
","}
void ReturnRefCounter(const std::string& handle, uint64_t offset /* unused */) {
  if(!CudaIPCGlobalEntities::alive) {
    return;
  }
std::lock_guard<std::mutex> lock(
cuda_ipc_global_entities.ref_counters_mutex_);
auto& map = cuda_ipc_global_entities.ref_counters_files_;
"
211,"}
for (const auto* node : graph->nodes()) {
    for (const auto* input : node->inputs()) {
      for (const auto* output : node->outputs()) {
if (liveness_map.count(input) && liveness_map.count(output)) {
liveness_map.at(input).insert(output);
liveness_map.at(output).insert(input);
}
}
}
}
return liveness_map;
","}
for (const auto* node : graph->nodes()) {
    auto inputs = node->inputs();
    auto outputs = node->outputs();
    for (const auto* input : inputs) {
      for (const auto* output : outputs) {
if (liveness_map.count(input) && liveness_map.count(output)) {
liveness_map.at(input).insert(output);
liveness_map.at(output).insert(input);
}
}
}
    // All inputs should be alive at the same time.
    for (size_t i = 0; i < inputs.size(); ++i) {
      for (size_t j = 0; j < inputs.size(); ++j) {
        if (liveness_map.count(inputs[i]) && liveness_map.count(inputs[j])) {
          liveness_map.at(inputs[i]).insert(inputs[j]);
          liveness_map.at(inputs[j]).insert(inputs[i]);
        }
      }
    }
    // All outputs should be alive at the same time.
    for (size_t i = 0; i < outputs.size(); ++i) {
      for (size_t j = 0; j < outputs.size(); ++j) {
        if (liveness_map.count(outputs[i]) && liveness_map.count(outputs[j])) {
          liveness_map.at(outputs[i]).insert(outputs[j]);
          liveness_map.at(outputs[j]).insert(outputs[i]);
        }
      }
    }
}
return liveness_map;
"
212,"if (!pack_hook_ || !unpack_hook_) {
return nullptr;
}
py::gil_scoped_acquire gil;
py::function pack_hook = py::reinterpret_borrow<py::function>(pack_hook_);
py::function unpack_hook = py::reinterpret_borrow<py::function>(unpack_hook_);
","if (!pack_hook_ || !unpack_hook_) {
return nullptr;
}
    std::lock_guard<std::mutex> lock(mutex_);
py::gil_scoped_acquire gil;
py::function pack_hook = py::reinterpret_borrow<py::function>(pack_hook_);
py::function unpack_hook = py::reinterpret_borrow<py::function>(unpack_hook_);
"
213,"#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <ATen/Tensor.h>
","#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/engine.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <torch/csrc/autograd/variable.h>
#include <ATen/Tensor.h>
"
214,"#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/engine.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <torch/csrc/autograd/variable.h>
#include <ATen/Tensor.h>
","#include <torch/csrc/autograd/saved_variable.h>
#include <torch/csrc/autograd/edge.h>
#include <torch/csrc/autograd/function.h>
#include <torch/csrc/autograd/variable.h>
#include <torch/csrc/autograd/anomaly_mode.h>
#include <torch/csrc/autograd/grad_mode.h>
#include <ATen/Tensor.h>
"
215,"Tensor LU_data_working_copy = is_LU_data_batched_column_major ? LU_data_broadcasted : cloneBatchedColumnMajor(LU_data_broadcasted);
Tensor LU_pivots_working_copy = LU_pivots_broadcasted.is_contiguous() ? LU_pivots_broadcasted : LU_pivots_broadcasted.contiguous();
  lu_solve_stub(self.device().type(), result, LU_data_working_copy, LU_pivots_working_copy);
return result;
}
Tensor& lu_solve_out(const Tensor& self, const Tensor& LU_data, const Tensor& LU_pivots, Tensor& result) {
checkSameDevice(""lu_solve"", result, self);
checkLinalgCompatibleDtype(""lu_solve"", result, self);
","Tensor LU_data_working_copy = is_LU_data_batched_column_major ? LU_data_broadcasted : cloneBatchedColumnMajor(LU_data_broadcasted);
Tensor LU_pivots_working_copy = LU_pivots_broadcasted.is_contiguous() ? LU_pivots_broadcasted : LU_pivots_broadcasted.contiguous();
  lu_solve_trans_stub(self.device().type(), result, LU_data_working_copy, LU_pivots_working_copy, trans);
return result;
}
Tensor lu_solve(const Tensor& self, const Tensor& LU_data, const Tensor& LU_pivots) {
  return at::native::_lu_solve_trans(self, LU_data, LU_pivots, ""N"");
}

Tensor& lu_solve_out(const Tensor& self, const Tensor& LU_data, const Tensor& LU_pivots, Tensor& result) {
checkSameDevice(""lu_solve"", result, self);
checkLinalgCompatibleDtype(""lu_solve"", result, self);
"
216,"return unsqueeze_node;
}
} // namespace jit
} // namespace torch
","return unsqueeze_node;
}
bool isValidToTransformToONNXConcatNode(Node* lc_node) {
  return !lc_node->inputs().empty();
}

Node* transformToONNXConcatNode(
    Graph* g,
    Node* lc_node,
    bool need_new_input,
    int opset_version) {
  // ListConstruct Int[] output case, we need to transform to ONNX
  // Concat to ensure the output is a single tensor(dynamic) type in
  // order to be consumed as inputs
  std::vector<Value*> unsqueezed;
  auto new_node = need_new_input ? g->return_node() : lc_node;

  for (auto* input : lc_node->inputs()) {
    auto new_input =
        need_new_input ? g->addInput()->copyMetadata(input) : input;

    Node* unsqueezed_node =
        createONNXUnsqueeze(g, new_node, new_input, 0, opset_version);
    unsqueezed.emplace_back(unsqueezed_node->output());
  }

  Node* concat_node = need_new_input
      ? g->insertNode(g->create(onnx::Concat, 1))
      : g->create(onnx::Concat, 1)->insertBefore(lc_node);
  concat_node->i_(attr::axis, 0);
  for (auto v : unsqueezed) {
    concat_node->addInput(v);
  }

  return concat_node;
}

} // namespace jit
} // namespace torch
"
217,"#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_pooling.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
namespace torch {
namespace jit {
","#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_pooling.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/remove_mutation.h>
namespace torch {
namespace jit {
"
218,"sizeof(THPVariableMeta),                     /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","sizeof(THPVariableMeta),                     /* tp_basicsize */
0,                                           /* tp_itemsize */
nullptr,                                     /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
219,"sizeof(THPStorage),                          /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPStorage_(dealloc),            /* tp_dealloc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
","sizeof(THPStorage),                          /* tp_basicsize */
0,                                           /* tp_itemsize */
(destructor)THPStorage_(dealloc),            /* tp_dealloc */
0,                                           /* tp_vectorcall_offset */
nullptr,                                     /* tp_getattr */
nullptr,                                     /* tp_setattr */
"
220,"TORCH_META_FUNC(mm)(const Tensor & self, const Tensor & mat2) {
TORCH_CHECK(self.dim() == 2, ""self must be a matrix"");
TORCH_CHECK(mat2.dim() == 2, ""mat2 must be a matrix"");
auto names = at::namedinference::compute_matmul_outnames(self, mat2);
set_output(0, {self.sizes()[0], mat2.sizes()[1]}, {}, self.options(), names);
","TORCH_META_FUNC(mm)(const Tensor & self, const Tensor & mat2) {
TORCH_CHECK(self.dim() == 2, ""self must be a matrix"");
TORCH_CHECK(mat2.dim() == 2, ""mat2 must be a matrix"");
  TORCH_CHECK(
      self.sizes()[1] == mat2.sizes()[0], ""mat1 and mat2 shapes cannot be multiplied ("",
      self.sizes()[0], ""x"", self.sizes()[1], "" and "", mat2.sizes()[0], ""x"", mat2.sizes()[1], "")"");
auto names = at::namedinference::compute_matmul_outnames(self, mat2);
set_output(0, {self.sizes()[0], mat2.sizes()[1]}, {}, self.options(), names);
"
221,"auto m2_strides = m2.strides();
auto m2_sizes = m2.sizes();
  // keeping TORCH_CHECKs here because othe mm methods also utilize this impl.
  // TODO move this to meta once all methods have migrated to structured kernel.
  TORCH_CHECK(
      m1_sizes[1] == m2_sizes[0], ""mat1 and mat2 shapes cannot be multiplied ("",
      m1_sizes[0], ""x"", m1_sizes[1], "" and "", m2_sizes[0], ""x"", m2_sizes[1], "")"");

TORCH_CHECK(
self_sizes[0] == m1_sizes[0] && self_sizes[1] == m2_sizes[1],
""input shape is incompatible with matrix multiplication ("",
","auto m2_strides = m2.strides();
auto m2_sizes = m2.sizes();
TORCH_CHECK(
self_sizes[0] == m1_sizes[0] && self_sizes[1] == m2_sizes[1],
""input shape is incompatible with matrix multiplication ("",
"
222,"return cpu_tensors;
}
void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
auto& schema_args = op.schema().arguments();
","return cpu_tensors;
}
c10::optional<c10::Device> compute_target_device(std::vector<at::Tensor>& t_args, std::vector<c10::List<at::Tensor>> tlist_args) {
  // Decide what device to move the output tensor(s) to.
  // The current convention is that we use the first tensor arg to pick the device
  // Barring that, we take the first tensor from a TensorList arg.
  if (t_args.size() > 0) {
    return t_args[0].device();
  } else {
    // We need to loop through all of the (potentially multiple) TensorList arguments
    // In case, e.g. the first one is empty but the second is not.
    for (auto& tens_list : tlist_args) {
      for (const auto i : c10::irange(tens_list.size())) {
        return tens_list.get(i).device();
      }
    }
  }
  return c10::nullopt;
}

void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
auto& schema_args = op.schema().arguments();
"
223,"std::vector<at::Tensor> tensor_args;
std::vector<int> tensor_args_indices;
// Step 1: Convert all non-CPU tensor inputs into CPU tensors
// and put them on the stack at the correct indices.
for (int64_t idx = 0; idx < arguments.size(); ++idx) {
","std::vector<at::Tensor> tensor_args;
std::vector<int> tensor_args_indices;
  std::vector<c10::List<at::Tensor>> tensorlist_args;

// Step 1: Convert all non-CPU tensor inputs into CPU tensors
// and put them on the stack at the correct indices.
for (int64_t idx = 0; idx < arguments.size(); ++idx) {
"
224,"if (!unsafe) {
auto min_length = lengths_value.min().item<int64_t>();
TORCH_CHECK((min_length >= 0), ""lengths contains negative value!"");
    TORCH_CHECK(min_length != 0 || initial.has_value());
TORCH_CHECK(lengths_value.sum().item<int64_t>() == data.size(axis));
}
","if (!unsafe) {
auto min_length = lengths_value.min().item<int64_t>();
TORCH_CHECK((min_length >= 0), ""lengths contains negative value!"");
TORCH_CHECK(lengths_value.sum().item<int64_t>() == data.size(axis));
}
"
225,"#undef MAKE_SMART_PTR
struct NnapiCompilation : torch::jit::CustomClassHolder {
  // NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init,modernize-use-equals-default)
  NnapiCompilation() {
    // Could possibly call load_platform_library here, but error reporting
    // can be complicated if the constructor is called during model loading.
    // Instead, delay all work until the explicit init call.
  }
  // NOLINTNEXTLINE(modernize-use-override,modernize-use-equals-default)
  ~NnapiCompilation() {
  }
void init(
at::Tensor serialized_model_tensor,
","#undef MAKE_SMART_PTR
struct NnapiCompilation : torch::jit::CustomClassHolder {
  // Could possibly call load_platform_library here, but error reporting
  // can be complicated if the constructor is called during model loading.
  // Instead, delay all work until the explicit init call.
  NnapiCompilation() = default;
  ~NnapiCompilation() override = default;
void init(
at::Tensor serialized_model_tensor,
"
226,"if(i > 0)
out << "", "";
if (tup->schema()) {
        out << tup->schema()->arguments()[i].name() << "" : "";
}
      out << *(tup->elements()[i]);
}
out << "")"";
} else if (t.kind() == TypeKind::FunctionType) {
","if(i > 0)
out << "", "";
if (tup->schema()) {
        auto arg = tup->schema()->arguments()[i];
        out << arg.name() << "" : "";
        out << *(tup->elements()[i]);
        if (arg.default_value()) {
          out << "" = "" << *arg.default_value();
        }
      }
      else {
        out << *(tup->elements()[i]);
}
}
out << "")"";
} else if (t.kind() == TypeKind::FunctionType) {
"
227,"auto pad_idx = pad.size() - ((i + 1) * 2);
auto new_dim = input_sizes[l_diff + i] + pad[pad_idx] + pad[pad_idx + 1];
TORCH_CHECK(new_dim > 0, ""The input size "", input_sizes[l_diff + i], "", plus negative padding "",
                 pad[pad_idx], "" and "", pad[pad_idx + 1], ""resulted in a negative output size, ""
                 ""which is invalid. Check dimension "", l_diff + i, ""of your input."");
new_shape.emplace_back(new_dim);
}
","auto pad_idx = pad.size() - ((i + 1) * 2);
auto new_dim = input_sizes[l_diff + i] + pad[pad_idx] + pad[pad_idx + 1];
TORCH_CHECK(new_dim > 0, ""The input size "", input_sizes[l_diff + i], "", plus negative padding "",
                 pad[pad_idx], "" and "", pad[pad_idx + 1], "" resulted in a negative output size, ""
                 ""which is invalid. Check dimension "", l_diff + i, "" of your input."");
new_shape.emplace_back(new_dim);
}
"
228,"};
});
// out variant takes precedence over native
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
REGISTER_OPERATOR_FUNCTOR(
static_runtime::to_copy,
","};
});
// out variant takes precedence over native
// NB: This impl doesn't work for cpu->cuda copy/cast or vice versa.
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
REGISTER_OPERATOR_FUNCTOR(
static_runtime::to_copy,
"
229,"for (auto V : instructions) {
auto I = dyn_cast<Instruction>(V);
// We only need to process call/invoke instructions.
      if (!I || !CallSite(I)) {
continue;
}
auto contextualNamespace = inferContextualNamespace(I);
","for (auto V : instructions) {
auto I = dyn_cast<Instruction>(V);
// We only need to process call/invoke instructions.
      if (!I || !_isCallSite(I)) {
continue;
}
auto contextualNamespace = inferContextualNamespace(I);
"
230,"}
return [](ProcessedNode* p_node) {
const auto in1_s = p_node->Input(1).toScalar();
if (p_node->Output(0).isNone()) {
      const auto& in0_t = p_node->Input(0).toTensor();
const auto dtype = p_node->Input(2).toOptional<c10::ScalarType>();
const auto layout = p_node->Input(3).toOptional<c10::Layout>();
const auto device = p_node->Input(4).toOptional<c10::Device>();
","}
return [](ProcessedNode* p_node) {
const auto in1_s = p_node->Input(1).toScalar();
    const auto& in0_t = p_node->Input(0).toTensor();
if (p_node->Output(0).isNone()) {
const auto dtype = p_node->Input(2).toOptional<c10::ScalarType>();
const auto layout = p_node->Input(3).toOptional<c10::Layout>();
const auto device = p_node->Input(4).toOptional<c10::Device>();
"
231,"#include <ATen/ATen.h>
#include <memory>
#include <sstream>
#include <stdexcept>
","#include <ATen/ATen.h>
#include <array>
#include <memory>
#include <sstream>
#include <stdexcept>
"
232,"std::vector<SugaredValuePtr> IterableTree::get_base_iterables() {
std::vector<SugaredValuePtr> base_iters{};
  // NOLINTNEXTLINE(performance-for-range-copy)
  for (SugaredValuePtr sv : children_) {
if (auto iv = std::dynamic_pointer_cast<IterableTree>(sv)) {
std::vector<SugaredValuePtr> child_iters = iv->get_base_iterables();
// merge child iters with the base_iters
","std::vector<SugaredValuePtr> IterableTree::get_base_iterables() {
std::vector<SugaredValuePtr> base_iters{};
  for (SugaredValuePtr& sv : children_) {
if (auto iv = std::dynamic_pointer_cast<IterableTree>(sv)) {
std::vector<SugaredValuePtr> child_iters = iv->get_base_iterables();
// merge child iters with the base_iters
"
233,"}
// NOLINTNEXTLINE(bugprone-branch-clone)
  auto size = tuple ? PyTuple_GET_SIZE(obj) : PyList_GET_SIZE(obj);
if (size > 0) {
PyObject* iobj = tuple ? PyTuple_GET_ITEM(obj, 0) : PyList_GET_ITEM(obj, 0);
if (!THPUtils_checkDouble(iobj) && !PyComplex_Check(iobj)) {
","}
// NOLINTNEXTLINE(bugprone-branch-clone)
  const auto size = tuple ? PyTuple_GET_SIZE(obj) : PyList_GET_SIZE(obj);
if (size > 0) {
PyObject* iobj = tuple ? PyTuple_GET_ITEM(obj, 0) : PyList_GET_ITEM(obj, 0);
if (!THPUtils_checkDouble(iobj) && !PyComplex_Check(iobj)) {
"
234,"std::vector<std::vector<at::Tensor>>& /* usused */,
std::vector<at::Tensor>& /* usused */,
const AllgatherOptions& /* usused */) {
  throw std::runtime_error(
""no support for allgather_coalesced in this process group"");
}
","std::vector<std::vector<at::Tensor>>& /* usused */,
std::vector<at::Tensor>& /* usused */,
const AllgatherOptions& /* usused */) {
  TORCH_CHECK(false,
""no support for allgather_coalesced in this process group"");
}
"
235,"func<int64_t>(args);                             \
break;                                           \
default:                                           \
      throw std::runtime_error(""Invalid scalar type""); \
}
#endif
","func<int64_t>(args);                             \
break;                                           \
default:                                           \
      TORCH_CHECK(false, ""Invalid scalar type""); \
}
#endif
"
236,"work = c10::make_intrusive<AsyncScatterCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    throw std::runtime_error(""Invalid backend"");
}
enqueue(work);
return work;
","work = c10::make_intrusive<AsyncScatterCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    TORCH_CHECK(false, ""Invalid backend"");
}
enqueue(work);
return work;
"
237,"at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    throw std::runtime_error(""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    throw std::runtime_error(""input tensor has to be dense"");
}
return tensor;
}
","at::Tensor& checkSingleTensor(std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false, ""ProcessGroupGloo::send takes a single tensor"");
}
auto& tensor = tensors[0];
if (!tensor.is_contiguous()) {
    TORCH_CHECK(false, ""input tensor has to be contiguous"");
}
if (tensor.is_sparse()) {
    TORCH_CHECK(false, ""input tensor has to be dense"");
}
return tensor;
}
"
238,"for (auto i = size_t{}; i < num_devices; ++i) {
if (tensor_lists[i].size() != world_size * num_devices) {
      throw std::runtime_error(
""Tensor list input to scatter/gather must match number of collective""
"" participants"");
}
","for (auto i = size_t{}; i < num_devices; ++i) {
if (tensor_lists[i].size() != world_size * num_devices) {
      TORCH_CHECK(false,
""Tensor list input to scatter/gather must match number of collective""
"" participants"");
}
"
239,"void BackgroundThread::initStopSignal() {
ghStopEvent_ = CreateEvent(NULL, TRUE, FALSE, NULL);
if (ghStopEvent_ == NULL) {
    throw std::runtime_error(
""Failed to create the control pipe to start the ""
""BackgroundThread run"");
}
","void BackgroundThread::initStopSignal() {
ghStopEvent_ = CreateEvent(NULL, TRUE, FALSE, NULL);
if (ghStopEvent_ == NULL) {
    TORCH_CHECK(false,
""Failed to create the control pipe to start the ""
""BackgroundThread run"");
}
"
240,"cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    TORCH_CHECK(false,
""Number of workers for FileStore should be greater than zero"");
}
}
","cleanupKey_(""cleanup/""),
regularPrefix_(""/"") {
if (numWorkers_ < 1) {
    throw std::runtime_error(
""Number of workers for FileStore should be greater than zero"");
}
}
"
241,"collectiveCounter_(0) {
auto& devices = options->devices;
if (devices.empty()) {
    TORCH_CHECK(false, ""No device(s) specified"");
}
// Create and connect a context for every device.
","collectiveCounter_(0) {
auto& devices = options->devices;
if (devices.empty()) {
    throw std::runtime_error(""No device(s) specified"");
}
// Create and connect a context for every device.
"
242,"invalidArgument(""unsupported layout"");
}
} else {
    TORCH_CHECK(false, ""Invalid backend"");
}
enqueue(work);
return work;
","invalidArgument(""unsupported layout"");
}
} else {
    throw std::runtime_error(""Invalid backend"");
}
enqueue(work);
return work;
"
243,"work = c10::make_intrusive<AsyncScatterCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    TORCH_CHECK(false, ""Invalid backend"");
}
enqueue(work);
return work;
","work = c10::make_intrusive<AsyncScatterCUDAWork>(
std::move(context), outputs, inputs, opts.rootRank, tag);
} else {
    throw std::runtime_error(""Invalid backend"");
}
enqueue(work);
return work;
"
244,"const AllgatherOptions& opts) {
checkSingleTensor(inputTensors);
if (outputTensors.size() != 1) {
    TORCH_CHECK(false,
""MPI process group only supports a single ""
""tensor op"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
    TORCH_CHECK(false,
""All gather: number of output tensors should equal ""
""to the world size"");
}
","const AllgatherOptions& opts) {
checkSingleTensor(inputTensors);
if (outputTensors.size() != 1) {
    throw std::runtime_error(
""MPI process group only supports a single ""
""tensor op"");
}
if (static_cast<size_t>(size_) != outputTensors[0].size()) {
    throw std::runtime_error(
""All gather: number of output tensors should equal ""
""to the world size"");
}
"
245,"for (const auto& t : tensors) {
if (!t.is_cuda() || t.is_sparse()) {
      TORCH_CHECK(false, ""Tensors must be CUDA and dense"");
}
if (t.scalar_type() != first.scalar_type()) {
      TORCH_CHECK(false, ""Tensors must have identical type"");
}
if (t.sizes() != first.sizes()) {
      TORCH_CHECK(false, ""Tensors must have identical size"");
}
if (t.strides() != first.strides()) {
      TORCH_CHECK(false, ""Tensors must have identical strides"");
}
if (!t.is_non_overlapping_and_dense()) {
      TORCH_CHECK(false, ""Tensors must be non-overlapping and dense"");
}
const auto inserted = usedDevices.insert(t.get_device()).second;
if (!inserted) {
      TORCH_CHECK(false, ""Tensors must be on distinct GPU devices"");
}
}
}
","for (const auto& t : tensors) {
if (!t.is_cuda() || t.is_sparse()) {
      throw std::runtime_error(""Tensors must be CUDA and dense"");
}
if (t.scalar_type() != first.scalar_type()) {
      throw std::runtime_error(""Tensors must have identical type"");
}
if (t.sizes() != first.sizes()) {
      throw std::runtime_error(""Tensors must have identical size"");
}
if (t.strides() != first.strides()) {
      throw std::runtime_error(""Tensors must have identical strides"");
}
if (!t.is_non_overlapping_and_dense()) {
      throw std::runtime_error(""Tensors must be non-overlapping and dense"");
}
const auto inserted = usedDevices.insert(t.get_device()).second;
if (!inserted) {
      throw std::runtime_error(""Tensors must be on distinct GPU devices"");
}
}
}
"
246,"std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const GatherOptions& /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support gather"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::scatter(
std::vector<at::Tensor>& /* unused */,
std::vector<std::vector<at::Tensor>>& /* unused */,
const ScatterOptions& /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support scatter"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::recvAnysource(
std::vector<at::Tensor>& /* unused */,
int /* unused */) {
  TORCH_CHECK(false, ""ProcessGroupNCCL does not support recvAnysource"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::_allgather_base(
","std::vector<std::vector<at::Tensor>>& /* unused */,
std::vector<at::Tensor>& /* unused */,
const GatherOptions& /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support gather"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::scatter(
std::vector<at::Tensor>& /* unused */,
std::vector<std::vector<at::Tensor>>& /* unused */,
const ScatterOptions& /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support scatter"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::recvAnysource(
std::vector<at::Tensor>& /* unused */,
int /* unused */) {
  throw std::runtime_error(""ProcessGroupNCCL does not support recvAnysource"");
}
c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::_allgather_base(
"
247,"void resetPeakStats() {
std::lock_guard<std::recursive_mutex> lock(mutex);
    for (size_t statType = 0;
         statType < static_cast<size_t>(StatType::NUM_TYPES);
         ++statType) {
reset_peak_stat(stats.allocation[statType]);
reset_peak_stat(stats.segment[statType]);
reset_peak_stat(stats.active[statType]);
","void resetPeakStats() {
std::lock_guard<std::recursive_mutex> lock(mutex);
    for (const auto statType :
         c10::irange(static_cast<size_t>(StatType::NUM_TYPES))) {
reset_peak_stat(stats.allocation[statType]);
reset_peak_stat(stats.segment[statType]);
reset_peak_stat(stats.active[statType]);
"
248,"}
void emptyCache() {
    int count = device_allocator.size();
    for (int i = 0; i < count; i++)
      device_allocator[i]->emptyCache();
}
void* getBaseAllocation(void* ptr, size_t* outSize) {
","}
void emptyCache() {
    for (auto& da : device_allocator)
      da->emptyCache();
}
void* getBaseAllocation(void* ptr, size_t* outSize) {
"
249,"break;
}
  throw std::runtime_error(""Unhandled ReduceOp"");
}
template <typename T, typename O>
","break;
}
  TORCH_CHECK(false, ""Unhandled ReduceOp"");
}
template <typename T, typename O>
"
250,"std::string err = ""MPI error in: "" + std::string(__FILE__) + "":"" + \
std::to_string(__LINE__) +                                     \
"", with error code: "" + std::to_string(mpiStatus);             \
      throw std::runtime_error(err);                                     \
}                                                                    \
} while (0)
","std::string err = ""MPI error in: "" + std::string(__FILE__) + "":"" + \
std::to_string(__LINE__) +                                     \
"", with error code: "" + std::to_string(mpiStatus);             \
      TORCH_CHECK(false, err);                                     \
}                                                                    \
} while (0)
"
251,"void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    throw std::runtime_error(
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
","void checkSingleTensor(const std::vector<at::Tensor>& tensors) {
if (tensors.size() != 1) {
    TORCH_CHECK(false,
""MPI process group does not support multi-GPU collectives"");
}
checkSingleTensorHelper(tensors[0]);
"
252,"} catch (const std::out_of_range& e) {
switch (reduceOp) {
case ReduceOp::BAND:
        throw std::runtime_error(""Cannot use ReduceOp.BAND with NCCL"");
break;
case ReduceOp::BOR:
        throw std::runtime_error(""Cannot use ReduceOp.BOR with NCCL"");
break;
case ReduceOp::BXOR:
        throw std::runtime_error(""Cannot use ReduceOp.BXOR with NCCL"");
break;
default:
        throw std::runtime_error(""Unhandled ReduceOp"");
break;
}
}
","} catch (const std::out_of_range& e) {
switch (reduceOp) {
case ReduceOp::BAND:
        TORCH_CHECK(false, ""Cannot use ReduceOp.BAND with NCCL"");
break;
case ReduceOp::BOR:
        TORCH_CHECK(false, ""Cannot use ReduceOp.BOR with NCCL"");
break;
case ReduceOp::BXOR:
        TORCH_CHECK(false, ""Cannot use ReduceOp.BXOR with NCCL"");
break;
default:
        TORCH_CHECK(false, ""Unhandled ReduceOp"");
break;
}
}
"
253,"if (timeout != kNoTimeout) {
const auto elapsed = std::chrono::high_resolution_clock::now() - start;
if (elapsed > timeout) {
        throw std::runtime_error(kConnectTimeoutMsg);
}
}
std::this_thread::sleep_for(std::chrono::seconds(1));
","if (timeout != kNoTimeout) {
const auto elapsed = std::chrono::high_resolution_clock::now() - start;
if (elapsed > timeout) {
        TORCH_CHECK(false, kConnectTimeoutMsg);
}
}
std::this_thread::sleep_for(std::chrono::seconds(1));
"
254,"DistEngine::getInstance().execute(context_id, roots, retain_graph);
} catch (std::exception& e) {
// FIXME: crashes if exception type is not RuntimeError
    throw std::runtime_error(e.what());
}
}
","DistEngine::getInstance().execute(context_id, roots, retain_graph);
} catch (std::exception& e) {
// FIXME: crashes if exception type is not RuntimeError
    TORCH_CHECK(false, e.what());
}
}
"
255,"ptr += headerEnt.second;
}
if (ptr != endp) {
    throw std::runtime_error(""failed bounds"");
}
return out;
}
","ptr += headerEnt.second;
}
if (ptr != endp) {
    TORCH_CHECK(false, ""failed bounds"");
}
return out;
}
"
256,"} break;
default: {
std::string msg =
          std::string(""Unhandled node kind (in computeOperandValue): "") +
          op.toQualString();
throw malformed_input(msg);
}
}
","} break;
default: {
std::string msg =
          std::string(""Unhandled node kind: "") + op.toQualString();
throw malformed_input(msg);
}
}
"
257,"// Iterate over bucket variables.
for (const auto variable_index : bucket_indices[bucket_index]) {
        TORCH_CHECK(
variable_index < replicas_[replica_index].size(),
""Out of range variable index specified."");
const auto& variable = replicas_[replica_index][variable_index];
if (!options.has_device()) {
options = options.device(variable.device());
} else {
          TORCH_CHECK(
variable.device() == options.device(),
""All parameters in a bucket must be "",
""placed on the same device."");
}
if (!options.has_dtype()) {
options = options.dtype(variable.dtype());
} else {
          TORCH_CHECK(
variable.dtype() == options.dtype(),
""All parameters in a bucket must have the same dtype."");
}
const auto length = variable.numel();
","// Iterate over bucket variables.
for (const auto variable_index : bucket_indices[bucket_index]) {
        TORCH_INTERNAL_ASSERT(
variable_index < replicas_[replica_index].size(),
""Out of range variable index specified."");
const auto& variable = replicas_[replica_index][variable_index];
if (!options.has_device()) {
options = options.device(variable.device());
} else {
          REDUCER_CHECK(
variable.device() == options.device(),
              logger_,
""All parameters in a bucket must be "",
""placed on the same device."");
}
if (!options.has_dtype()) {
options = options.dtype(variable.dtype());
} else {
          REDUCER_CHECK(
variable.dtype() == options.dtype(),
              logger_,
""All parameters in a bucket must have the same dtype."");
}
const auto length = variable.numel();
"
258,"unmarkedParamInfo);
kBaseErrorMsg += unmarked_param_indices_info;
}
    TORCH_CHECK(false, kBaseErrorMsg);
}
}
","unmarkedParamInfo);
kBaseErrorMsg += unmarked_param_indices_info;
}
    REDUCER_CHECK(false, logger_, kBaseErrorMsg);
}
}
"
259,"}
InlinedCallStack::InlinedCallStack(Function* fn, SourceRange source_range)
    : fn_(fn), source_range_(std::move(source_range)) {}
InlinedCallStack::InlinedCallStack(
Function* fn,
","}
InlinedCallStack::InlinedCallStack(Function* fn, SourceRange source_range)
    : fn_(fn), source_range_(std::move(source_range)) {
  if (fn_) {
    set_function_name(fn_->name());
  }
}
InlinedCallStack::InlinedCallStack(
Function* fn,
"
260,"c10::optional<ModuleInstanceInfo> module_instance_info)
: fn_(fn),
source_range_(std::move(source_range)),
      module_instance_info_(std::move(module_instance_info)) {}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
","c10::optional<ModuleInstanceInfo> module_instance_info)
: fn_(fn),
source_range_(std::move(source_range)),
      module_instance_info_(std::move(module_instance_info)) {
  if (fn_) {
    set_function_name(fn_->name());
  }
}
InlinedCallStack::InlinedCallStack(
InlinedCallStackPtr callee,
"
261,"return source_range_;
}
std::vector<InlinedCallStackEntry> InlinedCallStack::vec() {
std::vector<InlinedCallStackEntry> r;
c10::optional<InlinedCallStackPtr> current = intrusive_from_this();
","return source_range_;
}
Function* InlinedCallStack::function() const {
  return fn_;
}

void InlinedCallStack::set_function_name(std::string fn_name) {
  fn_name_ = std::move(fn_name);
}

std::string InlinedCallStack::function_name() const {
  return fn_name_;
}

std::vector<InlinedCallStackEntry> InlinedCallStack::vec() {
std::vector<InlinedCallStackEntry> r;
c10::optional<InlinedCallStackPtr> current = intrusive_from_this();
"
262,"schema_(std::move(graph_and_schema.second)) {
// check opt flags
if (opts.optimize_graph_output_memory) {
    if (!(opts_.optimize_memory && opts_.enable_out_variant)) {
      throw std::runtime_error(
          ""When optimize_graph_output_memory is true, optimize_memory and enable_out_variant must be set to true"");
    }
}
if (opts_.optimize_memory) {
    if (!opts_.enable_out_variant) {
      throw std::runtime_error(
          ""When optimize_memory is true, enable_out_variant must be set to true"");
    }
}
// map Value* to IValue (from inputs or prim::Constant) or null
std::unordered_map<Value*, IValue*> value_to_ivalue;
// map Value* to its SSA definition IR
","schema_(std::move(graph_and_schema.second)) {
// check opt flags
if (opts.optimize_graph_output_memory) {
    TORCH_CHECK(
        opts_.enable_out_variant && opts_.optimize_memory,
        ""When optimize_graph_output_memory is true, enable_out_variant and optimize_memory must be set to true"");
}
if (opts_.optimize_memory) {
    TORCH_CHECK(
        opts_.enable_out_variant,
        ""When optimize_memory is true, enable_out_variant must be set to true"");
}

// map Value* to IValue (from inputs or prim::Constant) or null
std::unordered_map<Value*, IValue*> value_to_ivalue;
// map Value* to its SSA definition IR
"
263,"output_ssa_defs_.emplace_back(value_to_ssa_def[output]);
}
AliasDb alias_db(graph_);
auto lm = GetLivenessInformation(graph_, alias_db);
external_values_ = lm.second;
if (opts_.optimize_memory) {
auto values = GetMemoryPlanningCandidates(graph_);
    // Note (penguin): since it does not make sense to have optimize_memory
    // enabled but enable_out_variant disabled, we check the flag dependence
    // during initialization of StaticModule so that the following condition
    // would not be true. This would make the code easier to understand
    // if (!opts_.enable_out_variant) {
    //   values.first = {};
    // }
value_to_same_storage_values_ =
GenerateSameStorageValues(lm, values, alias_db);
}
","output_ssa_defs_.emplace_back(value_to_ssa_def[output]);
}
  // Prepare for memory planning
AliasDb alias_db(graph_);
auto lm = GetLivenessInformation(graph_, alias_db);
external_values_ = lm.second;
if (opts_.optimize_memory) {
auto values = GetMemoryPlanningCandidates(graph_);
value_to_same_storage_values_ =
GenerateSameStorageValues(lm, values, alias_db);
}
"
264,"const std::string& top_module_type_name) const {
const auto it = callstack_ptr_map_.find(debug_handle);
if (it == callstack_ptr_map_.end()) {
    return ""debug_handle:"" + std::to_string(debug_handle);
}
return (getStackTraceWithModuleHierarchy(
              it->second, ""top"", top_module_type_name))
.first;
}
} // namespace jit
} // namespace torch
","const std::string& top_module_type_name) const {
const auto it = callstack_ptr_map_.find(debug_handle);
if (it == callstack_ptr_map_.end()) {
    return ""Debug info for handle, "" + std::to_string(debug_handle) +
        "", not found."";
}
return (getStackTraceWithModuleHierarchy(
              {it->second}, ""top"", top_module_type_name))
.first;
}
std::string MobileDebugTable::getSourceDebugString(
    const std::vector<int64_t>& debug_handles,
    const std::string& top_module_type_name) const {
  return getSourceDebugModuleHierarchyInfo(debug_handles, top_module_type_name)
      .first;
}

std::pair<std::string, std::string> MobileDebugTable::
    getSourceDebugModuleHierarchyInfo(
        const std::vector<int64_t>& debug_handles,
        const std::string& top_module_type_name) const {
  std::vector<DebugInfoPair> debug_infos;
  bool debug_handle_not_found{false};
  for (auto it = debug_handles.rbegin(); it != debug_handles.rend(); ++it) {
    auto debug_handle = *it;
    const auto cs_it = callstack_ptr_map_.find(debug_handle);
    if (cs_it == callstack_ptr_map_.end()) {
      debug_handle_not_found = true;
      break;
    }
    debug_infos.emplace_back(cs_it->second);
  }
  if (debug_handle_not_found) {
    std::string debug_handles_string = ""debug_handles:{"";
    for (const auto debug_handle : debug_handles) {
      debug_handles_string += std::to_string(debug_handle);
    }
    debug_handles_string += ""}"";
    debug_handles_string =
        ""Debug info for handles: "" + debug_handles_string + "", was not found."";
    return {debug_handles_string, debug_handles_string};
  }
  return (getStackTraceWithModuleHierarchy(
      debug_infos, ""top"", top_module_type_name));
}

} // namespace jit
} // namespace torch
"
265,"return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple =
          getFunctionTuple(module, setstate, debug_handle_manager);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
debug_info_elements.push_back(func_tuple.second);
","return;
}
if (setstate.isGraphFunction()) {
      auto func_tuple = getFunctionTuple(module, setstate, debug_info_recorder);
elements.push_back(func_tuple.first);
qn_cache.emplace(qn);
debug_info_elements.push_back(func_tuple.second);
"
266,"static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
moduleMethodsTuple(
      module, elements, debug_info_elements, debug_handle_manager);
auto telements = Tup(std::move(elements));
writeArchive(
telements,
","static_cast<int64_t>(caffe2::serialize::kProducedBytecodeVersion));
moduleMethodsTuple(
      module, elements, debug_info_elements, debug_info_recorder);
auto telements = Tup(std::move(elements));
writeArchive(
telements,
"
267,"return legacy_new_from_sequence(options, scalar_type, deviceOptional, r.pyobject(0));
}
return new_with_sizes(options, scalar_type, r.deviceOptional(1), r.intlist(0));
  } else if (r.idx == 5) {
auto deviceOptional = r.deviceOptional(1);
check_legacy_ctor_device(dispatch_key, deviceOptional);
return legacy_new_from_sequence(options, scalar_type, deviceOptional, r.pyobject(0));
","return legacy_new_from_sequence(options, scalar_type, deviceOptional, r.pyobject(0));
}
return new_with_sizes(options, scalar_type, r.deviceOptional(1), r.intlist(0));
  } else if (r.idx == 6) {
auto deviceOptional = r.deviceOptional(1);
check_legacy_ctor_device(dispatch_key, deviceOptional);
return legacy_new_from_sequence(options, scalar_type, deviceOptional, r.pyobject(0));
"
268,"m.impl(""std.names_dim"", CppFunction::makeFallthrough());
m.impl(""std.names_out"", CppFunction::makeFallthrough());
m.impl(""std.out"", CppFunction::makeFallthrough());
m.impl(""std_mean"", CppFunction::makeFallthrough());
m.impl(""std_mean.dim"", CppFunction::makeFallthrough());
m.impl(""std_mean.names_dim"", CppFunction::makeFallthrough());
m.impl(""stride.Dimname"", CppFunction::makeFallthrough());
m.impl(""stride.int"", CppFunction::makeFallthrough());
m.impl(""sub.Scalar"", CppFunction::makeFallthrough());
","m.impl(""std.names_dim"", CppFunction::makeFallthrough());
m.impl(""std.names_out"", CppFunction::makeFallthrough());
m.impl(""std.out"", CppFunction::makeFallthrough());
  m.impl(""std.correction"", CppFunction::makeFallthrough());
  m.impl(""std.correction_out"", CppFunction::makeFallthrough());
  m.impl(""std.correction_names"", CppFunction::makeFallthrough());
  m.impl(""std.correction_names_out"", CppFunction::makeFallthrough());
m.impl(""std_mean"", CppFunction::makeFallthrough());
m.impl(""std_mean.dim"", CppFunction::makeFallthrough());
m.impl(""std_mean.names_dim"", CppFunction::makeFallthrough());
  m.impl(""std_mean.correction"", CppFunction::makeFallthrough());
  m.impl(""std_mean.correction_names"", CppFunction::makeFallthrough());
m.impl(""stride.Dimname"", CppFunction::makeFallthrough());
m.impl(""stride.int"", CppFunction::makeFallthrough());
m.impl(""sub.Scalar"", CppFunction::makeFallthrough());
"
269,"m.impl(""var.names_dim"", CppFunction::makeFallthrough());
m.impl(""var.names_out"", CppFunction::makeFallthrough());
m.impl(""var.out"", CppFunction::makeFallthrough());
m.impl(""var_mean"", CppFunction::makeFallthrough());
m.impl(""var_mean.dim"", CppFunction::makeFallthrough());
m.impl(""var_mean.names_dim"", CppFunction::makeFallthrough());
m.impl(""zero_"", CppFunction::makeFallthrough());
m.impl(""zeros_like"", CppFunction::makeFallthrough());
","m.impl(""var.names_dim"", CppFunction::makeFallthrough());
m.impl(""var.names_out"", CppFunction::makeFallthrough());
m.impl(""var.out"", CppFunction::makeFallthrough());
  m.impl(""var.correction"", CppFunction::makeFallthrough());
  m.impl(""var.correction_out"", CppFunction::makeFallthrough());
  m.impl(""var.correction_names"", CppFunction::makeFallthrough());
  m.impl(""var.correction_names_out"", CppFunction::makeFallthrough());
m.impl(""var_mean"", CppFunction::makeFallthrough());
m.impl(""var_mean.dim"", CppFunction::makeFallthrough());
m.impl(""var_mean.names_dim"", CppFunction::makeFallthrough());
  m.impl(""var_mean.correction"", CppFunction::makeFallthrough());
  m.impl(""var_mean.correction_names"", CppFunction::makeFallthrough());
m.impl(""zero_"", CppFunction::makeFallthrough());
m.impl(""zeros_like"", CppFunction::makeFallthrough());
"
270,"}
}
Tensor var_backward(const Tensor & grad, const Tensor & self, bool unbiased) {
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (self.numel() - unbiased)) * grad * (self - self.mean());
}
Tensor var_backward(Tensor grad, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  if (self.dim() == 0) {
    return var_backward(grad, self, unbiased);
}
if (!keepdim && self.dim() > 1) {
grad = unsqueeze_multiple(grad, dim, self.sizes().size());
}
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (_safe_size(self.sizes(), dim) - unbiased)) * grad * (self - self.mean(dim, true));
}
Tensor std_backward(const Tensor & result, const Tensor & grad, const Tensor & self, bool unbiased) {
  return var_backward((grad / (result * 2)).masked_fill_(result == 0, 0), self, unbiased);
}

Tensor std_backward(const Tensor & result, Tensor grad, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  return var_backward((grad / (result * 2)).masked_fill_(result == 0, 0), self, dim, unbiased, keepdim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, IntArrayRef dim, bool keepdim) {
return sum_backward(grad, sizes, dim, keepdim) / _safe_size(sizes, dim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, int numel) {
return grad.expand(sizes) / numel;
}
Tensor var_std_mean_backward(const variable_list& grads, const Tensor & self, const Tensor & r1, const Tensor & r2, IntArrayRef dim, bool unbiased, bool keepdim, bool is_std) {
  Tensor grad;
  if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, dim, unbiased, keepdim) : var_backward(grads[0], self, dim, unbiased, keepdim);
  }
  if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), dim, keepdim);
    grad = grads[0].defined() ? grad + mean_grad : mean_grad;
}
  return grad;
}
Tensor var_std_mean_backward(const variable_list& grads, const Tensor & self, const Tensor & r1, const Tensor & r2, bool unbiased, bool is_std) {
Tensor grad;
if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, unbiased) : var_backward(grads[0], self, unbiased);
}
if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), self.numel());
    grad = grads[0].defined() ? grad + mean_grad : mean_grad;
}
return grad;
}
","}
}
static Tensor var_backward(const Tensor & grad, const Tensor & self, int64_t correction) {
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / (self.numel() - correction)) * grad * (self - self.mean());
}
Tensor var_backward(Tensor grad, const Tensor& self, c10::optional<IntArrayRef> dim_opt,
    c10::optional<int64_t> correction_opt, bool keepdim) {
  auto correction = correction_opt.value_or(1);
  if (self.dim() == 0 || !dim_opt.has_value()) {
    return var_backward(grad, self, correction);
}
  auto dim = dim_opt.value();
if (!keepdim && self.dim() > 1) {
grad = unsqueeze_multiple(grad, dim, self.sizes().size());
}
  const int64_t dof = _safe_size(self.sizes(), dim) - correction;
// NOLINTNEXTLINE(bugprone-narrowing-conversions,cppcoreguidelines-avoid-magic-numbers,cppcoreguidelines-narrowing-conversions)
  return (2.0 / dof) * grad * (self - self.mean(dim, /*keepdim=*/true));
}
Tensor std_backward(
    const Tensor& result, const Tensor& grad, const Tensor& self,
    c10::optional<IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  auto grad_var = (grad / (result * 2)).masked_fill_(result == 0, 0);
  return var_backward(grad_var, self, dim, correction, keepdim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, IntArrayRef dim, bool keepdim) {
return sum_backward(grad, sizes, dim, keepdim) / _safe_size(sizes, dim);
}
Tensor mean_backward(Tensor grad, const IntArrayRef sizes, int64_t numel) {
return grad.expand(sizes) / numel;
}
static Tensor mean_backward(
    const Tensor& grad, const IntArrayRef sizes, int64_t numel,
    c10::optional<IntArrayRef> dim, bool keepdim) {
  if (dim.has_value()) {
    return mean_backward(grad, sizes, *dim, keepdim);
  } else {
    return mean_backward(grad, sizes, numel);
}
}
Tensor var_std_mean_backward(
    const variable_list& grads, const Tensor& self, const Tensor& r1,
    const Tensor& r2, c10::optional<IntArrayRef> dim,
    c10::optional<int64_t> correction, bool keepdim, bool is_std) {
Tensor grad;
if (grads[0].defined()) {
    grad = is_std ? std_backward(r1, grads[0], self, dim, correction, keepdim)
                  : var_backward(grads[0], self, dim, correction, keepdim);
}
if (grads[1].defined()) {
    Tensor mean_grad = mean_backward(grads[1], self.sizes(), self.numel(), dim, keepdim);
    grad = grad.defined() ? grad + mean_grad : mean_grad;
}
return grad;
}
"
271,"unbiased: bool,
keepdim: bool):
def backward(grad_output):
                grad_self = AD_var_backward_1(grad_output, self, dim, unbiased, keepdim)
return grad_self, None, None, None
return torch.var(self, dim, unbiased, keepdim), backward
def tanh(self):
output = torch.tanh(self)
def backward(grad_output):
","unbiased: bool,
keepdim: bool):
def backward(grad_output):
                correction = AD_bool_to_int(unbiased)
                grad_self = AD_var_backward_1(grad_output, self, dim, correction, keepdim)
return grad_self, None, None, None
return torch.var(self, dim, unbiased, keepdim), backward
        def var_2(self,
                  dim: Optional[List[int]],
                  *,
                  correction: Optional[int],
                  keepdim: bool):
            def backward(grad_output):
                grad_self = AD_var_backward_2(grad_output, self, dim, correction, keepdim)
                return grad_self, None, None, None

            return torch.var(self, dim, correction=correction, keepdim=keepdim), backward

def tanh(self):
output = torch.tanh(self)
def backward(grad_output):
"
272,"libkineto::GenericTraceActivity op;
op.activityType = libkineto::ActivityType::CPU_OP;
op.activityName = std::string(fn.name().str());

op.startTime = ctx->startUs;
op.endTime = getTimeUs();
    op.device = 0;
op.correlation = ctx->correlationId;
// optimization - postpone shapesToStr till finalizeCPUTrace
// is called from disableProfiler
","libkineto::GenericTraceActivity op;
op.activityType = libkineto::ActivityType::CPU_OP;
op.activityName = std::string(fn.name().str());
    op.device = libkineto::processId();
op.startTime = ctx->startUs;
op.endTime = getTimeUs();
op.correlation = ctx->correlationId;
// optimization - postpone shapesToStr till finalizeCPUTrace
// is called from disableProfiler
"
273,"if (ins.op == OP || ins.op == OPN) {
auto node = code.instructions_source()[i];
opnames.emplace_back(node->schema().operator_name());
      int64_t debug_handle =
          debug_handle_manager.getNextDebugHandleForInlinedCallStackPtr(node);
      op_debug_handles.emplace_back(debug_handle);
}
// CALL nodes at this point represent built-in (i.e. non-Graph)
// functions that were not inlined. Here we convert the CALL
","if (ins.op == OP || ins.op == OPN) {
auto node = code.instructions_source()[i];
opnames.emplace_back(node->schema().operator_name());
}
// CALL nodes at this point represent built-in (i.e. non-Graph)
// functions that were not inlined. Here we convert the CALL
"
274,"//  ...)
// In addition, the module debugging information can be saved
// in mobile_debug.pkl. An example for it looks like:
// (4,
//  ('__torch__.m.forward',
//   (('module_debug_info', (top(A).foo(B).forward)))))
// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisited and redesigned if we want to
","//  ...)
// In addition, the module debugging information can be saved
// in mobile_debug_handles.pkl. An example for it looks like:
// (4,
//  ('__torch__.m.forward',
//   (('module_debug_handles', 10))))
//   Here 10 is the debug handle.
// We also store separately and optionally callstack_debug_map.
// This serializes inlined callstack (InlinedCallStack data structure)
// corresponding to the debug handles.
// Callstack_debug_map serializes tuples of
// (int64_t(debug_handle), int64_t(source_range_tag), InlinedCallStack)
// source_range_tag maps to .debug_pkl files where this tag maps it to
// source range.
// InlinedCallStack is serialized as:
// IValue(InlinedCallStack) = {IValue(ModuleInstanceInfo),
// int64_t(source_range_tag), IValue(InlinedCallStack)} ModuleInstanceInfo is
// serialized as a tuple of (class_type_name, instance_name)
// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisited and redesigned if we want to
"
275,"int N = ins_item[2].toInt();
// TODO: Save debug handles for all instructions, not just for OP
if (op_code == OP) {
        // In later PRs we will refactor this to always save debug handles.
        // Debug info, source range and inlined callstack ptr saving will become
        // optional.
        if (has_debug_info) {
          auto module_debug_tuple =
              module_debug_info_list[X].toTuple()->elements();
          std::string module_debug_info =
              module_debug_tuple[0].toString()->string();
          int64_t debug_handle = module_debug_tuple[1].toInt();
          function->set_module_info(module_debug_info, i);
function->append_instruction(op_code, X, N, debug_handle);
} else {
          function->set_module_info("""", i);
function->append_instruction(op_code, X, N);
}
} else {
","int N = ins_item[2].toInt();
// TODO: Save debug handles for all instructions, not just for OP
if (op_code == OP) {
        if (has_debug_handles) {
          // Why X is used to index into debug_handles?
          // X is the offset into opnames table and since debug handles
          // were ""appended"" in the debug handles vector, during serialization,
          // only for OP/OPN X is safe to index into it.
          // This is not super reliable and once we move to saving debug
          // handles for all instructions we can remove this strange behavior.
          int64_t debug_handle = debug_handles_list[X].toInt();
function->append_instruction(op_code, X, N, debug_handle);
} else {
function->append_instruction(op_code, X, N);
}
} else {
"
276,"//
auto bvals = readArchive(""bytecode"", mcu).toTuple()->elements();
  c10::optional<std::vector<IValue>> debug_info_bvals;
  if (reader_->hasRecord(""mobile_debug.pkl"")) {
    debug_info_bvals = readArchive(""mobile_debug"", mcu).toTuple()->elements();
}
  parseMethods(bvals, debug_info_bvals, *mcu);
auto meta_dict = readMobileMetadata(mcu);
auto m = mobile::Module(readArchive(""data"", mcu).toObject(), meta_dict, mcu);
#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
  MobileDebugTable debug_table = MobileDebugTable(reader_);
m.setDebugTable(std::move(debug_table));
#endif
return m;
","//
auto bvals = readArchive(""bytecode"", mcu).toTuple()->elements();
  c10::optional<std::vector<IValue>> debug_handles;
  if (reader_->hasRecord(""mobile_debug_handles.pkl"")) {
    debug_handles =
        readArchive(""mobile_debug_handles"", mcu).toTuple()->elements();
}
  parseMethods(bvals, debug_handles, *mcu);
auto meta_dict = readMobileMetadata(mcu);
auto m = mobile::Module(readArchive(""data"", mcu).toObject(), meta_dict, mcu);
#if defined(SYMBOLICATE_MOBILE_DEBUG_HANDLE)
  MobileDebugTable debug_table = MobileDebugTable(reader_, compilation_unit_);
m.setDebugTable(std::move(debug_table));
#endif
return m;
"
277,"#include <torch/csrc/jit/serialization/export.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/frontend/source_range.h>
#include <torch/csrc/jit/ir/attributes.h>
#include <torch/csrc/jit/ir/ir.h>
","#include <torch/csrc/jit/serialization/export.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/backends/backend_debug_handler.h>
#include <torch/csrc/jit/frontend/source_range.h>
#include <torch/csrc/jit/ir/attributes.h>
#include <torch/csrc/jit/ir/ir.h>
"
278,"namespace {
void export_opnames(const script::Module& m, std::set<std::string>& opnames) {
std::vector<c10::IValue> elements;
  c10::optional<std::vector<c10::IValue>> debug_info_elements;
  moduleMethodsTuple(
      m, elements, debug_info_elements, false /* save_mobile_debug_info */);
for (const auto& element : elements) {
auto table = element.toTuple()->elements()[1];
auto row =
","namespace {
void export_opnames(const script::Module& m, std::set<std::string>& opnames) {
std::vector<c10::IValue> elements;
  std::vector<c10::IValue> debug_info_elements;
  BackendDebugHandleManager dummy;
  moduleMethodsTuple(m, elements, debug_info_elements, dummy);
for (const auto& element : elements) {
auto table = element.toTuple()->elements()[1];
auto row =
"
279,"const char* msg,
int64_t sequence_nr,
const std::vector<std::vector<int64_t>>& shapes) const {
  if (sequence_nr >= 0 || shapes.size() > 0) {
std::stringstream s;
#ifdef __HIP_PLATFORM_HCC__
s << name.str();
","const char* msg,
int64_t sequence_nr,
const std::vector<std::vector<int64_t>>& shapes) const {
  if (sequence_nr >= -1 || shapes.size() > 0) {
std::stringstream s;
#ifdef __HIP_PLATFORM_HCC__
s << name.str();
"
280,"return result;
}
Tensor & detach_(Tensor & self) {
RECORD_FUNCTION(""detach_"", std::vector<c10::IValue>({self}));
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
","return result;
}
Tensor & detach_(c10::DispatchKeySet ks, Tensor & self) {
RECORD_FUNCTION(""detach_"", std::vector<c10::IValue>({self}));
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
"
281,"return self;
}
namespace {
TORCH_LIBRARY_IMPL(aten, InplaceOrView, m) {
m.impl(""copy_"", torch::dispatch(DispatchKey::InplaceOrView, TORCH_FN(InplaceOrView::copy_)));
}
} // namespace
} // namespace InplaceOrView
","return self;
}
  Tensor detach(c10::DispatchKeySet ks, const Tensor & self) {
    auto out = ([&]() {
      at::AutoDispatchBelowInplaceOrView guard;
      // Make an empty shallow copy, the as_view call below will fill in the proper fields
      return Tensor(self.getIntrusivePtr()->shallow_copy_and_detach(
        /*version_counter=*/0,
        /*allow_tensor_metadata_change=*/false));
    })();
    std::function<at::Tensor(const at::Tensor&)> func=nullptr;
    auto result = as_view(/* base */ self, /* output */ out, /* is_bw_differentiable */ false,
                          /* is_fw_differentiable */ true, /* view_func */ func, /* creation_meta */ CreationMeta::DEFAULT,
                          /*allow_tensor_metadata_change=*/false);

    return result;
  }

  Tensor _fw_primal(c10::DispatchKeySet ks, const Tensor & self, int64_t level) {
    auto tmp = ([&]() {
      at::AutoDispatchBelowInplaceOrView guard;
      // Make an empty shallow copy, the as_view call below will fill in the proper fields
      return Tensor(self.getIntrusivePtr()->shallow_copy_and_detach(
        /*version_counter=*/0,
        /*allow_tensor_metadata_change=*/false));
    })();
    std::function<at::Tensor(const at::Tensor&)> func=nullptr;
    if (!self.unsafeGetTensorImpl()->support_as_strided()) {
      auto size_vec = self.sizes().vec();
      func = [=](const at::Tensor& input_base) {
        return input_base.view(size_vec);
      };
    }
    auto result = as_view(/* base */ self, /* output */ tmp, /* is_bw_differentiable */ true,
                          /* is_fw_differentiable */ false, /* view_func */ func, /* creation_meta */ CREATION_META_DEFINITION);

    return result;
  }

namespace {
TORCH_LIBRARY_IMPL(aten, InplaceOrView, m) {
m.impl(""copy_"", torch::dispatch(DispatchKey::InplaceOrView, TORCH_FN(InplaceOrView::copy_)));
      m.impl(""detach"", torch::dispatch(DispatchKey::InplaceOrView, TORCH_FN(InplaceOrView::detach)));
      m.impl(""_fw_primal"", torch::dispatch(DispatchKey::InplaceOrView, TORCH_FN(InplaceOrView::_fw_primal)));
}
} // namespace
} // namespace InplaceOrView
"
282,"END_HANDLE_TH_ERRORS
}
//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, modernize-avoid-c-arrays)
// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  THPModule_initExtension,   METH_O,       nullptr},
{""_autograd_init"",  THPAutograd_initExtension, METH_NOARGS,  nullptr},
","END_HANDLE_TH_ERRORS
}
//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, cppcoreguidelines-avoid-non-const-global-variables, modernize-avoid-c-arrays)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  THPModule_initExtension,   METH_O,       nullptr},
{""_autograd_init"",  THPAutograd_initExtension, METH_NOARGS,  nullptr},
"
283,"0,                                     /* tp_itemsize */
(destructor)THCPEvent_dealloc,         /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
0,                                     /* tp_getattr */
0,                                     /* tp_setattr */
0,                                     /* tp_reserved */
0,                                     /* tp_repr */
0,                                     /* tp_as_number */
0,                                     /* tp_as_sequence */
0,                                     /* tp_as_mapping */
0,                                     /* tp_hash  */
0,                                     /* tp_call */
0,                                     /* tp_str */
0,                                     /* tp_getattro */
0,                                     /* tp_setattro */
0,                                     /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
0,                                     /* tp_traverse */
0,                                     /* tp_clear */
0,                                     /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
0,                                     /* tp_iter */
0,                                     /* tp_iternext */
THCPEvent_methods,                     /* tp_methods */
0,                                     /* tp_members */
THCPEvent_properties,                  /* tp_getset */
0,                                     /* tp_base */
0,                                     /* tp_dict */
0,                                     /* tp_descr_get */
0,                                     /* tp_descr_set */
0,                                     /* tp_dictoffset */
0,                                     /* tp_init */
0,                                     /* tp_alloc */
THCPEvent_pynew,                       /* tp_new */
};
","0,                                     /* tp_itemsize */
(destructor)THCPEvent_dealloc,         /* tp_dealloc */
0,                                     /* tp_vectorcall_offset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_getattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_setattr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_reserved */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_repr */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_number */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_sequence */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_mapping */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_hash  */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_call */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_str */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_getattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_setattro */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_as_buffer */
Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
nullptr,                                  /* tp_doc */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_traverse */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_clear */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_richcompare */
0,                                     /* tp_weaklistoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_iter */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_iternext */
THCPEvent_methods,                     /* tp_methods */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_members */
THCPEvent_properties,                  /* tp_getset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_base */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_dict */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_descr_get */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_descr_set */
0,                                     /* tp_dictoffset */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_init */
  // NOLINTNEXTLINE(modernize-use-nullptr)
0,                                     /* tp_alloc */
THCPEvent_pynew,                       /* tp_new */
};
"
284,"END_HANDLE_TH_ERRORS
}
static struct PyMethodDef _THCPModule_methods[] = {
{""_cuda_init"",        THCPModule_initExtension,    METH_NOARGS,  nullptr},
{""_cuda_setDevice"",   THCPModule_setDevice_wrap,   METH_O,       nullptr},
","END_HANDLE_TH_ERRORS
}
// NOLINTNEXTLINE(modernize-avoid-c-arrays, cppcoreguidelines-avoid-non-const-global-variables, cppcoreguidelines-avoid-c-arrays)
static struct PyMethodDef _THCPModule_methods[] = {
{""_cuda_init"",        THCPModule_initExtension,    METH_NOARGS,  nullptr},
{""_cuda_setDevice"",   THCPModule_setDevice_wrap,   METH_O,       nullptr},
"
285,"Interpreter::Interpreter(InterpreterManager* manager)
: handle_(nullptr), manager_(manager) {
char library_name[] = ""/tmp/torch_deployXXXXXX"";
int fd = mkstemp(library_name);
TORCH_INTERNAL_ASSERT(fd != -1, ""failed to create temporary file"");
","Interpreter::Interpreter(InterpreterManager* manager)
: handle_(nullptr), manager_(manager) {
  // NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)
char library_name[] = ""/tmp/torch_deployXXXXXX"";
int fd = mkstemp(library_name);
TORCH_INTERNAL_ASSERT(fd != -1, ""failed to create temporary file"");
"
286,"50.,
95.}; //{1., 5., 25., 50., 75., 90., 95., 99., 99.25, 99.5, 99.75, 99.9};
struct Report {
std::string benchmark;
std::string strategy;
","50.,
95.}; //{1., 5., 25., 50., 75., 90., 95., 99., 99.25, 99.5, 99.75, 99.9};
// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)
struct Report {
std::string benchmark;
std::string strategy;
"
287,"tupleElements.pop_back();
// Build AutogradMetadata.
int64_t autogradContextId, autogradMessageId;
autogradMessageId = tupleElements.back().toInt();
tupleElements.pop_back();
","tupleElements.pop_back();
// Build AutogradMetadata.
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
int64_t autogradContextId, autogradMessageId;
autogradMessageId = tupleElements.back().toInt();
tupleElements.pop_back();
"
288,"auto wrappedProfilingMsg = RpcWithProfilingReq(
msgType,
std::move(wrappedRpcMessage),
std::move(profilerConfig),
globallyUniqueProfilingId);
","auto wrappedProfilingMsg = RpcWithProfilingReq(
msgType,
std::move(wrappedRpcMessage),
      // NOLINTNEXTLINE(performance-move-const-arg)
std::move(profilerConfig),
globallyUniqueProfilingId);
"
289,"namespace {
#ifdef USE_C10D_GLOO
constexpr char* GLOO_SOCKET_IFNAME_ENV = ""GLOO_SOCKET_IFNAME"";
#endif
","namespace {
#ifdef USE_C10D_GLOO
// NOLINTNEXTLINE(clang-diagnostic-unused-const-variable,cppcoreguidelines-avoid-non-const-global-variables)
constexpr char* GLOO_SOCKET_IFNAME_ENV = ""GLOO_SOCKET_IFNAME"";
#endif
"
290,"}
void ProcessGroupAgent::handleSend(const SendWork& work) {
auto serializedPayload = std::make_unique<std::string>(std::move(
wireSerialize(work.message_.payload(), work.message_.tensors())));
","}
void ProcessGroupAgent::handleSend(const SendWork& work) {
  // NOLINTNEXTLINE(clang-diagnostic-pessimizing-move)
auto serializedPayload = std::make_unique<std::string>(std::move(
wireSerialize(work.message_.payload(), work.message_.tensors())));
"
291,"program, args.size(), args.data());
if (result != NVRTC_SUCCESS) {
size_t logsize;
at::globalContext().getNVRTC().nvrtcGetProgramLogSize(program, &logsize);
std::vector<char> log(logsize);
","program, args.size(), args.data());
if (result != NVRTC_SUCCESS) {
      // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
size_t logsize;
at::globalContext().getNVRTC().nvrtcGetProgramLogSize(program, &logsize);
std::vector<char> log(logsize);
"
292,"Val* IterDomain::extent() const {
TORCH_CHECK(isLoweredVal(extent_));
if (isThread()) {
if (extent_->getValType() == ValType::KirScalar) {
if (extent_->as<kir::Int>()->isConst()) {
return extent_;
","Val* IterDomain::extent() const {
TORCH_CHECK(isLoweredVal(extent_));
if (isThread()) {
    // NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
if (extent_->getValType() == ValType::KirScalar) {
if (extent_->as<kir::Int>()->isConst()) {
return extent_;
"
293,"for (const JitOp* node : block->nodes()) {
processJitNode(node);
if (node->kind() == aten::rand_like) {
disable_unroll = true;
}
if (node->kind() == aten::sum) {
has_reduction = true;
}
}
","for (const JitOp* node : block->nodes()) {
processJitNode(node);
if (node->kind() == aten::rand_like) {
        // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)
disable_unroll = true;
}
if (node->kind() == aten::sum) {
        // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)
has_reduction = true;
}
}
"
294,"return true;
} else if (val->type()->isSubtypeOf(
static_cast<c10::TypePtr>(IntType::get()))) {
CgValue cg_val;
if (auto ival = constant_as<int>(val)) {
cg_val = new Int(ival.value());
","return true;
} else if (val->type()->isSubtypeOf(
static_cast<c10::TypePtr>(IntType::get()))) {
      // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
CgValue cg_val;
if (auto ival = constant_as<int>(val)) {
cg_val = new Int(ival.value());
"
295,"std::to_string(v->start()));
}
if (prev == nullptr) {
gpu_thread_extents_[gpu_thread_index] = v->stop();
} else if (prev->isConstant() && immediateEquals(prev, 1)) {
","std::to_string(v->start()));
}
    // NOLINTNEXTLINE(bugprone-branch-clone)
if (prev == nullptr) {
gpu_thread_extents_[gpu_thread_index] = v->stop();
} else if (prev->isConstant() && immediateEquals(prev, 1)) {
"
296,"void CudaCodeGen::CompileToNVRTC(
const std::string& code,
const std::string& func_name) {
CUcontext pctx = 0;
AT_CUDA_DRIVER_CHECK(nvrtc().cuCtxGetCurrent(&pctx));
// Note: hacked at::DeviceGuard since at::DeviceGuard was failing to work
","void CudaCodeGen::CompileToNVRTC(
const std::string& code,
const std::string& func_name) {
  // NOLINTNEXTLINE(modernize-use-nullptr)
CUcontext pctx = 0;
AT_CUDA_DRIVER_CHECK(nvrtc().cuCtxGetCurrent(&pctx));
// Note: hacked at::DeviceGuard since at::DeviceGuard was failing to work
"
297,"// Acquires device and NVRTC properties (for compile arch and occupancy
// calculations)
cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
int major, minor;
bool compile_to_sass = false;
codegenOutputQuery(prop, major, minor, compile_to_sass);
// Creates the NVRTC program
nvrtcProgram program;
AT_CUDA_NVRTC_CHECK(nvrtc().nvrtcCreateProgram(
&program, code.c_str(), nullptr, 0, nullptr, nullptr));
","// Acquires device and NVRTC properties (for compile arch and occupancy
// calculations)
cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
int major, minor;
bool compile_to_sass = false;
codegenOutputQuery(prop, major, minor, compile_to_sass);
// Creates the NVRTC program
  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)
nvrtcProgram program;
AT_CUDA_NVRTC_CHECK(nvrtc().nvrtcCreateProgram(
&program, code.c_str(), nullptr, 0, nullptr, nullptr));
"
298,"scalar = opterm->scalar();
variables = opterm->variables();
}
if (expr->isConstant()) {
scalar = combine_scalars(scalar, expr);
} else {
","scalar = opterm->scalar();
variables = opterm->variables();
}
    // NOLINTNEXTLINE(clang-analyzer-core.CallAndMessage)
if (expr->isConstant()) {
scalar = combine_scalars(scalar, expr);
} else {
"
299,"if (properly_strided_output->stmt()) {
block->append_stmt(properly_strided_output->stmt());
}
bufs_[output] = properly_strided_output->buf();
const auto& tt = output->type()->expect<TensorType>();
auto sizes = *tt->sizes().concrete_sizes();
","if (properly_strided_output->stmt()) {
block->append_stmt(properly_strided_output->stmt());
}
    // NOLINTNEXTLINE(clang-analyzer-cplusplus.NewDeleteLeaks)
bufs_[output] = properly_strided_output->buf();
const auto& tt = output->type()->expect<TensorType>();
auto sizes = *tt->sizes().concrete_sizes();
"
300,"OpCode op;
if (input->node()->kind() == prim::Constant) {
op = LOADC;
      } else if (drop) {
        op = DROPR;
} else if (moved) {
op = MOVE;
} else {
op = LOAD;
}
insertInstruction(op, reg);
}
}
","OpCode op;
if (input->node()->kind() == prim::Constant) {
op = LOADC;
} else if (moved) {
op = MOVE;
} else {
op = LOAD;
}

      if (drop) {
        op = DROPR;
      }
insertInstruction(op, reg);
}
}
"
301,"{c10::Symbol::fromQualString(""aten::reshape""),
c10::Symbol::fromQualString(""static_runtime::reshape_copy"")},
{c10::Symbol::fromQualString(""aten::flatten""),
       c10::Symbol::fromQualString(""static_runtime::flatten_copy"")},
      {c10::Symbol::fromQualString(""aten::to""),
c10::Symbol::fromQualString(""static_runtime::to_copy"")}};
bool has_inplace_ops = HasInplaceOp(graph, db);
std::vector<std::pair<Node*, Node*>> replacement;
for (auto* n : graph->nodes()) {
    if (!supported.count(n->kind()) ||
        !opIsRegistered(supported.at(n->kind()))) {
continue;
}
DCHECK(n->outputs().size() == 1);
","{c10::Symbol::fromQualString(""aten::reshape""),
c10::Symbol::fromQualString(""static_runtime::reshape_copy"")},
{c10::Symbol::fromQualString(""aten::flatten""),
       c10::Symbol::fromQualString(""static_runtime::flatten_copy"")}};

  // for ops that have overloads, match the schema
  const std::vector<std::pair<c10::FunctionSchema, c10::Symbol>> supported_schema = {
      {torch::schema(
           ""aten::to.prim_dtype(Tensor(a) self, int? dtype=None, bool non_blocking=False, bool copy=False) -> Tensor(a|b)""),
       c10::Symbol::fromQualString(""static_runtime::to_copy"")},
      {torch::schema(
           ""to.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor""),
c10::Symbol::fromQualString(""static_runtime::to_copy"")}};
  auto match_schema = [&supported_schema](
                          const Node* node, c10::Symbol& out_matched_symbol) {
    for (auto& schema : supported_schema) {
      if (node->matches(schema.first)) {
        out_matched_symbol = schema.second;
        return true;
      }
    }
    return false;
  };

bool has_inplace_ops = HasInplaceOp(graph, db);
std::vector<std::pair<Node*, Node*>> replacement;
for (auto* n : graph->nodes()) {
    c10::Symbol new_symbol;
    if (supported.count(n->kind()) && opIsRegistered(supported.at(n->kind()))) {
      new_symbol = supported.at(n->kind());
    } else if (!match_schema(n, new_symbol)) {
continue;
}
DCHECK(n->outputs().size() == 1);
"
302,""". Please rename the out tensor's dims with `Tensor.rename`."");
}
Tensor& propagate_names_if_nonempty(Tensor& result,
DimnameList maybe_names,
bool validate_names) {
propagate_names_if_nonempty(result.unsafeGetTensorImpl(), maybe_names, validate_names);
",""". Please rename the out tensor's dims with `Tensor.rename`."");
}
const Tensor& propagate_names_if_nonempty(const Tensor& result,
DimnameList maybe_names,
bool validate_names) {
propagate_names_if_nonempty(result.unsafeGetTensorImpl(), maybe_names, validate_names);
"
303,"namespace at { namespace native {
// Returns true if resize is necessary
bool resize_output_check(Tensor& output, IntArrayRef shape) {
// Tests for resizing of tensors with one more elements
if (output.sizes().equals(shape)) {
return false;
","namespace at { namespace native {
// Returns true if resize is necessary
bool resize_output_check(const Tensor& output, IntArrayRef shape) {
// Tests for resizing of tensors with one more elements
if (output.sizes().equals(shape)) {
return false;
"
304,"// Call the sparse implementation in SparseTensor.cpp directly.
// A dynamic dispatch here is NOT necessary, so I didn't put
// this function in native_functions.yaml
Tensor& resize_as_sparse_(Tensor& self, const Tensor& src);
// TODO(VitalyFedyunin): Move it to HTML docs.
//
","// Call the sparse implementation in SparseTensor.cpp directly.
// A dynamic dispatch here is NOT necessary, so I didn't put
// this function in native_functions.yaml
const Tensor& resize_as_sparse_(const Tensor& self, const Tensor& src);
// TODO(VitalyFedyunin): Move it to HTML docs.
//
"
305,"return self;
}
Tensor& resize_(
    Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
if (torch::jit::tracer::isTracing()) {
","return self;
}
const Tensor& resize_(
    const Tensor& self,
IntArrayRef size,
c10::optional<MemoryFormat> optional_memory_format) {
if (torch::jit::tracer::isTracing()) {
"
306,"py::class_<ClassDef, TreeView>(m, ""ClassDef"")
.def(py::init([](const Ident& name,
std::vector<Stmt> body,
                       std::vector<Property> props) {
const auto& r = name.range();
return ClassDef::create(
r,
name,
Maybe<Expr>::create(r),
wrap_list(r, std::move(body)),
            wrap_list(r, std::move(props)));
}));
py::class_<Decl, TreeView>(m, ""Decl"").def(py::init(
[](const SourceRange& r, std::vector<Param> params, Expr* return_type) {
return Decl::create(
","py::class_<ClassDef, TreeView>(m, ""ClassDef"")
.def(py::init([](const Ident& name,
std::vector<Stmt> body,
                       std::vector<Property> props,
                       std::vector<Assign> assigns) {
const auto& r = name.range();
return ClassDef::create(
r,
name,
Maybe<Expr>::create(r),
wrap_list(r, std::move(body)),
            wrap_list(r, std::move(props)),
            wrap_list(r, std::move(assigns)));
}));

py::class_<Decl, TreeView>(m, ""Decl"").def(py::init(
[](const SourceRange& r, std::vector<Param> params, Expr* return_type) {
return Decl::create(
"
307,"weight_size = weight.size()
grad_input = torch.matmul(grad_output, weight)
grad_weight = torch.matmul(grad_output.reshape(-1, weight_size[0]).t(), input.reshape(-1, weight_size[1]))
                return grad_input, grad_weight, grad_bias
return result, backward
)"",
R""(
","weight_size = weight.size()
grad_input = torch.matmul(grad_output, weight)
grad_weight = torch.matmul(grad_output.reshape(-1, weight_size[0]).t(), input.reshape(-1, weight_size[1]))
                # Note: calling unchecked_unwrap_optional is only safe, when we
                #       directly return grad_bias directly back to bias.
                #       Because in the case where `bias is None`, unwrapped
                #       grad_bias would just be pruned away.
                return grad_input, grad_weight, grad_bias.unchecked_unwrap_optional
return result, backward
)"",
R""(
"
308,"// Same as calling synchronize().
bool ProcessGroupNCCL::WorkNCCL::wait(std::chrono::milliseconds timeout) {
synchronizeInternal(timeout);
// Always return true, because abort API is not implemented.
return true;
","// Same as calling synchronize().
bool ProcessGroupNCCL::WorkNCCL::wait(std::chrono::milliseconds timeout) {
  RECORD_PARAM_COMMS(
      rank_,      // rank
      ""wait"",     // colName
      0,          // inSize
      0,          // outSize
      at::kByte,  // dType
      {},         // inSplitSizes
      {});        // outSplitSizes
synchronizeInternal(timeout);
// Always return true, because abort API is not implemented.
return true;
"
309,"return size;
}
static Tensor wrapped_scalar_tensor(const Scalar& scalar) {
  auto tensor = scalar_to_tensor(scalar);
  tensor.unsafeGetTensorImpl()->set_wrapped_number(true);
  return tensor;
}

Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result) {
if (!at::isComplexType(self_st) && gradient_result.is_complex()) {
// R -> C
","return size;
}
Tensor handle_r_to_c(ScalarType self_st, Tensor gradient_result) {
if (!at::isComplexType(self_st) && gradient_result.is_complex()) {
// R -> C
"
310,"int i = 0;
#ifdef CPU_CAPABILITY_AVX2
  __m256i sum_v_epu32 = _mm256_setzero_si256();
// vectorized
  for (; i < len / 16 * 16; i += 16) {
    // (i15, ..., i0)
    __m128i src_epu8 = _mm_loadu_si128(reinterpret_cast<__m128i const*>(A + i));
    __m256i src_epu16 = _mm256_cvtepu8_epi16(src_epu8);
    // (i15 ^ 2, ..., i0 ^ 2)
    __m256i sq_epu16 = _mm256_mullo_epi16(src_epu16, src_epu16);
    // (i7 ^ 2, ..., i0 ^ 2)
    __m128i sq_lo_epu16 = _mm256_castsi256_si128(sq_epu16);
    // (i15 ^ 2, ..., i8 ^ 2)
    __m128i sq_hi_epu16 = _mm256_extractf128_si256(sq_epu16, 1);
    // widen to epu32
    __m256i sq_lo_epu32 = _mm256_cvtepu16_epi32(sq_lo_epu16);
    __m256i sq_hi_epu32 = _mm256_cvtepu16_epi32(sq_hi_epu16);
    // add to running sum
    sum_v_epu32 = _mm256_add_epi32(sum_v_epu32, sq_lo_epu32);
    sum_v_epu32 = _mm256_add_epi32(sum_v_epu32, sq_hi_epu32);
  }

alignas(64) int32_t temp[8];
  _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_v_epu32);
  for (int k = 0; k < 8; ++k) {
    row_sum += temp[k];
}
#endif // CPU_CAPABILITY_AVX2
","int i = 0;
#ifdef CPU_CAPABILITY_AVX2
// vectorized
  __m256i sum_v_epu32 = _mm256_setzero_si256();
alignas(64) int32_t temp[8];
  int overflow_threshold = 262144; // 2147483647(max of int32)/(256*256)*8 = 262144
  int loop = len / overflow_threshold + 1;
  for(int j=0; j<=loop; j++){
    for (; ((i < overflow_threshold * j) && (i < len / 16 * 16)); i += 16) {
      // (i15, ..., i0)
      __m128i src_epu8 = _mm_loadu_si128(reinterpret_cast<__m128i const*>(A + i));
      __m256i src_epu16 = _mm256_cvtepu8_epi16(src_epu8);
      // (i15 ^ 2, ..., i0 ^ 2)
      __m256i sq_epu16 = _mm256_mullo_epi16(src_epu16, src_epu16);
      // (i7 ^ 2, ..., i0 ^ 2)
      __m128i sq_lo_epu16 = _mm256_castsi256_si128(sq_epu16);
      // (i15 ^ 2, ..., i8 ^ 2)
      __m128i sq_hi_epu16 = _mm256_extractf128_si256(sq_epu16, 1);
      // widen to epu32
      __m256i sq_lo_epu32 = _mm256_cvtepu16_epi32(sq_lo_epu16);
      __m256i sq_hi_epu32 = _mm256_cvtepu16_epi32(sq_hi_epu16);
      // add to running sum
      sum_v_epu32 = _mm256_add_epi32(sum_v_epu32, sq_lo_epu32);
      sum_v_epu32 = _mm256_add_epi32(sum_v_epu32, sq_hi_epu32);
    }
    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_v_epu32);
    for (int k = 0; k < 8; ++k) {
      row_sum += temp[k];
    }
    sum_v_epu32 = _mm256_setzero_si256();
}
#endif // CPU_CAPABILITY_AVX2
"
311,".set_rcond(cond)
.set_rank(rank)
.set_s(singular_values)
.call_driver() // initial call to deduce optimal sizes for workspace arrays
.set_work()
.set_rwork()
",".set_rcond(cond)
.set_rank(rank)
.set_s(singular_values)
      .set_infos(infos)
.call_driver() // initial call to deduce optimal sizes for workspace arrays
.set_work()
.set_rwork()
"
312,"GRAPH_DUMP(""Graph before fixing controlflow: "", node->owningGraph());
auto* if_node = node;
auto* graph = if_node->owningGraph();
  FixupONNXSubblockOutputs(if_node);
ONNXFixupUninitializedOutput(if_node);
GRAPH_DUMP(""Graph after fixing controlflow: "", node->owningGraph());
return if_node->outputs().vec();
","GRAPH_DUMP(""Graph before fixing controlflow: "", node->owningGraph());
auto* if_node = node;
auto* graph = if_node->owningGraph();
  FixupONNXSubblockOutputs(node);
ONNXFixupUninitializedOutput(if_node);
GRAPH_DUMP(""Graph after fixing controlflow: "", node->owningGraph());
return if_node->outputs().vec();
"
313,"if (grad_fn) {
msg = c10::str(""Output "", diff_view_meta->output_nr_, "" of "", grad_fn->name(), "" is a view and "",
modified_obj, "" modified inplace."");
} else {
msg = c10::str(""A view was created in no_grad mode and "", modified_obj, "" modified inplace with grad mode enabled."");
}
","if (grad_fn) {
msg = c10::str(""Output "", diff_view_meta->output_nr_, "" of "", grad_fn->name(), "" is a view and "",
modified_obj, "" modified inplace."");
    } else if (creation_meta == CreationMeta::INFERENCE_MODE) {
      msg = c10::str(""A view was created in inference mode and "", modified_obj, "" modified inplace in normal mode."");
} else {
msg = c10::str(""A view was created in no_grad mode and "", modified_obj, "" modified inplace with grad mode enabled."");
}
"
314,"#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
#include <ATen/native/ReduceOpsUtils.h>
#include <ATen/native/TensorIterator.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/native/TensorDimApply.h>
","#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>
#include <ATen/native/ReduceOpsUtils.h>
#include <ATen/native/Resize.h>
#include <ATen/native/TensorIterator.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/native/TensorDimApply.h>
"
315,"scalar_t* gradOutput_p_k = gradOutput_p + k * oT * oH * oW;
int64_t* ind_p_k = ind_p + k * iT * iH * iW;
    int t, i, j, index;
int64_t maxp;
for (t = 0; t < iT; t++) {
for (i = 0; i < iH; i++) {
","scalar_t* gradOutput_p_k = gradOutput_p + k * oT * oH * oW;
int64_t* ind_p_k = ind_p + k * iT * iH * iW;
    int64_t t, i, j, index;
int64_t maxp;
for (t = 0; t < iT; t++) {
for (i = 0; i < iH; i++) {
"
316,"scalar_t scale = static_cast<scalar_t>(scale_);
        int elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per output:
","scalar_t scale = static_cast<scalar_t>(scale_);
        int64_t elt;
// For each elt in batch, do:
for (elt = 0; elt < batch_size; ++elt) {
// Matrix mulitply per output:
"
317,"std::string getSchemaInputTypesString(const FunctionSchema& schema) {
std::stringstream input_types;
const std::vector<Argument>& forward_args = schema.arguments();
  for (int i = 1; i < forward_args.size(); ++i) {
input_types << forward_args[i].type()->annotation_str();
if (forward_args.size() - 1 != i) {
input_types << "", "";
","std::string getSchemaInputTypesString(const FunctionSchema& schema) {
std::stringstream input_types;
const std::vector<Argument>& forward_args = schema.arguments();
  for (const auto i : c10::irange(1, forward_args.size())) {
input_types << forward_args[i].type()->annotation_str();
if (forward_args.size() - 1 != i) {
input_types << "", "";
"
318,"output.fill_(value);
auto c_output = output;
    for (int i = l_diff; i < l_inp; i++) {
auto pad_idx = 2 * (l_inp - i - 1);
if (pad[pad_idx] > 0) {
c_output = c_output.narrow(i, pad[pad_idx], c_output.size(i) - pad[pad_idx]);
","output.fill_(value);
auto c_output = output;
    for (const auto i : c10::irange(l_diff, l_inp)) {
auto pad_idx = 2 * (l_inp - i - 1);
if (pad[pad_idx] > 0) {
c_output = c_output.narrow(i, pad[pad_idx], c_output.size(i) - pad[pad_idx]);
"
319,"#include <ATen/quantized/QTensorImpl.h>
#include <ATen/quantized/Quantizer.h>
namespace at {
namespace native {
","#include <ATen/quantized/QTensorImpl.h>
#include <ATen/quantized/Quantizer.h>
#include <c10/util/irange.h>

namespace at {
namespace native {
"
320,"const Tensor& zero_points,
ScalarType dtype) {
std::vector<Tensor> quantized_tensors;
  for (auto i = 0; i < tensors.size(); ++i) {
quantized_tensors.push_back(at::quantize_per_tensor(
tensors[i],
scales[i].item<double>(),
","const Tensor& zero_points,
ScalarType dtype) {
std::vector<Tensor> quantized_tensors;
  for (const auto i : c10::irange(tensors.size())) {
quantized_tensors.push_back(at::quantize_per_tensor(
tensors[i],
scales[i].item<double>(),
"
321,"const int num_tasks = at::get_num_threads();
at::parallel_for(0, num_tasks, 1, [&](int64_t begin, int64_t end) {
fbgemm::DoNothing<> kNoOpObj{};
    for (int task_id = begin; task_id < end; ++task_id) {
if (q_scheme == c10::kPerTensorAffine) {
fbgemm::ReQuantizeOutput<
kReluFused,
","const int num_tasks = at::get_num_threads();
at::parallel_for(0, num_tasks, 1, [&](int64_t begin, int64_t end) {
fbgemm::DoNothing<> kNoOpObj{};
    for (const auto task_id : c10::irange(begin, end)) {
if (q_scheme == c10::kPerTensorAffine) {
fbgemm::ReQuantizeOutput<
kReluFused,
"
322,"#include <ATen/native/quantized/cpu/fbgemm_utils.h>
#include <torch/library.h>
torch::class_<EmbeddingPackedParamsBase> register_embedding_params();
/*
","#include <ATen/native/quantized/cpu/fbgemm_utils.h>
#include <torch/library.h>
#include <c10/util/irange.h>

torch::class_<EmbeddingPackedParamsBase> register_embedding_params();
/*
"
323,"output_row_scale_zp[1] = Xmin;
// Pack the weight values.
      for (int col = 0; col < embedding_cols; ++col) {
float X = input_row[col];
std::uint8_t quantized = std::max(
0,
","output_row_scale_zp[1] = Xmin;
// Pack the weight values.
      for (const auto col : c10::irange(embedding_cols)) {
float X = input_row[col];
std::uint8_t quantized = std::max(
0,
"
324,"return native::add_sparse_(self, other, -alpha);
}
Tensor& sub_out_sparse(Tensor& r, const Tensor& self, const Tensor& other, const Scalar& alpha) {
sub_check(self, other);
return at::add_out(r, self, other, -alpha);  // redispatch!
}
","return native::add_sparse_(self, other, -alpha);
}
Tensor& sub_out_sparse(const Tensor& self, const Tensor& other, const Scalar& alpha, Tensor& r) {
sub_check(self, other);
return at::add_out(r, self, other, -alpha);  // redispatch!
}
"
325,"auto r = parser.parse(args, kwargs, parsed_args);
if (r.idx == 0) {
bool type_inference = r.isNone(2);
    const auto inferred_dispatch_key = denseTypeIdWithDefault(r, 3, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(2, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(3));
// if no dtype provided, infer type based on value type.
","auto r = parser.parse(args, kwargs, parsed_args);
if (r.idx == 0) {
bool type_inference = r.isNone(2);
    const auto inferred_dispatch_key = typeIdWithDefault(r, 3, dispatch_key);
const auto inferred_scalar_type = r.scalartypeWithDefault(2, scalar_type);
at::OptionalDeviceGuard device_guard(r.deviceOptional(3));
// if no dtype provided, infer type based on value type.
"
326,"store_->wait(joinedKeys, timeout);
}
} // namespace c10d
","store_->wait(joinedKeys, timeout);
}
const std::chrono::milliseconds& PrefixStore::getTimeout() const noexcept {
  return store_->getTimeout();
}

void PrefixStore::setTimeout(const std::chrono::milliseconds& timeout) {
  store_->setTimeout(timeout);
}

} // namespace c10d
"
327,"fuse.RegisterRewritePattern(pattern2, fused_pattern2);
fuse.runOnGraph(graph);
}
void ClipRangesGatherRangesX2SigridHashPrecompute(
","fuse.RegisterRewritePattern(pattern2, fused_pattern2);
fuse.runOnGraph(graph);

  // reverse the ops that got fused in step 1 but not in step2
  fuse.RegisterRewritePattern(fused_pattern, pattern);
  fuse.runOnGraph(graph);
}
void ClipRangesGatherRangesX2SigridHashPrecompute(
"
328,"std::shared_ptr<torch::jit::Graph>& graph) {
#ifdef FBCODE_CAFFE2
PrecomputeMultiplierShiftForSigridHash(graph);
#endif
}
","std::shared_ptr<torch::jit::Graph>& graph) {
#ifdef FBCODE_CAFFE2
PrecomputeMultiplierShiftForSigridHash(graph);
  ConstantPropagation(graph);
  ConstantPooling(graph);
#endif
}
"
329,"PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
using namespace torch::autograd::profiler;
  auto tensor_module = THPObjectPtr(PyImport_ImportModule(""torch.tensor""));
if (!tensor_module)
return nullptr;
","PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
using namespace torch::autograd::profiler;
  auto tensor_module = THPObjectPtr(PyImport_ImportModule(""torch._tensor""));
if (!tensor_module)
return nullptr;
"
330,"TORCH_INTERNAL_ASSERT(false, ""Expected non-Tensor backend scalar"");
}
void assert_async_cpu(const Tensor& self) {
TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
","TORCH_INTERNAL_ASSERT(false, ""Expected non-Tensor backend scalar"");
}
void _assert_async_cpu(const Tensor& self) {
TORCH_CHECK(native::is_nonzero(self), ""Expected Tensor with single nonzero value, but got zero"");
}
"
331,"#include <c10d/logger.hpp>
namespace c10d {
","#include <c10d/logger.hpp>
#include <c10d/Utils.hpp>
#include <fmt/format.h>
namespace c10d {
"
332,"}
TORCH_INTERNAL_ASSERT(expect_sparse_gradients_.size() == replicas_.size());
  // Corresponding params' layouts (strides) must match across
  // replicas within this process and across processes.
  // (see Note:  ""Gradient Layout Contract"" in initialize_buckets).
  verify_replicas_within_process();
  verify_replica0_across_processes();

// Initialize variable bucketing.
// This can be reinitialized later after capturing runtime information.
{
","}
TORCH_INTERNAL_ASSERT(expect_sparse_gradients_.size() == replicas_.size());
// Initialize variable bucketing.
// This can be reinitialized later after capturing runtime information.
{
"
333,"const caffe2::TypeMeta dtype) {
VkDeviceSize size = c10::elementSize(c10::typeMetaToScalarType(dtype));
  // Forward declaration
  bool requires_image(IntArrayRef);

if (requires_image(sizes)) {
    // Forward declaration
    uvec3 image_extents(IntArrayRef);

const uvec3 extents = image_extents(sizes);
size *= extents.data[0u] * extents.data[1u] * (4u * extents.data[2u]);
}
","const caffe2::TypeMeta dtype) {
VkDeviceSize size = c10::elementSize(c10::typeMetaToScalarType(dtype));
if (requires_image(sizes)) {
const uvec3 extents = image_extents(sizes);
size *= extents.data[0u] * extents.data[1u] * (4u * extents.data[2u]);
}
"
334,"if (asyncErrorHandling_) {
workCleanupThread_ = std::thread(&ProcessGroupNCCL::workCleanupLoop, this);
}
LOG(INFO) << ""[Rank "" << rank_
<< ""] ProcessGroupNCCL initialized with following options:""
<< ""\nNCCL_ASYNC_ERROR_HANDLING: "" << asyncErrorHandling_
<< ""\nNCCL_BLOCKING_WAIT: "" << blockingWait_
<< ""\nTIMEOUT(ms): "" << opTimeout_.count()
            << ""\nUSE_HIGH_PRIORITY_STREAM: "" << isHighPriorityStream_;
}
ProcessGroupNCCL::~ProcessGroupNCCL() {
","if (asyncErrorHandling_) {
workCleanupThread_ = std::thread(&ProcessGroupNCCL::workCleanupLoop, this);
}

  const char * ncclDebugLevel = std::getenv(""NCCL_DEBUG"");

  if (!ncclDebugLevel) {
    ncclDebugLevel = ""UNSET"";
  }

LOG(INFO) << ""[Rank "" << rank_
<< ""] ProcessGroupNCCL initialized with following options:""
<< ""\nNCCL_ASYNC_ERROR_HANDLING: "" << asyncErrorHandling_
<< ""\nNCCL_BLOCKING_WAIT: "" << blockingWait_
<< ""\nTIMEOUT(ms): "" << opTimeout_.count()
            << ""\nUSE_HIGH_PRIORITY_STREAM: "" << isHighPriorityStream_
            << ""\nNCCL_DEBUG: "" << ncclDebugLevel;
}
ProcessGroupNCCL::~ProcessGroupNCCL() {
"
335,"if (isServer_) {
// Store daemon should end because of closed connection.
// daemon destructor should join the thread
    tcpStoreDaemon_.reset(nullptr);
tcputil::closeSocket(masterListenSocket_);
}
}
","if (isServer_) {
// Store daemon should end because of closed connection.
// daemon destructor should join the thread
    tcpStoreDaemon_ = nullptr;
tcputil::closeSocket(masterListenSocket_);
}
}
"
336,"return self.is_quantized();
}
#define DEFINE_CAST(T, name)                     \
  template <>                                    \
  TORCH_API T* Tensor::data_ptr() const {           \
    TORCH_CHECK(                                 \
        scalar_type() == ScalarType::name,       \
        ""expected scalar type "",                 \
        #name,                                   \
        "" but found "",                           \
        c10::toString(scalar_type()));           \
return static_cast<T*>(this->unsafeGetTensorImpl()->data());    \
}
","return self.is_quantized();
}
#define DEFINE_CAST(T, name)                                        \
  template <>                                                       \
  TORCH_API T* Tensor::data_ptr() const {                           \
    TORCH_CHECK(                                                    \
        scalar_type() == ScalarType::name,                          \
        ""expected scalar type ""                                     \
        #name                                                       \
        "" but found "",                                              \
        scalar_type());                                             \
return static_cast<T*>(this->unsafeGetTensorImpl()->data());    \
}
"
337,"Message&& requestMessage,
std::shared_ptr<LazyStreamContext> ctx) mutable {
if (error) {
          // FIXME This is not a correct way to check whether this error was
          // ""intentionally"" caused by the remote end shutting down. We should
          // find a better way, Perhaps sending an empty message?
          if ((error.isOfType<tensorpipe::PipeClosedError>() &&
               !rpcAgentRunning_.load()) ||
              error.isOfType<tensorpipe::EOFError>()) {
// This is expected.
} else {
LOG(WARNING)
<< ""RPC agent for "" << workerInfo_.name_
<< "" encountered error when reading incoming request from ""
                << pipe->getRemoteName() << "": "" << error.what()
                << "" (this is expected to happen during shutdown)"";
}
return;
}
","Message&& requestMessage,
std::shared_ptr<LazyStreamContext> ctx) mutable {
if (error) {
          if (shuttingDown_) {
// This is expected.
} else {
LOG(WARNING)
<< ""RPC agent for "" << workerInfo_.name_
<< "" encountered error when reading incoming request from ""
                << pipe->getRemoteName() << "": "" << error.what();
}
return;
}
"
338,"flush();
}
catch (const std::exception& e) {
    LOG(WARNING)
        << ""Vulkan: Context destructor raised an exception!  Error: ""
        << e.what();
}
catch (...) {
    LOG(WARNING) << ""Vulkan: Context destructor raised an unknown exception!"";
}
}
","flush();
}
catch (const std::exception& e) {
    TORCH_WARN(
        ""Vulkan: Context destructor raised an exception! Error: "",
        e.what());
}
catch (...) {
    TORCH_WARN(
        ""Vulkan: Context destructor raised an exception! ""
        ""Error: Unknown"");
}
}
"
339,"update_stat_array(stats.segment, 1, p.stat_types);
update_stat_array(stats.reserved_bytes, size, p.stat_types);
    return (p.block != nullptr);
}
bool free_cached_blocks()
","update_stat_array(stats.segment, 1, p.stat_types);
update_stat_array(stats.reserved_bytes, size, p.stat_types);
    // p.block came from new, not cudaMalloc.  It should not be nullptr here.
    TORCH_INTERNAL_ASSERT(p.block != nullptr && p.block->ptr != nullptr);
    return true;
}
bool free_cached_blocks()
"
340,"// or only transposed in the last two axis
const auto res_sizes = res.sizes();
if (canAvoidTensorAccessor) {
    scalar_t* mat1_data = static_cast<scalar_t*>(mat1.data_ptr());
    scalar_t* mat2_data = static_cast<scalar_t*>(mat2.data_ptr());
for (int64_t batch = 0; batch < batch_size; batch++) {
A.emplace_back(mat1_data + batch * mat1_sizes[1] * mat1_sizes[2]);
B.emplace_back(mat2_data + batch * mat2_sizes[1] * mat2_sizes[2]);
","// or only transposed in the last two axis
const auto res_sizes = res.sizes();
if (canAvoidTensorAccessor) {
    scalar_t* mat1_data = mat1.data_ptr<scalar_t>();
    scalar_t* mat2_data = mat2.data_ptr<scalar_t>();
for (int64_t batch = 0; batch < batch_size; batch++) {
A.emplace_back(mat1_data + batch * mat1_sizes[1] * mat1_sizes[2]);
B.emplace_back(mat2_data + batch * mat2_sizes[1] * mat2_sizes[2]);
"
341,"checkAllContiguous(c, {save_mean, save_var});
// TODO: TensorArg check should start handle memory format
TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));
  TORCH_CHECK(grad_output->is_contiguous(grad_output->suggest_memory_format()));
checkDimRange(c, input, 2, 6 /* exclusive */);
checkSameSize(c, input, grad_output);
auto num_features = input->size(1);
","checkAllContiguous(c, {save_mean, save_var});
// TODO: TensorArg check should start handle memory format
TORCH_CHECK(input->is_contiguous(input->suggest_memory_format()));
  TORCH_CHECK(grad_output->is_contiguous(input->suggest_memory_format()));
checkDimRange(c, input, 2, 6 /* exclusive */);
checkSameSize(c, input, grad_output);
auto num_features = input->size(1);
"
342,"std::pair<uint64_t, uint64_t> CUDAGeneratorImpl::philox_engine_inputs(uint64_t increment) {
at::cuda::assertNotCapturing(""Refactor this op to use CUDAGeneratorImpl::philox_cuda_state. ""
""Cannot call CUDAGeneratorImpl::philox_engine_inputs"");
uint64_t offset = this->philox_offset_per_thread_;
this->philox_offset_per_thread_ += increment;
return std::make_pair(this->seed_, offset);
","std::pair<uint64_t, uint64_t> CUDAGeneratorImpl::philox_engine_inputs(uint64_t increment) {
at::cuda::assertNotCapturing(""Refactor this op to use CUDAGeneratorImpl::philox_cuda_state. ""
""Cannot call CUDAGeneratorImpl::philox_engine_inputs"");
  // see Note [Why enforce RNG offset % 4 == 0?]
  TORCH_INTERNAL_ASSERT(this->philox_offset_per_thread_ % 4 == 0);
uint64_t offset = this->philox_offset_per_thread_;
this->philox_offset_per_thread_ += increment;
return std::make_pair(this->seed_, offset);
"
343,"#include <torch/csrc/jit/codegen/cuda/kernel_cache.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
#include <torch/csrc/jit/codegen/cuda/parser.h>
","#include <torch/csrc/jit/codegen/cuda/kernel_cache.h>

#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/codegen/cuda/ir_utils.h>
#include <torch/csrc/jit/codegen/cuda/parser.h>
"
344,"#include <torch/csrc/jit/codegen/cuda/shape_inference.h>
#include <c10/core/ScalarType.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/ir/constants.h>
","#include <torch/csrc/jit/codegen/cuda/shape_inference.h>

#include <c10/core/ScalarType.h>
#include <torch/csrc/jit/codegen/cuda/instrumentation.h>
#include <torch/csrc/jit/ir/constants.h>
"
345,"#include <torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h>
#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <torch/csrc/jit/codegen/fuser/compiler.h>
","#include <torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h>

#include <c10/util/Exception.h>
#include <c10/util/Optional.h>
#include <torch/csrc/jit/codegen/fuser/compiler.h>
"
346,"#include <torch/csrc/jit/codegen/fuser/cuda/fused_kernel.h>
#include <torch/csrc/jit/codegen/fuser/compiler.h>
#include <ATen/ATen.h>
","#include <torch/csrc/jit/codegen/fuser/cuda/fused_kernel.h>

#include <torch/csrc/jit/codegen/fuser/compiler.h>
#include <ATen/ATen.h>
"
347,"#include <torch/csrc/jit/passes/canonicalize.h>
#include <torch/csrc/jit/ir/ir_views.h>
namespace torch {
","#include <torch/csrc/jit/passes/canonicalize.h>

#include <torch/csrc/jit/ir/ir_views.h>
namespace torch {
"
348,"#include <torch/csrc/jit/passes/fold_conv_bn.h>
#include <torch/csrc/jit/ir/subgraph_matcher.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/graph_rewrite_helper.h>
","#include <torch/csrc/jit/passes/fold_conv_bn.h>

#include <torch/csrc/jit/ir/subgraph_matcher.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/graph_rewrite_helper.h>
"
349,"#include <torch/csrc/jit/passes/guard_elimination.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
","#include <torch/csrc/jit/passes/guard_elimination.h>

#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
"
350,"#include <torch/csrc/jit/passes/liveness.h>
#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir_views.h>
#include <torch/csrc/jit/passes/constant_pooling.h>
","#include <torch/csrc/jit/passes/liveness.h>

#include <torch/csrc/jit/ir/alias_analysis.h>
#include <torch/csrc/jit/ir/ir_views.h>
#include <torch/csrc/jit/passes/constant_pooling.h>
"
351,"#include <torch/csrc/jit/passes/onnx.h>
#include <ATen/core/functional.h>
#include <c10/util/Exception.h>
#include <torch/csrc/autograd/function.h>
","#include <torch/csrc/jit/passes/onnx.h>

#include <ATen/core/functional.h>
#include <c10/util/Exception.h>
#include <torch/csrc/autograd/function.h>
"
352,"#include <torch/csrc/jit/passes/onnx/helper.h>
#include <onnx/onnx_pb.h>
namespace torch {
","#include <torch/csrc/jit/passes/onnx/helper.h>

#include <onnx/onnx_pb.h>
namespace torch {
"
353,"#include <torch/csrc/jit/passes/onnx/shape_type_inference.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/onnx/helper.h>
#include <torch/csrc/jit/passes/onnx/scalar_type_analysis.h>
","#include <torch/csrc/jit/passes/onnx/shape_type_inference.h>

#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/onnx/helper.h>
#include <torch/csrc/jit/passes/onnx/scalar_type_analysis.h>
"
354,"#include <torch/csrc/jit/passes/subgraph_rewrite.h>
#include <torch/csrc/jit/ir/irparser.h>
#include <torch/csrc/jit/ir/subgraph_matcher.h>
","#include <torch/csrc/jit/passes/subgraph_rewrite.h>

#include <torch/csrc/jit/ir/irparser.h>
#include <torch/csrc/jit/ir/subgraph_matcher.h>
"
355,"#include <torch/csrc/jit/passes/utils/check_alias_annotation.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/passes/normalize_ops.h>
#include <torch/csrc/jit/runtime/operator.h>
","#include <torch/csrc/jit/passes/utils/check_alias_annotation.h>

#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/passes/normalize_ops.h>
#include <torch/csrc/jit/runtime/operator.h>
"
356,"#ifdef TORCH_ENABLE_LLVM
#include <torch/csrc/jit/tensorexpr/llvm_codegen.h>
#include <c10/util/Exception.h>
#include <torch/csrc/jit/tensorexpr/llvm_jit.h>
","#ifdef TORCH_ENABLE_LLVM
#include <torch/csrc/jit/tensorexpr/llvm_codegen.h>

#include <c10/util/Exception.h>
#include <torch/csrc/jit/tensorexpr/llvm_jit.h>
"
357,"}
Tensor pow_backward_exponent(Tensor grad, const Scalar & base, const Tensor& exponent, Tensor result) {
  auto base_ = base.isComplex() ? base.toComplexDouble() : base.toDouble();
  auto grad_lambda = [](auto a, auto b) { return (a * std::log(b)).conj(); };
  if (base_ == 0.0) {
auto cond = [](auto exp) {
if (exp.is_complex()) {
return at::logical_and(at::imag(exp) == 0, at::real(exp) >= 0);
","}
Tensor pow_backward_exponent(Tensor grad, const Scalar & base, const Tensor& exponent, Tensor result) {
  auto grad_lambda = [](Tensor a, Scalar b) { return (a * b.log()).conj(); };
  if (base.equal(0.0)) {
auto cond = [](auto exp) {
if (exp.is_complex()) {
return at::logical_and(at::imag(exp) == 0, at::real(exp) >= 0);
"
358,"}
if (model_version == 0x3L &&
opname == c10::OperatorName(""aten::_convolution"", """")) {
    // Since byte-code versions 0x4L, convolution has an additional
    // default-value argument (allow_tf32=True, see
    // https://github.com/pytorch/pytorch/pull/40737). This wrapper handles
    // backward compatibility with models of byte-code version <= 0x3L, where
    // this bool argument does not yet exist.
fn = [fn](Stack& stack) {
stack.push_back(true);
fn(stack);
","}
if (model_version == 0x3L &&
      model_version < caffe2::serialize::kProducedBytecodeVersion &&
opname == c10::OperatorName(""aten::_convolution"", """")) {
    // A default-value argument will be added in
    // https://github.com/pytorch/pytorch/pull/40737. This wrapper is used to
    // handle backward compatibility, where there is no default bool value in
    // old models.
fn = [fn](Stack& stack) {
stack.push_back(true);
fn(stack);
"
359,"function->set_register_size(register_size);
mcu.register_function(std::move(function));
}
}
// The deserializer class which loads the bytecode package from bc files.
class BytecodeDeserializer final {
 public:
  explicit BytecodeDeserializer(std::unique_ptr<PyTorchStreamReader> reader);
  mobile::Module deserialize(
      c10::optional<at::Device> device,
      ExtraFilesMap& extra_files);
  std::unordered_map<std::string, std::string> deserializeMetadata(
      c10::optional<at::Device> device);

 private:
  c10::IValue readArchive(
      const std::string& archive_name,
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::unordered_map<std::string, std::string> readMobileMetadata(
      std::shared_ptr<mobile::CompilationUnit> mcu);
  std::shared_ptr<CompilationUnit> compilation_unit_;
  std::unordered_set<std::string> imported_libs_;
  std::unique_ptr<PyTorchStreamReader> reader_;
  c10::optional<at::Device> device_;
};

BytecodeDeserializer::BytecodeDeserializer(
    std::unique_ptr<PyTorchStreamReader> reader)
    : compilation_unit_(std::make_shared<CompilationUnit>()),
      reader_(std::move(reader)) {}

std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
","function->set_register_size(register_size);
    // function schema
    if (schemaTable) { // (schema is optional for back compat)
      auto parseArgList = [this](const std::vector<IValue>& argTables) {
        std::vector<c10::Argument> args;
        for (auto&& argTable : argTables) {
          auto name =
              expect_field(argTable, ""name"", BYTECODE_INDEX_ARGUMENT_NAME)
                  .toStringRef();
          const auto& type = resolveTypeName(
              (expect_field(argTable, ""type"", BYTECODE_INDEX_ARGUMENT_TYPE))
                  .toStringRef());
          auto default_value = expect_field(
                                   argTable,
                                   ""default_value"",
                                   BYTECODE_INDEX_ARGUMENT_DEFAULT_VALUE)
                                   .toIValue();
          auto arg =
              c10::Argument(name, type, c10::nullopt /*N*/, default_value);
          args.emplace_back(std::move(arg));
        }
        return args;
      };
      const auto& arg_list =
          expect_field(
              *schemaTable, ""arguments"", BYTECODE_INDEX_SCHEMA_ARGUMENTS)
              .toTuple()
              ->elements();
      const auto& ret_list =
          expect_field(*schemaTable, ""returns"", BYTECODE_INDEX_SCHEMA_RETURNS)
              .toTuple()
              ->elements();
      c10::FunctionSchema schema(
          function_name,
          """" /*overload_name*/,
          parseArgList(arg_list),
          parseArgList(ret_list),
          false /*is_varargs*/,
          false /*is_varret*/);
      function->setSchema(std::move(schema));
    }

mcu.register_function(std::move(function));
}
}
std::unordered_map<std::string, std::string> BytecodeDeserializer::
deserializeMetadata(c10::optional<at::Device> device) {
device_ = device;
"
360,"}
}
c10::IValue Method::operator()(std::vector<IValue> stack) {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
","}
}
c10::IValue Method::operator()(std::vector<IValue> stack) const {
run(stack);
TORCH_INTERNAL_ASSERT(!stack.empty());
return stack.front();
"
361,"return result;
}
Tensor einsum(std::string eqn, TensorList tensors) {
  constexpr size_t number_of_letters = 26;
  std::string in_eqn;
  size_t pos;
  // The equation is given in terms of single lowercase letters ('a'..'z') and potentially an ellipsis.
  // Internally, we represent it using indices from 0 to num_total_dimensions, with each letter
  // mapped to an index and the ellipsis ('...') being mapped to a number of consequtive indices.
  // The mapping of letters to internal indices is given in letter_mapping. A value of -1 means that
  // the letter has not been assigned an index yet (because it has not been seen).
  // The ellipsis is defined by first_ell_idx (the first index) and num_ell_idxes (the number of indices).
  // A value of -1 for num_ell_idxes specifies that we have not seen an ellipsis yet.
  // Note: The internal indices are NOT the dimensions used internally. There is a mapping to them below.

  std::array<std::int64_t, number_of_letters> letter_mapping; // map letter to internal (numerical) label
  letter_mapping.fill(-1);
  int64_t num_ell_idxes = -1;
  int64_t first_ell_idx = 0;

  // The internal representation of the left hand side fo the equation (with ellipsis expanded) is stored in input_op_idxes.
  // For each operand, we have a vector mapping each dimension to an internal index.
  // We also keep track of the number of occurrences for each letter (to infer a right hand side if not given) and
  // of the last occurrence of each index.
  std::vector<std::vector<int64_t>> input_op_idxes;                   // the parsed operand indices
  std::array<std::int64_t, number_of_letters> num_letter_occurrences; // number of occurrence in the equation of this letter
  num_letter_occurrences.fill(0);
  std::vector<std::int64_t> last_idx_occurrence;                      // the last operator (left to right) using this index

  if ((pos = eqn.find(""->"")) != std::string::npos) { // check whether we have a right hand side. in_eq is the left hand side
    in_eqn = eqn.substr(0, pos);
  } else {
    in_eqn = eqn;
  }
  // remove spaces for einsum compatibility (#9929)
  in_eqn.erase(std::remove_if(in_eqn.begin(), in_eqn.end(), isspace), in_eqn.end());

  // next we parse in_eq (the left hand side) by iterating. It is a string of comma separated terms per index
  int64_t operand = 0;
  std::stringstream eqn_stream(in_eqn);
  std::string term;
  int64_t num_total_idxes = 0;
  while (! eqn_stream.eof()) {
    std::getline(eqn_stream, term, ',');  // term = string with indices of current term
    TORCH_CHECK((int64_t) tensors.size()>operand, ""more operands in equation than tensors""); // we cannot have a longer equation than operands. We need to check here before we use the dimension

    int64_t ell_char_count = 0;            // handling of ellipsis '...' is a bit tedious, we count the '.'
    // if there is an ellipsis, the number of dimensions it represents must be total dim - letter dimensions
    int64_t candidate_num_ell_idxes = tensors[operand].dim() - term.size() + 3;
    int64_t dims_in_term = 0;              // dimensions we have seen
    std::vector<int64_t> current_op_idxes; // mapping of operand dimensions to indices for current term
    for (auto &c : term) {                 // c = character with a single letter or '.'
      if (c == '.') {
        ell_char_count++;
        TORCH_CHECK(ell_char_count <= 3, ""can only have '.' in one ellispis '...' in term "", operand, "" of the equation"");
        if (ell_char_count == 3) {        // this completes the ellipsis
          if (num_ell_idxes == -1) {      // if we have not seen an ellipsis before, keep track of indices and size
            first_ell_idx = num_total_idxes;
            num_ell_idxes = candidate_num_ell_idxes;
            num_total_idxes += num_ell_idxes;
          }
          else {                          // we have seen an ellipsis before, so we check compatibility
            TORCH_CHECK(candidate_num_ell_idxes == num_ell_idxes,
                     ""ellipsis must represent "", num_ell_idxes, "" dimensions in all terms"");
          }
          for (int64_t i = 0; i < num_ell_idxes; ++i) { // map ellipsis dimensions in operand to indices
            current_op_idxes.push_back(first_ell_idx + i);
            last_idx_occurrence.push_back(operand);
          }
          dims_in_term += num_ell_idxes;                // keep track of dimensions
        }
      } else {                                          // a letter (hopefully)
        TORCH_CHECK((ell_char_count == 0) || (ell_char_count == 3), ""'.' must only occur in ellipsis, operand "", operand);
        TORCH_CHECK(('a' <= c) && (c <= 'z'), ""only lowercase letters a-z allowed as indices"");
        int64_t letter_num = c-'a';                     // letter_num  = position in letter_mapping
        if (letter_mapping[letter_num] == -1) {         // new letter, add internal index and mapping
          letter_mapping[letter_num] = num_total_idxes;
          num_total_idxes++;
          last_idx_occurrence.push_back(operand);
        } else {                                        // letter we have already seen
          last_idx_occurrence[letter_mapping[letter_num]] = operand;
        }
        num_letter_occurrences[letter_num]++;
        current_op_idxes.push_back(letter_mapping[letter_num]);
        dims_in_term++;
      }
}
    TORCH_CHECK(dims_in_term == tensors[operand].dim(), ""dimension mismatch for operand "", operand, "": equation "", dims_in_term, "" tensor "", tensors[operand].dim());
    input_op_idxes.push_back(std::move(current_op_idxes));
    operand++;
}
  // in the check below, we need ==, but > is captured above, so the error message can be specific that it is <.
  TORCH_CHECK((int64_t) tensors.size()==operand, ""more tensors than operands in equation"");

  // the following parses or infers output (right hand side)
  // it also assigns the idxes_to_preprocessed_dims (index -> dimension in preprocessed / output tensors)
  // for the output indices. -1 means that the index has not been assigned a dimension yet
  std::vector<int64_t> idxes_to_preprocessed_dims(num_total_idxes, -1);     // the position of the index in the tensor dimensions
  int64_t num_output_dims = 0;
  if (pos != std::string::npos) {            // parse the user provided right hand side
    int64_t ell_char_count = 0;
    for (auto &c : eqn.substr(pos+2)) {
      if (c == '.') {                        // '.' as part of ellipsis
        ell_char_count++;
        TORCH_CHECK(ell_char_count <= 3, ""can only have '.' in one ellispis '...' in right hand side of the equation"");
        if (ell_char_count == 3) {           // ellipsis complete
          TORCH_CHECK(num_ell_idxes >= 0, ""ellipsis '...' may only appear in right hand side if it does in left hand side"");
          for (int64_t i = 0; i < num_ell_idxes; ++i) {
            idxes_to_preprocessed_dims[first_ell_idx + i] = num_output_dims;
            num_output_dims++;
          }
        }
      } else if (! isspace(c)) {                              // letter (hopefully)
        TORCH_CHECK((ell_char_count == 0) || (ell_char_count == 3), ""'.' must only occur in ellipsis in the right hand side"");
        TORCH_CHECK(('a' <= c) && (c <= 'z'), ""only lowercase letters a-z allowed as indices"");
        int64_t letter_num = c-'a';
        TORCH_CHECK(idxes_to_preprocessed_dims[letter_mapping[letter_num]] == -1, ""index "", c, "" occurs twice in output"");
        idxes_to_preprocessed_dims[letter_mapping[letter_num]] = num_output_dims;
        num_output_dims++;
}
}
  } else { // create an inferred right hand side
    // the ellipsis (if in the lhs) comes first
    if (num_ell_idxes >= 0) {
      for (int64_t i = 0; i < num_ell_idxes; ++i) {
        idxes_to_preprocessed_dims[first_ell_idx + i] = num_output_dims;
        num_output_dims++;
}
}
    // then the indices that occur exactly once in alphabetic order
    for (size_t idx = 0; idx < number_of_letters; idx++) {
      if (num_letter_occurrences[idx] == 1) {
        idxes_to_preprocessed_dims[letter_mapping[idx]] = num_output_dims;
        num_output_dims++;
}
}
}
  // now we assign the idxes_to_preprocessed_dims (index -> dimension in preprocessed / output tensors)
  // for the non-output indices - those that are eventually summed over
  int64_t position = num_output_dims;
  for (int64_t i = 0; i < num_total_idxes; i++) {
    if (idxes_to_preprocessed_dims[i]==-1) {
      idxes_to_preprocessed_dims[i] = position;
      position++;
}
}
  // we now ""homogenize the dimensions"", i.e.
  // - take diagonals for duplicated indices
  // - permute the dimensions to match the order given by idxes_to_preprocessed_dims
  // - unsqueeze to create all dimensions for each index in each tensor where they are missing
  // we also check that sizes match
  // after this, all operands will have compatible shapes (i.e. all dimensions are aligned are broadcastable)
  std::vector<Tensor> preprocessed_operands;
  std::vector<std::int64_t> size_of_dims(num_total_idxes, -1); // keep track of sizes for each index, -1 means we have not seen a size yet
  for (int64_t op = 0; op < (int64_t) tensors.size(); op++) {
    auto preprocessed_op = tensors[op];
    std::vector<int64_t> idx_to_dim(num_total_idxes, -1); // the dimension which the index refers to in the original tensor, -1 means it does not appear
    std::vector<int64_t>& current_op_input_idxes = input_op_idxes[op];
    int64_t dim = 0; // there are two dimension indices: dim is after taking diagonals, i is in input
    for (size_t i = 0; i < current_op_input_idxes.size(); i++) {
      auto idx = current_op_input_idxes[i];
      auto dim_out = idxes_to_preprocessed_dims[idx];
      if (idx_to_dim[dim_out] == -1) { // first appearance
        idx_to_dim[dim_out] = dim;
        if (size_of_dims[idx] == -1) { // keep track of sizes
          size_of_dims[idx] = preprocessed_op.size(dim);
        }
        else {
          TORCH_CHECK(size_of_dims[idx] == preprocessed_op.size(dim), ""size of dimension does not match previous size, operand "", op, "", dim "", i);
}
        dim++;
      } else { // duplicate dimension in tensor --> take diagonal of idx_to_dim[dim_out] and dim and put the diagonal dimension to idx_to_dim[dim_out]
        TORCH_CHECK(size_of_dims[idx] == preprocessed_op.size(dim), ""size of dimension does not match previous size, operand "", op, "", dim "", i);
        preprocessed_op = preprocessed_op.diagonal(0, idx_to_dim[dim_out], dim);
        // diagonal moves the diagonal dimension to the back
        // now we permute the last dim back to idx_to_dim[dim_out]
        std::vector<int64_t> perm(preprocessed_op.dim(), 0);
        for (int64_t d = 0; d < preprocessed_op.dim(); d++) {
          if (d == idx_to_dim[dim_out]) {
            perm[d] = preprocessed_op.dim() - 1;
          } else {
            perm[d] = d - (d > idx_to_dim[dim_out]);
          }
}
        preprocessed_op = preprocessed_op.permute(perm);
}
}
    // now we permute the dimensions in the right order
    std::vector<int64_t> permutation; // permutation for this tensor
    for (auto &d : idx_to_dim) {
      if (d > -1) {
        permutation.push_back(d);
}
}
    preprocessed_op = preprocessed_op.permute(permutation);
    // finally, we insert dimensions for idxes not in the operand
    for (size_t dim = 0; dim < idx_to_dim.size(); dim++) {
      if (idx_to_dim[dim] == -1) {
        preprocessed_op = preprocessed_op.unsqueeze(dim);
}
}
    preprocessed_operands.push_back(std::move(preprocessed_op));
}
  // now we reduce the indices from left to right
  // numpy allows to optimize the path using various
  // algorithms (see eigen_path in numpy docs)
  // we start with the leftmost operator and reduce indices that
  // appear only there
  Tensor result = std::move(preprocessed_operands[0]);
  for (int64_t idx = 0; idx < num_total_idxes; idx++) {
    if ((last_idx_occurrence[idx] == 0)
        && (idxes_to_preprocessed_dims[idx]>=num_output_dims)) {
      result = result.sum(idxes_to_preprocessed_dims[idx], true);
}
}
  // now we process each tensor using sumproduct_pair
  for (int64_t i = 1; i < (int64_t) preprocessed_operands.size(); i++) {
std::vector<int64_t> sum_dims;
    for (int64_t idx = 0; idx < num_total_idxes; idx++) {
      if ((last_idx_occurrence[idx] == i)
          && (idxes_to_preprocessed_dims[idx]>=num_output_dims)) {
        sum_dims.push_back(idxes_to_preprocessed_dims[idx]);
}
}
    result = at::native::sumproduct_pair(result, std::move(preprocessed_operands[i]), sum_dims, true);
  }
  // finally, we squeeze out all non-result dimensions
  auto sizes = result.sizes().vec();
  for (int64_t dim = num_total_idxes-1; dim >= num_output_dims; dim--) {
    sizes.erase(sizes.begin() + dim);
}
  result = result.view(sizes);
return result;
}
","return result;
}
// There are roughly three parts to compute einsum:
// 1. Parse equation to extract the labels for each input operand and output
// 2. Unsqueeze missing dimensions from input operands and permute to align them
// 3. Compute result by multiplying input operands and summing contraction
//    dimensions We do the last part by reducing to bmm.
Tensor einsum(std::string equation, TensorList operands) {
  TORCH_CHECK(!operands.empty(), ""einsum() must provide at least one operand"");
  checkDeviceType(""einsum()"", operands, operands[0].device().type());

  // Code used to identify ELLIPSIS (""..."")
  constexpr int ELLIPSIS = '.';

  // Find arrow (->) to split equation into lhs and rhs
  const auto arrow_pos = equation.find(""->"");
  const auto lhs = equation.substr(0, arrow_pos);

  const auto num_ops = operands.size();

  // Convert labels for input operands into an index in [0, 25] and store
  // them in op_labels for each operand along with ELLIPSIS if present.
  std::vector<std::vector<int>> op_labels(num_ops);
  bool found_ell = false;
  std::size_t curr_op = 0;
  for (auto i = decltype(lhs.length()){0}; i < lhs.length(); ++i) {
    switch (lhs[i]) {
      case ' ':
        // Ignore spaces
        break;

      case '.':
        TORCH_CHECK(
            // Only one ellipsis per operand can be given
            !found_ell,
            ""einsum() found \'.\' for operand "",
            curr_op,
            "" for which an ellipsis was already found"");
        TORCH_CHECK(
            // Ensure it's a valid ellipsis
            i + 2 < lhs.length() && lhs[++i] == '.' && lhs[++i] == '.',
            ""einsum() found \'.\' for operand "",
            curr_op,
            "" that is not part of any ellipsis"");
        op_labels[curr_op].push_back(ELLIPSIS);
        found_ell = true;
        break;

      case ',':
        // Move onto next operand
        ++curr_op;
        TORCH_CHECK(
            curr_op < num_ops,
            ""einsum() fewer operands were provided than specified in the equation"");
        found_ell = false;
        break;

      default:
        // Parse label
        TORCH_CHECK(
            lhs[i] >= 'a' && lhs[i] <= 'z',
            ""einsum() operand subscript must be in range [a, z] but found "",
            lhs[i],
            "" for operand "",
            curr_op);
        // Convert label to index in [0, 25] and store
        op_labels[curr_op].push_back(lhs[i] - 'a');
}
}

  TORCH_CHECK(
      curr_op == num_ops - 1,
      ""einsum() more operands were provided than specified in the equation"");

  // Labels must be within [a, z].
  constexpr int TOTAL_LABELS = 'z' - 'a' + 1;
  std::vector<int> label_count(TOTAL_LABELS, 0);

  // The maximum number of dimensions covered by any ellipsis, needed when
  // unsqueezing missing dimensions from operands to permute and broadcast
  int64_t ell_num_dim = 0;

  // Compute label frequency and number of dimensions covered by ellipsis
  // We do this after parsing labels to make it more readable and simpler
  // to compute the number of dimensions covered by ellipsis.
  for (auto i = decltype(num_ops){0}; i < num_ops; ++i) {
    const auto operand = operands[i];
    const auto labels = op_labels[i];
    const int64_t ndims = operand.dim();
    int64_t nlabels = labels.size();
    bool has_ellipsis = false;

    for (const auto& label : labels) {
      if (label == ELLIPSIS) {
        --nlabels;
        has_ellipsis = true;
        ell_num_dim = std::max(ell_num_dim, ndims - nlabels);
      } else {
        ++label_count[label];
}
}

    TORCH_CHECK(
        has_ellipsis ? nlabels <= ndims : nlabels == ndims,
        ""einsum() the number of subscripts in the equation ("",
        nlabels,
        has_ellipsis ? "") is more than the number of dimensions (""
                     : "") does not match the number of dimensions ("",
        ndims,
        "") for operand "",
        i,
        has_ellipsis ? """" : "" and no ellipsis was given"");
  }

  // We want to align the dimensions of every input tensor to have
  // shape out_dims + sum_dims. For this, we create a mapping of label
  // to index into the permuted shape.
  std::vector<int64_t> label_perm_index(TOTAL_LABELS, -1);

  // Current index in the permuted shape
  int64_t perm_index = 0;

  // Start index of ellipsis dimensions in the permuted shape
  int64_t ell_index = 0;
  found_ell = false;

  if (arrow_pos == std::string::npos) {
    // Implicit output is ellipsis (...) + labels seen only once
    perm_index = ell_num_dim;
    found_ell = true;
    for (int label = 0; label < TOTAL_LABELS; ++label) {
      if (label_count[label] == 1) {
        label_perm_index[label] = perm_index++;
}
}
  } else {
    // Parse explicit output
    const auto rhs = equation.substr(arrow_pos + 2);
    for (auto i = decltype(rhs.length()){0}; i < rhs.length(); ++i) {
      switch (rhs[i]) {
        case ' ':
          // Ignore spaces
          break;

        case '.':
          TORCH_CHECK(
              // There can only be one ellipsis in the output
              !found_ell,
              ""einsum() found \'.\' for output but an ellipsis (...) was already found"");
          TORCH_CHECK(
              // Ensure ellipsis is correct
              i + 2 < rhs.length() && rhs[++i] == '.' && rhs[++i] == '.',
              ""einsum() found \'.\' for output that is not part of any ellipsis (...)"");
          ell_index = perm_index;
          perm_index += ell_num_dim;
          found_ell = true;
          break;

        default:
          TORCH_CHECK(
              // Labels must be in [a, z]
              rhs[i] >= 'a' && rhs[i] <= 'z',
              ""einsum() subscripts must be in range [a, z] but found "",
              rhs[i],
              "" for the output"");
          const auto label = rhs[i] - 'a';
          TORCH_CHECK(
              // Ensure label appeared at least once for some input operand and at
              // most once for the output
              label_count[label] > 0 && label_perm_index[label] == -1,
              ""einsum() output subscript "",
              rhs[i],
              label_perm_index[label] > -1
                  ? "" appears more than once in the output""
                  : "" does not appear in the equation for any input operand"");
          label_perm_index[label] = perm_index++;
}
}
}

  // Save output size before adding contraction dims (dims to sum out)
  const int64_t out_size = perm_index;

  // If ellipsis is not part of the output, add to contraction dimensions
  if (!found_ell) {
    ell_index = perm_index;
    perm_index += ell_num_dim;
  }

  // Add contraction labels (labels not present in output)
  for (int label = 0; label < TOTAL_LABELS; ++label) {
    if (label_count[label] > 0 && label_perm_index[label] == -1) {
      label_perm_index[label] = perm_index++;
}
}
  // Here we unsqueeze missing dimensions to make all operands have the same
  // number of dimensions. We take diagonals for repeated labels within the
  // same operand. Finally we permute the operands to align dimensions as
  // per the perm_out_index we computed above.
  std::vector<Tensor> permuted_operands;
  for (auto i = decltype(num_ops){0}; i < num_ops; ++i) {
    std::vector<int64_t> perm_shape(perm_index, -1);
    std::vector<int64_t> label_dim(TOTAL_LABELS, -1);
    Tensor operand = operands[i];
    const auto labels = op_labels[i];
    const auto original_sizes = operand.sizes();

    std::size_t j = 0;
    for (const auto& label : labels) {
      if (label == ELLIPSIS) {
        // Add missing dimensions covered by the ellipsis
        const int64_t num_missing_dim =
            ell_num_dim - (original_sizes.size() - labels.size() + 1);
        for (int64_t k = 0; k < num_missing_dim; ++k) {
          operand = operand.unsqueeze(j);
}
        for (int64_t k = 0; k < ell_num_dim; ++k) {
          perm_shape[ell_index + k] = j++;
}
      } else if (label_dim[label] != -1) {
        // Repeated label, take diagonal
        const auto dim = label_dim[label];
        TORCH_CHECK(
            operand.size(j) == operand.size(dim),
            ""einsum() subscript "",
            char(label + 'a'),
            "" is repeated for operand "",
            i,
            "" but the sizes don't match, "",
            operand.size(j),
            "" != "",
            operand.size(dim));
        operand = operand.diagonal(0, dim, j).movedim(-1, dim);
      } else {
        // Lookup output index for label
        label_dim[label] = j;
        perm_shape[label_perm_index[label]] = j++;
}
}

    // Add dimensions for missing labels
    for (int64_t& index : perm_shape) {
      if (index == -1) {
        operand = operand.unsqueeze(-1);
        index = j++;
}
}

    permuted_operands.push_back(operand.permute(perm_shape));
  }

  // Check if operands broadcast and keep track of last operand with
  // dimension size != 1 for optimizing reductions
  std::vector<std::size_t> dim_last_op(perm_index, 0);
  bool has_zero_size_dim = false;
  for (int64_t dim = 0; dim < perm_index; ++dim) {
    auto broadcast_size = permuted_operands[0].size(dim);
    for (auto i = decltype(num_ops){1}; i < num_ops; ++i) {
      const auto dim_size = permuted_operands[i].size(dim);
      if (broadcast_size != dim_size && broadcast_size != 1 && dim_size != 1) {
        std::ostringstream msg;
        msg << ""einsum() operands do not broadcast with remapped shapes [original->remapped]:"";
        for (auto j = decltype(num_ops){0}; j < num_ops; ++j) {
          msg << "" "" << operands[j].sizes() << ""->""
              << permuted_operands[j].sizes();
        }
        TORCH_CHECK(false, msg.str());
      }
      if (dim_size != 1) {
        broadcast_size = dim_size;
        dim_last_op[dim] = i;
}
}
    has_zero_size_dim |= broadcast_size == 0;
  }

  // Compute result
  Tensor result = permuted_operands[0];
  // Fast path for when an operand has zero sized dim
  if (has_zero_size_dim) {
    std::vector<int64_t> out_shape(out_size);
    for (int64_t i = 0; i < out_size; ++i) {
      out_shape[i] = permuted_operands[dim_last_op[i]].size(i);
    }
    return at::zeros(out_shape, result.options());
}
  // Sum out or squeeze dimensions that are size 1 for all later operands
  int64_t dim = out_size;
  for (int64_t i = dim; i < perm_index; ++i, ++dim) {
    if (dim_last_op[i] == 0) {
      if (result.size(dim) == 1) {
        result = result.squeeze(dim--);
      } else {
        result = result.sum(dim--);
      }
}
}
  for (auto i = decltype(num_ops){1}; i < num_ops; ++i) {
    Tensor operand = permuted_operands[i];
std::vector<int64_t> sum_dims;

    // Sum out or squeeze dimensions that are size 1 for all later operands
    dim = out_size;
    for (int64_t j = dim; j < perm_index; ++j, ++dim) {
      if (dim_last_op[j] < i) {
        operand = operand.squeeze(dim);
        --dim;
      } else if (dim_last_op[j] == i) {
        if (result.size(dim) == 1) {
          operand = operand.sum(dim);
          result = result.squeeze(dim);
          --dim;
        } else {
          sum_dims.push_back(dim);
        }
}
}

    // Multiply tensors and sum out dimensions in sum_dims
    if (sum_dims.empty()) {
      result = result.mul(operand);
    } else if (sum_dims.size() == result.sizes().size()) {
      result = result.flatten().dot(operand.flatten());
    } else {
      result = sumproduct_pair(result, operand, sum_dims, false);
    }
}
return result;
}
"
362,"OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  if (op.operatorIterator_->def_count == 0) {
    // NB: registerSchema is not idempotent! Only do it once!
    op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
    listeners_->callOnOperatorRegistered(op);
  } else {
    checkSchemaCompatibility(op, schema, debug);
  }
// NB: do not increment the counts until AFTER error checking
","OperatorName op_name = schema.operator_name();
auto op = findOrRegisterName_(op_name);
  TORCH_CHECK(op.operatorIterator_->def_count == 0, ""Tried to register an operator ("", schema, "") with the same name and overload name multiple times."",
                                                    "" Each overload's schema should only be registered with a single call to def()."",
                                                    "" Duplicate registration: "", debug, "". Original registration: "", op.operatorIterator_->op.debug());
  op.operatorIterator_->op.registerSchema(std::move(schema), std::move(debug));
  listeners_->callOnOperatorRegistered(op);
// NB: do not increment the counts until AFTER error checking
+op.operatorIterator_->def_count;
"
363,"running_mean{ running_mean_t, ""running_mean"", 4 },
running_var{ running_var_t, ""running_var"", 5 };
CheckedFrom c = ""miopen_batch_norm"";
  setMIOpenStreamToCurrent();
checkAllDefined(c, {input, weight, bias});
if (!training) {
","running_mean{ running_mean_t, ""running_mean"", 4 },
running_var{ running_var_t, ""running_var"", 5 };
CheckedFrom c = ""miopen_batch_norm"";
checkAllDefined(c, {input, weight, bias});
if (!training) {
"
364,"TensorArg input  { input_t,  ""input"",  1 },
weight { weight_t, ""weight"", 2 },
bias   { bias_t,   ""bias"",   3 };
  setMIOpenStreamToCurrent();
CheckedFrom c = ""miopen_convolution"";
auto output_t = miopen_convolution_forward(
c, input, weight, padding, stride, dilation, groups, benchmark, deterministic);
","TensorArg input  { input_t,  ""input"",  1 },
weight { weight_t, ""weight"", 2 },
bias   { bias_t,   ""bias"",   3 };
CheckedFrom c = ""miopen_convolution"";
auto output_t = miopen_convolution_forward(
c, input, weight, padding, stride, dilation, groups, benchmark, deterministic);
"
365,"TensorArg input  { input_t,  ""input"",  1 },
weight { weight_t, ""weight"", 2 },
bias   { bias_t,   ""bias"",   3 };
  setMIOpenStreamToCurrent();
CheckedFrom c = ""miopen_depthwise_convolution"";
auto output_t = miopen_depthwise_convolution_forward(
c, input, weight, padding, stride, dilation, groups, benchmark, deterministic);
","TensorArg input  { input_t,  ""input"",  1 },
weight { weight_t, ""weight"", 2 },
bias   { bias_t,   ""bias"",   3 };
CheckedFrom c = ""miopen_depthwise_convolution"";
auto output_t = miopen_depthwise_convolution_forward(
c, input, weight, padding, stride, dilation, groups, benchmark, deterministic);
"
366,"{
TensorArg grad_output { grad_output_t,  ""grad_output"", 1 },
weight      { weight_t, ""weight"", 2 };
  setMIOpenStreamToCurrent();
return miopen_convolution_forward(
""miopen_convolution_transpose_backward_input"",
grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
","{
TensorArg grad_output { grad_output_t,  ""grad_output"", 1 },
weight      { weight_t, ""weight"", 2 };
return miopen_convolution_forward(
""miopen_convolution_transpose_backward_input"",
grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
"
367,"std::vector<Frame> stack;
std::unordered_set<Node*> seen;
for (const auto & input : graph_root.next_edges()) {
if (seen.count(input.function.get()) > 0) continue;
stack.emplace_back(input.function.get());
while (!stack.empty()) {
","std::vector<Frame> stack;
std::unordered_set<Node*> seen;
for (const auto & input : graph_root.next_edges()) {
    if (!input.function.get()) continue;
if (seen.count(input.function.get()) > 0) continue;
stack.emplace_back(input.function.get());
while (!stack.empty()) {
"
368,"void UpdateTorchValueByOnnxValueInfo(
Value* v,
const onnx::ValueInfoProto& p_info,
    SymbolDimMap symbol_map) {
if (!p_info.has_type()) {
return;
}
","void UpdateTorchValueByOnnxValueInfo(
Value* v,
const onnx::ValueInfoProto& p_info,
    SymbolDimMap& symbol_map) {
if (!p_info.has_type()) {
return;
}
"
369,"`true` if the key was successfully deleted, and `false` if it was not.
.. warning::
    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore`. Using this API
    with the :class:`~torch.distributed.FileStore` or :class:`~torch.distributed.HashStore` will result in an exception.
Arguments:
key (str): The key to be deleted from the store
","`true` if the key was successfully deleted, and `false` if it was not.
.. warning::
    The ``delete_key`` API is only supported by the :class:`~torch.distributed.TCPStore` and :class:`~torch.distributed.HashStore`. Using this API
    with the :class:`~torch.distributed.FileStore` will result in an exception.
Arguments:
key (str): The key to be deleted from the store
"
370,"// each axis.
std::vector<const Expr*> tmp_params;
for (size_t i = 0; i < new_loop_vars.size(); ++i) {
    tmp_params.push_back(new Add(new_loop_vars[i], starts[i]));
}
// Replace acceses to the producer in the consumer with the cache.
  CacheReplacer replacer(producer, tmp_buf, starts);
Stmt* new_consumer =
IRSimplifier::simplify(consumer->accept_mutator(&replacer));
","// each axis.
std::vector<const Expr*> tmp_params;
for (size_t i = 0; i < new_loop_vars.size(); ++i) {
    tmp_params.push_back(new Add(new_loop_vars[i], info.start[i]));
}
// Replace acceses to the producer in the consumer with the cache.
  CacheReplacer replacer(producer, tmp_buf, info.start);
Stmt* new_consumer =
IRSimplifier::simplify(consumer->accept_mutator(&replacer));
"
371,">>>     work = process_group.allreduce(tensors)
>>>     return work.get_future()
                >>> ddp_model._register_comm_hook(state = None, hook = allreduce)
.. warning ::
``get_future`` API supports only NCCL backend and single-process single-device mode.
The ``torch._C.Future`` object returned by this API can be used in
                ``DistributedDataParallel._register_comm_hook``, but it is subject to some subtle
differences compared to ``torch.futures.Future`` due to compromises made for performance
reasons.
",">>>     work = process_group.allreduce(tensors)
>>>     return work.get_future()
                >>> ddp_model._egister_comm_hook(state = None, hook = allreduce)
.. warning ::
``get_future`` API supports only NCCL backend and single-process single-device mode.
The ``torch._C.Future`` object returned by this API can be used in
                ``DistributedDataParallel.register_comm_hook``, but it is subject to some subtle
differences compared to ``torch.futures.Future`` due to compromises made for performance
reasons.
"
372,"}
void logical_or_kernel(TensorIterator& iter) {
  // We use if-else here specifically for bool instead of using iter.common_dtype() like the CUDA implementation because
  // common_dtype() is unavailable for bfloat16.
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""logical_or_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return a || b;
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.dtype(), ""logical_or_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(a || b);
});
      });
}
}
void logical_xor_kernel(TensorIterator& iter) {
  // We use if-else here specifically for bool instead of using iter.common_dtype() like the CUDA implementation because
  // common_dtype() is unavailable for bfloat16.
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""logical_xor_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return bool(a) != bool(b);
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""logical_xor_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(bool(a) != bool(b));
","}
void logical_or_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""logical_or_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return a || b;
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""logical_or_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(a || b);
});
    });
}
}
void logical_xor_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""logical_xor_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> bool {
return bool(a) != bool(b);
});
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), ""logical_xor_cpu"", [&]() {
cpu_kernel(iter,
[](scalar_t a, scalar_t b) -> scalar_t {
return static_cast<scalar_t>(bool(a) != bool(b));
"
373,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.gt(b);
});
    });
}
}
void ge_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""ge_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a >= b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), ""ge_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.gt(b);
});
      });
}
}
void ge_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""ge_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a >= b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), ""ge_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
374,"op.original_tensor = op.tensor;
op.tensor = op.tensor.to(common_dtype_);
op.current_dtype = common_dtype_;
}
}
}
","op.original_tensor = op.tensor;
op.tensor = op.tensor.to(common_dtype_);
op.current_dtype = common_dtype_;
        op.target_dtype = common_dtype_;
}
}
}
"
375,"[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.ge(b);
});
      });
}
}
void eq_kernel(TensorIterator& iter) {
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), ""eq_cpu"", [&]() {
cpu_kernel(iter,
       [](scalar_t a, scalar_t b) -> bool {
         return a == b;
       });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), ""eq_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
","[](Vec256<scalar_t> a, Vec256<scalar_t> b) -> Vec256<scalar_t> {
return a.ge(b);
});
    });
}
}
void eq_kernel(TensorIterator& iter) {
  // See Note [special-case bool outputs]
if (iter.dtype() == ScalarType::Bool) {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), ""eq_cpu"", [&]() {
cpu_kernel(iter,
        [](scalar_t a, scalar_t b) -> bool {
          return a == b;
        });
});
} else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), ""eq_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t a, scalar_t b) -> scalar_t {
"
376,"std::vector<int64_t> tensor2_expand_size(expand_batch_portion);
tensor2_expand_size.insert(tensor2_expand_size.end(), {r2, c2});
  int expand_batch_product = std::accumulate(expand_batch_portion.begin(), expand_batch_portion.end(), 1, std::multiplies<int64_t>());
std::vector<int64_t> tensor1_view{expand_batch_product, r1, c1};
std::vector<int64_t> tensor2_view{expand_batch_product, r2, c2};
","std::vector<int64_t> tensor2_expand_size(expand_batch_portion);
tensor2_expand_size.insert(tensor2_expand_size.end(), {r2, c2});
  const int64_t expand_batch_product = prod_intlist(expand_batch_portion);
std::vector<int64_t> tensor1_view{expand_batch_product, r1, c1};
std::vector<int64_t> tensor2_view{expand_batch_product, r2, c2};
"
377,"std::vector<int64_t> tensor2_expand_size(expand_batch_portion);
tensor2_expand_size.insert(tensor2_expand_size.end(), {m2, p});
    int expand_batch_product = std::accumulate(expand_batch_portion.begin(), expand_batch_portion.end(),
                                               1, std::multiplies<int64_t>());
std::vector<int64_t> tensor1_bmm_view({expand_batch_product});
tensor1_bmm_view.insert(tensor1_bmm_view.end(), {n, m1});
","std::vector<int64_t> tensor2_expand_size(expand_batch_portion);
tensor2_expand_size.insert(tensor2_expand_size.end(), {m2, p});
    const int64_t expand_batch_product =
        prod_intlist(expand_batch_portion);
std::vector<int64_t> tensor1_bmm_view({expand_batch_product});
tensor1_bmm_view.insert(tensor1_bmm_view.end(), {n, m1});
"
378,"if (!compute_highest_degree_approx) {
constexpr std::array<
Tensor(*)(const Tensor&),
      total_n_degs - 1>
compute_Ts = {
compute_T1, compute_T2, compute_T4<scalar_t>,
compute_T8<scalar_t>, compute_T12<scalar_t>
","if (!compute_highest_degree_approx) {
constexpr std::array<
Tensor(*)(const Tensor&),
      total_n_degs - 1>
compute_Ts = {
compute_T1, compute_T2, compute_T4<scalar_t>,
compute_T8<scalar_t>, compute_T12<scalar_t>
"
379,"const int64_t batches = input_shape[0];
const int64_t num_channels = input_shape[1];
  const int64_t elements_per_batch = std::accumulate(
      input_shape.cbegin() + 1,
      input_shape.cend(),
      1LL,
      std::multiplies<int64_t>());
const int64_t M = batches * num_groups;
const int64_t N = elements_per_batch / num_groups;
","const int64_t batches = input_shape[0];
const int64_t num_channels = input_shape[1];
  const int64_t elements_per_batch =
      prod_intlist(input_shape.cbegin() + 1, input_shape.cend());
const int64_t M = batches * num_groups;
const int64_t N = elements_per_batch / num_groups;
"
380,"#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include <ATen/TypeDefault.h>
#include <torch/library.h>
#include ""torch/csrc/autograd/function.h""
","#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include <torch/library.h>
#include ""torch/csrc/autograd/function.h""
"
381,"#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include ""torch/csrc/autograd/FunctionsManual.h""
#include <ATen/TypeDefault.h>
#include <torch/library.h>
// ${generated_comment}
","#include ""torch/csrc/autograd/VariableTypeUtils.h""
#include ""torch/csrc/autograd/FunctionsManual.h""
#include <torch/library.h>
// ${generated_comment}
"
382,"auto indices_arg = TensorArg(indices, ""indices"", 1);
checkScalarType(""embedding"", indices_arg, kLong);
// TODO: use tensor.index() after improving perf
if (indices.dim() == 1) {
    return weight.index_select(0, indices);
}
auto size = indices.sizes().vec();
for (auto d : weight.sizes().slice(1)) {
size.push_back(d);
}
  return weight.index_select(0, indices.reshape(-1)).view(size);
}
Tensor embedding_backward(
","auto indices_arg = TensorArg(indices, ""indices"", 1);
checkScalarType(""embedding"", indices_arg, kLong);
  auto zerofill_padding = [&](Tensor& embedding) {
    if (padding_idx >= 0) {
      embedding.masked_fill_((indices == padding_idx).reshape({-1, 1}), 0);
    }
  };

// TODO: use tensor.index() after improving perf
if (indices.dim() == 1) {
    auto out = weight.index_select(0, indices);
    zerofill_padding(out);
    return out;
}
auto size = indices.sizes().vec();
for (auto d : weight.sizes().slice(1)) {
size.push_back(d);
}

  auto out = weight.index_select(0, indices.reshape(-1));
  zerofill_padding(out);
  return out.view(size);
}
Tensor embedding_backward(
"
383,"if (diff_view_meta->creation_meta != CreationMeta::MULTI_OUTPUT_SAFE) {
// Do not use handle_view_on_rebase here as check_inplace should have been called before this
// and either throw an error or clear the warning
        TORCH_INTERNAL_ASSERT(diff_view_meta->creation_meta == CreationMeta::DEFAULT);
TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
","if (diff_view_meta->creation_meta != CreationMeta::MULTI_OUTPUT_SAFE) {
// Do not use handle_view_on_rebase here as check_inplace should have been called before this
// and either throw an error or clear the warning
        // Temporary error message as a full fix is too risky for now
        // Should be an internal assert again
        if (diff_view_meta->creation_meta != CreationMeta::DEFAULT) {
          auto grad_fn = diff_view_meta->grad_fn_.get();
          auto grad_fn_name = grad_fn? grad_fn->name() : ""a function created in no_grad mode"";
          TORCH_CHECK(false, ""Output "", diff_view_meta->output_nr_, "" of "", grad_fn_name, "" is a view and ""
                      ""is being modified inplace but this inplace operation is not allowed. You should ""
                      ""either replace it by an out of place operation or do a .clone() of the Tensor ""
                      ""before modifying it inplace. Note that this can happen when using DataParallel or ""
                      ""DistributedDataParallel, which can send views of the original input to the forward ""
                      ""method of the model."");
        }
TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
"
384,"const T* X_data = X.data_ptr<T>();
const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
const T* beta_data = beta.defined() ? beta.data_ptr<T>() : nullptr;
  T* Y_data = Y->data_ptr<T>();
  T* mean_data = mean->data_ptr<T>();
  T* rstd_data = rstd->data_ptr<T>();
const T s = T(1) / static_cast<T>(D * HxW);
const bool gamma_null = (gamma_data == nullptr);
const bool beta_null = beta_data == nullptr;
","const T* X_data = X.data_ptr<T>();
const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
const T* beta_data = beta.defined() ? beta.data_ptr<T>() : nullptr;
  T* Y_data = Y.data_ptr<T>();
  T* mean_data = mean.data_ptr<T>();
  T* rstd_data = rstd.data_ptr<T>();
const T s = T(1) / static_cast<T>(D * HxW);
const bool gamma_null = (gamma_data == nullptr);
const bool beta_null = beta_data == nullptr;
"
385,"Tensor mean = at::empty({N, group}, X.options());
Tensor rstd = at::empty({N, group}, X.options());
GroupNormKernel(
      X.device().type(),
      X,
      gamma,
      beta,
      N,
      C,
      HxW,
      group,
      eps,
      &Y,
      &mean,
      &rstd);
return std::make_tuple(Y, mean, rstd);
}
","Tensor mean = at::empty({N, group}, X.options());
Tensor rstd = at::empty({N, group}, X.options());
GroupNormKernel(
      X.device().type(), X, gamma, beta, N, C, HxW, group, eps, Y, mean, rstd);
return std::make_tuple(Y, mean, rstd);
}
"
386,"time(t), allocation_id(id), size(s), type(e) {}
};
std::vector<MemEvent> create_and_sort_mem_events(
const std::vector<uint64_t>& allocation_sizes,
const std::vector<uint64_t>& allocation_lifetimes) {
","time(t), allocation_id(id), size(s), type(e) {}
};
bool overlaps(const MemBlock& a, const MemBlock& b) {
  // two blocks dont overlap if
  // |---a--------|--------------b--------|
  // strat_a     end_a <= start_b       end_b
  return
    !((a.end_offset <= b.start_offset) || (b.end_offset <= a.start_offset));
}

bool validate_allocation_plan(
    const std::vector<MemEvent>& alloc_events,
    const std::vector<uint64_t>& allocation_offsets) {
  std::set<MemBlock> allocations;
  for (const auto& event : alloc_events) {
    auto alloc_id = event.allocation_id;
    // Skip allocations not managed by AllocationPlan
    if (allocation_offsets[alloc_id] == std::numeric_limits<uint64_t>::max()) {
      continue;
    }
    auto start_offset = allocation_offsets[alloc_id];
    auto end_offset = allocation_offsets[alloc_id] + event.size;
    MemBlock mem_block(start_offset, end_offset);
    if (event.type == EventType::Allocate) {
      auto it = allocations.lower_bound(mem_block);
      if (it != allocations.end()) {
        auto next_block = *it;
        if (overlaps(next_block, mem_block)) {
          return false;
        }
      }
      if (it != allocations.begin()) {
        auto prev_block = *(--it);
        if (overlaps(prev_block, mem_block)) {
          return false;
        }
      }
      allocations.emplace(mem_block);
    } else if (event.type == EventType::Free) {
      auto it = allocations.find(mem_block);
      TORCH_CHECK((*it).end_offset == end_offset,
          ""Enf offset of allocation being freed must match the one recorded."");
      TORCH_CHECK(
          it != allocations.end(),
          ""ProfilingAllocator: Allocate event ""
          ""must have preceded deallocate event."");
      allocations.erase(it);
    } else {
      TORCH_CHECK(false, ""ProfilingAllocator: Invalid event type."");
    }
  }
  return true;
}

std::vector<MemEvent> create_and_sort_mem_events(
const std::vector<uint64_t>& allocation_sizes,
const std::vector<uint64_t>& allocation_lifetimes) {
"
387,"// synchronizes the dispatch table entry for a given dispatch key
// with the current state of kernel registrations in the dispatcher.
// note that this is not a complete update, due to relationships between
// dispatch keys (e.g. runtime keys and their associated autograd keys).
// This function should be considered a private helper for updateDispatchTable_()
void OperatorEntry::updateDispatchTableEntry_(const c10::Dispatcher& dispatcher, DispatchKey dispatch_key) {
auto dispatch_ix = static_cast<uint8_t>(dispatch_key);
","// synchronizes the dispatch table entry for a given dispatch key
// with the current state of kernel registrations in the dispatcher.
// note that this is not a complete update, due to relationships between
// dispatch keys (e.g. runtime keys and their associated autograd keys,
// or alias keys and their associated keysets).
// This function should be considered a private helper for updateDispatchTable_()
void OperatorEntry::updateDispatchTableEntry_(const c10::Dispatcher& dispatcher, DispatchKey dispatch_key) {
auto dispatch_ix = static_cast<uint8_t>(dispatch_key);
"
388,"// For any dispatch key, it'll pick a kernel using the following order:
//  (1) Use kernel if it's directly registered to this key
//  (2) Handle runtime keys that have kernels available from alias keys
  //    (2.1) Use kernel from DispatchKey::Math if available.
//          For autograd keys, we only use kernel from Math when there's no direct registration
  //          to its corresponding backend key.
//          For AutogradOther, we eagerly return ambiguousAutogradOtherKernel_ if there's registration to any of
//          its backends and ask backend extender to request a decicated Autograd key for the backend.
//          See Note [Ambiguity in AutogradOther kernel] for more details.
  //    (2.2) Use kernel from DispatchKey::Autograd if available
  //    (2.3) Special logic to handle catchAll for Autograd keys
//          For autograd backend keys, we use kernel from alias Math key (catchAll will be moved to Math)
//          if there's no direct registration to the backend key.
//          Tensor factory functions used to have no registration to Autograd key but only to catchAll.
//          In the past we directly call into backends(filled with catchAll) after BackendSelect.
//          Now that we first call Autograd backend keys after BackendSelect, we should fill those
//          with catchAll as well.
  //    The implementation of (2.1) & (2.3) relies on the invariant that for a given backend,
//    `computeDispatchTableEntryWithDebug()` will be called for that backend's autograd key after the
//    backend key. See Note [Refresh Runtime Autograd entries in dispatchTable_]
//  (3) Use fallthrough kernel that are registered as fallback.
//  (4) Use catchAll kernel if available
// Alias Key Precedence:
  //   Math > Autograd
// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
  // TODO: we can remove (2.3) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.
// 1. Operator registration
","// For any dispatch key, it'll pick a kernel using the following order:
//  (1) Use kernel if it's directly registered to this key
//  (2) Handle runtime keys that have kernels available from alias keys
  //    (2.1) Use kernel from DispatchKey::DefaultBackend if available.
  //          This is used to register a kernel that works for all backend in inference. But it requires
  //          separate registration for Autograd keys to support training.
  //    (2.2) Use kernel from DispatchKey::Math if available.
//          For autograd keys, we only use kernel from Math when there's no direct registration
  //          to its corresponding backend key or DefaultBackend. See Note [DefaultBackend and Math].
//          For AutogradOther, we eagerly return ambiguousAutogradOtherKernel_ if there's registration to any of
//          its backends and ask backend extender to request a decicated Autograd key for the backend.
//          See Note [Ambiguity in AutogradOther kernel] for more details.
  //          A DefaultBackend kernel prevents Math kernel being used for Autograd keys, but it doesn't
  //          cause confusion for AutogradOther. It's pretty straightforward to use Autograd (if available)
  //          in this case.
  //    (2.3) Use kernel from DispatchKey::Autograd if available
  //    (2.4) Special logic to handle catchAll for Autograd keys
//          For autograd backend keys, we use kernel from alias Math key (catchAll will be moved to Math)
//          if there's no direct registration to the backend key.
//          Tensor factory functions used to have no registration to Autograd key but only to catchAll.
//          In the past we directly call into backends(filled with catchAll) after BackendSelect.
//          Now that we first call Autograd backend keys after BackendSelect, we should fill those
//          with catchAll as well.
  //    The implementation of (2.2) & (2.4) relies on the invariant that for a given backend,
//    `computeDispatchTableEntryWithDebug()` will be called for that backend's autograd key after the
//    backend key. See Note [Refresh Runtime Autograd entries in dispatchTable_]
//  (3) Use fallthrough kernel that are registered as fallback.
//  (4) Use catchAll kernel if available
// Alias Key Precedence:
  //   DefaultBackend > Math > Autograd
  // Note [DefaultBackend and Math]
  //   When there're registrations to both DefaultBackend & Math & Autograd, from (2.2) we know DefaultBackend
  //   and Autograd kernels will be picked up and Math is overriden.
  //   This is fine and in practice DefaultBackend and Math shouldn't co-exist for an op.
// TODO: Update alias key precedence after we add new alias keys AutogradDispatchCPUOrCUDA .
  // TODO: we can remove (2.4) and (4) after TypeDefault registrations are moved from catchAll to Math
//       so that Math can populate to Autograd backend keys before fallback kernels.
// 1. Operator registration
"
389,"}
if (checkForNCCLErrors(ncclComms)) {
          LOG(INFO) << ""Received NCCL errors for communicators in the cache"";
if (blockingWait_ || asyncErrorHandling_) {
            LOG(INFO) << ""Aborting communicators that received errors"";
// We abort NCCL communicators that have received errors from this
// thread, and exceptions are set on the corresponding work objects.
// The workCleanupThread will then loop through the unfinished
","}
if (checkForNCCLErrors(ncclComms)) {
          LOG(INFO) << ""[Rank "" << rank_
                    << ""] Received NCCL errors for communicators in the cache"";
if (blockingWait_ || asyncErrorHandling_) {
            LOG(INFO) << ""[Rank "" << rank_
                      << ""] Aborting communicators that received errors"";
// We abort NCCL communicators that have received errors from this
// thread, and exceptions are set on the corresponding work objects.
// The workCleanupThread will then loop through the unfinished
"
390,"int srcRank,
int /* unused */) {
check_gpu_tensors(tensors);
  auto ret= pointToPoint(
tensors,
[&](at::Tensor& output,
ncclComm_t comm,
","int srcRank,
int /* unused */) {
check_gpu_tensors(tensors);
  auto ret = pointToPoint(
tensors,
[&](at::Tensor& output,
ncclComm_t comm,
"
391,"const auto& e_a1 = *it_a1;
const auto& e_a2 = *it_a2;
      if (!ivaluesEqual(e_a1.key(), e_a2.key()) &&
!ivaluesEqual(e_a1.value(), e_a2.value())) {
return false;
}
","const auto& e_a1 = *it_a1;
const auto& e_a2 = *it_a2;
      if (!ivaluesEqual(e_a1.key(), e_a2.key()) ||
!ivaluesEqual(e_a1.value(), e_a2.value())) {
return false;
}
"
392,"ncclUniqueId ncclID;
// For point-to-point communication, lower rank of the two will get unique id.
  if (rank_ == 0 || (isP2POp(opType) && p2pRank == 0)) {
C10D_NCCL_CHECK(ncclGetUniqueId(&ncclID));
}
","ncclUniqueId ncclID;
// For point-to-point communication, lower rank of the two will get unique id.
  if (rank_ == 0 || (commType != NCCLCommType::COLL && p2pRank == 0)) {
C10D_NCCL_CHECK(ncclGetUniqueId(&ncclID));
}
"
393,"outputTensors[i][j].copy_(outputFlattened[i][j], true);
}
}
      },
      OpType::ALLGATHER);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::allgather_coalesced(
","outputTensors[i][j].copy_(outputFlattened[i][j], true);
}
}
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::allgather_coalesced(
"
394,"getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        },
        OpType::ALLTOALL_BASE);
} else {
c10d::checkSplitSizes(inputSplitSizes, inputTensor, size_);
c10d::checkSplitSizes(outputSplitSizes, outputTensor, size_);
","getNcclDataType(input.scalar_type()),
comm,
stream.stream());
        });
} else {
c10d::checkSplitSizes(inputSplitSizes, inputTensor, size_);
c10d::checkSplitSizes(outputSplitSizes, outputTensor, size_);
"
395,"ncclUniqueId ncclID;
// For point-to-point communication, lower rank of the two will get unique id.
  if (rank_ == 0 || (commType != NCCLCommType::COLL && p2pRank == 0)) {
C10D_NCCL_CHECK(ncclGetUniqueId(&ncclID));
}
","ncclUniqueId ncclID;
// For point-to-point communication, lower rank of the two will get unique id.
  if (rank_ == 0 || (isP2POp(opType) && p2pRank == 0)) {
C10D_NCCL_CHECK(ncclGetUniqueId(&ncclID));
}
"
396,"at::cuda::CUDAStream& ncclStream = ncclStreams_[key][i];
(*work->cudaEvents_)[i].record(ncclStream);
work->ncclComms_[i] = ncclComms[i];
    work->blockingWait_ = blockingWait_;
    work->opTimeout_ = opTimeout_;
    work->store_ = store_;
}
if (asyncErrorHandling_) {
workEnqueue(work);
}
","at::cuda::CUDAStream& ncclStream = ncclStreams_[key][i];
(*work->cudaEvents_)[i].record(ncclStream);
work->ncclComms_[i] = ncclComms[i];
}
  // Set appropriate work parameters.
  work->blockingWait_ = blockingWait_;
  work->opTimeout_ = opTimeout_;
  work->store_ = store_;

if (asyncErrorHandling_) {
workEnqueue(work);
}
"
397,"root,
comm,
stream.stream());
      });
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
","root,
comm,
stream.stream());
      },
      OpType::BROADCAST);
}
std::shared_ptr<ProcessGroup::Work> ProcessGroupNCCL::reduce(
"
398,"//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, modernize-avoid-c-arrays)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  (PyCFunction)THPModule_initExtension,   METH_O,       nullptr},
  {""_autograd_init"",  (PyCFunction)THPAutograd_initExtension, METH_NOARGS,  nullptr},
{""_add_docstr"",     (PyCFunction)THPModule_addDocStr,       METH_VARARGS, nullptr},
{""_init_names"",     (PyCFunction)THPModule_initNames,       METH_O,       nullptr},
  {""_has_distributed"",(PyCFunction)THPModule_hasDistributed,  METH_NOARGS,  nullptr},
{""_set_default_tensor_type"", (PyCFunction)THPModule_setDefaultTensorType, METH_O, nullptr},
{""_set_default_dtype"", (PyCFunction)THPModule_setDefaultDtype, METH_O, nullptr},
{""_infer_size"",     (PyCFunction)THPModule_inferSize,         METH_VARARGS, nullptr},
{""_crash_if_csrc_asan"", (PyCFunction)THPModule_crashIfCsrcASAN, METH_O, nullptr},
{""_crash_if_csrc_ubsan"", (PyCFunction)THPModule_crashIfCsrcUBSAN, METH_O, nullptr},
{""_crash_if_aten_asan"", (PyCFunction)THPModule_crashIfATenASAN, METH_O, nullptr},
  {""_show_config"",    (PyCFunction)THPModule_showConfig, METH_NOARGS, nullptr},
  {""_parallel_info"",    (PyCFunction)THPModule_parallelInfo, METH_NOARGS, nullptr},
{""_set_backcompat_broadcast_warn"", (PyCFunction)THPModule_setBackcompatBroadcastWarn, METH_O, nullptr},
  {""_get_backcompat_broadcast_warn"", (PyCFunction)THPModule_getBackcompatBroadcastWarn, METH_NOARGS, nullptr},
{""_set_backcompat_keepdim_warn"", (PyCFunction)THPModule_setBackcompatKeepdimWarn, METH_O, nullptr},
  {""_get_backcompat_keepdim_warn"", (PyCFunction)THPModule_getBackcompatKeepdimWarn, METH_NOARGS, nullptr},
  {""get_num_threads"", (PyCFunction)THPModule_getNumThreads,     METH_NOARGS,  nullptr},
{""set_num_threads"", (PyCFunction)THPModule_setNumThreads,     METH_O,       nullptr},
  {""get_num_interop_threads"", (PyCFunction)THPModule_getNumInteropThreads,     METH_NOARGS,  nullptr},
{""set_num_interop_threads"", (PyCFunction)THPModule_setNumInteropThreads,     METH_O,       nullptr},
  {""_get_cudnn_enabled"", (PyCFunction)THPModule_userEnabledCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_enabled"", (PyCFunction)THPModule_setUserEnabledCuDNN, METH_O,  nullptr},
  {""_get_mkldnn_enabled"", (PyCFunction)THPModule_userEnabledMkldnn, METH_NOARGS,     nullptr},
{""_set_mkldnn_enabled"", (PyCFunction)THPModule_setUserEnabledMkldnn, METH_O,  nullptr},
  {""_get_cudnn_allow_tf32"", (PyCFunction)THPModule_allowTF32CuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_allow_tf32"", (PyCFunction)THPModule_setAllowTF32CuDNN, METH_O,  nullptr},
  {""_get_cudnn_benchmark"", (PyCFunction)THPModule_benchmarkCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_benchmark"", (PyCFunction)THPModule_setBenchmarkCuDNN, METH_O,  nullptr},
  {""_get_cudnn_deterministic"", (PyCFunction)THPModule_deterministicCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_deterministic"", (PyCFunction)THPModule_setDeterministicCuDNN, METH_O,  nullptr},
  {""_get_deterministic"", (PyCFunction)THPModule_deterministic, METH_NOARGS,     nullptr},
{""_set_deterministic"", (PyCFunction)THPModule_setDeterministic, METH_O,  nullptr},
  {""_get_cublas_allow_tf32"", (PyCFunction)THPModule_allowTF32CuBLAS, METH_NOARGS,     nullptr},
{""_set_cublas_allow_tf32"", (PyCFunction)THPModule_setAllowTF32CuBLAS, METH_O,  nullptr},
  {""_vmapmode_increment_nesting"", (PyCFunction)THPModule_vmapmode_increment_nesting, METH_NOARGS, nullptr},
  {""_vmapmode_decrement_nesting"", (PyCFunction)THPModule_vmapmode_decrement_nesting, METH_NOARGS, nullptr},
{""_to_dlpack"",      (PyCFunction)THPModule_toDLPack,          METH_O,       nullptr},
{""_from_dlpack"",    (PyCFunction)THPModule_fromDLPack,        METH_O,       nullptr},
{""set_flush_denormal"", (PyCFunction)THPModule_setFlushDenormal, METH_O,     nullptr},
  {""get_default_dtype"", (PyCFunction)THPModule_getDefaultDtype, METH_NOARGS,  nullptr},
  {""_get_default_device"", (PyCFunction)THPModule_getDefaultDevice, METH_NOARGS,   nullptr},
  {""_get_qengine"", (PyCFunction)THPModule_qEngine, METH_NOARGS, nullptr},
{""_set_qengine"", (PyCFunction)THPModule_setQEngine, METH_O, nullptr},
  {""_supported_qengines"", (PyCFunction)THPModule_supportedQEngines, METH_NOARGS, nullptr},
  {""_is_xnnpack_enabled"", (PyCFunction)THPModule_isEnabledXNNPACK, METH_NOARGS, nullptr},
  {""_is_torch_function_enabled"", (PyCFunction)THPModule_isEnabledTorchFunction, METH_NOARGS, nullptr},
{""_disabled_torch_function_impl"", (PyCFunction)THPModule_disable_torch_function, METH_VARARGS, nullptr},
{nullptr, nullptr, 0, nullptr}
};
","//NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays, modernize-avoid-c-arrays)
static PyMethodDef TorchMethods[] = {
{""_initExtension"",  (PyCFunction)THPModule_initExtension,   METH_O,       nullptr},
  {""_autograd_init"",  THPAutograd_initExtension, METH_NOARGS,  nullptr},
{""_add_docstr"",     (PyCFunction)THPModule_addDocStr,       METH_VARARGS, nullptr},
{""_init_names"",     (PyCFunction)THPModule_initNames,       METH_O,       nullptr},
  {""_has_distributed"",THPModule_hasDistributed,  METH_NOARGS,  nullptr},
{""_set_default_tensor_type"", (PyCFunction)THPModule_setDefaultTensorType, METH_O, nullptr},
{""_set_default_dtype"", (PyCFunction)THPModule_setDefaultDtype, METH_O, nullptr},
{""_infer_size"",     (PyCFunction)THPModule_inferSize,         METH_VARARGS, nullptr},
{""_crash_if_csrc_asan"", (PyCFunction)THPModule_crashIfCsrcASAN, METH_O, nullptr},
{""_crash_if_csrc_ubsan"", (PyCFunction)THPModule_crashIfCsrcUBSAN, METH_O, nullptr},
{""_crash_if_aten_asan"", (PyCFunction)THPModule_crashIfATenASAN, METH_O, nullptr},
  {""_show_config"",    THPModule_showConfig, METH_NOARGS, nullptr},
  {""_parallel_info"",    THPModule_parallelInfo, METH_NOARGS, nullptr},
{""_set_backcompat_broadcast_warn"", (PyCFunction)THPModule_setBackcompatBroadcastWarn, METH_O, nullptr},
  {""_get_backcompat_broadcast_warn"", THPModule_getBackcompatBroadcastWarn, METH_NOARGS, nullptr},
{""_set_backcompat_keepdim_warn"", (PyCFunction)THPModule_setBackcompatKeepdimWarn, METH_O, nullptr},
  {""_get_backcompat_keepdim_warn"", THPModule_getBackcompatKeepdimWarn, METH_NOARGS, nullptr},
  {""get_num_threads"", THPModule_getNumThreads,     METH_NOARGS,  nullptr},
{""set_num_threads"", (PyCFunction)THPModule_setNumThreads,     METH_O,       nullptr},
  {""get_num_interop_threads"", THPModule_getNumInteropThreads,     METH_NOARGS,  nullptr},
{""set_num_interop_threads"", (PyCFunction)THPModule_setNumInteropThreads,     METH_O,       nullptr},
  {""_get_cudnn_enabled"", THPModule_userEnabledCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_enabled"", (PyCFunction)THPModule_setUserEnabledCuDNN, METH_O,  nullptr},
  {""_get_mkldnn_enabled"", THPModule_userEnabledMkldnn, METH_NOARGS,     nullptr},
{""_set_mkldnn_enabled"", (PyCFunction)THPModule_setUserEnabledMkldnn, METH_O,  nullptr},
  {""_get_cudnn_allow_tf32"", THPModule_allowTF32CuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_allow_tf32"", (PyCFunction)THPModule_setAllowTF32CuDNN, METH_O,  nullptr},
  {""_get_cudnn_benchmark"", THPModule_benchmarkCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_benchmark"", (PyCFunction)THPModule_setBenchmarkCuDNN, METH_O,  nullptr},
  {""_get_cudnn_deterministic"", THPModule_deterministicCuDNN, METH_NOARGS,     nullptr},
{""_set_cudnn_deterministic"", (PyCFunction)THPModule_setDeterministicCuDNN, METH_O,  nullptr},
  {""_get_deterministic"", THPModule_deterministic, METH_NOARGS,     nullptr},
{""_set_deterministic"", (PyCFunction)THPModule_setDeterministic, METH_O,  nullptr},
  {""_get_cublas_allow_tf32"", THPModule_allowTF32CuBLAS, METH_NOARGS,     nullptr},
{""_set_cublas_allow_tf32"", (PyCFunction)THPModule_setAllowTF32CuBLAS, METH_O,  nullptr},
  {""_vmapmode_increment_nesting"", THPModule_vmapmode_increment_nesting, METH_NOARGS, nullptr},
  {""_vmapmode_decrement_nesting"", THPModule_vmapmode_decrement_nesting, METH_NOARGS, nullptr},
{""_to_dlpack"",      (PyCFunction)THPModule_toDLPack,          METH_O,       nullptr},
{""_from_dlpack"",    (PyCFunction)THPModule_fromDLPack,        METH_O,       nullptr},
{""set_flush_denormal"", (PyCFunction)THPModule_setFlushDenormal, METH_O,     nullptr},
  {""get_default_dtype"", THPModule_getDefaultDtype, METH_NOARGS,  nullptr},
  {""_get_default_device"", THPModule_getDefaultDevice, METH_NOARGS,   nullptr},
  {""_get_qengine"", THPModule_qEngine, METH_NOARGS, nullptr},
{""_set_qengine"", (PyCFunction)THPModule_setQEngine, METH_O, nullptr},
  {""_supported_qengines"", THPModule_supportedQEngines, METH_NOARGS, nullptr},
  {""_is_xnnpack_enabled"", THPModule_isEnabledXNNPACK, METH_NOARGS, nullptr},
  {""_is_torch_function_enabled"", THPModule_isEnabledTorchFunction, METH_NOARGS, nullptr},
{""_disabled_torch_function_impl"", (PyCFunction)THPModule_disable_torch_function, METH_VARARGS, nullptr},
{nullptr, nullptr, 0, nullptr}
};
"
399,"static PyMethodDef methods[] = { // NOLINT
{""_dist_autograd_init"",
     (PyCFunction)dist_autograd_init,
METH_NOARGS,
nullptr},
{nullptr, nullptr, 0, nullptr}};
","static PyMethodDef methods[] = { // NOLINT
{""_dist_autograd_init"",
     dist_autograd_init,
METH_NOARGS,
nullptr},
{nullptr, nullptr, 0, nullptr}};
"
400,"} // namespace
static PyMethodDef methods[] = { // NOLINT
    {""_rpc_init"", (PyCFunction)rpc_init, METH_NOARGS, nullptr},
{nullptr, nullptr, 0, nullptr}};
PyMethodDef* python_functions() {
","} // namespace
static PyMethodDef methods[] = { // NOLINT
    {""_rpc_init"", rpc_init, METH_NOARGS, nullptr},
{nullptr, nullptr, 0, nullptr}};
PyMethodDef* python_functions() {
"
401,"}
Tensor& smooth_l1_loss_out(Tensor& result, const Tensor& input, const Tensor& target, int64_t reduction, double beta) {
  if (beta <= 0)
return at::native::l1_loss_out(result, input, target, reduction);
if (reduction != Reduction::None) {
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
","}
Tensor& smooth_l1_loss_out(Tensor& result, const Tensor& input, const Tensor& target, int64_t reduction, double beta) {
  TORCH_CHECK(beta >= 0, ""smooth_l1_loss does not support negative values for beta."")
  if (beta == 0) {
return at::native::l1_loss_out(result, input, target, reduction);
  }
if (reduction != Reduction::None) {
Tensor loss;
auto iter = TensorIterator::binary_op(loss, input, target);
"
402,"}
}
}
}
void ProcessGroupNCCL::ncclCommWatchdog() {
try {
ncclCommWatchdogInternal();
    LOG(INFO) << ""NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
LOG(INFO) << ""NCCL watchdog thread terminated with exception: "" << e.what();
} catch (...) {
","}
}
}

  if (asyncErrorHandling_) {
    workMetaListCV_.notify_one();
    workCleanupThread_.join();
  }
}
void ProcessGroupNCCL::ncclCommWatchdog() {
try {
ncclCommWatchdogInternal();
    LOG(INFO) << ""[Rank "" << rank_ << ""] NCCL watchdog thread terminated normally"";
} catch (std::exception& e) {
LOG(INFO) << ""NCCL watchdog thread terminated with exception: "" << e.what();
} catch (...) {
"
403,"const std::string kSocketIfnameEnvVar = ""TP_SOCKET_IFNAME"";
const std::string kDefaultUvAddress = ""127.0.0.1"";
constexpr long kToMilliseconds = 1000;

const std::string kGilAverageWaitTime = ""agent.gil_average_wait_time_us"";
const std::string kThreadPoolSize = ""agent.thread_pool_size"";
const std::string kNumIdleThreads = ""agent.num_idle_threads"";
const std::string kClientActiveCalls = ""agent.client_active_calls"";
const std::string kServerActiveCalls = ""agent.server_active_calls"";
const std::string kServerActiveAsyncCalls = ""agent.server_active_async_calls"";
const std::string kRpcTimeoutErrorStr =
    ""RPC ran for more than set timeout ({} ms) and will now be marked with an error"";
inline void checkCPUTensor(const torch::Tensor& tensor) {
TORCH_CHECK(
","const std::string kSocketIfnameEnvVar = ""TP_SOCKET_IFNAME"";
const std::string kDefaultUvAddress = ""127.0.0.1"";
const std::string kGilAverageWaitTime = ""agent.gil_average_wait_time_us"";
const std::string kThreadPoolSize = ""agent.thread_pool_size"";
const std::string kNumIdleThreads = ""agent.num_idle_threads"";
const std::string kClientActiveCalls = ""agent.client_active_calls"";
const std::string kServerActiveCalls = ""agent.server_active_calls"";
const std::string kServerActiveAsyncCalls = ""agent.server_active_async_calls"";
inline void checkCPUTensor(const torch::Tensor& tensor) {
TORCH_CHECK(
"
404,"const auto self = SimpleSelf(classType);
cu->define(classname, props, propRcbs, methodDefs, methodRcbs, &self);
});
m.def(
""_jit_script_interface_compile"",
","const auto self = SimpleSelf(classType);
cu->define(classname, props, propRcbs, methodDefs, methodRcbs, &self);

        // Stitch in default arguments for methods. Properties don't need to be
        // considered since there is no way to invoke setters without passing in
        // a value.
        auto defs_it = methodDefs.begin();
        while (defs_it != methodDefs.end()) {
          auto def_name = (*defs_it).name().name();
          // If the method is not in the defaults map, assume there are
          // no default arguments for it.
          auto default_it = defaults.find(def_name);
          if (default_it == defaults.end()) {
            continue;
          }

          const auto method_name =
              QualifiedName(classname, (*defs_it).name().name());
          auto& method = cu->get_function(method_name);
          method.setSchema(getSchemaWithNameAndDefaults(
              defs_it->range(),
              method.getSchema(),
              at::nullopt,
              default_it->second));
          ++defs_it;
        }
});
m.def(
""_jit_script_interface_compile"",
"
405,"os() << *v->base_handle() << ""["" << *v->flat_index() << ""] = "";
}
os() << *v->value() << "";"";
}
void CudaPrinter::visit(const AtomicAdd* v) {
","os() << *v->base_handle() << ""["" << *v->flat_index() << ""] = "";
}
os() << *v->value() << "";"";
  os() << std::endl;
}
void CudaPrinter::visit(const AtomicAdd* v) {
"
406,"IRSimplifier::simplify(new Max(old_reach, v->stop(), true));
}
    // If a thread dimension has changed, insert a syncThreads in the enclosing
    // Block.
    if (last_thread_dim_ && !exprEquals(last_thread_dim_, v->stop())) {
      need_sync_ = true;
    }
    last_thread_dim_ = v->stop();

const Var* metaVar = gpu_thread_vars_[gpu_thread_index];
body = Substitute(Stmt::clone(body), {{v->var(), metaVar}});
}
","IRSimplifier::simplify(new Max(old_reach, v->stop(), true));
}
const Var* metaVar = gpu_thread_vars_[gpu_thread_index];
body = Substitute(Stmt::clone(body), {{v->var(), metaVar}});
}
"
407,"return new Block(stmts);
}
Stmt* TermExpander::mutate(const Block* v) {
Stmt* new_stmt = IRSimplifierBase::mutate(v);
Block* new_block = dynamic_cast<Block*>(new_stmt);
","return new Block(stmts);
}
Stmt* TermExpander::fuseSyncThreads(Block* block) {
  // only really first if highest level Block.
  bool first = block->get_parent() == nullptr;
  SyncThreads* last = nullptr;
  std::vector<Stmt*> stmts;
  bool did_anything = false;

  for (auto* s : *block) {
    SyncThreads* sync = dynamic_cast<SyncThreads*>(s);
    if (!sync) {
      first = false;
      last = nullptr;
      stmts.push_back(s);
      continue;
    }

    if (first || last) {
      did_anything = true;
      continue;
    }

    last = sync;
    first = false;
    stmts.push_back(s);
  }

  if (last) {
    stmts.pop_back();
    did_anything = true;
  }

  if (!did_anything) {
    return block;
  }

  // clean up parents.
  for (auto* s : stmts) {
    if (s->get_parent() == block) {
      block->remove_stmt(s);
    }
  }

  return new Block({stmts});
}

Stmt* TermExpander::mutate(const Block* v) {
Stmt* new_stmt = IRSimplifierBase::mutate(v);
Block* new_block = dynamic_cast<Block*>(new_stmt);
"
408,"void mergeRunCallbacks(
const RecordFunctionCallbacks& sorted_callbacks,
const CallbackHandles& sorted_handles,
bool is_start,
RecordFunction& rf) {
size_t num_executed = 0;
size_t idx_c = 0;
    for (size_t idx_h = 0; idx_h < sorted_handles.size(); ++idx_h) {
while (idx_c < sorted_callbacks.size() &&
sorted_callbacks[idx_c].second < sorted_handles[idx_h]) {
","void mergeRunCallbacks(
const RecordFunctionCallbacks& sorted_callbacks,
const CallbackHandles& sorted_handles,
      ObserverContextList& ctx_list,
bool is_start,
RecordFunction& rf) {
size_t num_executed = 0;
size_t idx_c = 0;
    for (size_t idx_h = 0; idx_h < sorted_handles.size() && idx_h < ctx_list.size(); ++idx_h) {
while (idx_c < sorted_callbacks.size() &&
sorted_callbacks[idx_c].second < sorted_handles[idx_h]) {
+idx_c;
"
409,"auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
    auto* src_data = src.contiguous().data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
","auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelect(src, output)) {
    auto src_contig = src.contiguous();
    auto* src_data = src_contig.data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
"
410,"auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelectScale(src, scale, output)) {
    auto* src_data = src.contiguous().data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
","auto* output_data = output.data_ptr<float>();
if (isFastPathIndexSelectScale(src, scale, output)) {
    auto src_contig = src.contiguous();
    auto* src_data = src_contig.data_ptr<float>();
int64_t output_size = offsets.numel() - 1;
auto* offsets_data = offsets.data_ptr<int64_t>();
std::vector<int64_t> offsets_include_last;
"
411,"std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_mode_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_mode_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
412,"std::tuple<Tensor &,Tensor &> _th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_sort_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_sort_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
413,"std::tuple<Tensor &,Tensor &> _th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_topk_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto values_ = checked_dense_tensor_unwrap(values, ""values"", 0, ""_th_topk_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
414,"Tensor & _th_fmod_out(Tensor & result, const Tensor & self, Scalar other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto result_ = checked_dense_tensor_unwrap(result, ""result"", 0, ""_th_fmod_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_fmod_out(Tensor & result, const Tensor & self, Scalar other) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Byte: {
auto result_ = checked_dense_tensor_unwrap(result, ""result"", 0, ""_th_fmod_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
415,"Tensor & _th_potri_out(Tensor & output, const Tensor & self, bool upper) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto output_ = checked_dense_tensor_unwrap(output, ""output"", 0, ""_th_potri_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _th_potri_out(Tensor & output, const Tensor & self, bool upper) {
// DeviceGuard omitted
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto output_ = checked_dense_tensor_unwrap(output, ""output"", 0, ""_th_potri_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
416,"std::tuple<Tensor &,Tensor &> _thnn_nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_nll_loss2d_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &> _thnn_nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto self_ = checked_dense_tensor_unwrap(self, ""self"", 1, ""_thnn_nll_loss2d_forward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
417,"Tensor & _thnn_rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_rrelu_with_noise_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","Tensor & _thnn_rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_rrelu_with_noise_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
418,"std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & columns, const Tensor & ones) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_conv2d_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
","std::tuple<Tensor &,Tensor &,Tensor &> _thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & columns, const Tensor & ones) {
const OptionalDeviceGuard device_guard(device_of(self));
auto dispatch_scalar_type = infer_scalar_type(self);

switch (dispatch_scalar_type) {
case ScalarType::Double: {
auto grad_output_ = checked_dense_tensor_unwrap(grad_output, ""grad_output"", 1, ""_thnn_conv2d_backward_out"", false, DeviceType::CUDA, dispatch_scalar_type);
"
419,"return at::native::std_out(result, self, dim, unbiased, keepdim);
}
Tensor &std_out(Tensor &result, const Tensor &self, IntArrayRef dim, bool unbiased, bool keepdim) {
return std_var_out(result, self, dim, unbiased, keepdim, true);
}
","return at::native::std_out(result, self, dim, unbiased, keepdim);
}
Tensor& std_out(Tensor& result, const Tensor& self, IntArrayRef dim, bool unbiased, bool keepdim) {
return std_var_out(result, self, dim, unbiased, keepdim, true);
}
"
420,"#include <c10/util/Logging.h>
#include <c10/util/string_utils.h>

#include <torch/csrc/jit/tensorexpr/analysis.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/tensorexpr/expr.h>
","#include <c10/util/Logging.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/tensorexpr/expr.h>
"
421,"// TODO: Fix the traversal, currently the order is non-deterministic
for (Tensor* tensor : intermediate_tensors_) {
if (output_tensors_.count(tensor) > 0) {
// No need to allocate memory if the tensors are given as input/output.
continue;
","// TODO: Fix the traversal, currently the order is non-deterministic
for (Tensor* tensor : intermediate_tensors_) {
    if (inlined_functions_.count(tensor->function()) ||
        inlined_random_functions_.count(tensor->function())) {
      // No need to allocate memory for intermediate tensors.
      continue;
    }
if (output_tensors_.count(tensor) > 0) {
// No need to allocate memory if the tensors are given as input/output.
continue;
"
422,"#include <c10/util/Logging.h>
#include <c10/util/string_utils.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/tensorexpr/expr.h>
","#include <c10/util/Logging.h>
#include <c10/util/string_utils.h>

#include <torch/csrc/jit/tensorexpr/analysis.h>
#include <torch/csrc/jit/tensorexpr/bounds_inference.h>
#include <torch/csrc/jit/tensorexpr/eval.h>
#include <torch/csrc/jit/tensorexpr/expr.h>
"
423,"const Tensor& input,
const Tensor& buffer) {
auto iter = TensorIteratorConfig()
    .set_check_mem_overlap(true)
.add_output(grad_input)
.add_input(input)
.add_input(buffer)
","const Tensor& input,
const Tensor& buffer) {
auto iter = TensorIteratorConfig()
.add_output(grad_input)
.add_input(input)
.add_input(buffer)
"
424,"Tensor& smooth_l1_loss_backward_out(Tensor& grad_input, const Tensor& grad_output, const Tensor& input, const Tensor& target, int64_t reduction) {
auto norm = reduction == Reduction::Mean ? 1. / input.numel() : 1.;
auto iter = at::TensorIteratorConfig()
    .set_check_mem_overlap(true)
.add_output(grad_input)
.add_input(input)
.add_input(target)
","Tensor& smooth_l1_loss_backward_out(Tensor& grad_input, const Tensor& grad_output, const Tensor& input, const Tensor& target, int64_t reduction) {
auto norm = reduction == Reduction::Mean ? 1. / input.numel() : 1.;
auto iter = at::TensorIteratorConfig()
.add_output(grad_input)
.add_input(input)
.add_input(target)
"
425,"static TensorIterator make_index_iterator(const AdvancedIndex& info) {
TensorIteratorConfig config;
  config.check_all_same_dtype(false)
.declare_static_dtype_and_device(info.src.scalar_type(), info.src.device())
.add_output(Tensor())
.add_input(info.src);
","static TensorIterator make_index_iterator(const AdvancedIndex& info) {
TensorIteratorConfig config;
  config.set_check_mem_overlap(false)
        .check_all_same_dtype(false)
.declare_static_dtype_and_device(info.src.scalar_type(), info.src.device())
.add_output(Tensor())
.add_input(info.src);
"
426,"static TensorIterator make_index_out_iterator(const AdvancedIndex& info, Tensor& result) {
TensorIteratorConfig config;
  config.check_all_same_dtype(false)
.add_output(result)
.add_input(info.src);
for (auto& index : info.indices) {
","static TensorIterator make_index_out_iterator(const AdvancedIndex& info, Tensor& result) {
TensorIteratorConfig config;
  // info.src is a restrided view of result
  config.set_check_mem_overlap(false)
        .check_all_same_dtype(false)
.add_output(result)
.add_input(info.src);
for (auto& index : info.indices) {
"
427,"// Phase 0. Inline functions, then clean up any artifacts that the inliner
//          left in that may inhibit optimization
Inline(*opt_graph);
    GRAPH_DUMP(""After Inline, before LowerGradOf"", opt_graph);
LowerGradOf(*opt_graph);
    GRAPH_DUMP(""After LowerGradOf, before specializeAutogradZero"", opt_graph);
specializeAutogradZero(opt_graph);
    GRAPH_DUMP(
        ""After specializeAutogradZero, before LowerSimpleTuples"", opt_graph);
LowerSimpleTuples(opt_graph);
    GRAPH_DUMP(""After LowerSimpleTuples, before ConstantPooling"", opt_graph);
ConstantPooling(opt_graph);
    GRAPH_DUMP(""After ConstantPooling, before runRequiredPasses"", opt_graph);
// Phase 1. Specialize to input definedness (this is very important for
//          gradient graphs), and run required passes to bring the graph
//          to an executable form.
runRequiredPasses(opt_graph);
    GRAPH_DUMP(
        ""After runRequiredPasses, before ConstantPropagation"", opt_graph);
// Phase 2. Propagate detailed information about the spec through the
//          graph (enabled more specializations in later passes).
","// Phase 0. Inline functions, then clean up any artifacts that the inliner
//          left in that may inhibit optimization
Inline(*opt_graph);
    GRAPH_DEBUG(""After Inline, before LowerGradOf\n"", *opt_graph);
LowerGradOf(*opt_graph);
    GRAPH_DEBUG(
        ""After LowerGradOf, before specializeAutogradZero\n"", *opt_graph);
specializeAutogradZero(opt_graph);
    GRAPH_DEBUG(
        ""After specializeAutogradZero, before LowerSimpleTuples\n"", *opt_graph);
LowerSimpleTuples(opt_graph);
    GRAPH_DEBUG(
        ""After LowerSimpleTuples, before ConstantPooling\n"", *opt_graph);
ConstantPooling(opt_graph);
    GRAPH_DEBUG(
        ""After ConstantPooling, before runRequiredPasses\n"", *opt_graph);
// Phase 1. Specialize to input definedness (this is very important for
//          gradient graphs), and run required passes to bring the graph
//          to an executable form.
runRequiredPasses(opt_graph);
    GRAPH_DEBUG(
        ""After runRequiredPasses, before ConstantPropagation\n"", *opt_graph);
// Phase 2. Propagate detailed information about the spec through the
//          graph (enabled more specializations in later passes).
"
428,"InlineAutodiffSubgraphs(
copy,
getAutodiffSubgraphInlining() ? autodiffSubgraphInlineThreshold : 1);
    GRAPH_DUMP(""After InlineAutodiffSubgraphs"", copy);
} else {
runNoGradOptimizations(copy);
}
EliminateDeadCode(copy);
  GRAPH_DUMP(""After runProfilingOptimizations:"", copy);
}
void ProfilingGraphExecutorImpl::runProfilingInsensitiveOptimizations(
std::shared_ptr<Graph>& graph) {
  GRAPH_DUMP(
      ""Before inlining (beginning of runProfilingInsensitiveOptimizations)"",
      graph);
// TODO: maybe this can go later in pipeline / directly in autodiff forward
// creation
if (getGraphExecutorOptimize()) {
Inline(*graph);
}
  GRAPH_DUMP(""After inlining, before ClearProfilingInformation"", graph);
ClearProfilingInformation(graph);
  GRAPH_DUMP(""After ClearProfilingInformation, before LowerGradOf"", graph);
LowerGradOf(*graph);
  GRAPH_DUMP(""After LowerGradOf, before ClearUndefinedness"", graph);
// clear any residual undefinedness
// as double backward graph inputs'
// may carry over undefinedness
","InlineAutodiffSubgraphs(
copy,
getAutodiffSubgraphInlining() ? autodiffSubgraphInlineThreshold : 1);
    GRAPH_DEBUG(""After InlineAutodiffSubgraphs\n"", *copy);
} else {
runNoGradOptimizations(copy);
}
EliminateDeadCode(copy);
  GRAPH_DEBUG(""After runProfilingOptimizations:\n"", *copy);
}
void ProfilingGraphExecutorImpl::runProfilingInsensitiveOptimizations(
std::shared_ptr<Graph>& graph) {
  GRAPH_DEBUG(
      ""Before inlining (beginning of runProfilingInsensitiveOptimizations)\n"",
      *graph);
// TODO: maybe this can go later in pipeline / directly in autodiff forward
// creation
if (getGraphExecutorOptimize()) {
Inline(*graph);
}
  GRAPH_DEBUG(""After inlining, before ClearProfilingInformation\n"", *graph);
ClearProfilingInformation(graph);
  GRAPH_DEBUG(""After ClearProfilingInformation, before LowerGradOf\n"", *graph);
LowerGradOf(*graph);
  GRAPH_DEBUG(""After LowerGradOf, before ClearUndefinedness\n"", *graph);
// clear any residual undefinedness
// as double backward graph inputs'
// may carry over undefinedness
"
429,"return;
}
  size_t begin_line = start(); // beginning of line to highlight
  size_t end_line = start(); // end of line to highlight
while (begin_line > 0 && str[begin_line - 1] != '\n')
-begin_line;
while (end_line < str.size() && str[end_line] != '\n')
","return;
}
  size_t range_end =
      (str.size() < end()
           ? str.size()
           : end()); // use instead of 'end()' because some ranges extend past
                     // the length of the source

  // determine CONTEXT line range
  size_t begin_line = start(); // beginning of lines to highlight
  size_t end_line = range_end;
while (begin_line > 0 && str[begin_line - 1] != '\n')
while (end_line < str.size() && str[end_line] != '\n')
"
430,"futureMessage->addCallback([this](const rpc::FutureMessage& futureMessage) {
if (futureMessage.hasError()) {
// If we have an error, let the local autograd engine know about it.
      std::runtime_error err((*futureMessage.error()).what());
std::unique_lock<std::mutex> lock(lock_);
if (graphTask_) {
graphTask_->set_exception_without_signal(nullptr);
lock.unlock();
if (!graphTask_->future_completed_.exchange(true)) {
          graphTask_->future_result_->setErrorIfNeeded(err.what());
}
} else {
LOG(WARNING) << ""Ignoring error since GraphTask is no longer valid: ""
                     << err.what();
}
}
});
","futureMessage->addCallback([this](const rpc::FutureMessage& futureMessage) {
if (futureMessage.hasError()) {
// If we have an error, let the local autograd engine know about it.
std::unique_lock<std::mutex> lock(lock_);
if (graphTask_) {
graphTask_->set_exception_without_signal(nullptr);
lock.unlock();
if (!graphTask_->future_completed_.exchange(true)) {
          graphTask_->future_result_->setErrorIfNeeded(
              std::make_exception_ptr(*futureMessage.error()));
}
} else {
LOG(WARNING) << ""Ignoring error since GraphTask is no longer valid: ""
                     << (*futureMessage.error()).what();
}
}
});
"
431,"ownerRRef->setValue(std::move(py_ivalue));
} catch (py::error_already_set& e) {
// py::error_already_set requires GIL to destruct, take special care.
      ownerRRef->setError(e.what());
py::gil_scoped_acquire acquire;
e.restore();
PyErr_Clear();
} catch (std::exception& e) {
      ownerRRef->setError(e.what());
}
markComplete(RemoteRet(rrefId, forkId).toMessage());
}
","ownerRRef->setValue(std::move(py_ivalue));
} catch (py::error_already_set& e) {
// py::error_already_set requires GIL to destruct, take special care.
      ownerRRef->setError(std::current_exception());
py::gil_scoped_acquire acquire;
e.restore();
PyErr_Clear();
} catch (std::exception& e) {
      ownerRRef->setError(std::current_exception());
}
markComplete(RemoteRet(rrefId, forkId).toMessage());
}
"
432,"whenValueSet->addCallback(
[responseFuture, messageId, rref, whenValueSet]() {
if (whenValueSet->hasError()) {
                responseFuture->setError(whenValueSet->error()->what());
return;
}
try {
","whenValueSet->addCallback(
[responseFuture, messageId, rref, whenValueSet]() {
if (whenValueSet->hasError()) {
                responseFuture->setError(
                    whenValueSet->tryRetrieveErrorMessage());
return;
}
try {
"
433,"future_->markCompleted(value);
}
void OwnerRRef::setError(const std::string& error) {
  future_->setErrorIfNeeded(error);
}
std::ostream& operator<<(std::ostream& os, const RRef& rref) {
","future_->markCompleted(value);
}
void OwnerRRef::setError(std::exception_ptr eptr) {
  future_->setErrorIfNeeded(std::move(eptr));
}
std::ostream& operator<<(std::ostream& os, const RRef& rref) {
"
434,"return result;
} else {
    AT_ASSERTM(self.sizes() == other.sizes(),
""mkldnn_mul_out: currently mkldnn not support broadcasting"");
ideep::tensor y = itensor_from_mkldnn(other);
ideep::binary::compute(x, y, z, dnnl::algorithm::binary_mul);
","return result;
} else {
    TORCH_CHECK(self.sizes() == other.sizes(),
""mkldnn_mul_out: currently mkldnn not support broadcasting"");
ideep::tensor y = itensor_from_mkldnn(other);
ideep::binary::compute(x, y, z, dnnl::algorithm::binary_mul);
"
435,"return y;
}
at::Tensor mkldnn_convolution(
    const at::Tensor& input,
    const at::Tensor& weight,
    const at::Tensor& bias,
IntArrayRef padding,
IntArrayRef stride,
IntArrayRef dilation,
","return y;
}
Tensor mkldnn_convolution(
    const Tensor& input,
    const Tensor& weight,
    const Tensor& bias,
IntArrayRef padding,
IntArrayRef stride,
IntArrayRef dilation,
"
436,"namespace at { namespace native {
Tensor mkldnn_relu(const Tensor& input) {
  AT_ERROR(""mkldnn_relu: ATen not compiled with MKLDNN support"");
}
Tensor& mkldnn_relu_(Tensor& input) {
  AT_ERROR(""mkldnn_relu_: ATen not compiled with MKLDNN support"");
}
}}
","namespace at { namespace native {
Tensor mkldnn_relu(const Tensor& input) {
  TORCH_CHECK(false, ""mkldnn_relu: ATen not compiled with MKLDNN support"");
}
Tensor& mkldnn_relu_(Tensor& input) {
  TORCH_CHECK(false, ""mkldnn_relu_: ATen not compiled with MKLDNN support"");
}
}}
"
437,"const Tensor& self,
const int64_t dim,
const bool half_to_float) {
  AT_ERROR(""mkldnn_softmax: ATen not compiled with MKLDNN support"");
}
} // namespace native
","const Tensor& self,
const int64_t dim,
const bool half_to_float) {
  TORCH_CHECK(false, ""mkldnn_softmax: ATen not compiled with MKLDNN support"");
}
} // namespace native
"
438,"return new_with_itensor_mkldnn(std::move(dst), self.options());
}
Tensor mkldnn_transpose(const Tensor & self, int64_t dim0, int64_t dim1) {
const ideep::tensor& x = itensor_from_mkldnn(self);
ideep::tensor y;
std::vector<int> axes(x.ndims());
","return new_with_itensor_mkldnn(std::move(dst), self.options());
}
Tensor mkldnn_transpose(const Tensor& self, int64_t dim0, int64_t dim1) {
const ideep::tensor& x = itensor_from_mkldnn(self);
ideep::tensor y;
std::vector<int> axes(x.ndims());
"
439,"namespace native {
Tensor mkldnn_sigmoid(const Tensor& self) {
  AT_ERROR(""mkldnn_sigmoid: ATen not compiled with MKLDNN support"");
}
Tensor& mkldnn_sigmoid_(Tensor& self) {
  AT_ERROR(""mkldnn_sigmoid_: ATen not compiled with MKLDNN support"");
}
} // namespace native
","namespace native {
Tensor mkldnn_sigmoid(const Tensor& self) {
  TORCH_CHECK(false, ""mkldnn_sigmoid: ATen not compiled with MKLDNN support"");
}
Tensor& mkldnn_sigmoid_(Tensor& self) {
  TORCH_CHECK(false, ""mkldnn_sigmoid_: ATen not compiled with MKLDNN support"");
}
} // namespace native
"
440,"Tensor trunc(const Tensor& self) { return unary_op_impl(self, at::trunc_out); }
Tensor& trunc_(Tensor& self) { return unary_op_impl_(self, at::trunc_out); }
Tensor& neg_out(Tensor& result, const Tensor& self) {
TORCH_CHECK(self.scalar_type() != kBool,
""Negation, the `-` operator, on a bool tensor is not supported. ""
","Tensor trunc(const Tensor& self) { return unary_op_impl(self, at::trunc_out); }
Tensor& trunc_(Tensor& self) { return unary_op_impl_(self, at::trunc_out); }
// Alias for trunc
Tensor& fix_out(Tensor& result, const Tensor& self) { return at::native::trunc_out(result, self); }
Tensor fix(const Tensor& self) { return at::native::trunc(self); }
Tensor& fix_(Tensor& self) { return at::native::trunc_(self); }

Tensor& neg_out(Tensor& result, const Tensor& self) {
TORCH_CHECK(self.scalar_type() != kBool,
""Negation, the `-` operator, on a bool tensor is not supported. ""
"
441,"Example::
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
    >>>      pred = model.forward()
    >>>      loss = loss_func(pred, loss)
    >>>      dist_autograd.backward(context_id, loss)
)"",
py::arg(""contextId""),
py::arg(""roots""),
","Example::
>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
    >>>     pred = model.forward()
    >>>     loss = loss_func(pred, loss)
    >>>     dist_autograd.backward(context_id, loss)
)"",
py::arg(""contextId""),
py::arg(""roots""),
"
442,"#include <ATen/native/xnnpack/Common.h>
#include <ATen/native/ConvUtils.h>
#include <ATen/native/utils/ParamUtils.h>
#include <ATen/native/xnnpack/Factory.h>
#include <ATen/native/xnnpack/Convolution.h>
namespace at {
","#include <ATen/native/xnnpack/Common.h>
#include <ATen/native/ConvUtils.h>
#include <ATen/native/utils/Factory.h>
#include <ATen/native/utils/ParamUtils.h>
#include <ATen/native/xnnpack/Convolution.h>
namespace at {
"
443,"}
std::vector<Edge> output_edges;
  if (inputs != nullptr) {
int num_inputs = PyTuple_GET_SIZE(inputs);
output_edges.reserve(num_inputs);
for (int i = 0; i < num_inputs; ++i) {
","}
std::vector<Edge> output_edges;
  if (!backward_api_called) {
int num_inputs = PyTuple_GET_SIZE(inputs);
output_edges.reserve(num_inputs);
for (int i = 0; i < num_inputs; ++i) {
"
444,"Tensor embedding(const Tensor & weight, const Tensor & indices,
int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
auto indices_arg = TensorArg(indices, ""indices"", 1);
checkScalarType(""embedding"", indices_arg, kLong);
","Tensor embedding(const Tensor & weight, const Tensor & indices,
int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
  TORCH_CHECK(weight.dim() >= 1, ""'weight' must be at least 1-D"");
auto indices_arg = TensorArg(indices, ""indices"", 1);
checkScalarType(""embedding"", indices_arg, kLong);
"
445,"return v->type()->cast<FunctionType>() && getFuncName(v) == functional;
}
c10::optional<std::string> getModuleName(Value* value) {
auto type = value->type()->cast<ClassType>();
if (type && type->name()) {
    static std::regex mangle_re(""\\.___torch_mangle_\\d+"");
    auto qualified_name =
        std::regex_replace(type->name()->qualifiedName(), mangle_re, """");
    return qualified_name;
}
return c10::nullopt;
}
","return v->type()->cast<FunctionType>() && getFuncName(v) == functional;
}
std::string removeTorchMangle(const std::string& orig_name) {
  static std::regex mangle_re(""\\.___torch_mangle_\\d+"");
  auto qualified_name = std::regex_replace(orig_name, mangle_re, """");
  return qualified_name;
}

c10::optional<std::string> getModuleName(Value* value) {
auto type = value->type()->cast<ClassType>();
if (type && type->name()) {
    return removeTorchMangle(type->name()->qualifiedName());
}
return c10::nullopt;
}
"
446,"#include <c10/cuda/CUDAGuard.h>
#include <c10d/Utils.hpp>

namespace c10d {
constexpr const char* const kNCCLAbortedCommStoreKey = ""NCCLABORTEDCOMM"";
","#include <c10/cuda/CUDAGuard.h>
#include <c10d/Utils.hpp>
namespace c10d {
constexpr const char* const kNCCLAbortedCommStoreKey = ""NCCLABORTEDCOMM"";
"
447,"template <>
void gemm<double>(CUDABLAS_GEMM_ARGTYPES(double)) {
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t opa = _cublasOpFromChar(transa);
cublasOperation_t opb = _cublasOpFromChar(transb);
","template <>
void gemm<double>(CUDABLAS_GEMM_ARGTYPES(double)) {
  globalContext().alertCuBLASConfigNotDeterministic();
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t opa = _cublasOpFromChar(transa);
cublasOperation_t opb = _cublasOpFromChar(transb);
"
448,"#ifndef __HIP_PLATFORM_HCC__
template <>
void gemv<c10::complex<double>>(CUDABLAS_GEMV_ARGTYPES(c10::complex<double>)) {
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t op = _cublasOpFromChar(trans);
_cublasAdjustLdLevel2(m, n, &lda);
","#ifndef __HIP_PLATFORM_HCC__
template <>
void gemv<c10::complex<double>>(CUDABLAS_GEMV_ARGTYPES(c10::complex<double>)) {
    globalContext().alertCuBLASConfigNotDeterministic();
cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();
cublasOperation_t op = _cublasOpFromChar(trans);
_cublasAdjustLdLevel2(m, n, &lda);
"
449,"#include <torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h>

namespace torch {
namespace jit {

namespace onnx {
using namespace ::c10::onnx;
}

void FixupONNXIfs(Block* block) {
  for (auto* node : block->nodes()) {
    if (node->kind() == ::c10::onnx::If) {
      auto* if_node = node;
      auto* graph = if_node->owningGraph();
      for (Block* block : node->blocks()) {
        FixupONNXIfs(block);
        if (block->nodes().begin() == block->nodes().end()) {
          // ONNX does not support empty blocks, must use some op which does
          // nothing
          Value* output = block->outputs()[0];
          Node* id_node = graph->create(onnx::Identity);
          id_node->insertBefore(block->return_node());
          id_node->addInput(output);
          id_node->output()->copyMetadata(output);
          block->return_node()->replaceInputWith(output, id_node->output());
        }
      }
    } else {
      for (Block* block : node->blocks()) {
        FixupONNXIfs(block);
      }
    }
  }
}

void FixupONNXConditionals(std::shared_ptr<Graph>& graph) {
  FixupONNXIfs(graph->block());
}

} // namespace jit
} // namespace torch
","++ /dev/null
"
450,"#include <torch/csrc/jit/passes/onnx/fixup_onnx_loop.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/onnx/peephole.h>

namespace torch {
namespace jit {

namespace onnx {
using namespace ::c10::onnx;
}

Node* CreateCastToBoolNode(Value* val, Graph* graph) {
  Node* cast_node = graph->create(onnx::Cast);
  cast_node->addInput(val);
  cast_node->i_(attr::to, /*Bool*/ 9);
  return cast_node;
}

Node* InsertCastForCond(Value* cond_val, Graph* graph, Node* consumer_node) {
  // prev:  cond_val -> consumer_node
  // after: cond_val -> cast -> consumer_node
  // NOTE: The cast is required because operators like PyTorch Greater/Less
  //       return tensor in type torch.uint8. However the type for condition
  //       input in ONNX Loop must be bool.
  Node* cast_node = CreateCastToBoolNode(cond_val, graph);
  cast_node->insertBefore(consumer_node);

  consumer_node->replaceInputWith(cond_val, cast_node->output());
  return cast_node;
}

bool IsCondCastRequired(Value* cond_val) {
  const auto& type = cond_val->type();
  if (auto tt = type->cast<TensorType>()) {
    if (auto scalar_type = tt->scalarType()) {
      return *scalar_type != c10::kBool;
    }
  }
  return !type->isSubtypeOf(BoolType::get());
}

void FixupONNXLoops(Block* block) {
  for (auto* node : block->nodes()) {
    if (node->kind() == ::c10::onnx::Loop) {
      auto* loop_node = node;
      auto* graph = loop_node->owningGraph();

      // add cast to condition input outside the loop.
      Value* cond_val = loop_node->inputs()[1];
      if (IsCondCastRequired(cond_val))
        InsertCastForCond(cond_val, graph, loop_node);

      // Setup Loop input cond and i.
      TORCH_INTERNAL_ASSERT(loop_node->blocks().size() == 1);
      auto* sub_block = loop_node->blocks()[0];
      Value* cond = sub_block->insertInput(1, ""cond"");
      cond->setType(BoolType::create());

      Value* i = sub_block->inputs()[0];
      i->setType(TensorType::fromNumberType(IntType::get()));

      // add cast to condition input inside the loop.
      Value* next_cond_val = sub_block->outputs()[0];
      if (IsCondCastRequired(next_cond_val))
        InsertCastForCond(next_cond_val, graph, sub_block->return_node());
    }
    for (Block* block : node->blocks()) {
      FixupONNXLoops(block);
    }
  }
}

namespace {
bool IsErasableSequence(const Node* loop_node, size_t i) {
  AT_ASSERT(loop_node->blocks().size() == 1);
  auto* sub_block = loop_node->blocks()[0];
  auto* out_node = sub_block->outputs()[i - 1]->node();
  auto* in_val = sub_block->inputs()[i];

  if (out_node->kind() != ::c10::onnx::SequenceInsert) {
    return false;
  }

  if (out_node->inputs().size() == 3) {
    // Non-default insert position is not supported.
    return false;
  }

  if (out_node->input(0) != in_val) {
    // Only SequenceInsert that applies on loop-carried sequence is supported.
    return false;
  }

  const auto seq_node_kind = loop_node->inputs()[i]->node()->kind();
  if (seq_node_kind != ::c10::onnx::SequenceEmpty) {
    // Initial sequence must be empty.
    return false;
  }

  if (out_node->output()->uses().size() != 1) {
    // The sequence is not supported to be used elsewhere.
    return false;
  }

  return true;
}
} // anonymous namespace

// ONNX::Loop does not support Sequence type as loop-carried dependencies. Only
// tensors are supported. This pass converts Sequence loop-carried dependencies
// to scan_outputs. In opset 11, only the below pattern is supported.
//
// PTIR graph:
//  ...
//  %res.1 : Tensor[] = prim::ListConstruct()
//  %res : Tensor[] = prim::Loop(%11, %22, %res.1)
//    block0(%i.1 : Tensor, %res.6 : Tensor[]):
//      ...
//      %res.3 : Tensor[] = aten::append(%res.6, %17)
//      -> (%22, %res.3)
//  return (%res.3)
//
// ONNX graph:
//  ...
//  %res : Tensor = onnx::Loop(%11, %22)
//    block0(%i.1 : Tensor):
//      ...
//      -> (%22, %17)
//  %res_seq : Tensor[] = onnx::SplitToSequence[keepdims=0](%res)
//  return (%res_seq)
void ConvertSequenceDependencies(Block* block) {
  for (auto* node : block->nodes()) {
    for (Block* block : node->blocks()) {
      ConvertSequenceDependencies(block);
    }

    if (node->kind() == ::c10::onnx::Loop) {
      auto* loop_node = node;
      auto* graph = loop_node->owningGraph();

      AT_ASSERT(loop_node->blocks().size() == 1);
      auto* sub_block = loop_node->blocks()[0];

      std::vector<size_t> idx_to_remove;
      // loop sub-block inputs are  (iter, cond, loop-carried dependencies)
      // loop sub-block outputs are (      cond, loop-carried dependencies, scan
      // outputs) loop inputs are            (iter, cond, loop-carried
      // dependencies) loop outputs are           (            loop-carried
      // dependencies, scan outputs)
      for (size_t i = 2; i < sub_block->inputs().size(); ++i) {
        if (IsErasableSequence(loop_node, i)) {
          auto* seq_node = sub_block->outputs()[i - 1]->node();
          // Replace sequence output with the inserted element.
          auto inserted_value = seq_node->input(1);
          sub_block->return_node()->replaceInputWith(
              seq_node->output(), inserted_value);

          // Split the added scan_output back to expected tensor sequence.
          auto loop_output = loop_node->output(i - 2);
          Node* split_node =
              loop_node->owningGraph()->create(onnx::SplitToSequence);
          loop_output->replaceAllUsesWith(split_node->output());
          split_node->i_(attr::keepdims, 0);
          split_node->addInput(loop_output);
          split_node->insertAfter(loop_node);
          split_node->output()->copyMetadata(loop_output);

          // Update loop output metadata.
          loop_output->copyMetadata(inserted_value);
          loop_output->setType(c10::unshapedType(loop_output->type()));

          // The node that produces sequence should be safe to remove now.
          seq_node->destroy();

          idx_to_remove.push_back(i);
        }
      }

      for (size_t i = 0; i < idx_to_remove.size(); ++i) {
        size_t idx = idx_to_remove[i] - i;

        sub_block->eraseInput(idx);
        loop_node->removeInput(idx);

        // Swap output order. Move all scan outputs to the back.
        sub_block->return_node()->addInput(
            sub_block->return_node()->inputs().at(idx - 1));
        sub_block->return_node()->removeInput(idx - 1);

        auto loop_out = loop_node->addOutput();
        loop_out->copyMetadata(loop_node->outputs().at(idx - 2));
        loop_node->outputs().at(idx - 2)->replaceAllUsesWith(loop_out);
        loop_node->eraseOutput(idx - 2);
      }
    }
  }
}

static void FuseSequenceSplitConcat(Block* b) {
  for (auto it = b->nodes().begin(), end = b->nodes().end(); it != end; ++it) {
    for (auto* child_block : it->blocks()) {
      FuseSequenceSplitConcat(child_block);
    }
    if (it->kind() == onnx::ConcatFromSequence &&
        it->input()->node()->kind() == onnx::SplitToSequence) {
      if (it->input()->uses().size() > 1) {
        continue;
      }

      auto split_node = it->input()->node();
      auto concat_node = *it;

      const auto split_axis =
          split_node->hasAttribute(attr::axis) ? split_node->i(attr::axis) : 0;
      const auto split_keepdims = split_node->hasAttribute(attr::keepdims)
          ? split_node->i(attr::keepdims)
          : 1;
      const auto concat_axis = concat_node->i(attr::axis);
      const auto concat_new_axis = concat_node->hasAttribute(attr::new_axis)
          ? concat_node->i(attr::new_axis)
          : 0;
      const bool has_input_split = split_node->inputs().size() == 2;

      if (has_input_split) {
        continue;
      }

      if (split_keepdims == concat_new_axis) {
        continue;
      }

      if (split_axis != concat_axis) {
        continue;
      }

      concat_node->output()->replaceAllUsesWith(split_node->input());
    }
  }
}

void FixupONNXLoops(std::shared_ptr<Graph>& graph) {
  FixupONNXLoops(graph->block());
  ConvertSequenceDependencies(graph->block());
  FuseSequenceSplitConcat(graph->block());
  EliminateDeadCode(
      graph->block(),
      true,
      DCESideEffectPolicy::ALLOW_DELETING_NODES_WITH_SIDE_EFFECTS);
}

} // namespace jit
} // namespace torch
","++ /dev/null
"
451,"#include <torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h>
#include <torch/csrc/jit/passes/onnx/constant_fold.h>
#include <torch/csrc/jit/passes/onnx/eval_peephole.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_conditionals.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_loop.h>
#include <torch/csrc/jit/passes/onnx/function_substitution.h>
#include <torch/csrc/jit/passes/onnx/peephole.h>
#include <torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h>
","#include <torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.h>
#include <torch/csrc/jit/passes/onnx/constant_fold.h>
#include <torch/csrc/jit/passes/onnx/eval_peephole.h>
#include <torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h>
#include <torch/csrc/jit/passes/onnx/function_substitution.h>
#include <torch/csrc/jit/passes/onnx/peephole.h>
#include <torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h>
"
452,"#if !defined(TH_REAL_IS_HALF) /* non half part */
void THTensor_(maskedSelect)(THTensor *tensor, THTensor *src, THByteTensor *mask)
{
  at::NoNamesGuard guard;
  ptrdiff_t numel = THTensor_wrap(mask).sum().item<int64_t>();
  scalar_t *tensor_data;

#ifdef DEBUG
  THAssert(numel <= LONG_MAX);
#endif
  THTensor_(resize1d)(tensor,numel);
  tensor_data = tensor->data<scalar_t>();
  TH_TENSOR_APPLY2(scalar_t, src, unsigned char, mask,
                   if (*mask_data > 1)
                   {
                     THFree(mask_counter);
                     THFree(src_counter);
                     THError(""Mask tensor can take 0 and 1 values only"");
                   }
                   else if (*mask_data == 1)
                   {
                     *tensor_data = *src_data;
                     tensor_data++;
                   });
}

void THTensor_(maskedSelectBool)(THTensor *tensor, THTensor *src, THBoolTensor *mask)
{
  at::NoNamesGuard guard;
  ptrdiff_t numel = THTensor_wrap(mask).sum().item<int64_t>();
  scalar_t *tensor_data;

#ifdef DEBUG
  THAssert(numel <= LONG_MAX);
#endif
  THTensor_(resize1d)(tensor,numel);
  tensor_data = tensor->data<scalar_t>();
  TH_TENSOR_APPLY2(scalar_t, src, bool, mask,
                   if (*mask_data)
                   {
                     *tensor_data = *src_data;
                     tensor_data++;
                   });
}

void THTensor_(maskedCopy)(THTensor *tensor, THByteTensor *mask, THTensor* src )
{
THTensor *srct = THTensor_(newContiguous)(src);
","#if !defined(TH_REAL_IS_HALF) /* non half part */
void THTensor_(maskedCopy)(THTensor *tensor, THByteTensor *mask, THTensor* src )
{
THTensor *srct = THTensor_(newContiguous)(src);
"
453,"}
}
    uint32_t extCount;
    vkEnumerateInstanceExtensionProperties(nullptr, &extCount, nullptr);
std::vector<VkExtensionProperties> extProps(extCount);
    vkEnumerateInstanceExtensionProperties(nullptr, &extCount, extProps.data());
bool foundExt = false;
for (VkExtensionProperties p : extProps) {
if (strcmp(VK_EXT_DEBUG_REPORT_EXTENSION_NAME, p.extensionName) == 0) {
","}
}
    uint32_t extCount = 0;
    VK_CHECK(vkEnumerateInstanceExtensionProperties(nullptr, &extCount, nullptr));
std::vector<VkExtensionProperties> extProps(extCount);
    VK_CHECK(vkEnumerateInstanceExtensionProperties(nullptr, &extCount, extProps.data()));
bool foundExt = false;
for (VkExtensionProperties p : extProps) {
if (strcmp(VK_EXT_DEBUG_REPORT_EXTENSION_NAME, p.extensionName) == 0) {
"
454,"groups,
output_min ? output_min->to<float>() : vulkan::ContextConv2D::kMin,
output_max ? output_max->to<float>() : vulkan::ContextConv2D::kMax);
  auto conv2d_op_context = c10::make_intrusive<VulkanConv2dOpContext>(
std::move(weight),
std::move(bias),
std::move(padding),
","groups,
output_min ? output_min->to<float>() : vulkan::ContextConv2D::kMin,
output_max ? output_max->to<float>() : vulkan::ContextConv2D::kMax);
  return c10::make_intrusive<VulkanConv2dOpContext>(
std::move(weight),
std::move(bias),
std::move(padding),
"
455,"r.scalartypeWithDefault(1, scalar_type),
r.deviceOptional(2),
r.pyobject(0),
        false,
        false,
        type_inference);
}
throw std::runtime_error(""tensor(): invalid arguments"");
}
","r.scalartypeWithDefault(1, scalar_type),
r.deviceOptional(2),
r.pyobject(0),
        /*copy_variables=*/false,
        /*copy_numpy=*/false,
        /*type_inference=*/type_inference);
}
throw std::runtime_error(""tensor(): invalid arguments"");
}
"
456,"#include <algorithm>
#include <ATen/ATen.h>
#include <ATen/Config.h>
#if AT_BUILD_WITH_BLAS()
extern ""C"" double ddot_(int *n, double *x, int *incx, double *y, int *incy);
extern ""C"" float sdot_(int *n, float *x, int *incx, float *y, int *incy);
extern ""C"" void dscal_(int *n, double *a, double *x, int *incx);
extern ""C"" void sscal_(int *n, float *a, float *x, int *incx);
extern ""C"" void dgemv_(char *trans, int *m, int *n, double *alpha, double *a, int *lda, double *x, int *incx, double *beta, double *y, int *incy);
","#include <algorithm>
#include <ATen/ATen.h>
#include <ATen/Config.h>
#include <TH/THGeneral.h>
#if AT_BUILD_WITH_BLAS()
extern ""C"" double ddot_(int *n, double *x, int *incx, double *y, int *incy);
extern ""C"" void dscal_(int *n, double *a, double *x, int *incx);
extern ""C"" void sscal_(int *n, float *a, float *x, int *incx);
extern ""C"" void dgemv_(char *trans, int *m, int *n, double *alpha, double *a, int *lda, double *x, int *incx, double *beta, double *y, int *incy);
"
457,".VS(copyMetadata)
.VS(isCompleteTensor)
.VS(requires_grad)
.def(""toIValue"", [](Value& n) { return toIValue(&n); })
.def(""type"", [](Value& v) { return v.type(); });
#undef VS
",".VS(copyMetadata)
.VS(isCompleteTensor)
.VS(requires_grad)
      .def(
          ""requiresGrad"",
          [](Value& n) { n.type()->expect<TensorType>()->requiresGrad(); })
.def(""toIValue"", [](Value& n) { return toIValue(&n); })
.def(""type"", [](Value& v) { return v.type(); });
#undef VS
"
458,"static void check_valid_identifier(const std::string& name) {
TORCH_CHECK(
Dimname::isValidName(name),
      ""Invalid name: a valid identifier must contain alphabetical characters and/or underscore, got: '"",
name, ""'."");
}
","static void check_valid_identifier(const std::string& name) {
TORCH_CHECK(
Dimname::isValidName(name),
      ""Invalid name: a valid identifier contains only digits, alphabetical ""
      ""characters, and/or underscore and starts with a non-digit. got: '"",
name, ""'."");
}
"
459,"// in the end in any case (and if it turns out you DON'T have the right
// information at the site, as is the case with backend specific
// per-op registrations, you will get the right behavior!)
    TORCH_CHECK(false,
      *ns_opt == *ns_,
""Explicitly provided namespace ("", *ns_opt, "") in schema string ""
""does not match namespace of enclosing "", toString(kind_), "" block ("", *ns_, "").  ""
""Move this definition to the (unique) TORCH_LIBRARY block corresponding to this namespace ""
","// in the end in any case (and if it turns out you DON'T have the right
// information at the site, as is the case with backend specific
// per-op registrations, you will get the right behavior!)
    TORCH_CHECK(*ns_opt == *ns_,
""Explicitly provided namespace ("", *ns_opt, "") in schema string ""
""does not match namespace of enclosing "", toString(kind_), "" block ("", *ns_, "").  ""
""Move this definition to the (unique) TORCH_LIBRARY block corresponding to this namespace ""
"
460,"throw e;
}

void ThrowEnforceFiniteNotMet(
const char* file,
const int line,
const char* condition,
const std::string& msg,
const void* caller) {
    throw c10::EnforceFiniteError(
      file, line, condition, msg, (*GetFetchStackTrace())(), caller
    );
}
// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, std::string msg)
    : Error(std::move(msg), str(""Exception raised from "", source_location, "" (most recent call first):\n"", (*GetFetchStackTrace())())) {
}
using APIUsageLoggerType = std::function<void(const std::string&)>;
","throw e;
}
void ThrowEnforceFiniteNotMet(
const char* file,
const int line,
const char* condition,
const std::string& msg,
const void* caller) {
  throw c10::EnforceFiniteError(
      file, line, condition, msg, (*GetFetchStackTrace())(), caller);
}
// PyTorch-style error message
// (This must be defined here for access to GetFetchStackTrace)
Error::Error(SourceLocation source_location, std::string msg)
    : Error(
          std::move(msg),
          str(""Exception raised from "",
              source_location,
              "" (most recent call first):\n"",
              (*GetFetchStackTrace())())) {}
using APIUsageLoggerType = std::function<void(const std::string&)>;
"
461,"!wrappedPayload.empty(), ""Wrapped payload should not be empty."");
// Create the ivalues to send over. We need to send the original message type
// and id, as well as some profiling metadata.
  std::vector<at::IValue> ivalues{wrappedMsgType, profilerConfig_.toIValue()};
// Pickle it into a char payload to be sent over the wire.
std::vector<torch::Tensor> tensorTable;
std::vector<char> profilingPayload =
","!wrappedPayload.empty(), ""Wrapped payload should not be empty."");
// Create the ivalues to send over. We need to send the original message type
// and id, as well as some profiling metadata.
  std::vector<at::IValue> ivalues{
      wrappedMsgType, profilerConfig_.toIValue(), profilingKeyId_.toIValue()};
// Pickle it into a char payload to be sent over the wire.
std::vector<torch::Tensor> tensorTable;
std::vector<char> profilingPayload =
"
462,"return profilerConfig_;
}
std::unique_ptr<RpcWithProfilingReq> RpcWithProfilingReq::fromMessage(
const rpc::Message& message) {
rpc::MessageType origMsgType = message.type();
","return profilerConfig_;
}
const rpc::ProfilingId& RpcWithProfilingReq::getProfilingId() const {
  return profilingKeyId_;
}

std::unique_ptr<RpcWithProfilingReq> RpcWithProfilingReq::fromMessage(
const rpc::Message& message) {
rpc::MessageType origMsgType = message.type();
"
463,"return profiledEvents_;
}
void RpcWithProfilingResp::setWrappedRpc(
std::unique_ptr<RpcCommandBase> wrappedRpc) {
wrappedRpc_ = std::move(wrappedRpc);
","return profiledEvents_;
}
const rpc::ProfilingId& RpcWithProfilingResp::getProfilingId() const {
  return profilingId_;
}

void RpcWithProfilingResp::setWrappedRpc(
std::unique_ptr<RpcCommandBase> wrappedRpc) {
wrappedRpc_ = std::move(wrappedRpc);
"
464,"py::class_<NoneType, Type, std::shared_ptr<NoneType>>(m, ""NoneType"")
.def_static(""get"", &NoneType::get);
  py::class_<TupleType, NamedType, std::shared_ptr<TupleType>>(m, ""TupleType"")
.def(
py::init([](std::vector<TypePtr> a) { return TupleType::create(a); }))
      .def_static(
          ""createNamed"",
          [](const std::string& name,
             const std::vector<std::string>& field_names,
             const std::vector<TypePtr>& types) {
            return TupleType::createNamed(name, field_names, types);
          })
.def(""elements"", [](TupleType& self) {
std::vector<TypePtr> types;
for (const auto& type : self.elements()) {
","py::class_<NoneType, Type, std::shared_ptr<NoneType>>(m, ""NoneType"")
.def_static(""get"", &NoneType::get);
  py::class_<TupleType, Type, std::shared_ptr<TupleType>>(m, ""TupleType"")
.def(
py::init([](std::vector<TypePtr> a) { return TupleType::create(a); }))
.def(""elements"", [](TupleType& self) {
std::vector<TypePtr> types;
for (const auto& type : self.elements()) {
"
465,"Module& input_module,
const std::string& method_name,
bool inplace,
QuantType quant_type) {
Module module = input_module.clone(inplace);
InsertQuantDeQuantHelper h;
","Module& input_module,
const std::string& method_name,
bool inplace,
    bool debug,
QuantType quant_type) {
Module module = input_module.clone(inplace);
InsertQuantDeQuantHelper h;
"
466,"// once
std::unordered_set<Value*> quantized_values_;
// Map from original weight value to GraphFunction corresponding to the
// subgraph that includes the weight observer and dependent nodes.
std::unordered_map<Value*, std::unique_ptr<GraphFunction>>
weight_to_graph_fn_;

  QuantType quant_type_ = QuantType::STATIC;
  bool debug_ = false;
};
void InsertQuantDeQuantHelper::collectObserverNodesAndValueToQuantize(
","// once
std::unordered_set<Value*> quantized_values_;
  QuantType quant_type_ = QuantType::STATIC;

// Map from original weight value to GraphFunction corresponding to the
// subgraph that includes the weight observer and dependent nodes.
std::unordered_map<Value*, std::unique_ptr<GraphFunction>>
weight_to_graph_fn_;
};
void InsertQuantDeQuantHelper::collectObserverNodesAndValueToQuantize(
"
467,"void setQuantType(QuantType quant_type) {
quant_type_ = quant_type;
}
// Cleanup observer nodes from graph and observer modules
// from module object and ClassType
void cleanup(Module& module);
","void setQuantType(QuantType quant_type) {
quant_type_ = quant_type;
}

  void setDebug(bool debug) {
    debug_ = debug;
  }

// Cleanup observer nodes from graph and observer modules
// from module object and ClassType
void cleanup(Module& module);
"
468,"// once
std::unordered_set<Value*> quantized_values_;
  QuantType quant_type_ = QuantType::STATIC;

// Map from original weight value to GraphFunction corresponding to the
// subgraph that includes the weight observer and dependent nodes.
std::unordered_map<Value*, std::unique_ptr<GraphFunction>>
weight_to_graph_fn_;
};
void InsertQuantDeQuantHelper::collectObserverNodesAndValueToQuantize(
","// once
std::unordered_set<Value*> quantized_values_;
// Map from original weight value to GraphFunction corresponding to the
// subgraph that includes the weight observer and dependent nodes.
std::unordered_map<Value*, std::unique_ptr<GraphFunction>>
weight_to_graph_fn_;

  QuantType quant_type_ = QuantType::STATIC;
  bool debug_ = false;
};
void InsertQuantDeQuantHelper::collectObserverNodesAndValueToQuantize(
"
469,"}
std::ostream& operator<<(std::ostream& os, GloballyUniqueId const& globalId) {
  return os << ""GloballyUniqueId("" << globalId.createdOn_ << "", ""
            << globalId.localId_ << "")"";
}
///////////////////////////  SerializedPyObj   ///////////////////////////
","}
std::ostream& operator<<(std::ostream& os, GloballyUniqueId const& globalId) {
  return os << ""GloballyUniqueId(created_on="" << globalId.createdOn_
            << "", local_id="" << globalId.localId_ << "")"";
}
///////////////////////////  SerializedPyObj   ///////////////////////////
"
470,"},
aliasAnalysisFromSchema()),
Operator(
         ""aten::local_value(RRef(t) self) -> t"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
TORCH_CHECK(
","},
aliasAnalysisFromSchema()),
Operator(
         ""aten::local_value(RRef(t) self) -> t(*)"",
[](Stack& stack) {
auto rref = pop(stack).toRRef();
TORCH_CHECK(
"
471,": start_version_{_start_version},
end_version_{_end_version},
sym_{_sym} {}

const uint64_t start_version_;
const uint64_t end_version_;
const Symbol sym_;
",": start_version_{_start_version},
end_version_{_end_version},
sym_{_sym} {}
const uint64_t start_version_;
const uint64_t end_version_;
const Symbol sym_;
"
472,"output_max);
}
Tensor Conv2dClampRun::operator()(
const Tensor& input,
    const c10::intrusive_ptr<vulkan::Conv2dOpContext>& op_context) {
return op_context->run(input);
}
","output_max);
}
Tensor conv2d_clamp_run(
const Tensor& input,
    const c10::intrusive_ptr<at::native::vulkan::Conv2dOpContext>& op_context) {
return op_context->run(input);
}
"
473,"// each graph is only quantized with one type of QScheme
std::unordered_map<Graph*, c10::QScheme> qscheme_for_graph_;
bool is_dynamic_ = false;
// Map from original weight value to GraphFunction corresponding to the
","// each graph is only quantized with one type of QScheme
std::unordered_map<Graph*, c10::QScheme> qscheme_for_graph_;
  // Set of quantized values, so that we quantize each value only
  // once
  std::unordered_set<Value*> quantized_values_;

bool is_dynamic_ = false;
// Map from original weight value to GraphFunction corresponding to the
"
474,"// replace uses of original output of the general op with quantized
// output
original_output->replaceAllUsesAfterNodeWith(quant, quantized_output);
  insertDeQuantForAllUse(graph, quantized_output, quantized_output);
}
void propagateDequantize(Value* output, const std::vector<Value*> inputs) {
","// replace uses of original output of the general op with quantized
// output
original_output->replaceAllUsesAfterNodeWith(quant, quantized_output);
  const auto& outputs =
      insertDeQuantForAllUse(graph, quantized_output, quantized_output);
  for (auto* output : outputs) {
    if (is_scalar) {
      // Convert the dequantized Tensor back to Scalar
      Node* item = insertItem(graph, output, FloatType::get());
      Value* scalar = item->output();
      output->replaceAllUsesAfterNodeWith(item, scalar);
      output = scalar;
    }
    quantized_values_.insert(output);
  }
}
void propagateDequantize(Value* output, const std::vector<Value*> inputs) {
"
475,"""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
    auto sizes = self.sizes();
    if (sizes[dim.value()] == 1) {
      if (keepdim) {
        result = at::zeros(sizes, self.options().dtype(at::kLong));
      } else {
        auto sizes_vec = sizes.vec();
        sizes_vec.erase(sizes_vec.begin() + dim.value());
        result = at::zeros(sizes_vec, self.options().dtype(at::kLong));
      }
      return result;
    }
in = self;
} else {
in = self.reshape({-1});
","""tensor with no elements because the operation does not have an identity"");
Tensor in;
if (dim) {
in = self;
} else {
in = self.reshape({-1});
"
476,"#include <torch/csrc/distributed/rpc/request_callback_impl.h>
#include <torch/csrc/distributed/rpc/utils.h>
#include <tensorpipe/channel/basic/context.h>
#ifdef TP_ENABLE_CMA
#include <tensorpipe/channel/cma/context.h>
#endif
#ifdef TP_ENABLE_SHM
#include <tensorpipe/transport/shm/context.h>
#endif
#include <tensorpipe/transport/uv/context.h>

#include <arpa/inet.h>
#include <ifaddrs.h>
#include <netdb.h>
","#include <torch/csrc/distributed/rpc/request_callback_impl.h>
#include <torch/csrc/distributed/rpc/utils.h>
#include <arpa/inet.h>
#include <ifaddrs.h>
#include <netdb.h>
"
477,"pipe,
[this, pipe](
const tensorpipe::Error& error, Message&& requestMessage) mutable {
        // TODO: Handle server pipe read error
if (error) {
          LOG(WARNING) << ""Server read message: "" << error.what();
return;
}
","pipe,
[this, pipe](
const tensorpipe::Error& error, Message&& requestMessage) mutable {
        // FIXME Find a way for the client to tell the server they are done with
        // the pipe and are intentionally shutting it down. Perhaps sending an
        // empty message?
if (error) {
          LOG(WARNING) << ""RPC agent for "" << workerInfo_.name_
                       << "" encountered error when reading incoming request: ""
                       << error.what();
return;
}
"
478,"tensorpipe::ContextOptions().name(workerInfo_.name_))),
addressStore_(std::move(addressStore)),
worldSize_(worldSize),
      opts_(std::move(opts)) {
collectNames();
// Initialize the time-series metrics tracking map
","tensorpipe::ContextOptions().name(workerInfo_.name_))),
addressStore_(std::move(addressStore)),
worldSize_(worldSize),
      opts_(std::move(opts)),
      processGroup_(std::move(processGroup)) {
collectNames();
// Initialize the time-series metrics tracking map
"
479,"return defaultIP;
}
void TensorPipeAgent::markFutureAsComplete(
std::shared_ptr<AtomicFutureMessage> futureMessage,
Message message) {
","return defaultIP;
}
void TensorPipeAgent::increaseCallCount(int32_t& count) {
  {
    std::unique_lock<std::mutex> lock(callCountMutex_);
    ++count;
  }
  callCountCV_.notify_all();
}

void TensorPipeAgent::decreaseCallCount(int32_t& count) {
  {
    std::unique_lock<std::mutex> lock(callCountMutex_);
    --count;
  }
  callCountCV_.notify_all();
}

void TensorPipeAgent::markFutureAsComplete(
std::shared_ptr<AtomicFutureMessage> futureMessage,
Message message) {
"
480,"Scalar value) {
if (isIntegralType(tensor1.scalar_type(), /*includeBool=*/ true)
&& isIntegralType(tensor2.scalar_type(), /*includeBool=*/ true)) {
    TORCH_WARN_ONCE(
      ""Integer division with addcdiv is deprecated, and in a future  "",
""release addcdiv will perform a true division of tensor1 and tensor2. "",
      ""The current addcdiv behavior can be replicated using floor_divide "",
""for integral inputs (self + value * tensor1 // tensor2) and "",
""division for float inputs (self + value * tensor1 / tensor2). "",
      ""The new addcdiv behavior can be implemented with true_divide "",
""(self + value * torch.true_divide(tensor1, tensor2)."");
}
checkBackend(""addcdiv_cpu"", result, self.options().backend());
","Scalar value) {
if (isIntegralType(tensor1.scalar_type(), /*includeBool=*/ true)
&& isIntegralType(tensor2.scalar_type(), /*includeBool=*/ true)) {
    TORCH_CHECK(false,
      ""Integer division with addcdiv is no longer supported, and in a future  "",
""release addcdiv will perform a true division of tensor1 and tensor2. "",
      ""The historic addcdiv behavior can be implemented using floor_divide "",
""for integral inputs (self + value * tensor1 // tensor2) and "",
""division for float inputs (self + value * tensor1 / tensor2). "",
      ""The future addcdiv behavior can be implemented with true_divide "",
""(self + value * torch.true_divide(tensor1, tensor2)."");
}
checkBackend(""addcdiv_cpu"", result, self.options().backend());
"
481,"torch::autograd::set_device(torch::autograd::CPU_DEVICE);
graph_task->owner_ = torch::autograd::CPU_DEVICE;
  while(!cpu_ready_queue->empty()) {
std::shared_ptr<GraphTask> local_graph_task;
{
// Scope this block of execution since NodeTask is not needed after this
","torch::autograd::set_device(torch::autograd::CPU_DEVICE);
graph_task->owner_ = torch::autograd::CPU_DEVICE;
  while (!cpu_ready_queue->empty()) {
std::shared_ptr<GraphTask> local_graph_task;
{
// Scope this block of execution since NodeTask is not needed after this
"
482,"}
}
// Emit mutating assignments like `foo[0] = bar`
void emitSubscriptAssign(
const SourceRange& stmtRange,
","}
}
  NamedValue emitValueToTensor(
      const NamedValue& value,
      const NamedValue& matchTypeOf) {
    // Add implicit conversion of int/float/bool types to tensors
    // Used in emitSubscriptAssign to convert:
    //   `tensor(...)[x] = 99` to `tensor(...)[x] = tensor(99)`
    // Mirrors the `valueToTensor` behavior in python_variable_indexing.cpp
    const auto kind = value.type()->kind();
    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||
        kind == c10::TypeKind::FloatType) {
      auto dtype = graph->insert(prim::dtype, {matchTypeOf}, {});
      auto device = graph->insert(prim::device, {matchTypeOf}, {});
      auto converted = graph->insert(
          aten::tensor,
          {value},
          {NamedValue(""dtype"", dtype), NamedValue(""device"", device)});
      return NamedValue(value.loc(), converted);
    }

    return value;
  }

// Emit mutating assignments like `foo[0] = bar`
void emitSubscriptAssign(
const SourceRange& stmtRange,
"
483,"rec._end();
}
void _call_end_callbacks_on_fut(
const at::Tensor& handle,
const c10::intrusive_ptr<c10::ivalue::Future>& fut) {
// Save and pass thread local state into the callback
at::ThreadLocalState tls_state;
  // Add a callback onto the future to mark run RecordFunction's end callbacks
  // when the future is completed.
  fut->addCallback(
      // Copy handle and tls_state by value to persist after the python
      // context manager is exited.
      [handle, tls_state = std::move(tls_state)]() {
TORCH_INTERNAL_ASSERT(
handle.defined(),
""Undefined RecordFunction handle. This can happen if the handle is ""
","rec._end();
}
c10::intrusive_ptr<c10::ivalue::Future> _call_end_callbacks_on_fut(
const at::Tensor& handle,
const c10::intrusive_ptr<c10::ivalue::Future>& fut) {
// Save and pass thread local state into the callback
at::ThreadLocalState tls_state;
  // Profiling callback that ends the associated record_function
  // and returns the value of the passed in future.
  std::function<c10::IValue(void)> futureProfilingFunc =
      [fut, handle, tls_state = std::move(tls_state)]() {
TORCH_INTERNAL_ASSERT(
handle.defined(),
""Undefined RecordFunction handle. This can happen if the handle is ""
"
484,"});
});
} else {
    AT_DISPATCH_FLOATING_TYPES(iter.dtype(), ""fmod_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t x, scalar_t d) -> scalar_t {
","});
});
} else {
    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.dtype(), ""fmod_cpu"", [&]() {
cpu_kernel_vec(
iter,
[](scalar_t x, scalar_t d) -> scalar_t {
"
485,"});
});
} else {
    AT_DISPATCH_FLOATING_TYPES(iter.dtype(), ""fmod_scalar_cpu"", [&]() {
const auto div = divisor.to<scalar_t>();
const auto div_vec = Vec256<scalar_t>(div);
cpu_kernel_vec(
","});
});
} else {
    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.dtype(), ""fmod_scalar_cpu"", [&]() {
const auto div = divisor.to<scalar_t>();
const auto div_vec = Vec256<scalar_t>(div);
cpu_kernel_vec(
"
486,"}
e->accept(this);
if (prec >= self_prec) {
      os() << ""("";
}
};
withParens(v->ret_val1());
","}
e->accept(this);
if (prec >= self_prec) {
      os() << "")"";
}
};
withParens(v->ret_val1());
"
487,"const void* caller) {
c10::Error e(file, line, condition, msg, (*GetFetchStackTrace())(), caller);
if (FLAGS_caffe2_use_fatal_for_enforce) {
    LOG(FATAL) << e.msg_stack()[0];
}
throw e;
}
","const void* caller) {
c10::Error e(file, line, condition, msg, (*GetFetchStackTrace())(), caller);
if (FLAGS_caffe2_use_fatal_for_enforce) {
    LOG(FATAL) << e.msg();
}
throw e;
}
"
488,"#include <torch/csrc/jit/runtime/profiling_record.h>
#include <torch/csrc/jit/passes/clear_profiling.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/runtime/graph_executor.h>
","#include <torch/csrc/jit/runtime/profiling_record.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/clear_profiling.h>
#include <torch/csrc/jit/passes/constant_propagation.h>
#include <torch/csrc/jit/runtime/graph_executor.h>
"
489,"texpr_fuser_enabled_ = val;
}
static bool tensorExprFuserEnabled() {
static const char* enable_c_str = std::getenv(""PYTORCH_TENSOREXPR"");
if (!enable_c_str) {
return texpr_fuser_enabled_;
","texpr_fuser_enabled_ = val;
}
bool tensorExprFuserEnabled() {
static const char* enable_c_str = std::getenv(""PYTORCH_TENSOREXPR"");
if (!enable_c_str) {
return texpr_fuser_enabled_;
"
490,"const Var* var_new = dynamic_cast<const Var*>(var_new_expr);
const Expr* start_new = start->accept_mutator(this);
const Expr* stop_new = stop->accept_mutator(this);
  Stmt* body_new = body->accept_mutator(this);
  if (!body_new) {
    return new Block({});
  }
const Expr* loops = new Sub(stop_new, start_new);
loops = loops->accept_mutator(this);
","const Var* var_new = dynamic_cast<const Var*>(var_new_expr);
const Expr* start_new = start->accept_mutator(this);
const Expr* stop_new = stop->accept_mutator(this);
  Stmt* body_new = body;
const Expr* loops = new Sub(stop_new, start_new);
loops = loops->accept_mutator(this);
"
491,"// we will transfer the caffe2_log_level setting to glog to override that.
FLAGS_minloglevel = std::min(FLAGS_caffe2_log_level, FLAGS_minloglevel);
// If caffe2_log_level is explicitly set, let's also turn on logtostderr.
  if (FLAGS_caffe2_log_level < google::GLOG_ERROR) {
FLAGS_logtostderr = 1;
}
// Also, transfer the caffe2_log_level verbose setting to glog.
","// we will transfer the caffe2_log_level setting to glog to override that.
FLAGS_minloglevel = std::min(FLAGS_caffe2_log_level, FLAGS_minloglevel);
// If caffe2_log_level is explicitly set, let's also turn on logtostderr.
  if (FLAGS_caffe2_log_level < google::GLOG_WARNING) {
FLAGS_logtostderr = 1;
}
// Also, transfer the caffe2_log_level verbose setting to glog.
"
492,"return self;
}
static auto registry = torch::RegisterOperators()
  .op(torch::RegisterOperators::options()
    .schema(""aten::resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"")
    .aliasAnalysis(AliasAnalysisKind::FROM_SCHEMA)
    .impl_unboxedOnlyKernel<decltype(resize_), &resize_>(DispatchKey::CPU))
  .op(torch::RegisterOperators::options()
    .schema(""aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)"")
    .aliasAnalysis(AliasAnalysisKind::FROM_SCHEMA)
    .impl_unboxedOnlyCatchAllKernel<decltype(resize_as_), &resize_as_>())
  ;
} // namespace native
} // namespace at
","return self;
}
TORCH_LIBRARY_IMPL(aten, CPU, m) {
  m.impl_UNBOXED(""resize_"", resize_);
}

TORCH_LIBRARY_IMPL(aten, CatchAll, m) {
  m.impl_UNBOXED(""resize_as_"", resize_as_);
}
} // namespace native
} // namespace at
"
493,"return qy;
}
// Keep the registry in the anonymous namespace.
namespace {

template <bool ReLUFused = false>
class QBatchNorm2d final : public torch::OperatorKernel {
 public:
  Tensor operator()(
      Tensor qx,
      Tensor weight,
      Tensor bias,
      Tensor mean,
      Tensor var,
      double eps,
      double output_scale,
      int64_t output_zero_point) {
    return q_batch_norm2d_impl<ReLUFused>(
        qx, weight, bias, mean, var, eps, output_scale, output_zero_point);
  }
};

template <bool ReLUFused = false>
class QBatchNorm3d final : public torch::OperatorKernel {
 public:
  Tensor operator()(
      Tensor qx,
      Tensor weight,
      Tensor bias,
      Tensor mean,
      Tensor var,
      double eps,
      double output_scale,
      int64_t output_zero_point) {
    return q_batch_norm3d_impl<ReLUFused>(
        qx, weight, bias, mean, var, eps, output_scale, output_zero_point);
  }
};

static auto registry = torch::RegisterOperators().op(
    ""quantized::batch_norm2d(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm2d<false>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm2d_relu(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm2d<true>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm3d(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm3d<false>>(
        DispatchKey::QuantizedCPU))
.op(
    ""quantized::batch_norm3d_relu(Tensor qx, ""
    ""Tensor weight, ""
    ""Tensor bias, ""
    ""Tensor mean, ""
    ""Tensor var, ""
    ""float eps, ""
    ""float output_scale, ""
    ""int output_zero_point) -> Tensor"",
    torch::RegisterOperators::options().kernel<QBatchNorm3d<true>>(
        DispatchKey::QuantizedCPU));
} // namespace
} // namespace native
} // namespace at
","return qy;
}
TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
  m.impl(""batch_norm2d"",      q_batch_norm2d_impl<false>);
  m.impl(""batch_norm2d_relu"", q_batch_norm2d_impl<true>);
  m.impl(""batch_norm3d"",      q_batch_norm3d_impl<false>);
  m.impl(""batch_norm3d_relu"", q_batch_norm3d_impl<true>);
}
} // namespace native
} // namespace at
"
494,"c10::RegisterOperators()
.op(""quantized::linear_prepack(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
.kernel<QLinearPackWeightInt8>(DispatchKey::QuantizedCPU))
.op(""quantized::linear_prepack_fp16(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
.kernel<QLinearPackWeightFp16>(DispatchKey::CPU))
.op(""_quantized::linear_prepack(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
.kernel<QLinearPackWeightInt8>(DispatchKey::QuantizedCPU))
.op(""_quantized::linear_prepack_fp16(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::PURE_FUNCTION)
.kernel<QLinearPackWeightFp16>(DispatchKey::CPU));
} // namespace
","c10::RegisterOperators()
.op(""quantized::linear_prepack(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
.kernel<QLinearPackWeightInt8>(DispatchKey::QuantizedCPU))
.op(""quantized::linear_prepack_fp16(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
.kernel<QLinearPackWeightFp16>(DispatchKey::CPU))
.op(""_quantized::linear_prepack(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
.kernel<QLinearPackWeightInt8>(DispatchKey::QuantizedCPU))
.op(""_quantized::linear_prepack_fp16(Tensor W, Tensor? B=None) -> Tensor W_prepack"",
c10::RegisterOperators::options()
            .aliasAnalysis(at::AliasAnalysisKind::FROM_SCHEMA)
.kernel<QLinearPackWeightFp16>(DispatchKey::CPU));
} // namespace
"
495,"#include <ATen/core/jit_type.h>
#include <aten/src/ATen/ExpandUtils.h>
#include <torch/csrc/api/include/torch/utils.h>
#include <torch/csrc/autograd/profiler.h>
#include <torch/csrc/jit/ir/ir.h>
","#include <ATen/core/jit_type.h>
#include <aten/src/ATen/ExpandUtils.h>
#include <c10/core/DefaultDtype.h>
#include <torch/csrc/api/include/torch/utils.h>
#include <torch/csrc/autograd/profiler.h>
#include <torch/csrc/jit/ir/ir.h>
"
496,"// Something is wrong if all variables contained in this bucket replica have
// already been marked as ready.
if (replica.pending == 0) {
    // Receiving a call to `mark_variable_ready` twice for the same variable
    // is only possible if the variable was initially deemed unused, and was
    // marked ready from the `prepare_for_backward` function, only to become
    // part of the autograd graph at a later point in time.
    TORCH_INTERNAL_ASSERT(has_marked_unused_parameters_);
    TORCH_CHECK(
        false,
""Expected to mark a variable ready only once. "",
"""",
        ""This error is caused by use of a module parameter outside the "",
        ""`forward` function. The return value of the `forward` function "",
        ""is inspected by the distributed data parallel wrapper to figure "",
        ""out if any of the module's parameters went unused. If this is the "",
        ""case, it knows they won't receive gradients in a backward pass. "",
        ""If any of those parameters are then used outside `forward`, this "",
        ""error condition is triggered. "",
        """",
        ""You can disable unused parameter detection by passing the keyword ""
        ""argument `find_unused_parameters=False` to "",
""`torch.nn.parallel.DistributedDataParallel`."");
}
if (bucket.expect_sparse_gradient) {
","// Something is wrong if all variables contained in this bucket replica have
// already been marked as ready.
if (replica.pending == 0) {
    const auto common_error = c10::str(
""Expected to mark a variable ready only once. "",
"""",
        ""This error is caused by one of the following reasons: "",
        ""1) Use of a module parameter outside the `forward` function. "",
        ""Please make sure model parameters are not shared across multiple "",
        ""concurrent forward-backward passes"",
        ""2) Reused parameters in multiple reentrant backward passes. For "",
        ""example, if you use multiple `checkpoint` functions to wrap the "",
        ""same part of your model, it would result in the same set of "",
        ""parameters been used by different reentrant backward passes "",
        ""multiple times, and hence marking a variable ready multiple times. "",
        ""DDP does not support such use cases yet."");
    TORCH_CHECK(
        has_marked_unused_parameters_,
        common_error,
        ""3) Incorrect unused parameter detection. The return value of the "",
        ""`forward` function is inspected by the distributed data parallel "",
        ""wrapper to figure out if any of the module's parameters went "",
        ""unused. For unused parameters, DDP would not expect gradients from "",
        ""then. However, if an unused parameter becomes part of the autograd "",
        ""graph at a later point in time (e.g., in a reentrant backward when "",
        ""using `checkpoint`), the gradient will show up unexpectedly. If all "",
        ""parameters in the model participate in the backward pass, you can "",
        ""disable unused parameter detection by passing the keyword argument "",
        ""`find_unused_parameters=False` to "",
""`torch.nn.parallel.DistributedDataParallel`."");
    TORCH_CHECK(!has_marked_unused_parameters_, common_error);
}
if (bucket.expect_sparse_gradient) {
"
497,"l.computeInline(l.getLoopBodyFor(tensorOutputs_[i]));
Tensor* tensor = tensorOutputs[i];
      const Var* index = tensor->arg(0);
int loopLevels = getTECudaPointwiseLoopLevels();
const int kDefaultLoopLevels = 2;
loopLevels = (loopLevels > 0) ? loopLevels : kDefaultLoopLevels;
","l.computeInline(l.getLoopBodyFor(tensorOutputs_[i]));
Tensor* tensor = tensorOutputs[i];
int loopLevels = getTECudaPointwiseLoopLevels();
const int kDefaultLoopLevels = 2;
loopLevels = (loopLevels > 0) ? loopLevels : kDefaultLoopLevels;
"
498,"const Expr* result = body->accept_mutator(this);
// Remove the caller/callee relationship.
      for (int i = 0; i < buf->ndim(); i++) {
const Var* func_callee_arg = dynamic_cast<const Var*>(func->arg(i));
auto iter = inline_mapping_.find(func_callee_arg);
if (iter == inline_mapping_.end()) {
","const Expr* result = body->accept_mutator(this);
// Remove the caller/callee relationship.
      for (size_t i = 0; i < buf->ndim(); i++) {
const Var* func_callee_arg = dynamic_cast<const Var*>(func->arg(i));
auto iter = inline_mapping_.find(func_callee_arg);
if (iter == inline_mapping_.end()) {
"
499,"void RpcAgent::cleanup() {
rpcAgentRunning_.store(false);
if (rpcRetryThread_.joinable()) {
    rpcRetryMapCV_.notify_one();
rpcRetryThread_.join();
}
}
","void RpcAgent::cleanup() {
rpcAgentRunning_.store(false);
  // We must notify the condition variable so it stops waiting in the
  // retry thread, otherwise this thread cannot be joined.
  rpcRetryMapCV_.notify_one();
if (rpcRetryThread_.joinable()) {
rpcRetryThread_.join();
}
}
"
500,"} else {
Tensor tensor;
if (tensor_.is_quantized()) {
      Tensor tensor = tensor_.dequantize().to(kCPU, kDouble).contiguous();
} else {
tensor = tensor_.to(kCPU, kDouble).contiguous();
}
","} else {
Tensor tensor;
if (tensor_.is_quantized()) {
      tensor = tensor_.dequantize().to(kCPU, kDouble).contiguous();
} else {
tensor = tensor_.to(kCPU, kDouble).contiguous();
}
"
501,"// Massage a C++ variable_list into a Python arguments tuple
auto num_inputs = inputs.size();
THPObjectPtr pyInputs(PyTuple_New(num_inputs));
  if (!pyInputs) throw python_error();
auto& output_info = py_fn->output_info;
for (size_t i = 0; i < num_inputs; ++i) {
PyObject* input;
","// Massage a C++ variable_list into a Python arguments tuple
auto num_inputs = inputs.size();
THPObjectPtr pyInputs(PyTuple_New(num_inputs));
  if (!pyInputs) throw_python_error();
auto& output_info = py_fn->output_info;
for (size_t i = 0; i < num_inputs; ++i) {
PyObject* input;
"
502,"}
}
bool InsertObserversHelper::valueNeedsToBeQuantized(Value* v) {
if (!v->type()->isSubtypeOf(TensorType::get()) ||
isBiasOfConvOrLinear(v)) {
","}
}
// TODO: remove this as a class method
bool InsertObserversHelper::valueNeedsToBeQuantized(Value* v) {
if (!v->type()->isSubtypeOf(TensorType::get()) ||
isBiasOfConvOrLinear(v)) {
"
503,"/* need_inputs */ false,
/* sampled */ false);
#endif
    JITCallGuard guard;
}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
preModuleLoadSetup();
module_ = torch::jit::load(std::move(modelPath->toStdString()));
module_.eval();
}
","/* need_inputs */ false,
/* sampled */ false);
#endif
}
PytorchJni(facebook::jni::alias_ref<jstring> modelPath) {
preModuleLoadSetup();
    JITCallGuard guard;
module_ = torch::jit::load(std::move(modelPath->toStdString()));
module_.eval();
}
"
504,"if (self_or_result.numel() == 0) {
return self_or_result;
} else if (contraction_size == 0) {
    return self_or_result.zero_();
}
auto batch_items_contiguous_or_transposed = [&](const Tensor& t) {
","if (self_or_result.numel() == 0) {
return self_or_result;
} else if (contraction_size == 0) {
    if (is_bmm_out) {
      return self_or_result.zero_();
    } else {
      return self_or_result.mul_(beta);
    }
}
auto batch_items_contiguous_or_transposed = [&](const Tensor& t) {
"
505,"auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1);

  // We have to manually set the diagonal to 0 due to an issue with MAGMA's getrf_batched routine
  if (self.dim() > 2 && self.is_cuda()) {
    u_diagonal.index_put_(infos.nonzero_numpy(), at::zeros({}, self.options()));
  }
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
","auto n = self.size(-1);
auto num_exchanges = (at::arange(1, n + 1, pivs.options()) != pivs).sum(-1, /*keepdim=*/false, /*dtype=*/self.scalar_type()).fmod_(2);
auto u_diagonal = lu.diagonal(/*offset=*/0, /*dim1=*/-2, /*dim2=*/-1);
return std::tuple<Tensor, Tensor>(num_exchanges.mul_(-2).add_(1), u_diagonal);
}
"
506,"auto slot = getAttributeSlot(name);
attributeNames_.erase(attributeNames_.begin() + slot);
attributeTypes_.erase(attributeTypes_.begin() + slot);
}
void ClassType::addMethod(Function* method) {
","auto slot = getAttributeSlot(name);
attributeNames_.erase(attributeNames_.begin() + slot);
attributeTypes_.erase(attributeTypes_.begin() + slot);
  if (is_module()) {
    parameterSlots_->erase(parameterSlots_->begin() + slot);
  }
}
void ClassType::addMethod(Function* method) {
"
507,"// Note that at this point cuda_malloc_with_retry has already returned all
// possible ""cached"" memory to the driver. The only remaining ""cached""
// memory is split from a larger block that is partially in-use.
        AT_ERROR(
""CUDA out of memory. Tried to allocate "", format_size(alloc_size),
"" (GPU "", device, ""; "",
format_size(device_total), "" total capacity; "",
","// Note that at this point cuda_malloc_with_retry has already returned all
// possible ""cached"" memory to the driver. The only remaining ""cached""
// memory is split from a larger block that is partially in-use.
        TORCH_CHECK_WITH(CUDAOutOfMemoryError, false,
""CUDA out of memory. Tried to allocate "", format_size(alloc_size),
"" (GPU "", device, ""; "",
format_size(device_total), "" total capacity; "",
"
508,") {
for (int64_t i = 0; i < index_dim_size; ++i) {
int64_t idx_dim = index_data[i * index_dim_stride];
TORCH_CHECK(idx_dim >= 0 && idx_dim < self_dim_size,
          ""index "", idx_dim,
          "" is out of bounds for dimension "", dim,
          "" with size "", self_dim_size);
self_data[idx_dim * self_dim_stride] += src_data[i * src_dim_stride];
}
    }, /*serial_exec=*/true
  );
}
} // anonymous napespace
",") {
for (int64_t i = 0; i < index_dim_size; ++i) {
int64_t idx_dim = index_data[i * index_dim_stride];
        // we are not putting idx_dim in the error message because it disables
        // loop optimizations in clang-7
TORCH_CHECK(idx_dim >= 0 && idx_dim < self_dim_size,
                    ""index "", index_data[i * index_dim_stride], "" is out of bounds for dimension "", dim,
                    "" with size "", self_dim_size);
self_data[idx_dim * self_dim_stride] += src_data[i * src_dim_stride];
}
    },
      /*serial_exec=*/true);
}
} // anonymous napespace
"
509,"data = (scalar_t*)cpu_data.get();
THCudaCheck(cudaMemcpy(data, THWStorage_(data)(LIBRARY_STATE self), size * sizeof(scalar_t), cudaMemcpyDeviceToHost));
#endif
  if (torch::utils::THP_nativeByteOrder() ==
      torch::utils::THPByteOrder::THP_LITTLE_ENDIAN)
    doWrite(fd, &size, sizeof(int64_t));
  else {
    int64_t nsize; // convert big endian cpu to little endian storage
    torch::utils::THP_encodeInt64Buffer(
        (uint8_t*)&nsize,
        (const int64_t*)&size,
        torch::utils::THPByteOrder::THP_LITTLE_ENDIAN,
        1);
    doWrite(fd, &nsize, sizeof(int64_t));
}
// fast track for bytes and little endian
if (sizeof(scalar_t) == 1 ||
","data = (scalar_t*)cpu_data.get();
THCudaCheck(cudaMemcpy(data, THWStorage_(data)(LIBRARY_STATE self), size * sizeof(scalar_t), cudaMemcpyDeviceToHost));
#endif
  if (save_size) {
    if (torch::utils::THP_nativeByteOrder() ==
        torch::utils::THPByteOrder::THP_LITTLE_ENDIAN)
      doWrite(fd, &size, sizeof(int64_t));
    else {
      int64_t nsize; // convert big endian cpu to little endian storage
      torch::utils::THP_encodeInt64Buffer(
          (uint8_t*)&nsize,
          (const int64_t*)&size,
          torch::utils::THPByteOrder::THP_LITTLE_ENDIAN,
          1);
      doWrite(fd, &nsize, sizeof(int64_t));
    }
}
// fast track for bytes and little endian
if (sizeof(scalar_t) == 1 ||
"
510,"/**
* Gets a nondeterministic random number from /dev/urandom or time,
* seeds the CPUGenerator with it and then returns that number.
 *
* FIXME: You can move this function to Generator.cpp if the algorithm
* in getNonDeterministicRandom is unified for both CPU and CUDA
*/
","/**
* Gets a nondeterministic random number from /dev/urandom or time,
* seeds the CPUGenerator with it and then returns that number.
 *
* FIXME: You can move this function to Generator.cpp if the algorithm
* in getNonDeterministicRandom is unified for both CPU and CUDA
*/
"
511,"/**
* Gets a random 32 bit unsigned integer from the engine
 *
* See Note [Acquire lock when using random generators]
*/
uint32_t CPUGenerator::random() {
","/**
* Gets a random 32 bit unsigned integer from the engine
 *
* See Note [Acquire lock when using random generators]
*/
uint32_t CPUGenerator::random() {
"
512,"}
});
});
}
if (!result.is_contiguous()) {
","}
});
});
  } else {
    AT_DISPATCH_ALL_TYPES(r.scalar_type(), ""logspace_cpu"", [&]() {
      double scalar_base = static_cast<double>(base); // will be autopromoted anyway
      scalar_t scalar_start = start.to<scalar_t>();
      scalar_t scalar_end = end.to<scalar_t>();
      scalar_t *data_ptr = r.data_ptr<scalar_t>();
      double step = static_cast<double>(scalar_end - scalar_start) / (steps-1);
      at::parallel_for(0, steps, internal::GRAIN_SIZE, [&](int64_t p_begin, int64_t p_end) {
        for (int64_t i=p_begin; i < p_end; i++) {
          data_ptr[i] = std::pow(scalar_base, scalar_start + step*i);
        }
      });
    });
}
if (!result.is_contiguous()) {
"
513,".kernel<QLinearDynamicInt8<true>>(DispatchKey::CPUTensorId))
.op(""quantized::linear_dynamic_fp16(Tensor X, Tensor W_prepack) -> Tensor Y"",
torch::RegisterOperators::options()
                .kernel<QLinearDynamicFp16<true>>(DispatchKey::CPUTensorId))
        .op(""quantized::linear_relu_dynamic_fp16(Tensor X, Tensor W_prepack) -> Tensor Y"",
            torch::RegisterOperators::options()
                .kernel<QLinearDynamicFp16<true>>(DispatchKey::CPUTensorId));
} // namespace
} // namespace native
",".kernel<QLinearDynamicInt8<true>>(DispatchKey::CPUTensorId))
.op(""quantized::linear_dynamic_fp16(Tensor X, Tensor W_prepack) -> Tensor Y"",
torch::RegisterOperators::options()
                .kernel<QLinearDynamicFp16<false>>(DispatchKey::CPUTensorId));
} // namespace
} // namespace native
"
514,"const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = unsafe_chunk_no_version_check(gates, 4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
","const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = gates.chunk(4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
"
515,"}
void rebase_history(const Variable& self, Edge gradient_edge) {
    TORCH_INTERNAL_ASSERT(gradient_edge.function != nullptr);
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<DifferentiableViewMeta*>(get_autograd_meta(self));
      TORCH_INTERNAL_ASSERT(diff_view_meta->allow_rebase_history);
      TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
      TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
gradient_edge.function->num_inputs() == 1,
""Functions which modify views in-place must return a single Variable"");
","}
void rebase_history(const Variable& self, Edge gradient_edge) {
    AT_ASSERT(gradient_edge.function != nullptr);
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<DifferentiableViewMeta*>(get_autograd_meta(self));
      AT_ASSERT(gradient_edge.input_nr == 0);
      AT_ASSERT(gradient_edge.function);
TORCH_CHECK(
gradient_edge.function->num_inputs() == 1,
""Functions which modify views in-place must return a single Variable"");
"
516,"}
void rebase_history(const Variable& self, Edge gradient_edge) {
    AT_ASSERT(gradient_edge.function != nullptr);
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<DifferentiableViewMeta*>(get_autograd_meta(self));
      AT_ASSERT(gradient_edge.input_nr == 0);
      AT_ASSERT(gradient_edge.function);
TORCH_CHECK(
gradient_edge.function->num_inputs() == 1,
""Functions which modify views in-place must return a single Variable"");
","}
void rebase_history(const Variable& self, Edge gradient_edge) {
    TORCH_INTERNAL_ASSERT(gradient_edge.function != nullptr);
if (self.is_view()) {
// NB: is_view() ==> get_autograd_meta()
auto diff_view_meta = static_cast<DifferentiableViewMeta*>(get_autograd_meta(self));
      TORCH_INTERNAL_ASSERT(diff_view_meta->allow_rebase_history);
      TORCH_INTERNAL_ASSERT(gradient_edge.input_nr == 0);
      TORCH_INTERNAL_ASSERT(gradient_edge.function);
TORCH_CHECK(
gradient_edge.function->num_inputs() == 1,
""Functions which modify views in-place must return a single Variable"");
"
517,"const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = gates.chunk(4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
","const auto gates = params.linear_hh(hx).add_(
pre_compute_input ? input : params.linear_ih(input));
    auto chunked_gates = unsafe_chunk_no_version_check(gates, 4, 1);
auto ingate = chunked_gates[0].sigmoid_();
auto forgetgate = chunked_gates[1].sigmoid_();
auto cellgate = chunked_gates[2].tanh_();
"
518,"// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisted and redesigned if we want to
// suppot backward compatibility in future.
namespace torch {
namespace jit {
","// Note that currently the backward compatibility is not supported by bytecode.
// This format and process need to be revisted and redesigned if we want to
// support backward compatibility in future.
namespace torch {
namespace jit {
"
519,"// TreeTokens will be used to label nodes of the graph, if the nodes will fit
// our mm/add tree pattern. Basically we do dynamic programming on DAGs, where
// when we reach node N with inputs A and B, then A and B have already been
// procesed, and we can try to unify their TreeTokens (if they have them)
// and build a larger tree.
struct TreeToken {
uint64_t tree_size = 0; // NOTE: measured in number of leaves i.e. mm ops
","// TreeTokens will be used to label nodes of the graph, if the nodes will fit
// our mm/add tree pattern. Basically we do dynamic programming on DAGs, where
// when we reach node N with inputs A and B, then A and B have already been
// processed, and we can try to unify their TreeTokens (if they have them)
// and build a larger tree.
struct TreeToken {
uint64_t tree_size = 0; // NOTE: measured in number of leaves i.e. mm ops
"
520,"// Remove observer modules from last one to first one in order to
// reduce the time complexity, assuming all the observer modules
// are added after the existing modules, we'll have complexity of
  // O(N) where N is number of observer moduels with this optimization
if (observer_modules_to_remove_.count(g)) {
const auto& observers = observer_modules_to_remove_.at(g);
for (int64_t i = observers.size() - 1; i >= 0; --i) {
","// Remove observer modules from last one to first one in order to
// reduce the time complexity, assuming all the observer modules
// are added after the existing modules, we'll have complexity of
  // O(N) where N is number of observer modules with this optimization
if (observer_modules_to_remove_.count(g)) {
const auto& observers = observer_modules_to_remove_.at(g);
for (int64_t i = observers.size() - 1; i >= 0; --i) {
"
521,"return false;
}
// Applies implict conversion from value trying to turn it into type
// concrete_type. It succeeds if `return_value->isSubtypeOf(concrete_type)`
Value* tryConvertToType(
const SourceRange& loc,
","return false;
}
// Applies implicit conversion from value trying to turn it into type
// concrete_type. It succeeds if `return_value->isSubtypeOf(concrete_type)`
Value* tryConvertToType(
const SourceRange& loc,
"
522,"case prim::ListUnpack:
case prim::PythonOp:
case prim::GetAttr:
    case prim::unchecked_cast:
return analyzeExtractor(node);
case prim::ConstantChunk:
return analyzeChunk(node);
case prim::BroadcastingChunk:
","case prim::ListUnpack:
case prim::PythonOp:
case prim::GetAttr:
return analyzeExtractor(node);
    case prim::unchecked_cast:
      return makePointerTo(node->output(), node->input());
case prim::ConstantChunk:
return analyzeChunk(node);
case prim::BroadcastingChunk:
"
523,"MemoryFormat memory_format=MemoryFormat::Contiguous) {
AT_ASSERT(options.device().is_cpu());
native::check_size_nonnegative(sizes);
  auto* allocator = at::getCPUAllocator();
int64_t nelements = at::prod_intlist(sizes);
auto dtype = options.dtype();
TORCH_CHECK(isQIntType(typeMetaToScalarType(dtype)),
","MemoryFormat memory_format=MemoryFormat::Contiguous) {
AT_ASSERT(options.device().is_cpu());
  at::Allocator* allocator = at::getCPUAllocator();

#ifdef USE_PYTORCH_QNNPACK
  if (at::globalContext().qEngine() == at::QEngine::QNNPACK) {
    static QAllocator qallocator;
    allocator = &qallocator;
  }
#endif

native::check_size_nonnegative(sizes);
int64_t nelements = at::prod_intlist(sizes);
auto dtype = options.dtype();
TORCH_CHECK(isQIntType(typeMetaToScalarType(dtype)),
"
