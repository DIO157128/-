,source,target
0,"repository, run:
python -m tools.autograd.gen_autograd \
       build/aten/src/ATen/Declarations.yaml \
aten/src/ATen/native/native_functions.yaml \
aten/src/ATen/native/tags.yaml \
$OUTPUT_DIR \
","repository, run:
python -m tools.autograd.gen_autograd \
aten/src/ATen/native/native_functions.yaml \
aten/src/ATen/native/tags.yaml \
$OUTPUT_DIR \
"
1,"# This function is for debug use only.
# onnx_shape_inference = False by default.
def _set_onnx_shape_inference(onnx_shape_inference: bool):
GLOBALS.onnx_shape_inference = onnx_shape_inference
","# This function is for debug use only.
# onnx_shape_inference = True by default.
def _set_onnx_shape_inference(onnx_shape_inference: bool):
GLOBALS.onnx_shape_inference = onnx_shape_inference
"
2,"metadata=_create_shard_metadata(tensor.size()),
)
def _create_checkpoint_shard_for(storage: TensorStorageMetadata) -> ShardStorageMetadata:
    return ShardStorageMetadata(
        # The metadata device is not used during loading.
        shard_metadata=_create_shard_metadata(storage.size),
        storage_key=storage.storage_key,
    )

def _reshard_and_prepare_read_request(
state_dict: Dict[str, Any], metadata_from_storage: Metadata
) -> Tuple[List[BytesReadRequest], List[TensorReadRequest]]:
","metadata=_create_shard_metadata(tensor.size()),
)
def _reshard_and_prepare_read_request(
state_dict: Dict[str, Any], metadata_from_storage: Metadata
) -> Tuple[List[BytesReadRequest], List[TensorReadRequest]]:
"
3,"""""""
tensor_read_requests = []
bytes_read_requests = []
for fqn, obj in state_dict.items():
md = metadata_from_storage.state_dict_metadata[fqn]
if isinstance(obj, ShardedTensor):
","""""""
tensor_read_requests = []
bytes_read_requests = []
    storage_md = cast(Dict[MetadataIndex, str], metadata_from_storage.storage_data)
for fqn, obj in state_dict.items():
md = metadata_from_storage.state_dict_metadata[fqn]
if isinstance(obj, ShardedTensor):
"
4,"bytes_io = io.BytesIO()
brr = BytesReadRequest(
bytes=bytes_io,
                    storage_key=md.storage_key,
fqn=fqn
)
bytes_read_requests.append(brr)
","bytes_io = io.BytesIO()
brr = BytesReadRequest(
bytes=bytes_io,
                    storage_key=storage_md[MetadataIndex(fqn)],
fqn=fqn
)
bytes_read_requests.append(brr)
"
5,"in memory) its behavior is undefined.
Args:
    size (tuple of ints): the shape of the output tensor
    stride (tuple of ints): the strides of the output tensor
Keyword args:
{dtype}
","in memory) its behavior is undefined.
Args:
    size (tuple of int): the shape of the output tensor
    stride (tuple of int): the strides of the output tensor
Keyword args:
{dtype}
"
6,"Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Example:
>>> with torch.cuda.profiler.profile():
","Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
            Default: ``False``
Example:
>>> with torch.cuda.profiler.profile():
"
7,"has_sparse_grad: bool):
for i, param in enumerate(params):
        d_p = d_p_list[i]
if weight_decay != 0:
d_p = d_p.add(param, alpha=weight_decay)
","has_sparse_grad: bool):
for i, param in enumerate(params):
        d_p = d_p_list[i] if not maximize else -d_p_list[i]
if weight_decay != 0:
d_p = d_p.add(param, alpha=weight_decay)
"
8,"# TODO: might need to update according to supported input types
""torch.ops.aten.add"": None,
""torch.ops.aten.sub"": None,
            ""torch.ops.aten.rsub"": None,
""torch.ops.aten.div"": None,
""torch.ops.aten.atan2"": None,
""torch.ops.aten.mul"": None,
","# TODO: might need to update according to supported input types
""torch.ops.aten.add"": None,
""torch.ops.aten.sub"": None,
            # ""torch.ops.aten.rsub"": None,    # rsub decomp is supported at aten2aten level
""torch.ops.aten.div"": None,
""torch.ops.aten.atan2"": None,
""torch.ops.aten.mul"": None,
"
9,"with preserve_rng_state():
# Set input tensors that require grad to leaves
flat_tensor_args = pytree.tree_map(
                        lambda x: x.detach().requires_grad_(x.requires_grad), flat_tensor_args
)
with torch.set_grad_enabled(grad_state):
out = flat_fn(*flat_tensor_args)
","with preserve_rng_state():
# Set input tensors that require grad to leaves
flat_tensor_args = pytree.tree_map(
                        lambda x: x.detach().requires_grad_(x.requires_grad)
                        if isinstance(x, Tensor) else x, flat_tensor_args
)
with torch.set_grad_enabled(grad_state):
out = flat_fn(*flat_tensor_args)
"
10,"mem_sz = _size_of(node.meta['tensor_meta'])
# Heuristic to bias towards nodes closer to the backwards pass
        mem_sz = int(mem_sz + node.dist_from_fw)
if is_materialized(node):
return mem_sz
","mem_sz = _size_of(node.meta['tensor_meta'])
# Heuristic to bias towards nodes closer to the backwards pass
        mem_sz = int(mem_sz + node.dist_from_bw)
if is_materialized(node):
return mem_sz
"
11,"fw_compiler: Callable,
bw_compiler: Optional[Callable] = None,
partition_fn: Callable = default_partition,
        decompositions: Dict = {},
hasher_type: str = ""StaticShapeHasher"",
static_argnums: Optional[Tuple[int]] = None,
) -> Callable:
","fw_compiler: Callable,
bw_compiler: Optional[Callable] = None,
partition_fn: Callable = default_partition,
        decompositions: Optional[Dict] = None,
hasher_type: str = ""StaticShapeHasher"",
static_argnums: Optional[Tuple[int]] = None,
) -> Callable:
"
12,"return wrapped
def make_fx(f, decomposition_table={}):
@functools.wraps(f)
def wrapped(*args):
phs = pytree.tree_map(lambda x: fx.PH, args)
","return wrapped
def make_fx(f, decomposition_table=None):
    if decomposition_table is None:
        decomposition_table = {}

@functools.wraps(f)
def wrapped(*args):
phs = pytree.tree_map(lambda x: fx.PH, args)
"
13,"'logaddexp',
'logaddexp2',
'lcm',
'maximum',
'minimum',
'mul.Tensor',
","'logaddexp',
'logaddexp2',
'lcm',
    '_linalg_check_errors',
'maximum',
'minimum',
'mul.Tensor',
"
14,"# be a scalar or a vector of size [C], and in the forward pass it's
# broadcast against [N, C, ...]. So now, we need to do the corresponding
# reduction, which is harder than we'd like...
    cur_weight = weight
    for _ in range(2, grad_output.dim()):
        cur_weight = cur_weight.unsqueeze(-1)
input_grad = torch.where(self > 0, grad_output, cur_weight * grad_output)
weight_grad_collector = torch.where(self > 0, grad_output.new_zeros(()), self * grad_output)
out = weight_grad_collector.sum_to_size(cur_weight.shape)
","# be a scalar or a vector of size [C], and in the forward pass it's
# broadcast against [N, C, ...]. So now, we need to do the corresponding
# reduction, which is harder than we'd like...
    cur_weight = _unsqueeze_to_dim(weight, self.dim() - 1)
input_grad = torch.where(self > 0, grad_output, cur_weight * grad_output)
weight_grad_collector = torch.where(self > 0, grad_output.new_zeros(()), self * grad_output)
out = weight_grad_collector.sum_to_size(cur_weight.shape)
"
15,"@register_decomposition(aten.binary_cross_entropy_backward)
def binary_cross_entropy_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Optional[Tensor] = None, reduction: int = Reduction.MEAN.value) -> Tensor:
    if weight is None:
        weight = self.new_ones(())
    result = weight * (self - target) / self / (1 - self)
if reduction == Reduction.MEAN.value:
        result = result * (1.0 / self.numel())
    return result * grad_output
@register_decomposition(aten._euclidean_dist)
","@register_decomposition(aten.binary_cross_entropy_backward)
def binary_cross_entropy_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Optional[Tensor] = None, reduction: int = Reduction.MEAN.value) -> Tensor:
    EPSILON = 1e-12
    result = grad_output * (self - target) / torch.clamp(self * (1 - self), min=EPSILON)
    if weight is not None:
        result = result * weight
if reduction == Reduction.MEAN.value:
        result = result / self.numel()
    return result
@register_decomposition(aten._euclidean_dist)
"
16,"if node.name not in forward_node_names:
continue
# Since we can't save tuple of tensor values, we need to flatten out what we're saving
        if 'tensor_meta' not in node.meta:
users = node.users
assert all([user.target == operator.getitem for user in users])
for user in users:
","if node.name not in forward_node_names:
continue
# Since we can't save tuple of tensor values, we need to flatten out what we're saving
        if 'tensor_meta' not in node.meta and node.op == 'call_function':
users = node.users
assert all([user.target == operator.getitem for user in users])
for user in users:
"
17,"aten.hardswish_backward,
aten.tanh_backward,
aten.silu_backward,
]
)
default_decompositions = get_decompositions(default_decompositions)
","aten.hardswish_backward,
aten.tanh_backward,
aten.silu_backward,
        aten.cudnn_batch_norm,
        aten.cudnn_batch_norm_backward,
]
)
default_decompositions = get_decompositions(default_decompositions)
"
18,"return x
# @register_decomposition(aten.cudnn_batch_norm)
# def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
#     a, b, c = aten.native_batch_norm(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon)
#     return (a,b, c, aten.new_empty(input, (1,)))

# @register_decomposition(aten.cudnn_batch_norm_backward)
# def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], save_mean: Optional[Tensor], save_var: Optional[Tensor], epsilon: float, reserveSpace: Tensor):
#     return aten.native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var, True, epsilon, [True, True, True])
","return x
@register_decomposition(aten.cudnn_batch_norm)
def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: bool, exponential_average_factor: float, epsilon: float):
    a, b, c = aten.native_batch_norm(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon)
    # Cudnn return running mean and variance when training is True
    if training:
        return (a, b, c, input.new_zeros((1,)))
    return (a, input.new_zeros((1,)), input.new_zeros((1,)), input.new_zeros((1,)))


@register_decomposition(aten.cudnn_batch_norm_backward)
def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], save_mean: Optional[Tensor], save_var: Optional[Tensor], epsilon: float, reserveSpace: Tensor):
    return aten.native_batch_norm_backward(grad_output, input, weight, running_mean, running_var, save_mean, save_var, True, epsilon, [True, True, True])
"
19,"else:
d_weight = aten.mul(grad_out, x_hat)
else:
        d_weight = aten.new_empty(input, (0,))
if output_mask[2] and bias is not None:
if len(outer_dim_indices) > 0:
","else:
d_weight = aten.mul(grad_out, x_hat)
else:
        d_weight = None
if output_mask[2] and bias is not None:
if len(outer_dim_indices) > 0:
"
20,"To put the state back into a model, use `load_state`.
""""""
    weights, weight_descriptors = extract_weights(model)
    buffers, buf_descriptors = extract_buffers(model)
def fun(weights, buffers, data):
mutable_model = copy.deepcopy(model)
","To put the state back into a model, use `load_state`.
""""""
    weights, weight_descriptors, _ = extract_weights(model)
    buffers, buf_descriptors, _ = extract_buffers(model)
def fun(weights, buffers, data):
mutable_model = copy.deepcopy(model)
"
21,"This is the callable object returned by :func:`make_functional`.
""""""
    def __init__(self, stateless_model, param_names):
super(FunctionalModule, self).__init__()
self.stateless_model = stateless_model
self.param_names = param_names
        self.split_names = make_split_names(param_names)
@staticmethod
def _create_from(model):
# TODO: We don't need to copy the model to create a stateless copy
model_copy = copy.deepcopy(model)
        params, param_names = extract_weights(model_copy)
        return FunctionalModule(model_copy, param_names), params
def forward(self, params, *args, **kwargs):
# Temporarily load the state back onto self.stateless_model
        old_state = _swap_state(self.stateless_model, self.split_names, params)
try:
return self.stateless_model(*args, **kwargs)
finally:
# Remove the loaded state on self.stateless_model
            _swap_state(self.stateless_model, self.split_names, old_state)
def make_functional(model: nn.Module):
","This is the callable object returned by :func:`make_functional`.
""""""
    def __init__(self, stateless_model, param_names, names_map):
super(FunctionalModule, self).__init__()
self.stateless_model = stateless_model
self.param_names = param_names
        self.names_map = names_map
@staticmethod
def _create_from(model):
# TODO: We don't need to copy the model to create a stateless copy
model_copy = copy.deepcopy(model)
        params, param_names, names_map = extract_weights(model_copy)
        return FunctionalModule(model_copy, param_names, names_map), params
def forward(self, params, *args, **kwargs):
# Temporarily load the state back onto self.stateless_model
        old_state = _swap_state(self.stateless_model, self.names_map, params)
try:
return self.stateless_model(*args, **kwargs)
finally:
# Remove the loaded state on self.stateless_model
            _swap_state(self.stateless_model, self.names_map, old_state)
def make_functional(model: nn.Module):
"
22,"compile_cache = None
# Polyfilled from pytorch core while we figure out the `remove_duplicate` issues.
def _named_members(mod, get_members_fn, prefix='', recurse=True, remove_duplicate=True):
    r""""""Helper method for yielding various names + members of modules.""""""
    memo = set()
    modules = mod.named_modules(prefix=prefix, remove_duplicate=remove_duplicate) if recurse else [(prefix, mod)]
    for module_prefix, module in modules:
        members = get_members_fn(module)
        for k, v in members:
            if v is None or v in memo:
                continue
            if remove_duplicate:
                memo.add(v)
            name = module_prefix + ('.' if module_prefix else '') + k
            yield name, v


def _named_parameters(mod, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True):
    gen = _named_members(
        mod,
        lambda module: module._parameters.items(),
        prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)
    for elem in gen:
        yield elem


def _named_buffers(mod, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True):
    gen = _named_members(
        mod,
        lambda module: module._buffers.items(),
        prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)
    for elem in gen:
        yield elem


def aot_module(mod, *args, **kwargs):
def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
","compile_cache = None
def aot_module(mod, *args, **kwargs):
def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
"
23,"Note that this function modifies the model in place and after this
call, mod.parameters() will be empty.
""""""
orig_params = tuple(mod.parameters())
# Remove all the parameters in the model
names = []
for name, p in list(mod.named_parameters()):
","Note that this function modifies the model in place and after this
call, mod.parameters() will be empty.
""""""
    num_orig_params_with_duplicates = len(tuple(_named_parameters(mod, remove_duplicate=False)))
orig_params = tuple(mod.parameters())
    if len(orig_params) != num_orig_params_with_duplicates:
        raise_parameter_tying_error()

# Remove all the parameters in the model
names = []
for name, p in list(mod.named_parameters()):
"
24,"def clamp_max(self: Tensor, min: float):
return aten.clamp(self, max=max)
@register_decomposition(aten._fused_dropout)
def _fused_dropout_decomposition(input, p, generator=None):
mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
","def clamp_max(self: Tensor, min: float):
return aten.clamp(self, max=max)

@register_decomposition(aten._fused_dropout)
def _fused_dropout_decomposition(input, p, generator=None):
mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
"
25,"if it % 100 == 0:
print('Iteration %d -- Outer Loss: %.4f' % (it, loss2))
    losses.append(loss2)
t_A = torch.tensor(0.0).uniform_(0.1, 0.5)
t_b = torch.tensor(0.0).uniform_(0.0, math.pi)
","if it % 100 == 0:
print('Iteration %d -- Outer Loss: %.4f' % (it, loss2))
    losses.append(loss2.detach())
t_A = torch.tensor(0.0).uniform_(0.1, 0.5)
t_b = torch.tensor(0.0).uniform_(0.0, math.pi)
"
26,"nnc_jit,
memory_efficient_fusion,
debug_compile,
)
from .._src.partitioners import (
    partition_with_recompute_fwd_in_bwd,
default_partition,
draw_graph,
draw_joint_graph,
","nnc_jit,
memory_efficient_fusion,
debug_compile,
    print_compile
)
from .._src.partitioners import (
    min_cut_rematerialization_partition,
default_partition,
draw_graph,
draw_joint_graph,
"
27,"def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
        # import pdb; pdb.set_trace()
return _stateless.functional_call(mod, params_and_buffers, args, kwargs)
compiled_f = compiled_function(functional_call, *args, **kwargs)
","def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
return _stateless.functional_call(mod, params_and_buffers, args, kwargs)
compiled_f = compiled_function(functional_call, *args, **kwargs)
"
28,"return compiled_f(
dict(self.orig_module.named_parameters()),
dict(self.orig_module.named_buffers()),
*args,
**kwargs
)
","return compiled_f(
dict(self.orig_module.named_parameters()),
dict(self.orig_module.named_buffers()),
                # to replace once appropriate PR lands in PyTorch core
                # dict(self.orig_module.named_parameters(remove_duplicate=False)),
                # dict(self.orig_module.named_buffers(remove_duplicate=False)),
*args,
**kwargs
)
"
29,"def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
# import pdb; pdb.set_trace()
        return _stateless.functional_call(mod, params_and_buffers, *args, **kwargs)
compiled_f = compiled_function(functional_call, *args, **kwargs)
","def functional_call(named_params, named_buffers, *args, **kwargs):
params_and_buffers = {**named_params, **named_buffers}
# import pdb; pdb.set_trace()
        return _stateless.functional_call(mod, params_and_buffers, args, kwargs)
compiled_f = compiled_function(functional_call, *args, **kwargs)
"
30,"import copy
import operator
from functorch._C import CompileCache
from .decompositions import register_decomposition, decomposition_table
from typing import List, Dict, Any, Tuple
pytree._register_pytree_node(immutable_collections.immutable_list, lambda x: (
","import copy
import operator
from functorch._C import CompileCache
from .decompositions import register_decomposition
from typing import List, Dict, Any, Tuple
pytree._register_pytree_node(immutable_collections.immutable_list, lambda x: (
"
31,"return out
return beta * self + out
@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return aten.clamp(self, min=min)
","return out
return beta * self + out

@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return aten.clamp(self, min=min)
"
32,"fw_compiler,
bw_compiler,
partition_fn,
        decompose=True,
hasher_type=hasher_type,
)
","fw_compiler,
bw_compiler,
partition_fn,
        decompositions=decomposition_table,
hasher_type=hasher_type,
)
"
33,"fw_compiler,
bw_compiler,
partition_fn,
        decompose=True,
hasher_type=hasher_type,
)
","fw_compiler,
bw_compiler,
partition_fn,
        decompositions=decomposition_table,
hasher_type=hasher_type,
)
"
34,"from torch.fx import Tracer, GraphModule
import torch.fx as fx
from .decompositions import decomposition_table
from enum import Enum
import warnings
from contextlib import contextmanager
","from torch.fx import Tracer, GraphModule
import torch.fx as fx
from .decompositions import decomposition_table
from contextlib import contextmanager
"
35,"return CompiledModule()
aot_function = compiled_function
aot_module = compiled_module
\ No newline at end of file
","return CompiledModule()
aot_function = compiled_function
\ No newline at end of file
aot_module = compiled_module
"
36,"def draw_joint_graph(graph, joint_inputs, file_name=""full_graph.png""):
draw_graph(graph, file_name)
    return partition_backwards(graph, joint_inputs)
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn):
joint_forward_backward = create_joint_forward_backward(flat_fn)
","def draw_joint_graph(graph, joint_inputs, file_name=""full_graph.png""):
draw_graph(graph, file_name)
    return default_partition(graph, joint_inputs)
def create_compiled_function(flat_fn, fw_compiler, bw_compiler, partition_fn):
joint_forward_backward = create_joint_forward_backward(flat_fn)
"
37,"import torch.utils._pytree as _pytree
from torch.utils._pytree import tree_flatten, tree_unflatten
# TODO: The following function should only be used with vmap.
# torch.return_types should be registered as PyTree nodes.
","import torch.utils._pytree as _pytree
from torch.utils._pytree import tree_flatten, tree_unflatten, TreeSpec
from typing import List, Any
# TODO: The following function should only be used with vmap.
# torch.return_types should be registered as PyTree nodes.
"
38,"class PythonTensor(object):
def __init__(self, out, proxy):
if isinstance(out, torch.Tensor):
            self.value = torch.empty_like(out)
else:
self.value = torch.empty(out)
self.proxy = proxy
","class PythonTensor(object):
def __init__(self, out, proxy):
if isinstance(out, torch.Tensor):
            self.value = torch.clone(out)
else:
self.value = torch.empty(out)
self.proxy = proxy
"
39,"\ No newline at end of file
","++ b/functorch/functorch/_src/python_key.py
import functools
import torch._C.key as key
from torch.fx import PythonTensor
import torch

class ModuleWrap(torch.nn.Module):
    def __init__(self, mod, inps):
        super().__init__()
        self.mod = mod
        self.inps = inps
        @functools.wraps(mod.forward)
        def forward_wrapped(self, *args):
            new_args = []
            for inp, arg in zip(inps, args):
                if isinstance(inp, torch.Tensor):
                    new_arg = key.addKey(PythonTensor(inp.shape, arg))
                else:
                    new_arg = inp
                new_args.append(new_arg)
            out = self.mod(*new_args)
            return key.removeKey(out).proxy

        type(self).forward = forward_wrapped

def key_wrap(f, inps):
    @functools.wraps(f)
    def wrapped(*args):
        new_args = []
        for inp, arg in zip(inps, args):
            if isinstance(inp, torch.Tensor):
                new_arg = key.addKey(PythonTensor(inp.shape, arg))
            else:
                new_arg = inp
            new_args.append(new_arg)
        out = f(*new_args)
        if key.hasKey(out):
            return key.removeKey(out).proxy
        else:
            return out
    return wrapped
\ No newline at end of file
"
40,")
    grouped: List[PythonSignatureGroup] = []
    for sig, base in bases.items():
        outplace = outplaces.get(sig)
        grouped.append(
            PythonSignatureGroup(
                # prefer the signature with optional out=... arguments because it's the
                # superset that can be used to parse input for both base and outplace.
                signature=outplace.signature
                if outplace is not None
                else base.signature,
                base=base.function,
                outplace=outplace.function if outplace is not None else None,
            )
)

return sort_overloads(grouped)
"," ""\n"".join(f""- {candidate}"" for candidate in candidates)
)
    grouped = [
        PythonSignatureGroup.from_pairs(
            functional=base,
            out=outplaces.get(sig),
)
        for sig, base in bases.items()
    ]
return sort_overloads(grouped)
"
41," attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
:math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
          :math:`S` is the source sequence length. If ``average_weights=False``, returns attention weights per
head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
""""""
tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)
","attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
:math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
""""""
tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)
"
42,"from torch.fx.tensor_type import Dyn, TensorType
from torch.nn.modules.conv import Conv2d
from torch.nn.modules.batchnorm import BatchNorm2d
from torch.fx.experimental.graph_gradual_typechecker import get_parameter
_INFERENCE_RULES: Dict[Target, Callable] = {}
","from torch.fx.tensor_type import Dyn, TensorType
from torch.nn.modules.conv import Conv2d
from torch.nn.modules.batchnorm import BatchNorm2d
_INFERENCE_RULES: Dict[Target, Callable] = {}
"
43,"class ConstraintGenerator:
def __init__(self, traced, graph=None):
self.traced = traced  # traced or tracer.root
self.constraints = []
self.symbol_dict = {}
self.graph = traced.graph if hasattr(traced, 'graph') else graph
","class ConstraintGenerator:
def __init__(self, traced, graph=None):
self.traced = traced  # traced or tracer.root
        self.traced_params = dict(self.traced.named_parameters())
self.constraints = []
self.symbol_dict = {}
self.graph = traced.graph if hasattr(traced, 'graph') else graph
"
44,"import torch
from .expanded_weights_impl import ExpandedWeight
","from typing import Optional

import torch
from .expanded_weights_impl import ExpandedWeight
"
45,"env_callable=lambda fn: {
""unboxed_ops"": [ComputeCodegenUnboxedKernels(selector)(fn)]
},
        num_shards=10,
sharded_keys={""unboxed_ops""},
)
","env_callable=lambda fn: {
""unboxed_ops"": [ComputeCodegenUnboxedKernels(selector)(fn)]
},
        num_shards=1 if selected_op_num < sharding_threshold else 10,
sharded_keys={""unboxed_ops""},
)
"
46,"# apply mask to the dense part of the input values:
_, w1 = intersection(mask_flat_indices, maskin_input_flat_indices)
where_mask_values = mask.values()[w1]
        where_input_values = torch.where(where_mask_values, where_input_values,
                                         where_input_values.new_full([], fill_value.item()))
# the set of flat indices of unspecified input and masked-in elements:
maskin_zero_flat_indices = _apply(minus(maskin_flat_indices, maskin_input_flat_indices))
","# apply mask to the dense part of the input values:
_, w1 = intersection(mask_flat_indices, maskin_input_flat_indices)
where_mask_values = mask.values()[w1]
        where_input_values = torch.where(where_mask_values, where_input_values, fill_value)
# the set of flat indices of unspecified input and masked-in elements:
maskin_zero_flat_indices = _apply(minus(maskin_flat_indices, maskin_input_flat_indices))
"
47,"node: Node,
graph: Graph,
modules: Dict[str, torch.nn.Module],
        custom_module_class_mapping: Dict[Callable, Callable],
statically_quantized_custom_module_nodes: Set[Node]):
"""""" Converts an observed custom module to a quantized custom module based on
`custom_module_class_mapping`
","node: Node,
graph: Graph,
modules: Dict[str, torch.nn.Module],
        custom_module_class_mapping: Dict[QuantType, Dict[Type, Type]],
statically_quantized_custom_module_nodes: Set[Node]):
"""""" Converts an observed custom module to a quantized custom module based on
`custom_module_class_mapping`
"
48,"weight_observer_range_neg_127_to_127,
per_channel_weight_observer_range_neg_127_to_127,
default_reuse_input_observer,
)
import warnings

class QConfig(namedtuple('QConfig', ['activation', 'weight'])):
""""""
","weight_observer_range_neg_127_to_127,
per_channel_weight_observer_range_neg_127_to_127,
default_reuse_input_observer,
    ObserverBase,
)
import warnings
import copy

__all__ = [
    ""QConfig"",
    # TODO: deprecated, remove
    ""QConfigDynamic"",
    ""default_qconfig"",
    ""default_debug_qconfig"",
    ""default_per_channel_qconfig"",
    ""default_dynamic_qconfig"",
    ""float16_dynamic_qconfig"",
    ""float16_static_qconfig"",
    ""per_channel_dynamic_qconfig"",
    ""float_qparams_weight_only_qconfig"",
    ""float_qparams_weight_only_qconfig_4bit"",
    ""default_qat_qconfig"",
    ""default_dynamic_qat_qconfig"",
    ""default_weight_only_qconfig"",
    ""default_activation_only_qconfig"",
    ""default_qat_qconfig_v2"",
    ""default_reuse_input_qconfig"",
    ""default_symmetric_qnnpack_qconfig"",
    ""default_per_channel_symmetric_qnnpack_qconfig"",
    ""default_symmetric_qnnpack_qat_qconfig"",
    ""default_per_channel_symmetric_qnnpack_qat_qconfig"",
    ""default_embedding_qat_qconfig"",
    ""default_embedding_qat_qconfig_4bit"",
    ""get_default_qconfig"",
    ""get_default_qat_qconfig"",
    ""get_default_qconfig_dict"",
    ""get_default_qat_qconfig_dict"",
    ""assert_valid_qconfig"",
    ""add_module_to_qconfig_obs_ctr"",
    ""QConfigAny"",
    ""obs_or_fq_ctr_equals"",
    ""qconfig_equals"",
    ""activation_is_memoryless"",
    ""is_reuse_input_qconfig"",
]
class QConfig(namedtuple('QConfig', ['activation', 'weight'])):
""""""
"
49,"# Qconfig weight and activation can be either a partial wrapper,
# or an observer class. Special handling is required (above) for
# comparing partial wrappers.
            if(isinstance(q1.activation, _PartialWrapper)):
                activation_same = _partial_wrapper_equals(q1.activation, q2.activation)
            else:
                activation_same = q1.activation == q2.activation
            if(isinstance(q1.weight, _PartialWrapper)):
                weight_same = _partial_wrapper_equals(q1.weight, q2.weight)
            else:
                weight_same = q1.weight == q2.weight

return activation_same and weight_same
except AttributeError:
return q1 == q2
","# Qconfig weight and activation can be either a partial wrapper,
# or an observer class. Special handling is required (above) for
# comparing partial wrappers.
            activation_same = obs_or_fq_ctr_equals(q1.activation, q2.activation)
            weight_same = obs_or_fq_ctr_equals(q1.weight, q2.weight)
return activation_same and weight_same
except AttributeError:
return q1 == q2
"
50,"logging.debug(""took %dms"", (end_time - start_time) * 1000)
def check_files(
binary: str,
    files: List[str],
) -> List[LintMessage]:
try:
        proc = run_command([binary] + files)
except OSError as err:
return [
LintMessage(
","logging.debug(""took %dms"", (end_time - start_time) * 1000)
def check_file(
binary: str,
    file: str,
) -> List[LintMessage]:
try:
        proc = run_command([binary, file])
except OSError as err:
return [
LintMessage(
"
51,"``True`` or ``False`` depending on whether input layer should be wrapped.
Returns: None (`model` is modified inplace)
""""""
return _recursive_wrap(
module=model,
auto_wrap_policy=partial(lambda_auto_wrap_policy, lambda_fn=check_fn),
","``True`` or ``False`` depending on whether input layer should be wrapped.
Returns: None (`model` is modified inplace)
""""""
    # TODO: Importing inside function to avoid circular import issue between FSDP and
    # checkpoint_wrapper. This can be resolved once wrap() APIs are decoupled from FSDP code.
    from torch.distributed.fsdp.wrap import _recursive_wrap, lambda_auto_wrap_policy
return _recursive_wrap(
module=model,
auto_wrap_policy=partial(lambda_auto_wrap_policy, lambda_fn=check_fn),
"
52,"def name(func: FunctionSchema, *, faithful_name_for_out_overloads: bool = False) -> str:
name = str(func.name.name)
    if func.is_functional_fn():
        name += ""_functional""
if func.is_symint_fn():
name += ""_symint""
if func.is_out_fn():
","def name(func: FunctionSchema, *, faithful_name_for_out_overloads: bool = False) -> str:
name = str(func.name.name)
if func.is_symint_fn():
name += ""_symint""
if func.is_out_fn():
"
53,"``True`` or ``False`` depending on whether input layer should be wrapped.
Returns: None (`model` is modified inplace)
""""""
    # TODO: Importing inside function to avoid circular import issue between FSDP and
    # checkpoint_wrapper. This can be resolved once wrap() APIs are decoupled from FSDP code.
    from torch.distributed.fsdp.wrap import _recursive_wrap, lambda_auto_wrap_policy
return _recursive_wrap(
module=model,
auto_wrap_policy=partial(lambda_auto_wrap_policy, lambda_fn=check_fn),
","``True`` or ``False`` depending on whether input layer should be wrapped.
Returns: None (`model` is modified inplace)
""""""
return _recursive_wrap(
module=model,
auto_wrap_policy=partial(lambda_auto_wrap_policy, lambda_fn=check_fn),
"
54,"# call `replace()` twice separately
tensor_name = tensor_name.replace(FSDP_WRAPPED_MODULE + ""."", """")
tensor_name = tensor_name.replace(FPW_MODULE + ""."", """")
return tensor_name
","# call `replace()` twice separately
tensor_name = tensor_name.replace(FSDP_WRAPPED_MODULE + ""."", """")
tensor_name = tensor_name.replace(FPW_MODULE + ""."", """")
    # TODO: Explicitly replacing checkpoint_wrapper prefix is not ideal,
    # as it increases coupling between CheckpointWrapper and FSDP. This is also not
    # scalable for additional wrapped modules, we should come up with a general solution
    # for this issue.
    tensor_name = tensor_name.replace(_CHECKPOINT_PREFIX + ""."", """")
return tensor_name
"
55,"MODULE_NAME_DICT_KEY = ""module_name""
MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY = ""module_name_object_type_order""

def _get_default_qconfig_mapping(is_qat: bool, backend: str, version: int):
""""""
Return the default QConfigMapping for the given quantization type and backend.
""""""
","MODULE_NAME_DICT_KEY = ""module_name""
MODULE_NAME_OBJECT_TYPE_ORDER_DICT_KEY = ""module_name_object_type_order""
_FIXED_QPARAMS_OP_TO_OBSERVER: Dict[Union[Callable, str], _PartialWrapper] = {
    torch.nn.Hardsigmoid: default_fixed_qparams_range_0to1_observer,
    torch.nn.functional.hardsigmoid: default_fixed_qparams_range_0to1_observer,
    ""hardsigmoid"": default_fixed_qparams_range_0to1_observer,
    ""hardsigmoid_"": default_fixed_qparams_range_0to1_observer,
    torch.nn.Sigmoid: default_fixed_qparams_range_0to1_observer,
    torch.sigmoid: default_fixed_qparams_range_0to1_observer,
    ""sigmoid"": default_fixed_qparams_range_0to1_observer,
    ""sigmoid_"": default_fixed_qparams_range_0to1_observer,
    torch.nn.Softmax: default_fixed_qparams_range_0to1_observer,
    torch.nn.Tanh: default_fixed_qparams_range_neg1to1_observer,
    torch.tanh: default_fixed_qparams_range_neg1to1_observer,
    ""tanh"": default_fixed_qparams_range_neg1to1_observer,
    ""tanh_"": default_fixed_qparams_range_neg1to1_observer,
}


def _get_default_qconfig_mapping(is_qat: bool, backend: str, version: int) -> QConfigMapping:
""""""
Return the default QConfigMapping for the given quantization type and backend.
""""""
"
56,"s = first + '\n' + s
return s
r""""""This tracks hooks common to all modules that are executed before/after
calling forward and backward. This is global state used for debugging/profiling
","s = first + '\n' + s
return s
class _WrappedHook:
    def __init__(self, hook: Callable, module: Optional[""Module""] = None):
        self.hook: Callable = hook
        functools.update_wrapper(self, hook)

        self.with_module: bool = False

        if module is not None:
            self.module: weakref.ReferenceType[""Module""] = weakref.ref(module)
            self.with_module = True

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        if self.with_module:
            module = self.module()
            if module is None:
                raise RuntimeError(""You are trying to call the hook of a dead Module!"")
            return self.hook(module, *args, **kwargs)
        return self.hook(*args, **kwargs)

    def __getstate__(self) -> Dict:
        result = {""hook"": self.hook, ""with_module"": self.with_module}
        if self.with_module:
            result[""module""] = self.module()

        return result

    def __setstate__(self, state: Dict):
        self.hook = state[""hook""]
        self.with_module = state[""with_module""]

        if self.with_module:
            if state[""module""] is None:
                raise RuntimeError(""You are trying to revive the hook of a dead Module!"")
            self.module = weakref.ref(state[""module""])

r""""""This tracks hooks common to all modules that are executed before/after
calling forward and backward. This is global state used for debugging/profiling
"
57,"checkruns = node[""checkRuns""]
if workflow_run is not None:
conclusions[workflow_run[""workflow""][""name""]] = (node[""conclusion""], node[""url""])
while checkruns is not None:
for checkrun_node in checkruns[""nodes""]:
conclusions[checkrun_node[""name""]] = (checkrun_node[""conclusion""], checkrun_node[""detailsUrl""])
if bool(checkruns[""pageInfo""][""hasNextPage""]):
rc = gh_graphql(GH_GET_PR_NEXT_CHECK_RUNS,
","checkruns = node[""checkRuns""]
if workflow_run is not None:
conclusions[workflow_run[""workflow""][""name""]] = (node[""conclusion""], node[""url""])
                has_failing_check = False
while checkruns is not None:
for checkrun_node in checkruns[""nodes""]:
                        if checkrun_node[""conclusion""] == 'FAILURE':
                            has_failing_check = True
conclusions[checkrun_node[""name""]] = (checkrun_node[""conclusion""], checkrun_node[""detailsUrl""])
if bool(checkruns[""pageInfo""][""hasNextPage""]):
rc = gh_graphql(GH_GET_PR_NEXT_CHECK_RUNS,
"
58,"#       Part of https://github.com/pytorch/data/issues/284
datapipe = args[0]
msg = ""thrown by __iter__ of""
                full_msg = f""{msg} {datapipe.__class__.__name__}({_generate_input_args_string(datapipe)})""
                if len(e.args) >= 1 and msg not in e.args[0]:
                    e.args = (e.args[0] + f'\nThis exception is {full_msg}',) + e.args[1:]
raise
namespace['__iter__'] = wrap_generator
","#       Part of https://github.com/pytorch/data/issues/284
datapipe = args[0]
msg = ""thrown by __iter__ of""
                single_iterator_msg = ""single iterator per IterDataPipe constraint""
                if hasattr(e.args, '__len__'):
                    full_msg = f""{msg} {datapipe.__class__.__name__}({_generate_input_args_string(datapipe)})""
                    if len(e.args) == 0:  # If an exception message doesn't exist
                        e.args = (f'\nThis exception is {full_msg}',)
                    elif msg not in e.args[0] and single_iterator_msg not in e.args[0]:
                        e.args = (e.args[0] + f'\nThis exception is {full_msg}',) + e.args[1:]
raise
namespace['__iter__'] = wrap_generator
"
59,"from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
    parser.add_argument(""--on-mandatory"", action=""store_true"")
parser.add_argument(""--on-green"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
","from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
parser.add_argument(""--on-green"", action=""store_true"")
    parser.add_argument(""--on-mandatory"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
"
60,"env_callable=lambda fn: {
""definitions"": [ComputeUnboxingFunctions(Target.DEFINITION, selector)(fn)]
},
        num_shards=5,
sharded_keys={""definitions""},
)
cpu_fm.write(
","env_callable=lambda fn: {
""definitions"": [ComputeUnboxingFunctions(Target.DEFINITION, selector)(fn)]
},
        num_shards=1 if selected_op_num < sharding_threshold else 5,
sharded_keys={""definitions""},
)
cpu_fm.write(
"
61,"key = low_priority_aliases[var]
if key not in build_options:
build_options[key] = val

# The default value cannot be easily obtained in CMakeLists.txt. We set it here.
py_lib_path = sysconfig.get_path(""purelib"")
cmake_prefix_path = build_options.get(""CMAKE_PREFIX_PATH"", None)
","key = low_priority_aliases[var]
if key not in build_options:
build_options[key] = val
# The default value cannot be easily obtained in CMakeLists.txt. We set it here.
py_lib_path = sysconfig.get_path(""purelib"")
cmake_prefix_path = build_options.get(""CMAKE_PREFIX_PATH"", None)
"
62,"return f""https://github.com/{suffix_str}""
def merge(pr_num: int, repo: GitRepo, dry_run: bool = False, timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
start_time = time.time()
","return f""https://github.com/{suffix_str}""
def merge_on_green(pr_num: int, repo: GitRepo, dry_run: bool = False, timeout_minutes: int = 400) -> None:
repo = GitRepo(get_git_repo_dir(), get_git_remote_name())
org, project = repo.gh_owner_and_name()
start_time = time.time()
"
63,"from __future__ import annotations
from typing import Any, Union, Sequence, Optional, Tuple, List
from enum import Enum
from functools import reduce, cmp_to_key
import operator
","from __future__ import annotations
from typing import Any, Union, Sequence, Optional, Tuple, List, Callable, Type
from enum import Enum
from functools import reduce, cmp_to_key
import operator
"
64,"gh_post_comment(org, project, args.pr_num, ""Cross-repo ghstack merges are not supported"", dry_run=args.dry_run)
return
    if args.on_green:
        try:
            merge_on_green(args.pr_num, repo, dry_run=args.dry_run)
        except Exception as e:
            handle_exception(e)
    else:
        try:
            pr.merge_into(repo, dry_run=args.dry_run, force=args.force, comment_id=args.comment_id)
        except Exception as e:
            handle_exception(e)
if __name__ == ""__main__"":
","gh_post_comment(org, project, args.pr_num, ""Cross-repo ghstack merges are not supported"", dry_run=args.dry_run)
return
    try:
        merge(args.pr_num, repo, dry_run=args.dry_run)
    except Exception as e:
        handle_exception(e)

if __name__ == ""__main__"":
"
65,""""""")
add_docstr_all('as_strided', r""""""
as_strided(size, stride, storage_offset=0) -> Tensor
See :func:`torch.as_strided`
"""""")
",""""""")
add_docstr_all('as_strided', r""""""
as_strided(size, stride, storage_offset=None) -> Tensor
See :func:`torch.as_strided`
"""""")
"
66,"bias: Optional[Tensor],
eps: float,
) -> Tuple[Tensor, Tensor, Tensor]:
    input_shape = input.shape
    input_ndim = input.dim()
    axis = input_ndim - len(normalized_shape)
    M = prod(input_shape[:axis])  # type: ignore[arg-type]

    # Hmm... not sure how I get around this...
    # Basically, native_batch_norm doesn't support 0-entry tensors, while
    # native_layer_norm does (and is tested by OpInfos!)
    if M > 0:
        input_reshaped = input.view(1, M, -1)
else:
        return (input, input.new_zeros((0,)), input.new_zeros((0,)))

    # Unlike Batch Normalization, which applies scalar scale and bias for each
    # entire channel/plane with the affine option, Layer Normalization applies
    # per-element scale and bias. E.g. For input {N, C, H, W}, weight for
    # batchnorm has shape {C} while weight for layernorm has shape {H, W} or {W}.
    out, mean, rstd = aten.native_batch_norm(
        input_reshaped,
        weight=None,
        bias=None,
        running_mean=None,
        running_var=None,
        training=True,
        momentum=0.0,
        eps=eps,
    )
    out = out.view(input_shape)
    if weight is not None:
        out = out * weight
    if bias is not None:
        out = out + bias

    stat_shape = list(input_shape[:axis])
    for _ in range(axis, input.dim()):
        stat_shape.append(1)
    mean = mean.view(stat_shape)
    rstd = rstd.view(stat_shape)
return (out, mean, rstd)
# TODO: Correct the type promotion semantics
@register_decomposition(aten.native_layer_norm_backward)
@pw_cast_for_opmath
def native_layer_norm_backward(
grad_out: Tensor,
input: Tensor,
","bias: Optional[Tensor],
eps: float,
) -> Tuple[Tensor, Tensor, Tensor]:
    computation_dtype = utils.get_computation_dtype(input.dtype)
    axis = input.dim() - len(normalized_shape)
    if prod(list(input.shape[:axis])) == 0:
        mean = input.new_zeros((0,), dtype=computation_dtype)
        rstd = input.new_zeros((0,), dtype=computation_dtype)
        out = input
else:
        reduction_dims = list(range(axis, input.dim()))
        out, mean, rstd = normalize(input, reduction_dims, eps)

        if weight is not None:
            out = out * weight
        if bias is not None:
            out = out + bias

        out = out.to(dtype=input.dtype)

    if input.device.type == 'cpu':
        mean = mean.to(dtype=input.dtype)
        rstd = rstd.to(dtype=input.dtype)
return (out, mean, rstd)
def _maybe_cast(x: Optional[Tensor], dtype) -> Optional[Tensor]:
    if x is not None:
        return x.to(dtype)
    return x
# TODO: Correct the type promotion semantics
@register_decomposition(aten.native_layer_norm_backward)
def native_layer_norm_backward(
grad_out: Tensor,
input: Tensor,
"
67,"torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@_sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
","torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
"
68,"@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
","@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
            # Dispatch to custom user provided op first if it exists.
            if func in _CUSTOM_SHARDED_OPS:
                return _CUSTOM_SHARDED_OPS[func](types, args, kwargs, st._process_group)

# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
"
69,"encoder_layer: an instance of the TransformerEncoderLayer() class (required).
num_layers: the number of sub-encoder-layers in the encoder (required).
norm: the layer normalization component (optional).
Examples::
>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
","encoder_layer: an instance of the TransformerEncoderLayer() class (required).
num_layers: the number of sub-encoder-layers in the encoder (required).
norm: the layer normalization component (optional).
        enable_nested_tensor: if True, input will automatically convert to nested tensor
            (and convert back on output). This will improve the overall performance of
            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).
Examples::
>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
"
70,"the entire (nested) model into memory and taking the local model's
``state_dict`` on every rank, which could result in OOM if the model
cannot fit on a single GPU. As a result, :func:`state_dict_type` API is
        available to configure between `state_dict` implementations. User can
        thus use `with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`
context manager to perform a local checkpoint that will store only local
shards of the module. Currently, the only supported implementations are
``StateDictType.LOCAL_STATE_DICT`` and ``StateDictType.FULL_STATE_DICT``
","the entire (nested) model into memory and taking the local model's
``state_dict`` on every rank, which could result in OOM if the model
cannot fit on a single GPU. As a result, :func:`state_dict_type` API is
        available to configure between ``state_dict`` implementations. User can
        thus use ``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)``
context manager to perform a local checkpoint that will store only local
shards of the module. Currently, the only supported implementations are
``StateDictType.LOCAL_STATE_DICT`` and ``StateDictType.FULL_STATE_DICT``
"
71,"Returns:
Total norm of the parameters (viewed as a single vector).
        .. note:: This is analogous to `torch.nn.utils.clip_grad_norm_` but
handles the partitioning and multiple devices per rank under the
hood. The default torch util is not applicable here, because each
rank only has a partial view of all the grads in the model, so
","Returns:
Total norm of the parameters (viewed as a single vector).
        .. note:: This is analogous to ``torch.nn.utils.clip_grad_norm_`` but
handles the partitioning and multiple devices per rank under the
hood. The default torch util is not applicable here, because each
rank only has a partial view of all the grads in the model, so
"
72,"state['activation'] = F.relu
super(TransformerEncoderLayer, self).__setstate__(state)
    def is_fastpath(self):
        if (not self.norm_first and not self.training and
                self.self_attn.batch_first and
                self.self_attn._qkv_same_embed_dim and self.activation_relu_or_gelu and
                self.norm1.eps == self.norm2.eps):
            return True
        return False

def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,
src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layer.
","state['activation'] = F.relu
super(TransformerEncoderLayer, self).__setstate__(state)
def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,
src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layer.
"
73,"""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
","""""""
__constants__ = ['norm']
    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):
super(TransformerEncoder, self).__init__()
self.layers = _get_clones(encoder_layer, num_layers)
self.num_layers = num_layers
self.norm = norm
        self.enable_nested_tensor = enable_nested_tensor
def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:
r""""""Pass the input through the encoder layers in turn.
"
74,"return self.new_empty((nplane, output_h, output_w))
else:
return self.new_empty((nbatch, nplane, output_h, output_w))
","return self.new_empty((nplane, output_h, output_w))
else:
return self.new_empty((nbatch, nplane, output_h, output_w))

@torch.library.impl(meta_lib, ""dot"")
def meta_dot(self, tensor):
    check(
        self.dim() == 1 and tensor.dim() == 1,
        f""1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors""
    )
    return self.new_empty(())

@torch.library.impl(meta_lib, ""var_mean.correction"")
def meta_var_mean_correction(self, dim, *, correction, keepdim=False):
    dim = utils.reduction_dims(self.shape, dim)
    if keepdim:
        output_shape = tuple(self.shape[i] if i not in dim else 1 for i in range(self.ndim))
    else:
        output_shape = utils.compute_reduction_output_shape(self.shape, dim)
    result1 = self.new_empty(output_shape, dtype=toRealValueType(self.dtype))
    result2 = self.new_empty(output_shape)
    return result1, result2

@torch.library.impl(meta_lib, ""inverse"")
def meta_inverse(self):
    # Bug: https://github.com/pytorch/pytorch/issues/77498
    if self.numel() == 0:
        return torch.empty_like(self)
    r = self.new_empty(self.shape)
    r.transpose_(-2, -1)
    return r

@torch.library.impl(meta_lib, ""bernoulli.out"")
def meta_bernoulli(self, *, generator=None, out):
    torch._resize_output_(out, self.size(), self.device)
    return out

@torch.library.impl(meta_lib, ""_adaptive_avg_pool2d"")
def meta_adaptive_avg_pool2d(self, output_size):
    check(self.ndim == 3 or self.ndim == 4, f""Expected 3D or 4D tensor, but got {self.shape}"")
    return self.new_empty(self.shape[:-2] + tuple(output_size))

@torch.library.impl(meta_lib, ""_adaptive_avg_pool3d"")
def meta_adaptive_avg_pool3d(self, output_size):
    check(self.ndim == 4 or self.ndim == 5, f""Expected 4D or 5D tensor, but got {self.shape}"")
    return self.new_empty(self.shape[:-3] + tuple(output_size))
"
75,"_prepare_sharded_tensor_read,
_shards_get_overlap_region_wrt_saved_tensor
)
from .storage import StorageReader
def _reshard_and_prepare_read_request(
state_dict: Dict[str, Any], metadata_from_storage: Metadata
","_prepare_sharded_tensor_read,
_shards_get_overlap_region_wrt_saved_tensor
)
from .storage import (
    StorageReader,
)
from .api import CheckpointException
def _reshard_and_prepare_read_request(
state_dict: Dict[str, Any], metadata_from_storage: Metadata
"
76,"after backward computation for synchronizing and sharding gradients.
Sharded optimizer states are updated locally.
NO_SHARD: This is similar to PyTorch ``DistributedDataParallel`` API. Parameters, gradients
              and optimizer states are replicated among ranks, all_reduce is inserted after
backward computation is done for synchronizing gradients. Full optimizer states
are updated in each rank.
HYBRID_SHARD(future support): apply FULL_SHARD algorithm in the intra node and
","after backward computation for synchronizing and sharding gradients.
Sharded optimizer states are updated locally.
NO_SHARD: This is similar to PyTorch ``DistributedDataParallel`` API. Parameters, gradients
              and optimizer states are replicated among ranks, ``all_reduce`` is inserted after
backward computation is done for synchronizing gradients. Full optimizer states
are updated in each rank.
HYBRID_SHARD(future support): apply FULL_SHARD algorithm in the intra node and
"
77,"import torch.nn as nn
from torch import Tensor
from ._utils import _replace_by_prefix
ParamOffset = Tuple[int, int]
","import torch.nn as nn
from torch import Tensor
from torch.distributed.utils import _replace_by_prefix
ParamOffset = Tuple[int, int]
"
78,"export_modules_as_functions=False,
):
    if export_modules_as_functions and opset_version < 15:
        raise ValueError(
            ""`export_modules_as_functions` is not supported for `opset_version` < 15.""
            ""This is because `opset_version` < 15 implies IR version < 8, which means ""
            ""no local function support. ""
        )
    export_modules_as_functions = _setup_trace_module_map(
        model, export_modules_as_functions
    )

if isinstance(model, torch.nn.DataParallel):
raise ValueError(
""torch.nn.DataParallel is not supported by ONNX ""
","export_modules_as_functions=False,
):
if isinstance(model, torch.nn.DataParallel):
raise ValueError(
""torch.nn.DataParallel is not supported by ONNX ""
"
79,"if opset_version is None:
opset_version = _default_onnx_opset_version
if not operator_export_type:
if torch.onnx._CAFFE2_ATEN_FALLBACK:
operator_export_type = torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
","if opset_version is None:
opset_version = _default_onnx_opset_version

        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                ""`export_modules_as_functions` is not supported for `opset_version` < 15.""
                ""This is because `opset_version` < 15 implies IR version < 8, which means ""
                ""no local function support. ""
            )
        export_modules_as_functions = _setup_trace_module_map(
            model, export_modules_as_functions
        )

if not operator_export_type:
if torch.onnx._CAFFE2_ATEN_FALLBACK:
operator_export_type = torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
"
80,"# Use default config a state_dict config is not set.
if state_dict_config is None:
state_dict_config = _state_dict_type_to_config[state_dict_type]()
        for module in FullyShardedDataParallel.fsdp_modules(module):
if prev_state_dict_type is None:
                prev_state_dict_type = module._state_dict_type
if prev_state_dict_config is None:
                prev_state_dict_config = module._state_dict_config
            if prev_state_dict_type != module._state_dict_type:
raise RuntimeError(
""All FSDP module should the same state_dict_type.""
)
            if type(prev_state_dict_config) != type(module._state_dict_config):
raise RuntimeError(
""All FSDP modules should have the same type of state_dict_config.""
)
","# Use default config a state_dict config is not set.
if state_dict_config is None:
state_dict_config = _state_dict_type_to_config[state_dict_type]()
        for submodule in FullyShardedDataParallel.fsdp_modules(module):
if prev_state_dict_type is None:
                prev_state_dict_type = submodule._state_dict_type
if prev_state_dict_config is None:
                prev_state_dict_config = submodule._state_dict_config
            if prev_state_dict_type != submodule._state_dict_type:
raise RuntimeError(
""All FSDP module should the same state_dict_type.""
)
            if type(prev_state_dict_config) != type(submodule._state_dict_config):
raise RuntimeError(
""All FSDP modules should have the same type of state_dict_config.""
)
"
81,"_process_pos_dim_tensor_state,
_unflatten_optim_state,
)
from ._utils import _apply_to_modules, _apply_to_tensors, _replace_by_prefix
from .flatten_params_wrapper import (
FLAT_PARAM,
FPW_MODULE,
FlatParameter,
FlattenParamsWrapper,
)
from .wrap import _recursive_wrap
if TYPE_CHECKING:
from collections import OrderedDict  # noqa: F401
","_process_pos_dim_tensor_state,
_unflatten_optim_state,
)
from ._utils import (
    _apply_to_modules, _apply_to_tensors, _replace_by_prefix,
    _override_batchnorm_mixed_precision, _contains_batchnorm
)
from .flatten_params_wrapper import (
FLAT_PARAM,
FPW_MODULE,
FlatParameter,
FlattenParamsWrapper,
)
from .wrap import _recursive_wrap, _wrap_batchnorm_individually, _or_policy
if TYPE_CHECKING:
from collections import OrderedDict  # noqa: F401
"
82,"""""""
return True

def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
","""""""
return True
def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
"
83,"def _wrap(module: nn.Module, wrapper_cls: Callable, **kwargs) -> nn.Module:
assert wrapper_cls is not None
return wrapper_cls(module, **kwargs)
","def _wrap(module: nn.Module, wrapper_cls: Callable, **kwargs) -> nn.Module:
assert wrapper_cls is not None
    if hasattr(module, '_wrap_overrides'):
        # If module has a _wrap_overrides attribute, we force overriding the
        # FSDP config with these attributes for this module. Currently this
        # is only used to disable mixed precision for BatchNorm when
        # auto_wrapping.
        overrides = {**kwargs, **module._wrap_overrides}  # type: ignore[arg-type]
        return wrapper_cls(module, **overrides)

return wrapper_cls(module, **kwargs)
"
84,"""""""
return True
def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
","""""""
return True

def transformer_auto_wrap_policy(
module: nn.Module,
recurse: bool,
"
85,"def _wrap(module: nn.Module, wrapper_cls: Callable, **kwargs) -> nn.Module:
assert wrapper_cls is not None
return wrapper_cls(module, **kwargs)
","def _wrap(module: nn.Module, wrapper_cls: Callable, **kwargs) -> nn.Module:
assert wrapper_cls is not None
    if hasattr(module, '_wrap_overrides'):
        # If module has a _wrap_overrides attribute, we force overriding the
        # FSDP config with these attributes for this module. Currently this
        # is only used to disable mixed precision for BatchNorm when
        # auto_wrapping.
        overrides = {**kwargs, **module._wrap_overrides}  # type: ignore[arg-type]
        return wrapper_cls(module, **overrides)

return wrapper_cls(module, **kwargs)
"
86,"@register_decomposition(aten.prelu_backward)
@cast_for_opmath
def prelu_backward(
grad_output: Tensor, self: Tensor, weight: Tensor
) -> Tuple[Tensor, Tensor]:
","@register_decomposition(aten.prelu_backward)
@pw_cast_for_opmath
def prelu_backward(
grad_output: Tensor, self: Tensor, weight: Tensor
) -> Tuple[Tensor, Tensor]:
"
87,"@register_decomposition(aten.log_sigmoid_backward)
@cast_for_opmath
def log_sigmoid_backward(grad_output: Tensor, self: Tensor, buffer: Tensor) -> Tensor:
in_negative = self < 0
max_deriv = torch.where(in_negative, 1, 0)
","@register_decomposition(aten.log_sigmoid_backward)
@pw_cast_for_opmath
def log_sigmoid_backward(grad_output: Tensor, self: Tensor, buffer: Tensor) -> Tensor:
in_negative = self < 0
max_deriv = torch.where(in_negative, 1, 0)
"
88,"@register_decomposition(aten.huber_loss)
@cast_for_opmath
def huber_loss(
self: Tensor,
target: Tensor,
","@register_decomposition(aten.huber_loss)
@pw_cast_for_opmath
def huber_loss(
self: Tensor,
target: Tensor,
"
89,"@register_decomposition(aten._log_softmax_backward_data)
@cast_for_opmath
def _log_softmax_backward_data(
grad_output: Tensor, output: Tensor, dim: int, input_dtype: int
):
","@register_decomposition(aten._log_softmax_backward_data)
@pw_cast_for_opmath
def _log_softmax_backward_data(
grad_output: Tensor, output: Tensor, dim: int, input_dtype: int
):
"
90,"@register_decomposition(aten.std.correction)
@cast_for_opmath
def std_decomposition(
x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False
):
","@register_decomposition(aten.std.correction)
@reduction_complex_to_real
def std_decomposition(
x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False
):
"
91,"# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
def apply_loss_reduction(loss: Tensor, reduction: int):
    if reduction == Reduction.MEAN.value:
        return torch.mean(loss)
    elif reduction == Reduction.SUM.value:
        return torch.sum(loss)
    else:
        return loss


def to_real_dtype(dtype: torch.dtype):
if dtype == torch.complex32:
return torch.float16
","# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
def to_real_dtype(dtype: torch.dtype):
if dtype == torch.complex32:
return torch.float16
"
92,"Iterable,
Iterator,
List,
NamedTuple,
Optional,
Set,
","Iterable,
Iterator,
List,
    Mapping,
NamedTuple,
Optional,
Set,
"
93,"def _load_local_state_dict(
self,
        state_dict: ""OrderedDict[str, torch.Tensor]"",
*args,
) -> NamedTuple:
""""""
","def _load_local_state_dict(
self,
        state_dict: Mapping[str, Any],
*args,
) -> NamedTuple:
""""""
"
94,"# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
@register_decomposition(aten.mse_loss_backward)
@cast_for_opmath
def mse_loss_backward(
","# return (max_deriv - sign * (buffer / (1 + buffer))) * grad_output
def apply_loss_reduction(loss: Tensor, reduction: int):
    if reduction == Reduction.MEAN.value:
        return torch.mean(loss)
    elif reduction == Reduction.SUM.value:
        return torch.sum(loss)
    else:
        return loss


def to_real_dtype(dtype: torch.dtype):
    if dtype == torch.complex32:
        return torch.float16
    elif dtype == torch.complex64:
        return torch.float32
    elif dtype == torch.complex128:
        return torch.float64


@register_decomposition(aten.l1_loss)
def l1_loss(
    self: Tensor, target: Tensor, reduction: int = Reduction.MEAN.value
) -> Tensor:
    loss = (self - target).abs()
    # PyTorch semantics result in the output of l1_loss having the corresponding
    # real dtype to self.  This may not happen without explicit casting if say
    # self: complex64 and target: float64, which results in loss: float64
    float_type = to_real_dtype(self.dtype)
    return apply_loss_reduction(loss, reduction).to(float_type)


@register_decomposition(aten.l1_loss_backward)
@cast_for_opmath
def l1_loss_backward(
    grad_output: Tensor,
    self: Tensor,
    target: Tensor,
    reduction: int = Reduction.MEAN.value,
):
    sign = torch.sign(self - target)

    norm = sign / self.numel() if reduction == Reduction.MEAN.value else sign
    return grad_output * norm


@register_decomposition(aten.mse_loss)
def mse_loss(
    self: Tensor, target: Tensor, reduction: int = Reduction.MEAN.value
) -> Tensor:
    loss = (self - target) ** 2
    return apply_loss_reduction(loss, reduction)


@register_decomposition(aten.mse_loss_backward)
@cast_for_opmath
def mse_loss_backward(
"
95,"cuda_flags = ['-DWITH_HIP'] + cflags + COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS
cuda_flags += extra_cuda_cflags
cuda_flags += _get_rocm_arch_flags(cuda_flags)
        sources = [s if not _is_cuda_file(s) else
                   os.path.abspath(os.path.join(
                       path, get_hip_file_path(os.path.relpath(s, path), is_pytorch_extension=True)))
                   for s in sources]
elif with_cuda:
cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags()
if IS_WINDOWS:
","cuda_flags = ['-DWITH_HIP'] + cflags + COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS
cuda_flags += extra_cuda_cflags
cuda_flags += _get_rocm_arch_flags(cuda_flags)
elif with_cuda:
cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags()
if IS_WINDOWS:
"
96,"-- a/torch/utils/hipify/hipify_python.py
""""""Helper method to see if filename ends with certain extension""""""
return any(filename.endswith(e) for e in extensions)
def matched_files_iter(
root_path: str,
        includes: Iterable = ('*',),
ignores: Iterable = (),
extensions: Iterable = (),
out_of_place_only: bool = False,
is_pytorch_extension: bool = False) -> Iterator[str]:
    def _fnmatch(filepath, patterns):
        return any(fnmatch.fnmatch(filepath, pattern) for pattern in patterns)
exact_matches = set(includes)
","++ b/torch/utils/hipify/hipify_python.py
""""""Helper method to see if filename ends with certain extension""""""
return any(filename.endswith(e) for e in extensions)
def _fnmatch(filepath, patterns):
    return any(fnmatch.fnmatch(filepath, pattern) for pattern in patterns)

def matched_files_iter(
root_path: str,
        includes: Iterable = (),
ignores: Iterable = (),
extensions: Iterable = (),
out_of_place_only: bool = False,
is_pytorch_extension: bool = False) -> Iterator[str]:
exact_matches = set(includes)
"
97,"header_filepath = header_path_to_check
# If not found, look in include dirs one by one and first match wins
if header_filepath is None:
                    for include in includes:
                        header_dir_to_check = os.path.join(output_directory, os.path.dirname(include))
header_path_to_check = os.path.abspath(os.path.join(header_dir_to_check, f))
if os.path.exists(header_path_to_check):
header_dir = header_dir_to_check
","header_filepath = header_path_to_check
# If not found, look in include dirs one by one and first match wins
if header_filepath is None:
                    for header_include_dir in header_include_dirs:
                        header_dir_to_check = os.path.join(output_directory, header_include_dir)
header_path_to_check = os.path.abspath(os.path.join(header_dir_to_check, f))
if os.path.exists(header_path_to_check):
header_dir = header_dir_to_check
"
98,"project_directory.rstrip(""/"")
output_directory = project_directory + ""_amd""
# Copy from project directory to output directory if not done already.
if not os.path.exists(output_directory):
shutil.copytree(project_directory, output_directory)
","project_directory.rstrip(""/"")
output_directory = project_directory + ""_amd""
    if project_directory != output_directory:
        includes = [include.replace(project_directory, output_directory) for include in includes]
        ignores = [ignore.replace(project_directory, output_directory) for ignore in ignores]

# Copy from project directory to output directory if not done already.
if not os.path.exists(output_directory):
shutil.copytree(project_directory, output_directory)
"
99,"# and (2) don't want to worry about method-only operators.
@dataclass(frozen=True)
class ComputeOperators:
    target: Union[
        Literal[Target.DECLARATION],
        Literal[Target.DEFINITION]
    ]
static_dispatch_backend_indices: List[BackendIndex]
@method_with_native_function
","# and (2) don't want to worry about method-only operators.
@dataclass(frozen=True)
class ComputeOperators:
    target: Union[Literal[Target.DECLARATION], Literal[Target.DEFINITION]]
static_dispatch_backend_indices: List[BackendIndex]
@method_with_native_function
"
100,"return at::_ops::{f.func.name.unambiguous_name()}::call({exprs_str});
}}
""""""
result = generate_defn(faithful=False)
if sig_group.faithful_signature is not None:
result += generate_defn(faithful=True)
","return at::_ops::{f.func.name.unambiguous_name()}::call({exprs_str});
}}
""""""

result = generate_defn(faithful=False)
if sig_group.faithful_signature is not None:
result += generate_defn(faithful=True)
"
101,"native_functions,
key_fn=key_func,
env_callable=lambda fn: {
            'operator_headers': [f'#include <ATen/ops/{fn.root_name}.h>'],
            'definitions': [ComputeOperators(Target.DEFINITION,
                                             static_dispatch_backend_indices=static_dispatch_idx)(fn)]},
base_env={
            'static_dispatch_extra_headers': static_dispatch_extra_headers(static_dispatch_idx),
},
num_shards=5,
        sharded_keys={'operator_headers', 'definitions', 'static_dispatch_extra_headers'}
)
cpu_fm.write(""Functions.cpp"", lambda: {})
","native_functions,
key_fn=key_func,
env_callable=lambda fn: {
            ""operator_headers"": [f""#include <ATen/ops/{fn.root_name}.h>""],
            ""definitions"": [
                ComputeOperators(
                    Target.DEFINITION,
                    static_dispatch_backend_indices=static_dispatch_idx,
                )(fn)
            ],
        },
base_env={
            ""static_dispatch_extra_headers"": static_dispatch_extra_headers(
                static_dispatch_idx
            ),
},
num_shards=5,
        sharded_keys={
            ""operator_headers"",
            ""definitions"",
            ""static_dispatch_extra_headers"",
        },
)
cpu_fm.write(""Functions.cpp"", lambda: {})
"
102,"overall_group_size = dist.get_world_size(group=self.process_group)
if list(period_group_size_dict.values())[-1] != overall_group_size:
raise ValueError(
                ""The last value in arg ``period_process_group_dict`` ""
                ""must be equal to the size of arg ``process_group``."")
self.period_process_group_dict = OrderedDict()
logger.info(""Model averaging hierarchy:"")
","overall_group_size = dist.get_world_size(group=self.process_group)
if list(period_group_size_dict.values())[-1] != overall_group_size:
raise ValueError(
                f""The last value in arg ``period_process_group_dict`` {list(period_group_size_dict.values())[-1]} ""
                ""must be equal to the size of arg ``process_group`` {overall_group_size}.""
            )
self.period_process_group_dict = OrderedDict()
logger.info(""Model averaging hierarchy:"")
"
103,"params = list(inspect.signature(symbolic_fn).parameters.values())
return params and issubclass(params[0].annotation, SymbolicContext)
def _run_symbolic_function(g, block, n, inputs, env, operator_export_type=OperatorExportTypes.ONNX):
# NB: Returning None means the node gets cloned as is into
# the new graph
","params = list(inspect.signature(symbolic_fn).parameters.values())
return params and issubclass(params[0].annotation, SymbolicContext)
def _get_aten_op_overload_name(n: Node) -> str:
    from torch.onnx.symbolic_helper import is_caffe2_aten_fallback

    # Returns `overload_name` attribute to ATen ops on non-Caffe2 builds
    schema = n.schema()
    if not schema.startswith(""aten::"") or is_caffe2_aten_fallback():
        return """"
    return torch._C.parse_schema(schema).overload_name

def _run_symbolic_function(g, block, n, inputs, env, operator_export_type=OperatorExportTypes.ONNX):
# NB: Returning None means the node gets cloned as is into
# the new graph
"
104,"from torch._utils import ExceptionWrapper
def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
# This setting is thread local, and prevents the copy in pin_memory from
# consuming all CPU cores.
torch.set_num_threads(1)
","from torch._utils import ExceptionWrapper
def _pin_memory_loop(in_queue, out_queue, device_id, done_event, device):
# This setting is thread local, and prevents the copy in pin_memory from
# consuming all CPU cores.
torch.set_num_threads(1)
"
105,"self._index_sampler = loader._index_sampler
self._num_workers = loader.num_workers
self._prefetch_factor = loader.prefetch_factor
        self._pin_memory = loader.pin_memory and torch.cuda.is_available()
self._timeout = loader.timeout
self._collate_fn = loader.collate_fn
self._sampler_iter = iter(self._index_sampler)
","self._index_sampler = loader._index_sampler
self._num_workers = loader.num_workers
self._prefetch_factor = loader.prefetch_factor
        # for other backends, pin_memory_device need to set. if not set
        # default behaviour is CUDA device. if pin_memory_device is selected
        # and pin_memory is not set, the default behaviour false.
        if (len(loader.pin_memory_device) == 0):
            self._pin_memory = loader.pin_memory and torch.cuda.is_available()
            self._pin_memory_device = None
        else:
            if not loader.pin_memory:
                warn_msg = (""pin memory device is set and pin_memory flag is not used then device pinned memory won't be used""
                            ""please set pin_memory to true, if you need to use the device pin memory"")
                warnings.warn(warn_msg)

            self._pin_memory = loader.pin_memory
            self._pin_memory_device = loader.pin_memory_device
self._timeout = loader.timeout
self._collate_fn = loader.collate_fn
self._sampler_iter = iter(self._index_sampler)
"
106,"reject_reason = (f""{num_matching_files} files matched rule {rule_name}, but there are still non-matching files: "" +
f""{','.join(non_matching_files[:5])}{', ...' if len(non_matching_files) > 5 else ''}"")
continue
# If rule requires approvers but they aren't the ones that reviewed PR
if len(approvers_intersection) == 0 and len(rule_approvers_set) > 0:
if reject_reason_score < 10000:
","reject_reason = (f""{num_matching_files} files matched rule {rule_name}, but there are still non-matching files: "" +
f""{','.join(non_matching_files[:5])}{', ...' if len(non_matching_files) > 5 else ''}"")
continue
        # If rule needs approvers but PR has not been reviewed, skip it
        if len(rule.approved_by) > 0 and len(approved_by) == 0:
            if reject_reason_score < 10000:
                reject_reason_score = 10000
                reject_reason = f""Matched rule {rule_name}, but PR has not been reviewed yet""
            continue

        rule_approvers_set = set()
        for approver in rule.approved_by:
            if ""/"" in approver:
                org, name = approver.split(""/"")
                rule_approvers_set.update(gh_get_team_members(org, name))
            else:
                rule_approvers_set.add(approver)
        approvers_intersection = approved_by.intersection(rule_approvers_set)
# If rule requires approvers but they aren't the ones that reviewed PR
if len(approvers_intersection) == 0 and len(rule_approvers_set) > 0:
if reject_reason_score < 10000:
"
107,"if run_url is not None:
msg += f""\nRaised by {run_url}""
gh_post_comment(org, project, args.pr_num, msg, dry_run=args.dry_run)
if __name__ == ""__main__"":
","if run_url is not None:
msg += f""\nRaised by {run_url}""
gh_post_comment(org, project, args.pr_num, msg, dry_run=args.dry_run)
        import traceback
        traceback.print_exc()
if __name__ == ""__main__"":
"
108,"self,
period,
warmup_steps=0,
        process_group=None,
):
super().__init__(process_group)
if warmup_steps < 0:
","self,
period,
warmup_steps=0,
        process_group=None
):
super().__init__(process_group)
if warmup_steps < 0:
"
109,"return self.period_process_group_dict[period]
return None
    def average_parameters(self, params):
        r""""""
        Averages parameters if ``step`` is no less than ``warmup_steps``
and it can be divided by a period in the keys of ``period_process_group_dict``,
where ``step`` is increased by 1 at each iteration in the training loop.
If ``step`` can be divided by multiple periods in the keys of ``period_process_group_dict``,
only the largest period is used, and the corresponding process group is used for averaging parameters.
""""""
if self.step >= self.warmup_steps:
group = self._find_process_group()
if group is not None:
                utils.average_parameters(iter(params), group)
self.step += 1
","return self.period_process_group_dict[period]
return None
    def average_parameters(self, params: Union[Iterable[torch.nn.Parameter], Iterable[Dict[str, torch.nn.Parameter]]]):
        """"""
        Averages parameters or parameter groups of an optimizer if ``step`` is no less than ``warmup_steps``
and it can be divided by a period in the keys of ``period_process_group_dict``,
where ``step`` is increased by 1 at each iteration in the training loop.
If ``step`` can be divided by multiple periods in the keys of ``period_process_group_dict``,
only the largest period is used, and the corresponding process group is used for averaging parameters.
        Args:
            params: The parameters of a model or parameter groups of an optimizer.
""""""
if self.step >= self.warmup_steps:
group = self._find_process_group()
if group is not None:
                utils.average_parameters_or_parameter_groups(params, group)
self.step += 1
"
110,"import torch.utils.hooks as hooks
def _wrap_type_error_to_not_implemented(f):
# functools.wraps doesn't work well with methods in python 2
method_assignments = ('__name__', '__doc__')
assigned = functools.WRAPPER_ASSIGNMENTS
@functools.wraps(f, assigned=assigned)
def wrapped(*args, **kwargs):
        if has_torch_function(args):
            return handle_torch_function(wrapped, args, *args, **kwargs)
try:
return f(*args, **kwargs)
except TypeError:
return NotImplemented
","import torch.utils.hooks as hooks
def _handle_torch_function_and_wrap_type_error_to_not_implemented(f):
# functools.wraps doesn't work well with methods in python 2
method_assignments = ('__name__', '__doc__')
assigned = functools.WRAPPER_ASSIGNMENTS
@functools.wraps(f, assigned=assigned)
def wrapped(*args, **kwargs):
try:
            # See https://github.com/pytorch/pytorch/issues/75462
            if has_torch_function(args):
                return handle_torch_function(wrapped, args, *args, **kwargs)
return f(*args, **kwargs)
except TypeError:
return NotImplemented
"
111,"v_proj_weight=v_proj_weight,
static_k=static_k,
static_v=static_v,
)
is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
","v_proj_weight=v_proj_weight,
static_k=static_k,
static_v=static_v,
            average_attn_weights=average_attn_weights,
)
is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
"
112,"lambda query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v,
add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training=True, key_padding_mask=None,
need_weights=True, attn_mask=None, use_separate_proj_weight=False, q_proj_weight=None, k_proj_weight=None,
            v_proj_weight=None, static_k=None, static_v=None: -1),
torch.nn.functional.multi_margin_loss: (lambda input, target, p=1, margin=1.0, weight=None, size_average=None,
reduce=None, reduction='mean': -1),
torch.nn.functional.multilabel_margin_loss: (lambda input, target, size_average=None, reduce=None,
","lambda query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v,
add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training=True, key_padding_mask=None,
need_weights=True, attn_mask=None, use_separate_proj_weight=False, q_proj_weight=None, k_proj_weight=None,
            v_proj_weight=None, static_k=None, static_v=None, average_attn_weights=None: -1),
torch.nn.functional.multi_margin_loss: (lambda input, target, p=1, margin=1.0, weight=None, size_average=None,
reduce=None, reduction='mean': -1),
torch.nn.functional.multilabel_margin_loss: (lambda input, target, size_average=None, reduce=None,
"
113,"return self.extend(parameters)
def __dir__(self):
        return list(range(self._size))
def append(self, value: Any) -> 'ParameterList':
""""""Appends a given value at the end of the list.
","return self.extend(parameters)
def __dir__(self):
        keys = super(ParameterList, self).__dir__()
        keys = [key for key in keys if not key.isdigit()]
        return keys
def append(self, value: Any) -> 'ParameterList':
""""""Appends a given value at the end of the list.
"
114,""""""")
FW_DERIVATIVE_FORBID_TEMPLATE = CodeTemplate(""""""\
TORCH_CHECK_NOT_IMPLEMENTED(!(${cond}), ""Trying to use forward AD with ${msg} that does not support it."");
"""""")
FW_DERIVATIVE_FORBID_LIST_TEMPLATE = CodeTemplate(""""""\
for (const auto& _t: ${arg}) {
    TORCH_CHECK_NOT_IMPLEMENTED(!(${cond}), ""Trying to use forward AD with ${msg} that does not support it."");
}
"""""")
",""""""")
FW_DERIVATIVE_FORBID_TEMPLATE = CodeTemplate(""""""\
TORCH_CHECK_NOT_IMPLEMENTED(!(${cond}), ""Trying to use forward AD with ${name} that does not support it ${msg}"");
"""""")
FW_DERIVATIVE_FORBID_LIST_TEMPLATE = CodeTemplate(""""""\
for (const auto& _t: ${arg}) {
    TORCH_CHECK_NOT_IMPLEMENTED(!(${cond}), ""Trying to use forward AD with ${name} that does not support it ${msg}"");
}
"""""")
"
115,"RuntimeError: If shapes are incompatible.
""""""
# This wrapper exists to support variadic args.
    # TODO Movie this to C++ once the jit has better support for torch.Size.
if not torch.jit.is_tracing():
max_len = 0
for shape in shapes:
","RuntimeError: If shapes are incompatible.
""""""
# This wrapper exists to support variadic args.
    # TODO Move this to C++ once the jit has better support for torch.Size.
if not torch.jit.is_tracing():
max_len = 0
for shape in shapes:
"
116,".. warning::
Module should be already placed on the destination device or
        device is set properly using torch.cuda.set_device(device_id).
FSDP will get compute device from module first, if module device
is CPU, FSDP will then get compute device from current device.
",".. warning::
Module should be already placed on the destination device or
        device is set properly using ``torch.cuda.set_device(device_id)``.
FSDP will get compute device from module first, if module device
is CPU, FSDP will then get compute device from current device.
"
117,">>> ) -> bool:
>>>     return unwrapped_params >= min_num_params
        backward_prefetch: (Optional[BackwardPrefetch]):
This is an experimental feature that is subject to change in the
the near future. It allows users to enable two different backward_prefetch
algorithms to help backward communication and computation overlapping.
",">>> ) -> bool:
>>>     return unwrapped_params >= min_num_params
        backward_prefetch (Optional[BackwardPrefetch]):
This is an experimental feature that is subject to change in the
the near future. It allows users to enable two different backward_prefetch
algorithms to help backward communication and computation overlapping.
"
118,"corresponding to the local param shard will persist after the
context manager exits (unless ``writeback=False``, in which case
changes will be discarded). In the case where FSDP does not shard
            the parameters, currently only when world_size == 1, the
modification is persisted regardless of ``writeback``.
.. warning:: Note that ``rank0_only=True`` in conjunction with
","corresponding to the local param shard will persist after the
context manager exits (unless ``writeback=False``, in which case
changes will be discarded). In the case where FSDP does not shard
            the parameters, currently only when ``world_size == 1``, the
modification is persisted regardless of ``writeback``.
.. warning:: Note that ``rank0_only=True`` in conjunction with
"
119,"graph = tracer.trace(root, concrete_args)
name = root.__class__.__name__ if isinstance(root, torch.nn.Module) else root.__name__
return GraphModule(tracer.root, graph, name)
","graph = tracer.trace(root, concrete_args)
name = root.__class__.__name__ if isinstance(root, torch.nn.Module) else root.__name__
return GraphModule(tracer.root, graph, name)


@wrap
def _assert_is_none(value, msg):
    assert value is None, msg
"
120,"default_fused_per_channel_wt_fake_quant,
default_embedding_fake_quant,
default_embedding_fake_quant_4bit,
)
from .observer import (
","default_fused_per_channel_wt_fake_quant,
default_embedding_fake_quant,
default_embedding_fake_quant_4bit,
    fused_wt_fake_quant_range_neg_127_to_127,
    fused_per_channel_wt_fake_quant_range_neg_127_to_127,
)
from .observer import (
"
121,"reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
Given running min/max as :math:`x_\text{min}` and :math:`x_\text{max}`,
scale :math:`s` and zero point :math:`z` are computed as:
","reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.
Given running min/max as :math:`x_\text{min}` and :math:`x_\text{max}`,
scale :math:`s` and zero point :math:`z` are computed as:
"
122,"default_per_channel_weight_observer,
default_placeholder_observer,
default_weight_observer,
default_reuse_input_observer,
)
import warnings
","default_per_channel_weight_observer,
default_placeholder_observer,
default_weight_observer,
    weight_observer_range_neg_127_to_127,
    per_channel_weight_observer_range_neg_127_to_127,
default_reuse_input_observer,
)
import warnings
"
123,"for pattern, pattern_info in self.patterns.items():
if pattern.matches(module):
if pattern_info.action == _ModuleProviderAction.MOCK:
                        mocked_modules[module].append(field)
                    return
if dependencies:
all_dependencies = []
","for pattern, pattern_info in self.patterns.items():
if pattern.matches(module):
if pattern_info.action == _ModuleProviderAction.MOCK:
                        raise NotImplementedError(
                            f""Object '{field}' from module {module} was mocked out during packaging ""
                            f""but is being used in resource - {resource} in package {package}. ""
                            ""If this error is happening during 'save_pickle', please ensure that your ""
                            ""pickled object doesn't contain any mocked objects. Try interning or externing""
                            f""{module} if {field} is supposed to be in the package.""
                        )
                    else:
                        return
if dependencies:
all_dependencies = []
"
124,"pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()

name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
","pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()
        mocked_modules = defaultdict(list)
name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
"
125,"module: nn.Module,
process_group: Optional[ProcessGroup] = None,
cpu_offload: Optional[CPUOffload] = None,
        fsdp_auto_wrap_policy: Optional[Callable] = None,
backward_prefetch: Optional[BackwardPrefetch] = None,
):
torch._C._log_api_usage_once(""torch.distributed.fsdp"")
super().__init__()
        # if fsdp_auto_wrap_policy is specified, submodules should not be
# already wrapped, otherwise we'd attempt to double wrap them resulting
# in errors.
        if fsdp_auto_wrap_policy is not None:
self._check_wrapped(
module,
check_fn=lambda mod: not isinstance(mod, FullyShardedDataParallel),
","module: nn.Module,
process_group: Optional[ProcessGroup] = None,
cpu_offload: Optional[CPUOffload] = None,
        auto_wrap_policy: Optional[Callable] = None,
backward_prefetch: Optional[BackwardPrefetch] = None,
):
torch._C._log_api_usage_once(""torch.distributed.fsdp"")
super().__init__()
        # if auto_wrap_policy is specified, submodules should not be
# already wrapped, otherwise we'd attempt to double wrap them resulting
# in errors.
        if auto_wrap_policy is not None:
self._check_wrapped(
module,
check_fn=lambda mod: not isinstance(mod, FullyShardedDataParallel),
"
126,"from ._symbolic_trace import Tracer
from ._compatibility import compatibility
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
@compatibility(is_backward_compatible=True)
class Interpreter:
","from ._symbolic_trace import Tracer
from ._compatibility import compatibility
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
import inspect
@compatibility(is_backward_compatible=True)
class Interpreter:
"
127,"Return:
qconfig
""""""

    if backend == 'fbgemm':
        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
                          weight=default_per_channel_weight_observer)
    elif backend == 'qnnpack':
        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),
                          weight=default_weight_observer)
else:
        qconfig = default_qconfig
return qconfig
default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
","Return:
qconfig
""""""
    if version == 0:
        if backend == 'fbgemm':
            qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
                              weight=default_per_channel_weight_observer)
        elif backend == 'qnnpack':
            qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),
                              weight=default_weight_observer)
        else:
            qconfig = default_qconfig
else:
        raise AssertionError(""Version number: "" + str(version) +
                             "" in get_default_qconfig is not supported. Version number must be 0"")

return qconfig
default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
"
128,"import collections.abc
from contextlib import contextmanager
from typing import Optional, List, Sequence
","import collections.abc
import copy
from contextlib import contextmanager
from typing import Optional, List, Sequence
"
129,"else:
return _convert(ret, cls)
def __dlpack__(self, stream=None):
""""""
Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_
","else:
return _convert(ret, cls)
    __torch_dispatch__ = _C._disabled_torch_dispatch_impl

def __dlpack__(self, stream=None):
""""""
Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_
"
130,"torch.sigmoid,
torch.squeeze,
torch.stack,
torch.tanh,
torch.unsqueeze,
torch.cat,
","torch.sigmoid,
torch.squeeze,
torch.stack,
    torch.sum,
torch.tanh,
torch.unsqueeze,
torch.cat,
"
131,"raise ValueError(""Input shape must be `(N, C, T, H, W)`!"")
return ops.quantized.conv_transpose3d(
input, self._packed_params, self.scale, self.zero_point)
","raise ValueError(""Input shape must be `(N, C, T, H, W)`!"")
return ops.quantized.conv_transpose3d(
input, self._packed_params, self.scale, self.zero_point)

    @classmethod
    def from_reference(cls, ref_qconvt, output_scale, output_zero_point):
        return _ConvTransposeNd.from_reference(cls, ref_qconvt, output_scale, output_zero_point)
"
132,"""""""
raise RuntimeError(
""Reached a code path in Module.set_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md ""
""to report this bug."")
def _apply(self, fn):
","""""""
raise RuntimeError(
""Reached a code path in Module.set_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml ""
""to report this bug."")
def _apply(self, fn):
"
133,"self.main_datapipe,
self.num_instances,
self.buffer_size,
            dill_function,
self.drop_none,
) = state
        if DILL_AVAILABLE:
            self.classifier_fn = dill.loads(dill_function)  # type: ignore[assignment]
        else:
            self.classifier_fn = dill_function  # type: ignore[assignment]
self._datapipe_iterator = None
self.current_buffer_usage = 0
self.child_buffers = [deque() for _ in range(self.num_instances)]
","self.main_datapipe,
self.num_instances,
self.buffer_size,
            serialized_fn_with_method,
self.drop_none,
) = state
        self.classifier_fn = deserialize_fn(serialized_fn_with_method)
self._datapipe_iterator = None
self.current_buffer_usage = 0
self.child_buffers = [deque() for _ in range(self.num_instances)]
"
134,"def __init__(self, *args):
self._importers: List[Importer] = list(args)
    def _check_if_fileless_package(self, module):
if not hasattr(module, ""__path__""):
return False
if not hasattr(module, ""__file__""):
","def __init__(self, *args):
self._importers: List[Importer] = list(args)
    def _is_torchpackage_dummy(self, module):
        """"""Returns true iff this module is an empty PackageNode in a torch.package.

        If you intern `a.b` but never use `a` in your code, then `a` will be an
        empty module with no source. This can break cases where we are trying to
        re-package an object after adding a real dependency on `a`, since
        OrderedImportere will resolve `a` to the dummy package and stop there.

        See: https://github.com/pytorch/pytorch/pull/71520#issuecomment-1029603769
        """"""
        if not getattr(module, ""__torch_package__"", False):
            return False
if not hasattr(module, ""__path__""):
return False
if not hasattr(module, ""__file__""):
"
135,"training: apply dropout if is ``True``. Default: ``True``
inplace: If set to ``True``, will do this operation in-place. Default: ``False``
""""""
    # This is 100% the same code as dropout2d. We duplicate this code so that
    # stack traces are not confusing.
if has_torch_function_unary(input):
return handle_torch_function(dropout3d, (input,), input, p=p, training=training, inplace=inplace)
if p < 0.0 or p > 1.0:
raise ValueError(""dropout probability has to be between 0 and 1, "" ""but got {}"".format(p))
    return _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
def feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:
","training: apply dropout if is ``True``. Default: ``True``
inplace: If set to ``True``, will do this operation in-place. Default: ``False``
""""""
if has_torch_function_unary(input):
return handle_torch_function(dropout3d, (input,), input, p=p, training=training, inplace=inplace)
if p < 0.0 or p > 1.0:
raise ValueError(""dropout probability has to be between 0 and 1, "" ""but got {}"".format(p))
    inp_dim = input.dim()
    if inp_dim not in (4, 5):
        warn_msg = (f""dropout3d: Received a {inp_dim}-D input to dropout3d, which is deprecated ""
                    ""and will result in an error in a future release. To retain the behavior ""
                    ""and silence this warning, please use dropout instead. Note that dropout3d ""
                    ""exists to provide channel-wise dropout on inputs with 3 spatial dimensions, ""
                    ""a channel dimension, and an optional batch dimension (i.e. 4D or 5D inputs)."")
        warnings.warn(warn_msg)

    is_batched = inp_dim == 5
    if not is_batched:
        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)

    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)

    if not is_batched:
        result = result.squeeze_(0) if inplace else result.squeeze(0)
    return result
def feature_alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:
"
136,"""of the TensorRT region!""
)
layer = network.add_activation(input_val, operation_type)
    if alpha:
layer.alpha = alpha
    if beta:
layer.beta = beta
set_layer_name(layer, target, name)
return layer.get_output(0)
","""of the TensorRT region!""
)
layer = network.add_activation(input_val, operation_type)
    if alpha is not None:
layer.alpha = alpha
    if beta is not None:
layer.beta = beta
set_layer_name(layer, target, name)
return layer.get_output(0)
"
137,"elif os == ""windows"":
# We don't build CUDA 10.2 for window see https://github.com/pytorch/pytorch/issues/65648
arches += list_without(CUDA_ARCHES, [""10.2""])
    for arch_version in [""cpu""] + CUDA_ARCHES:
for libtorch_variant in libtorch_variants:
# We don't currently build libtorch for rocm
# one of the values in the following list must be exactly
","elif os == ""windows"":
# We don't build CUDA 10.2 for window see https://github.com/pytorch/pytorch/issues/65648
arches += list_without(CUDA_ARCHES, [""10.2""])
    for arch_version in arches:
for libtorch_variant in libtorch_variants:
# We don't currently build libtorch for rocm
# one of the values in the following list must be exactly
"
138,"return upgraderBytecodeList;
}
} // namespace jit
} // namespace torch

"""""")
UPGRADER_MOBILE_FILE_NAME = ""upgrader_mobile.cpp""
","return upgraderBytecodeList;
}
// clang-format on

} // namespace jit
} // namespace torch
"""""")
UPGRADER_MOBILE_FILE_NAME = ""upgrader_mobile.cpp""
"
139,"""it's type is {type(register_size_from_yaml)}. An int type is expected."")
return str(register_size_from_yaml)
def construct_one_operator_in_version_map(operator_name: str, upgrader_list: List[Any]) -> str:
    upgraders_in_version_map_part = []
    for one_upgrader in upgrader_list:
        upgraders_in_version_map_part.append(
            ONE_UPGRADER_IN_VERSION_MAP.substitute(
                upgrader_min_version=one_upgrader[0],
                upgrader_max_version=one_upgrader[1],
                upgrader_name=one_upgrader[2],
                bytecode_func_index=one_upgrader[3]
            )
        )
    return ONE_OPERATOR_IN_VERSION_MAP.substitute(
        operator_name=operator_name,
        upgrader_list_in_version_map="""".join(upgraders_in_version_map_part)
    )

def construct_version_maps(upgrader_bytecode_function_to_index_map: Dict[str, Any]) -> str:
version_map = torch._C._get_operator_version_map()
sorted_version_map_ = sorted(version_map.items(), key=lambda item: item[0])  # type: ignore[no-any-return]
    sorted_version_map = {
        name: sorted(lst, key=lambda e: e.upgrader_name) for name, lst in sorted_version_map_  # type: ignore[no-any-return]
    }
operator_list_in_version_map_part = []
for op_name in sorted_version_map:
","""it's type is {type(register_size_from_yaml)}. An int type is expected."")
return str(register_size_from_yaml)
def construct_version_maps(upgrader_bytecode_function_to_index_map: Dict[str, Any]) -> str:
version_map = torch._C._get_operator_version_map()
sorted_version_map_ = sorted(version_map.items(), key=lambda item: item[0])  # type: ignore[no-any-return]
    sorted_version_map = {name: lst for name, lst in sorted_version_map_}
operator_list_in_version_map_part = []
for op_name in sorted_version_map:
"
140,"index = 0
for upgrader_bytecode in upgrader_dict:
for upgrader_name, bytecode in upgrader_bytecode.items():
upgrader_bytecode_function_to_index_map[upgrader_name] = index
index += 1
return upgrader_bytecode_function_to_index_map
","index = 0
for upgrader_bytecode in upgrader_dict:
for upgrader_name, bytecode in upgrader_bytecode.items():
            if upgrader_name in EXCLUE_UPGRADER_SET:
                continue
upgrader_bytecode_function_to_index_map[upgrader_name] = index
index += 1
return upgrader_bytecode_function_to_index_map
"
141,"self.merge_ghstack_into(repo)
if not dry_run:
            repo.push(self.default_branch())
@dataclass
","self.merge_ghstack_into(repo)
if not dry_run:
            repo.push(self.default_branch(), dry_run)
@dataclass
"
142,"import builtins
import math
import warnings
if TYPE_CHECKING:
","import builtins
import math
import warnings
import inspect
if TYPE_CHECKING:
"
143,"return _InsertPoint(self, n.append)
@compatibility(is_backward_compatible=True)
    def placeholder(self, name: str, type_expr: Optional[Any] = None) -> Node:
""""""
Insert a ``placeholder`` node into the Graph. A ``placeholder`` represents
a function input.
","return _InsertPoint(self, n.append)
@compatibility(is_backward_compatible=True)
    def placeholder(self, name: str, type_expr: Optional[Any] = None,
                    default_value : Any = inspect.Signature.empty) -> Node:
""""""
Insert a ``placeholder`` node into the Graph. A ``placeholder`` represents
a function input.
"
144,"to undefined behavior.
.. warning ::
        Only one pair of hooks is allowed at a time. Recursively nesting this
        context-manager is not yet supported.
""""""
def __init__(self, pack_hook: Callable[[torch.Tensor], Any], unpack_hook: Callable[[Any], torch.Tensor]):
self.pack_hook = pack_hook
self.unpack_hook = unpack_hook
def __enter__(self):
        torch._C._autograd._register_saved_tensors_default_hooks(self.pack_hook, self.unpack_hook)
def __exit__(self, *args: Any):
        torch._C._autograd._reset_saved_tensors_default_hooks()
class save_on_cpu():
""""""Context-manager under which tensors saved by the forward pass will be
stored on cpu, then retrieved for backward.
","to undefined behavior.
.. warning ::
        Only one pair of hooks is allowed at a time. When recursively nesting this
        context-manager, only the inner-most pair of hooks will be applied.
""""""
def __init__(self, pack_hook: Callable[[torch.Tensor], Any], unpack_hook: Callable[[Any], torch.Tensor]):
self.pack_hook = pack_hook
self.unpack_hook = unpack_hook
def __enter__(self):
        torch._C._autograd._push_saved_tensors_default_hooks(self.pack_hook, self.unpack_hook)
def __exit__(self, *args: Any):
        torch._C._autograd._pop_saved_tensors_default_hooks()
class save_on_cpu(saved_tensors_hooks):
""""""Context-manager under which tensors saved by the forward pass will be
stored on cpu, then retrieved for backward.
"
145,"return kernel_positions
# Grab positional ranges of all kernel launches
    get_kernel_positions = list(find_kernel_bounds(string))
output_string = string
# Replace each CUDA kernel with a HIP kernel.
","return kernel_positions
    # Replace comments and string literals from the code so that find_kernel_bounds does not
    # wrongly capture kernels in comments and string literals.
    # This function replaces them with ""x"" to keep positions.
    def mask_comments(string):
        in_comment = ''
        prev_c = ''
        new_string = ''
        for c in string:
            if in_comment == '':
                # Outside comments
                if c == '/' and prev_c == '/':
                    in_comment = '//'
                elif c == '*' and prev_c == '/':
                    in_comment = '/*'
                elif c == '""' and prev_c != '\\' and prev_c != ""'"":
                    in_comment = '""'
            elif in_comment == '//':
                # In // xxx
                if c == '\r' or c == '\n':
                    in_comment = ''
            elif in_comment == '/*':
                # In /* xxx */
                if c == '/' and prev_c == '*':
                    in_comment = ''
            elif in_comment == '""':
                # In """"
                if c == '""' and prev_c != '\\':
                    in_comment = ''
            prev_c = c
            if in_comment == '':
                new_string += c
            else:
                new_string += 'x'
        return new_string

# Grab positional ranges of all kernel launches
    get_kernel_positions = list(find_kernel_bounds(mask_comments(string)))
output_string = string
# Replace each CUDA kernel with a HIP kernel.
"
146,"factory_kwargs = {'device': device, 'dtype': dtype}
super(BatchNorm2d, self).__init__(num_features, **factory_kwargs)
self.eps = eps
        self.scale = 1.0
        self.zero_point = 0
def forward(self, input):
return torch.ops.quantized.batch_norm2d(input, self.weight, self.bias, self.running_mean,
","factory_kwargs = {'device': device, 'dtype': dtype}
super(BatchNorm2d, self).__init__(num_features, **factory_kwargs)
self.eps = eps
        self.register_buffer('scale', torch.tensor(1.0, **factory_kwargs))
        self.register_buffer('zero_point', torch.tensor(0, **factory_kwargs))
def forward(self, input):
return torch.ops.quantized.batch_norm2d(input, self.weight, self.bias, self.running_mean,
"
147,"number = self._estimate_block_size(min_run_time=0.05)
def time_hook() -> float:
            return self._timer.timeit(number)
def stop_hook(times: List[float]) -> bool:
if len(times) > 3:
","number = self._estimate_block_size(min_run_time=0.05)
def time_hook() -> float:
            return self._timeit(number)
def stop_hook(times: List[float]) -> bool:
if len(times) > 3:
"
148,"def default_collate(batch):
    r""""""Puts each data field into a tensor with outer dimension batch size""""""
elem = batch[0]
elem_type = type(elem)
if isinstance(elem, torch.Tensor):
","def default_collate(batch):
    r""""""
        Function that takes in a batch of data and puts the elements within the batch
        into a tensor with an additional outer dimension - batch size. The exact output type can be
        a :class:`torch.Tensor`, a `Sequence` of :class:`torch.Tensor`, a
        Collection of :class:`torch.Tensor`, or left unchanged, depending on the input type.
        This is used as the default function for collation when
        `batch_size` or `batch_sampler` is defined in :class:`~torch.utils.data.DataLoader`.

        Here is the general input type (based on the type of the element within the batch) to output type mapping:
        * :class:`torch.Tensor` -> :class:`torch.Tensor` (with an added outer dimension batch size)
        * NumPy Arrays -> :class:`torch.Tensor`
        * `float` -> :class:`torch.Tensor`
        * `int` -> :class:`torch.Tensor`
        * `str` -> `str` (unchanged)
        * `bytes` -> `bytes` (unchanged)
        * `Mapping[K, V_i]` -> `Mapping[K, default_collate([V_1, V_2, ...])]`
        * `NamedTuple[V1_i, V2_i, ...]` -> `NamedTuple[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]`
        * `Sequence[V1_i, V2_i, ...]` -> `Sequence[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]`

        Args:
            batch: a single batch to be collated
        Examples:
            >>> # Example with a batch of `int`s:
            >>> default_collate([0, 1, 2, 3])
            tensor([0, 1, 2, 3])
            >>> # Example with a batch of `str`s:
            >>> default_collate(['a', 'b', 'c'])
            ['a', 'b', 'c']
            >>> # Example with `Map` inside the batch:
            >>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]
            {'A': tensor([  0, 100]), 'B': tensor([  1, 100])}
            >>> # Example with `NamedTuple` inside the batch:
            >>> Point = namedtuple('Point', ['x', 'y'])
            >>> default_collate([Point(0, 0), Point(1, 1)])
            Point(x=tensor([0, 1]), y=tensor([0, 1]))
            >>> # Example with `Tuple` inside the batch:
            >>> default_collate([(0, 1), (2, 3)])
            [tensor([0, 2]), tensor([1, 3])]
            >>> # Example with `List` inside the batch:
            >>> default_collate([[0, 1], [2, 3]])
            [tensor([0, 2]), tensor([1, 3])]
    """"""
elem = batch[0]
elem_type = type(elem)
if isinstance(elem, torch.Tensor):
"
149,"print('Adjusting learning rate'
' of group {} to {:.4e}.'.format(group, lr))
else:
                print('Epoch {:5d}: adjusting learning rate'
                      ' of group {} to {:.4e}.'.format(epoch, group, lr))
def step(self, epoch=None):
","print('Adjusting learning rate'
' of group {} to {:.4e}.'.format(group, lr))
else:
                epoch_str = (""%.2f"" if isinstance(epoch, float) else
                             ""%.5d"") % epoch
                print('Epoch {}: adjusting learning rate'
                      ' of group {} to {:.4e}.'.format(epoch_str, group, lr))
def step(self, epoch=None):
"
150,"'isnan', 'isposinf', 'isneginf', 'isinf', 'signbit', 'isin',
# Functions return none are not differentiable
'record_stream',
}
# The C -> R functions at the time of adding this are still being audited and tested
","'isnan', 'isposinf', 'isneginf', 'isinf', 'signbit', 'isin',
# Functions return none are not differentiable
'record_stream',
    # These functions are not differentiable
    'logical_and', 'logical_xor', 'logical_not', 'logical_or',
}
# The C -> R functions at the time of adding this are still being audited and tested
"
151,"# future dequants, to make the logic easier to understand
(arg_as_input_target_dtype != torch.float) and
# if arg is a bool tensor or not a tensor, do not insert observer
            (arg_as_output_target_dtype not in (torch.bool, None))
)
else:
","# future dequants, to make the logic easier to understand
(arg_as_input_target_dtype != torch.float) and
# if arg is a bool tensor or not a tensor, do not insert observer
            (arg_as_output_target_dtype not in (torch.bool, None)) and
            # if qconfig is reuse_input qconfig, we won't insert extra observer for input
            not is_reuse_input_qconfig_
)
else:
"
152,"return _is_memoryless(act.activation_post_process)
else:
return _is_memoryless(act)
","return _is_memoryless(act.activation_post_process)
else:
return _is_memoryless(act)

def is_reuse_input_qconfig(qconfig: Union[QConfig, QConfigDynamic, None]):
    return qconfig is not None and \
        isinstance(qconfig.activation(), ReuseInputObserver) and \
        isinstance(qconfig.weight(), NoopObserver)
"
153,"def _check_input_dim(self, input):
if input.dim() == 2:
raise ValueError(
                'InstanceNorm1d returns 0-filled tensor to 2D tensor.'
                'This is because InstanceNorm1d reshapes inputs to'
                '(1, N * C, ...) from (N, C,...) and this makes'
'variances 0.'
)
if input.dim() != 3:
","def _check_input_dim(self, input):
if input.dim() == 2:
raise ValueError(
                'InstanceNorm1d returns 0-filled tensor to 2D tensor. '
                'This is because InstanceNorm1d reshapes inputs to '
                '(1, N * C, ...) from (N, C,...) and this makes '
'variances 0.'
)
if input.dim() != 3:
"
154,"function = functools.partial(IterableDataset.functions[attribute_name], self)
return function
else:
            raise AttributeError
def __getstate__(self):
if IterableDataset.getstate_hook is not None:
","function = functools.partial(IterableDataset.functions[attribute_name], self)
return function
else:
            raise AttributeError(""'{0}' object has no attribute '{1}"".format(self.__class__.__name__, attribute_name))
def __getstate__(self):
if IterableDataset.getstate_hook is not None:
"
155,"from torch.jit._freeze import freeze, optimize_for_inference, run_frozen_optimizations
import warnings
from importlib.machinery import SourceFileLoader

import os
try:
    shape_function_fp = (
        f""{os.path.dirname(os.path.realpath(torch.__file__))}/include/torch/csrc/jit/runtime/shape_functions.h""
    )
    _shapes = SourceFileLoader(""shape_functions"", shape_function_fp).load_module()  # type: ignore[call-arg]
except Exception as e:
    warnings.warn(f""Couldn't load shape functions: {e}"")

# For backwards compatibility
_fork = fork
_wait = wait
","from torch.jit._freeze import freeze, optimize_for_inference, run_frozen_optimizations
# For backwards compatibility
_fork = fork
_wait = wait
"
156,"elem_size = len(next(it))
if not all(len(elem) == elem_size for elem in it):
raise RuntimeError('each element in list of batch should be of equal size')
        transposed = zip(*batch)
        return [default_collate(samples) for samples in transposed]
raise TypeError(default_collate_err_msg_format.format(elem_type))
","elem_size = len(next(it))
if not all(len(elem) == elem_size for elem in it):
raise RuntimeError('each element in list of batch should be of equal size')
        transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.

        if isinstance(elem, tuple):
            return [default_collate(samples) for samples in transposed]  # Backwards compatibility.
        else:
            try:
                return elem_type([default_collate(samples) for samples in transposed])
            except TypeError:
                # The sequence type may not support `__init__(iterable)` (e.g., `range`).
                return [default_collate(samples) for samples in transposed]
raise TypeError(default_collate_err_msg_format.format(elem_type))
"
157,"return {reverse_lambda.inner_call()}
}}
);
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, self, view_meta);
// See  Note [Propagating strides in the functionalization pass]
at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
return out;
","return {reverse_lambda.inner_call()}
}}
);
      auto out = at::functionalization::impl::create_functional_tensor_with_view_meta(tmp_output, {view_tensor_name}, view_meta);
// See  Note [Propagating strides in the functionalization pass]
at::functionalization::impl::set_sizes_strides_offset(out, reference_tensor_output);
return out;
"
158,"buf = tensor.numpy().tobytes()[:tensor_size]
return _unpickler(io.BytesIO(buf)).load()
def all_gather_object(object_list, obj, group=None):
""""""
","buf = tensor.numpy().tobytes()[:tensor_size]
return _unpickler(io.BytesIO(buf)).load()
def _check_for_nccl_backend(group):
    pg = group or _get_default_group()
    # It is not expected for PG to be wrapped many times, but support it just
    # in case
    while isinstance(pg, _ProcessGroupWrapper):
        pg = pg.wrapped_pg

    return (
        is_nccl_available() and
        isinstance(pg, ProcessGroupNCCL)
    )
def all_gather_object(object_list, obj, group=None):
""""""
"
159,"nn.Conv3d: nnq.Conv3d,
nn.ConvTranspose1d: nnq.ConvTranspose1d,
nn.ConvTranspose2d: nnq.ConvTranspose2d,
nn.ELU: nnq.ELU,
nn.Embedding: nnq.Embedding,
nn.EmbeddingBag: nnq.EmbeddingBag,
","nn.Conv3d: nnq.Conv3d,
nn.ConvTranspose1d: nnq.ConvTranspose1d,
nn.ConvTranspose2d: nnq.ConvTranspose2d,
    nn.ConvTranspose3d: nnq.ConvTranspose3d,
nn.ELU: nnq.ELU,
nn.Embedding: nnq.Embedding,
nn.EmbeddingBag: nnq.EmbeddingBag,
"
160,"print(json.dumps(err_msg._asdict()), flush=True)
exit(0)
with concurrent.futures.ThreadPoolExecutor(
max_workers=os.cpu_count(),
thread_name_prefix=""Thread"",
","print(json.dumps(err_msg._asdict()), flush=True)
exit(0)
    abs_build_dir = Path(args.build_dir).resolve()

with concurrent.futures.ThreadPoolExecutor(
max_workers=os.cpu_count(),
thread_name_prefix=""Thread"",
"
161,"and if only nondeterministic algorithms are available they will throw a
:class:`RuntimeError` when called.
The following normally-nondeterministic operations will act
deterministically when ``mode=True``:
","and if only nondeterministic algorithms are available they will throw a
:class:`RuntimeError` when called.
    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative
        interface for this feature.

The following normally-nondeterministic operations will act
deterministically when ``mode=True``:
"
162,"return list(map(_to_device, devices))

class TensorPipeRpcBackendOptions(_TensorPipeRpcBackendOptionsBase):
r""""""
The backend options for
","return list(map(_to_device, devices))
class TensorPipeRpcBackendOptions(_TensorPipeRpcBackendOptionsBase):
r""""""
The backend options for
"
163,"requests, the agent will properly synchronize CUDA streams for
all devices in this ``List``.
""""""
def __init__(
self,
*,
","requests, the agent will properly synchronize CUDA streams for
all devices in this ``List``.
""""""

def __init__(
self,
*,
"
164,"torch_dtype_from_trt(self.engine.get_binding_dtype(idx))
for idx in self.output_binding_indices_in_order
]
def _check_initialized(self):
if not self.initialized:
","torch_dtype_from_trt(self.engine.get_binding_dtype(idx))
for idx in self.output_binding_indices_in_order
]
        self.hidden_output_dtypes: Sequence[torch.dtype] = [
            torch_dtype_from_trt(self.engine.get_binding_dtype(idx))
            for idx in self.hidden_output_binding_indices_in_order
        ]
def _check_initialized(self):
if not self.initialized:
"
165,"for p in group['params']:
state = self.state[p]
state['step'] = 0
                state['sum'] = torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)
def share_memory(self):
for group in self.param_groups:
","for p in group['params']:
state = self.state[p]
state['step'] = 0
                init_value = complex(initial_accumulator_value, initial_accumulator_value) if torch.is_complex(p) \
                    else initial_accumulator_value
                state['sum'] = torch.full_like(p, init_value, memory_format=torch.preserve_format)
def share_memory(self):
for group in self.param_groups:
"
166,"from torch.nn.intrinsic import _FusedModule
from .fx import Fuser  # noqa: F401
from .fx import prepare, convert  # noqa: F401
from .fx._convert_new import _convert_new  # noqa: F401
from .fx import get_fbgemm_backend_config_dict  # noqa: F401
from .fx import get_tensorrt_backend_config_dict  # noqa: F401
from .fx.graph_module import ObservedGraphModule, QuantizedGraphModule
","from torch.nn.intrinsic import _FusedModule
from .fx import Fuser  # noqa: F401
from .fx import prepare, convert  # noqa: F401
from .fx import get_fbgemm_backend_config_dict  # noqa: F401
from .fx import get_tensorrt_backend_config_dict  # noqa: F401
from .fx.graph_module import ObservedGraphModule, QuantizedGraphModule
"
167,"convert_custom_config_dict,
is_standalone_module=True,
)

def _convert_fx_new(
        graph_module: GraphModule, is_reference: bool = False,
        convert_custom_config_dict: Dict[str, Any] = None,
        _remove_qconfig: bool = True) -> QuantizedGraphModule:
    assert is_reference
    if convert_custom_config_dict is None:
        convert_custom_config_dict = {}

    _check_is_graph_module(graph_module)
    check_is_valid_convert_custom_config_dict(convert_custom_config_dict)

    quantized = _convert_new(
        graph_module, is_reference, convert_custom_config_dict,
        False, _remove_qconfig_flag=_remove_qconfig)

    preserved_attributes = convert_custom_config_dict.get(""preserved_attributes"", [])
    for attr_name in preserved_attributes:
        setattr(quantized, attr_name, getattr(graph_module, attr_name))
    return quantized
","convert_custom_config_dict,
is_standalone_module=True,
)
"
168,"class QuantizationTracer(Tracer):
def __init__(
            self,
            skipped_module_names: List[str],
            skipped_module_classes: List[Callable]):
super().__init__()
self.skipped_module_names = skipped_module_names
self.skipped_module_classes = skipped_module_classes
","class QuantizationTracer(Tracer):
def __init__(
        self, skipped_module_names: List[str], skipped_module_classes: List[Callable]
    ):
super().__init__()
self.skipped_module_names = skipped_module_names
self.skipped_module_classes = skipped_module_classes
"
169,"""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.prepare_qat_fx"")
    assert model.training, 'prepare_qat_fx only works for models in  ' + \
        'train mode'
return _prepare_fx(
model,
qconfig_dict,
prepare_custom_config_dict,
        backend_config_dict=backend_config_dict)
def _convert_fx(
        graph_module: GraphModule, is_reference: bool,
        convert_custom_config_dict: Dict[str, Any] = None,
        is_standalone_module: bool = False,
        _remove_qconfig: bool = True) -> QuantizedGraphModule:
"""""" `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`
""""""
if convert_custom_config_dict is None:
","""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.prepare_qat_fx"")
    assert model.training, ""prepare_qat_fx only works for models in  "" + ""train mode""
return _prepare_fx(
model,
qconfig_dict,
prepare_custom_config_dict,
        backend_config_dict=backend_config_dict,
    )

def _convert_fx(
    graph_module: GraphModule,
    is_reference: bool,
    convert_custom_config_dict: Optional[Dict[str, Any]] = None,
    is_standalone_module: bool = False,
    _remove_qconfig: bool = True,
) -> QuantizedGraphModule:
"""""" `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`
""""""
if convert_custom_config_dict is None:
"
170,"const_subgraph: Optional[torch.fx.Graph] = None,
fx_const_folded_attrs_name: str = None,
):
super().__init__(root, graph)
self.const_subgraph_module = (
None
","const_subgraph: Optional[torch.fx.Graph] = None,
fx_const_folded_attrs_name: str = None,
):
        # In init, we set graph's owning module to root which will make graph's
        # owning module be None because graph already have a owning module. We
        # need owning module to run DCE. To work around we set the number of
        # graph's owners to 0.
        graph._owners = 0
super().__init__(root, graph)
self.const_subgraph_module = (
None
"
171,"add_docstr_all('index_add_',
r""""""
index_add_(dim, index, tensor, *, alpha=1) -> Tensor
Accumulate the elements of :attr:`alpha` times :attr:`tensor` into the :attr:`self`
tensor by adding to the indices in the order given in :attr:`index`. For example,
if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\ th row of
:attr:`tensor` is subtracted from the ``j``\ th row of :attr:`self`.
The :attr:`dim`\ th dimension of :attr:`tensor` must have the same size as the
length of :attr:`index` (which must be a vector), and all other dimensions must
match :attr:`self`, or an error will be raised.
","add_docstr_all('index_add_',
r""""""
index_add_(dim, index, source, *, alpha=1) -> Tensor
Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`
tensor by adding to the indices in the order given in :attr:`index`. For example,
if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\ th row of
``source`` is subtracted from the ``j``\ th row of :attr:`self`.
The :attr:`dim`\ th dimension of ``source`` must have the same size as the
length of :attr:`index` (which must be a vector), and all other dimensions must
match :attr:`self`, or an error will be raised.
"
172,"other_failures_fmt.append(fmt)
# upper boundary on width
        width = min(width, 80)
return Template(_MSG_FORMAT_TEMPLATE).substitute(
boarder=boarder_delim * width,
            title=title.center(width),
section=section_delim * width,
root_failure=root_failure_fmt,
other_failures=""\n"".join(other_failures_fmt or [""  <NO_OTHER_FAILURES>""]),
","other_failures_fmt.append(fmt)
# upper boundary on width
        width = min(width, 60)
return Template(_MSG_FORMAT_TEMPLATE).substitute(
boarder=boarder_delim * width,
            title=title,
section=section_delim * width,
root_failure=root_failure_fmt,
other_failures=""\n"".join(other_failures_fmt or [""  <NO_OTHER_FAILURES>""]),
"
173,"if batch_size is not None and batch_sampler is None:
# auto_collation without custom batch_sampler
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)  # type: ignore[arg-type]
self.batch_size = batch_size
self.drop_last = drop_last
        self.sampler = sampler  # type: ignore[assignment]
self.batch_sampler = batch_sampler
self.generator = generator
","if batch_size is not None and batch_sampler is None:
# auto_collation without custom batch_sampler
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)
self.batch_size = batch_size
self.drop_last = drop_last
        self.sampler = sampler
self.batch_sampler = batch_sampler
self.generator = generator
"
174,"]
]
@compatibility(is_backward_compatible=False)
class OperatorSupport:
""""""
`_support_dict` maps node.target typename to supported inputs dtypes.
","]
]
SupportDict = t.Mapping[TargetTypeName, SupportedArgumentDTypes]


@compatibility(is_backward_compatible=False)
class OperatorSupportBase(abc.ABC):
    """"""Interface for determining if a fx.Node is supported by a backend""""""
    @abc.abstractmethod
    def is_node_supported(
        self, submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node
    ) -> bool:
        raise NotImplementedError()

@compatibility(is_backward_compatible=False)
class OperatorSupport(OperatorSupportBase):
""""""
`_support_dict` maps node.target typename to supported inputs dtypes.
"
175,"# When you build own implementation just override it with dataframe_wrapper.set_df_wrapper(new_wrapper_class)
default_wrapper = PandasWrapper

def get_df_wrapper():
return default_wrapper
def set_df_wrapper(wrapper):
default_wrapper = wrapper
def create_dataframe(data, columns=None):
wrapper = get_df_wrapper()
    wrapper.create_dataframe(data, columns)
def is_dataframe(data):
wrapper = get_df_wrapper()
    wrapper.is_dataframe(data)
def is_column(data):
wrapper = get_df_wrapper()
    wrapper.is_column(data)
def concat(buffer):
wrapper = get_df_wrapper()
    wrapper.concat(buffer)
def iterate(data):
wrapper = get_df_wrapper()
    wrapper.iterate(data)
def get_item(data, idx):
wrapper = get_df_wrapper()
    wrapper.get_item(data, idx)
def get_len(df):
wrapper = get_df_wrapper()
    wrapper.get_len(df)
","# When you build own implementation just override it with dataframe_wrapper.set_df_wrapper(new_wrapper_class)
default_wrapper = PandasWrapper
def get_df_wrapper():
return default_wrapper
def set_df_wrapper(wrapper):
    global default_wrapper
default_wrapper = wrapper
def create_dataframe(data, columns=None):
wrapper = get_df_wrapper()
    return wrapper.create_dataframe(data, columns)
def is_dataframe(data):
wrapper = get_df_wrapper()
    return wrapper.is_dataframe(data)
def is_column(data):
wrapper = get_df_wrapper()
    return wrapper.is_column(data)
def concat(buffer):
wrapper = get_df_wrapper()
    return wrapper.concat(buffer)
def iterate(data):
wrapper = get_df_wrapper()
    return wrapper.iterate(data)
def get_item(data, idx):
wrapper = get_df_wrapper()
    return wrapper.get_item(data, idx)
def get_len(df):
wrapper = get_df_wrapper()
    return wrapper.get_len(df)
"
176,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v' + cuda_version
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
import ctypes
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]))
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
prev_error_mode = kernel32.SetErrorMode(0x0001)
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v' + cuda_version
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
import ctypes
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path, cupti_path]))
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
prev_error_mode = kernel32.SetErrorMode(0x0001)
"
177,"::
from torch.distributed.elastic import events
  event = Event(name=""test_event"", source=EventSource.WORKER, metadata={...})
  events.get_events_logger(destination=""default"").info(event)
""""""
","::
from torch.distributed.elastic import events
  event = events.Event(name=""test_event"", source=events.EventSource.WORKER, metadata={...})
  events.get_logging_handler(destination=""console"").info(event)
""""""
"
178,"normalized, onesided, length, return_complex)
del torch.unique_dim


if TYPE_CHECKING:
# These _impl functions return a variable number of tensors as output with
# __torch_function__; tuple unpacking is done already rather than being
","normalized, onesided, length, return_complex)
if TYPE_CHECKING:
# These _impl functions return a variable number of tensors as output with
# __torch_function__; tuple unpacking is done already rather than being
"
179,"# Now we have a mapping from const output names to the index they are passed
# into submod_1, so swap in getattrs for placeholders.
ph_idx = 0
for node in split.submod_1.graph.nodes:
if node.op != ""placeholder"":
continue
","# Now we have a mapping from const output names to the index they are passed
# into submod_1, so swap in getattrs for placeholders.
ph_idx = 0

for node in split.submod_1.graph.nodes:
if node.op != ""placeholder"":
continue
"
180,"return graph
# We accept dictionnaries and strings as ONNX inputs,
# but they should be only for configuration use.
# we detect here if these inputs are modified, and if so
# we warn the user that the changes won't take effect in the
","return graph
# We accept dictionaries and strings as ONNX inputs,
# but they should be only for configuration use.
# we detect here if these inputs are modified, and if so
# we warn the user that the changes won't take effect in the
"
181,"for input, traced_input in zip(input_states[0], input_states[1]):
if isinstance(input, dict):
if list(input.keys()) != list(traced_input.keys()):
                warning = ""We detected that you are modifying a dictionnary that is an input to your "" \
""model. "" \
""Note that dictionaries are allowed as inputs in ONNX but they should be "" \
""handled with care. "" \
","for input, traced_input in zip(input_states[0], input_states[1]):
if isinstance(input, dict):
if list(input.keys()) != list(traced_input.keys()):
                warning = ""We detected that you are modifying a dictionary that is an input to your "" \
""model. "" \
""Note that dictionaries are allowed as inputs in ONNX but they should be "" \
""handled with care. "" \
"
182,"normalized_kwargs = node.kwargs
else:
normalized_args = ()
            normalized_kwargs = get_normalized_kwargs(
                node, normalization_info.arg_replacement_tuples
            )
if (
normalization_info.needs_shapes_for_normalization
","normalized_kwargs = node.kwargs
else:
normalized_args = ()
            try:
                normalized_kwargs = get_normalized_kwargs(
                    node, normalization_info.arg_replacement_tuples
                )
            except Exception:
                print(
                    f""Error during kwarg normalization for: {node.format_node()}; ""
                    f""arg_replacement_tuples={normalization_info.arg_replacement_tuples}""
                )
                raise
if (
normalization_info.needs_shapes_for_normalization
"
183,"comm_hook=powerSGD.batched_powerSGD_hook,
matrix_approximation_rank=2,
)
def register_ddp_comm_hook(
","comm_hook=powerSGD.batched_powerSGD_hook,
matrix_approximation_rank=2,
)
    NOOP = partial(
        _ddp_comm_hook_wrapper, comm_hook=debugging.noop_hook,
    )
def register_ddp_comm_hook(
"
184,"# Parametrizations
from .experimental.pruner.parametrization import PruningParametrization
from .experimental.pruner.parametrization import ActivationReconstruction
# Pruner
from .experimental.pruner.base_pruner import BasePruner
","# Parametrizations
from .experimental.pruner.parametrization import PruningParametrization
from .experimental.pruner.parametrization import ActivationReconstruction
from .experimental.pruner.parametrization import BiasHook
# Pruner
from .experimental.pruner.base_pruner import BasePruner
"
185,"from torch.nn.modules.container import ModuleDict, ModuleList
from .parametrization import PruningParametrization, ActivationReconstruction
from torch.ao.sparsity import BaseSparsifier, fqn_to_module
","from torch.nn.modules.container import ModuleDict, ModuleList
from .parametrization import PruningParametrization, ActivationReconstruction, BiasHook
from torch.ao.sparsity import BaseSparsifier, fqn_to_module
"
186,"if module.bias is not None:
module.register_parameter('_bias', nn.Parameter(module.bias.detach()))
module.bias = None
            self.bias_handles.append(module.register_forward_hook(self.bias_hook))
def squash_mask(self, use_path=False, *args, **kwargs):
for config in self.module_groups:
","if module.bias is not None:
module.register_parameter('_bias', nn.Parameter(module.bias.detach()))
module.bias = None
            self.bias_handles.append(module.register_forward_hook(BiasHook(module.parametrizations.weight[0], self.prune_bias)))
def squash_mask(self, use_path=False, *args, **kwargs):
for config in self.module_groups:
"
187,"from typing import Any, Callable, List, Optional
import torch
import torch.distributed as dist
from torch.distributed.optim import ZeroRedundancyOptimizer
from torch.distributed.optim.zero_redundancy_optimizer import _OverlapStatus
from torch.nn.parallel.distributed import DistributedDataParallel
# Functional optimizers require passing a list of gradients to their `step()`
","import weakref
from typing import Any, Callable, List, Optional
import torch
import torch.distributed as dist
from torch.distributed.optim import ZeroRedundancyOptimizer
from torch.distributed.optim.zero_redundancy_optimizer import (
    _get_global_rank,
    _OverlapStatus,
)
from torch.nn.parallel.distributed import DistributedDataParallel
# Functional optimizers require passing a list of gradients to their `step()`
"
188,"# all-reduce future since that would add synchronization that delays
# all optimizer computation to wait for that last all-reduce
for bucket_index in range(num_buckets):
            assigned_rank = zero._ddp_bucket_index_to_rank(bucket_index)
            if assigned_rank == rank:
# Wait on the bucket's all-reduce future to ensure correct
# gradients
assert bucket_index in overlap_info.bucket_index_to_future, \
","# all-reduce future since that would add synchronization that delays
# all optimizer computation to wait for that last all-reduce
for bucket_index in range(num_buckets):
            assigned_ranks = overlap_info.assigned_ranks_per_bucket[bucket_index]
            if rank in assigned_ranks:
# Wait on the bucket's all-reduce future to ensure correct
# gradients
assert bucket_index in overlap_info.bucket_index_to_future, \
"
189,"curr_bucket = overlap_info.bucket_index_to_bucket[bucket_index]
_perform_local_step(curr_bucket, zero, rank)
            _broadcast_bucket(bucket_index, zero, assigned_rank)
# Ensure that all parameter updates are finished before the
# next forward pass
        overlap_info.wait_for_broadcasts(num_buckets, rank)
overlap_info.clear_per_iter_info()
return fut
","curr_bucket = overlap_info.bucket_index_to_bucket[bucket_index]
_perform_local_step(curr_bucket, zero, rank)
            _broadcast_bucket(bucket_index, zero)
# Ensure that all parameter updates are finished before the
# next forward pass
        overlap_info.wait_for_broadcasts()
overlap_info.clear_per_iter_info()
return fut
"
190,"hook: Callable[[Any, dist.GradBucket], torch.futures.Future],
ddp: DistributedDataParallel,
zero: ZeroRedundancyOptimizer,
) -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:
r""""""
Modifies the given ``hook`` to overlap the :class:`ZeroRedundancyOptimizer`
","hook: Callable[[Any, dist.GradBucket], torch.futures.Future],
ddp: DistributedDataParallel,
zero: ZeroRedundancyOptimizer,
    shard_buckets: bool = False,
) -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:
r""""""
Modifies the given ``hook`` to overlap the :class:`ZeroRedundancyOptimizer`
"
191,"parameter in that bucket, where ``rank`` is this process's own
rank; the keys of this :class:`dict` are the bucket indices
assigned to this rank.
broadcast_handles (List[Work]): :class:`list` of async work handles for
the parameter broadcasts.
bucket_index_to_future (Dict[int, torch.futures.Future]):
","parameter in that bucket, where ``rank`` is this process's own
rank; the keys of this :class:`dict` are the bucket indices
assigned to this rank.
        num_bucket_assignments (int): total number of bucket assignments across
            all ranks; this is equal to the number of
            :class:`DistributedDataParallel` gradient buckets if
            ``shard_buckets=False`` and possibly greater otherwise.
        total_size (int, optional): total size of all buckets (i.e. sum of
            ``param.numel()`` for all ``param`` across all buckets) if
            ``shard_buckets=True``; otherwise, ``None``.
broadcast_handles (List[Work]): :class:`list` of async work handles for
the parameter broadcasts.
bucket_index_to_future (Dict[int, torch.futures.Future]):
"
192,"""strategy will be used""
)
        # `self._buckets` is used if `parameters_as_bucket_view=True` or
        # `overlap_with_ddp=True`, in which case parameter data is flattened
        # into buckets (i.e. contiguous tensors)
        # If `overlap_with_ddp=True`, the bucketing requires an additional
        # dimension to match the DDP gradient bucketing
self.parameters_as_bucket_view = parameters_as_bucket_view
        self._buckets: Union[
            List[List[torch.Tensor]],
            List[List[Dict[int, torch.Tensor]]]
        ] = []  # type: ignore[assignment]
self._build_param_buckets()
# Optional consolidated optimizer state, only populated if this rank
","""strategy will be used""
)
        # `self._buckets` is used if `parameters_as_bucket_view=True`, in
        # which case parameter data is flattened into contiguous bucket tensors
self.parameters_as_bucket_view = parameters_as_bucket_view
        self._buckets: List[List[torch.Tensor]] = []
self._build_param_buckets()
# Optional consolidated optimizer state, only populated if this rank
"
193,"params_sorted = sorted(param_group[""params""], key=lambda t: t.numel(), reverse=True)
for param in params_sorted:
# Greedily add the parameter to rank with smallest size so far
                        rank = sizes.index(min(sizes))
param_group_params_per_rank[rank].append(param)
sizes[rank] += param.numel()
# Apply the constructed partition of the parameter group
","params_sorted = sorted(param_group[""params""], key=lambda t: t.numel(), reverse=True)
for param in params_sorted:
# Greedily add the parameter to rank with smallest size so far
                        rank = self._get_min_index(sizes)
param_group_params_per_rank[rank].append(param)
sizes[rank] += param.numel()
# Apply the constructed partition of the parameter group
"
194,"import argparse
import ast
from caffe2.python import workspace, brew
from caffe2.python.model_helper import ModelHelper
from caffe2.python.predictor import mobile_exporter
def parse_kwarg(kwarg_str):
    key, value = kwarg_str.split(""="")
try:
value = ast.literal_eval(value)
except ValueError:
","import argparse
import ast
from caffe2.python.model_helper import ModelHelper
from caffe2.python.predictor import mobile_exporter
from caffe2.python import workspace, brew
def parse_kwarg(kwarg_str):
    key, value = kwarg_str.split('=')
try:
value = ast.literal_eval(value)
except ValueError:
"
195,")
        code.append(
            """"""\
int64_t end_offset = offsets[rangeIndex + 1];
      int64_t length = end_offset - offsets[rangeIndex];""""""
        )
code.append(
""      for (""
"," ""        return false;\n""
 ""      }""
)
        code.append(""""""\
int64_t end_offset = offsets[rangeIndex + 1];
      int64_t length = end_offset - offsets[rangeIndex];"""""")
code.append(
""      for (""
 ""int64_t""
"
196,"############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(
        u""""""
Welcome to the developer reference for the PyTorch C++ API.
    """"""
    ),
}
# Tell sphinx what the primary language being documented is.
primary_domain = ""cpp""
# Tell sphinx what the pygments highlight language should be.
highlight_language = ""cpp""
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
","############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(u'''
Welcome to the developer reference for the PyTorch C++ API.
    '''),
}
# Tell sphinx what the primary language being documented is.
primary_domain = 'cpp'
# Tell sphinx what the pygments highlight language should be.
highlight_language = 'cpp'
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
"
197,"# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = ""1.6""
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    ""sphinx.ext.autodoc"",
    ""sphinx.ext.autosummary"",
    ""sphinx.ext.doctest"",
    ""sphinx.ext.intersphinx"",
    ""sphinx.ext.todo"",
    ""sphinx.ext.coverage"",
    ""sphinx.ext.napoleon"",
    ""sphinx.ext.viewcode"",
    ""sphinxcontrib.katex"",
    ""sphinx.ext.autosectionlabel"",
]
# build the templated autosummary files
","# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = '1.6'
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.napoleon',
    'sphinx.ext.viewcode',
    'sphinxcontrib.katex',
    'sphinx.ext.autosectionlabel',
]
# build the templated autosummary files
"
198,"# Start a new plot
pylab.clf()
        pylab.grid(color=""k"", alpha=0.2, linestyle=""--"")
# Plot the current function
plot_function(function)
","# Start a new plot
pylab.clf()
        pylab.grid(color='k', alpha=0.2, linestyle='--')
# Plot the current function
plot_function(function)
"
199,"example = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.script(model, example)
optimized_scripted_module = optimize_for_mobile(traced_script_module)
exported_optimized_scripted_module = (
    optimized_scripted_module._save_for_lite_interpreter(""model.ptl"")
)
","example = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.script(model, example)
optimized_scripted_module = optimize_for_mobile(traced_script_module)
exported_optimized_scripted_module = optimized_scripted_module._save_for_lite_interpreter(""model.ptl"")
"
200,"source_lib = os.path.join(rpath, omp_lib_name)
if not os.path.exists(source_lib):
continue
            target_lib = os.path.join(self.build_lib, ""torch"", ""lib"", omp_lib_name)
self.copy_file(source_lib, target_lib)
break
","source_lib = os.path.join(rpath, omp_lib_name)
if not os.path.exists(source_lib):
continue
            target_lib = os.path.join(self.build_lib, 'torch', 'lib', omp_lib_name)
self.copy_file(source_lib, target_lib)
break
"
201,"on an NVIDIA GPU with compute capability >= 3.0.
""""""
import ctypes
import os
import platform
import sys
import textwrap
import warnings

from .autocast_mode import autocast

if sys.version_info < (3,):
    raise Exception(
        ""Python 2 has reached end-of-life and is no longer supported by PyTorch.""
    )
from ._utils import _import_dotted_name
from ._utils_internal import (
    get_file_path,
    prepare_multiprocessing_environment,
    USE_RTLD_GLOBAL_WITH_LIBTORCH,
    USE_GLOBAL_DEPS,
)

# TODO(torch_deploy) figure out how to freeze version.py in fbcode build
if sys.executable == ""torch_deploy"":
__version__ = ""torch-deploy-1.8""
else:
from .torch_version import __version__ as __version__
from typing import Set, Type, TYPE_CHECKING

from ._six import string_classes as _string_classes
__all__ = [
    ""typename"",
    ""is_tensor"",
    ""is_storage"",
    ""set_default_tensor_type"",
    ""set_rng_state"",
    ""get_rng_state"",
    ""manual_seed"",
    ""initial_seed"",
    ""seed"",
    ""save"",
    ""load"",
    ""set_printoptions"",
    ""chunk"",
    ""split"",
    ""stack"",
    ""matmul"",
    ""no_grad"",
    ""enable_grad"",
    ""rand"",
    ""randn"",
    ""inference_mode"",
    ""DoubleStorage"",
    ""FloatStorage"",
    ""LongStorage"",
    ""IntStorage"",
    ""ShortStorage"",
    ""CharStorage"",
    ""ByteStorage"",
    ""BoolStorage"",
    ""DoubleTensor"",
    ""FloatTensor"",
    ""LongTensor"",
    ""IntTensor"",
    ""ShortTensor"",
    ""CharTensor"",
    ""ByteTensor"",
    ""BoolTensor"",
    ""Tensor"",
    ""lobpcg"",
    ""use_deterministic_algorithms"",
    ""are_deterministic_algorithms_enabled"",
    ""set_warn_always"",
    ""is_warn_always_enabled"",
]
################################################################################
# Load the extension module
################################################################################
if sys.platform == ""win32"":
    pfiles_path = os.getenv(""ProgramFiles"", ""C:\\Program Files"")
    py_dll_path = os.path.join(sys.exec_prefix, ""Library"", ""bin"")
    th_dll_path = os.path.join(os.path.dirname(__file__), ""lib"")
# When users create a virtualenv that inherits the base environment,
# we will need to add the corresponding library directory into
# DLL search directories. Otherwise, it will rely on `PATH` which
# is dependent on user settings.
if sys.exec_prefix != sys.base_exec_prefix:
        base_py_dll_path = os.path.join(sys.base_exec_prefix, ""Library"", ""bin"")
else:
        base_py_dll_path = """"
    dll_paths = list(
        filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path])
    )
    if all(
        [not os.path.exists(os.path.join(p, ""nvToolsExt64_1.dll"")) for p in dll_paths]
    ):
nvtoolsext_dll_path = os.path.join(
            os.getenv(
                ""NVTOOLSEXT_PATH"",
                os.path.join(pfiles_path, ""NVIDIA Corporation"", ""NvToolsExt""),
            ),
            ""bin"",
            ""x64"",
        )
else:
        nvtoolsext_dll_path = """"

    import glob
from .version import cuda as cuda_version

    if cuda_version and all(
        [not glob.glob(os.path.join(p, ""cudart64*.dll"")) for p in dll_paths]
    ):
        cuda_version_1 = cuda_version.replace(""."", ""_"")
        cuda_path_var = ""CUDA_PATH_V"" + cuda_version_1
        default_path = os.path.join(
            pfiles_path, ""NVIDIA GPU Computing Toolkit"", ""CUDA"", ""v"" + cuda_version
        )
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), ""bin"")
else:
        cuda_path = """"
dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
    kernel32 = ctypes.WinDLL(""kernel32.dll"", use_last_error=True)
    with_load_library_flags = hasattr(kernel32, ""AddDllDirectory"")
prev_error_mode = kernel32.SetErrorMode(0x0001)
kernel32.LoadLibraryW.restype = ctypes.c_void_p
","on an NVIDIA GPU with compute capability >= 3.0.
""""""
import os
import sys
import platform
import textwrap
import ctypes
import warnings
from .autocast_mode import autocast
if sys.version_info < (3,):
    raise Exception(""Python 2 has reached end-of-life and is no longer supported by PyTorch."")
from ._utils import _import_dotted_name
from ._utils_internal import get_file_path, prepare_multiprocessing_environment, \
    USE_RTLD_GLOBAL_WITH_LIBTORCH, USE_GLOBAL_DEPS
# TODO(torch_deploy) figure out how to freeze version.py in fbcode build
if sys.executable == 'torch_deploy':
__version__ = ""torch-deploy-1.8""
else:
from .torch_version import __version__ as __version__
from ._six import string_classes as _string_classes
from typing import Set, Type, TYPE_CHECKING

__all__ = [
    'typename', 'is_tensor', 'is_storage', 'set_default_tensor_type',
    'set_rng_state', 'get_rng_state', 'manual_seed', 'initial_seed', 'seed',
    'save', 'load', 'set_printoptions', 'chunk', 'split', 'stack', 'matmul',
    'no_grad', 'enable_grad', 'rand', 'randn', 'inference_mode',
    'DoubleStorage', 'FloatStorage', 'LongStorage', 'IntStorage',
    'ShortStorage', 'CharStorage', 'ByteStorage', 'BoolStorage',
    'DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',
    'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor', 'Tensor',
    'lobpcg', 'use_deterministic_algorithms',
    'are_deterministic_algorithms_enabled',
    'set_warn_always', 'is_warn_always_enabled',
]
################################################################################
# Load the extension module
################################################################################
if sys.platform == 'win32':
    pfiles_path = os.getenv('ProgramFiles', 'C:\\Program Files')
    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')
    th_dll_path = os.path.join(os.path.dirname(__file__), 'lib')
# When users create a virtualenv that inherits the base environment,
# we will need to add the corresponding library directory into
# DLL search directories. Otherwise, it will rely on `PATH` which
# is dependent on user settings.
if sys.exec_prefix != sys.base_exec_prefix:
        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')
else:
        base_py_dll_path = ''
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))
    if all([not os.path.exists(os.path.join(p, 'nvToolsExt64_1.dll')) for p in dll_paths]):
nvtoolsext_dll_path = os.path.join(
            os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')
else:
        nvtoolsext_dll_path = ''
from .version import cuda as cuda_version
    import glob
    if cuda_version and all([not glob.glob(os.path.join(p, 'cudart64*.dll')) for p in dll_paths]):
        cuda_version_1 = cuda_version.replace('.', '_')
        cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
        default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
        cuda_path = ''
dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
prev_error_mode = kernel32.SetErrorMode(0x0001)
kernel32.LoadLibraryW.restype = ctypes.c_void_p
"
202,"""""""
_C._set_default_dtype(d)

def use_deterministic_algorithms(mode):
    r""""""Sets whether PyTorch operations must use ""deterministic""
algorithms. That is, algorithms which, given the same input, and when
run on the same software and hardware, always produce the same output.
When enabled, operations will use deterministic algorithms when available,
","""""""
_C._set_default_dtype(d)
def use_deterministic_algorithms(mode):
    r"""""" Sets whether PyTorch operations must use ""deterministic""
algorithms. That is, algorithms which, given the same input, and when
run on the same software and hardware, always produce the same output.
When enabled, operations will use deterministic algorithms when available,
"
203,"# the public API. The ""regular"" import lines are there solely for the runtime
# side effect of adding to the imported module's members for other users.
from torch import cuda as cuda
from torch import distributions as distributions
from torch import fft as fft
from torch import futures as futures
from torch import hub as hub
from torch import jit as jit
from torch import linalg as linalg
from torch import multiprocessing as multiprocessing
from torch import nn as nn
from torch import onnx as onnx
from torch import optim as optim
from torch import profiler as profiler
from torch import quantization as quantization
from torch import random as random
from torch import sparse as sparse
from torch import special as special
from torch import testing as testing
from torch.autograd import (
    no_grad as no_grad,
    enable_grad as enable_grad,
    set_grad_enabled as set_grad_enabled,
    inference_mode as inference_mode,
)
_C._init_names(list(torch._storage_classes))
# attach docstrings to torch and tensor functions
from . import _torch_docs, _tensor_docs, _storage_docs

del _torch_docs, _tensor_docs, _storage_docs
","# the public API. The ""regular"" import lines are there solely for the runtime
# side effect of adding to the imported module's members for other users.
from torch import cuda as cuda
from torch import cpu as cpu
from torch import autograd as autograd
from torch.autograd import (
    no_grad as no_grad,
    enable_grad as enable_grad,
    set_grad_enabled as set_grad_enabled,
    inference_mode as inference_mode,
)
from torch import fft as fft
from torch import futures as futures
from torch import nn as nn
import torch.nn.intrinsic
import torch.nn.quantizable
import torch.nn.quantized
# AO depends on nn, as well as quantized stuff -- so should be after those.
from torch import ao as ao
from torch import optim as optim
import torch.optim._multi_tensor
from torch import multiprocessing as multiprocessing
from torch import sparse as sparse
from torch import special as special
import torch.utils.backcompat
from torch import onnx as onnx
from torch import jit as jit
from torch import linalg as linalg
from torch import hub as hub
from torch import random as random
from torch import distributions as distributions
from torch import testing as testing
import torch.backends.cuda
import torch.backends.mkl
import torch.backends.mkldnn
import torch.backends.openmp
import torch.backends.quantized
from torch import quantization as quantization
import torch.utils.data
from torch import __config__ as __config__
from torch import __future__ as __future__
from torch import profiler as profiler
_C._init_names(list(torch._storage_classes))
# attach docstrings to torch and tensor functions
from . import _torch_docs, _tensor_docs, _storage_docs
del _torch_docs, _tensor_docs, _storage_docs
"
204,"path = os.path.join(path, appauthor, appname)
else:
path = os.path.join(path, appname)
    elif system == ""darwin"":
        path = os.path.expanduser(""/Library/Application Support"")
if appname:
path = os.path.join(path, appname)
else:
# XDG default for $XDG_DATA_DIRS
# only first, if multipath is False
        path = os.getenv(
            ""XDG_DATA_DIRS"", os.pathsep.join([""/usr/local/share"", ""/usr/share""])
        )
        pathlist = [
            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)
        ]
if appname:
if version:
appname = os.path.join(appname, version)
","path = os.path.join(path, appauthor, appname)
else:
path = os.path.join(path, appname)
    elif system == 'darwin':
        path = os.path.expanduser('/Library/Application Support')
if appname:
path = os.path.join(path, appname)
else:
# XDG default for $XDG_DATA_DIRS
# only first, if multipath is False
        path = os.getenv('XDG_DATA_DIRS',
                         os.pathsep.join(['/usr/local/share', '/usr/share']))
        pathlist = [os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)]
if appname:
if version:
appname = os.path.join(appname, version)
"
205,"path = os.path.join(path, appname)
if opinion:
path = os.path.join(path, ""Cache"")
    elif system == ""darwin"":
        path = os.path.expanduser(""~/Library/Caches"")
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv(""XDG_CACHE_HOME"", os.path.expanduser(""~/.cache""))
if appname:
path = os.path.join(path, appname)
if appname and version:
","path = os.path.join(path, appname)
if opinion:
path = os.path.join(path, ""Cache"")
    elif system == 'darwin':
        path = os.path.expanduser('~/Library/Caches')
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv('XDG_CACHE_HOME', os.path.expanduser('~/.cache'))
if appname:
path = os.path.join(path, appname)
if appname and version:
"
206,"class AppDirs(object):
""""""Convenience wrapper for getting application dirs.""""""

    def __init__(
        self, appname=None, appauthor=None, version=None, roaming=False, multipath=False
    ):
self.appname = appname
self.appauthor = appauthor
self.version = version
","class AppDirs(object):
""""""Convenience wrapper for getting application dirs.""""""
    def __init__(self, appname=None, appauthor=None, version=None,
            roaming=False, multipath=False):
self.appname = appname
self.appauthor = appauthor
self.version = version
"
207,"def parseExpr(expr, module):
try:
value, len_parsed = parseNestedExpr(expr, module)
            assert len_parsed == len(
                expr
            ), ""whole expression was not parsed, falling back to c++ parser""
return value
except Exception:
""""""
","def parseExpr(expr, module):
try:
value, len_parsed = parseNestedExpr(expr, module)
            assert len_parsed == len(expr), ""whole expression was not parsed, falling back to c++ parser""
return value
except Exception:
""""""
"
208,"name_to_type = {
name: parameter.annotation
for name, parameter in signature.parameters.items()
        if parameter.annotation is not inspect.Parameter.empty
        and not isinstance(parameter.annotation, str)
}
# Then, get the literal type annotations from the function declaration
","name_to_type = {
name: parameter.annotation
for name, parameter in signature.parameters.items()
        if parameter.annotation is not inspect.Parameter.empty and not isinstance(parameter.annotation, str)
}
# Then, get the literal type annotations from the function declaration
"
209,"""""""
# cls is a type here, so `ismethod` is false since the methods on the type
# aren't bound to anything, so Python treats them as regular functions
    fns = [
        getattr(cls, name)
        for name in cls.__dict__
        if inspect.isroutine(getattr(cls, name))
    ]
captures = {}
for fn in fns:
","""""""
# cls is a type here, so `ismethod` is false since the methods on the type
# aren't bound to anything, so Python treats them as regular functions
    fns = [getattr(cls, name) for name in cls.__dict__ if inspect.isroutine(getattr(cls, name))]
captures = {}
for fn in fns:
"
210,"return lookup_in_class
def boolean_dispatch(
    arg_name, arg_index, default, if_true, if_false, module_name, func_name
):
""""""
Dispatches to either of 2 script functions based on a boolean argument.
In TorchScript, the boolean argument must be constant so that the correct
function to use can be determined at compile time.
""""""

def fn(*args, **kwargs):
dispatch_flag = False
if arg_name in kwargs:
","return lookup_in_class
def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):
""""""
Dispatches to either of 2 script functions based on a boolean argument.
In TorchScript, the boolean argument must be constant so that the correct
function to use can be determined at compile time.
""""""
def fn(*args, **kwargs):
dispatch_flag = False
if arg_name in kwargs:
"
211,"# The Python docs are very clear that `__module__` can be None, but I can't
# figure out when it actually would be.
if module_name is None:
        raise RuntimeError(
            f""Could not get qualified name for class '{name}': ""
            ""__module__ can't be None.""
        )
# if getattr(sys.modules[module_name], name) is not obj:
#     raise RuntimeError(f""Could not get qualified name for class '{name}': ""
","# The Python docs are very clear that `__module__` can be None, but I can't
# figure out when it actually would be.
if module_name is None:
        raise RuntimeError(f""Could not get qualified name for class '{name}': ""
                           ""__module__ can't be None."")
# if getattr(sys.modules[module_name], name) is not obj:
#     raise RuntimeError(f""Could not get qualified name for class '{name}': ""
"
212,"def symeig(A: Tensor, largest: Optional[bool] = False) -> Tuple[Tensor, Tensor]:
    """"""Return eigenpairs of A with specified ordering.""""""
if largest is None:
largest = False
    E, Z = torch.linalg.eigh(A, UPLO=""U"")
# assuming that E is ordered
if largest:
E = torch.flip(E, dims=(-1,))
","def symeig(A: Tensor, largest: Optional[bool] = False) -> Tuple[Tensor, Tensor]:
    """"""Return eigenpairs of A with specified ordering.
    """"""
if largest is None:
largest = False
    E, Z = torch.linalg.eigh(A, UPLO='U')
# assuming that E is ordered
if largest:
E = torch.flip(E, dims=(-1,))
"
213,"for i in range(N):
A_ = bA[i]
B_ = bB[i] if bB is not None else None
            X_ = (
                torch.randn((m, n), dtype=dtype, device=device) if bX is None else bX[i]
            )
assert len(X_.shape) == 2 and X_.shape == (m, n), (X_.shape, (m, n))
            iparams[""batch_index""] = i
worker = LOBPCG(A_, B_, X_, iK, iparams, fparams, bparams, method, tracker)
worker.run()
bE[i] = worker.E[:k]
","for i in range(N):
A_ = bA[i]
B_ = bB[i] if bB is not None else None
            X_ = torch.randn((m, n), dtype=dtype, device=device) if bX is None else bX[i]
assert len(X_.shape) == 2 and X_.shape == (m, n), (X_.shape, (m, n))
            iparams['batch_index'] = i
worker = LOBPCG(A_, B_, X_, iK, iparams, fparams, bparams, method, tracker)
worker.run()
bE[i] = worker.E[:k]
"
214,"class LOBPCG(object):
    """"""Worker class of LOBPCG methods.""""""

    def __init__(
        self,
        A,  # type: Optional[Tensor]
        B,  # type: Optional[Tensor]
        X,  # type: Tensor
        iK,  # type: Optional[Tensor]
        iparams,  # type: Dict[str, int]
        fparams,  # type: Dict[str, float]
        bparams,  # type: Dict[str, bool]
        method,  # type: str
        tracker,  # type: None
    ):
# type: (...) -> None
# constant parameters
","class LOBPCG(object):
    """"""Worker class of LOBPCG methods.
    """"""

    def __init__(self,
                 A,        # type: Optional[Tensor]
                 B,        # type: Optional[Tensor]
                 X,        # type: Tensor
                 iK,       # type: Optional[Tensor]
                 iparams,  # type: Dict[str, int]
                 fparams,  # type: Dict[str, float]
                 bparams,  # type: Dict[str, bool]
                 method,   # type: str
                 tracker   # type: None
                 ):
# type: (...) -> None
# constant parameters
"
215,"E_, Z = _utils.symeig(_utils.qform(self.A, S_), largest)
# Update E, X, P
            self.X[:, nc:] = mm(S_, Z[:, : n - nc])
            self.E[nc:] = E_[: n - nc]
            P = mm(
                S_,
                mm(
                    Z[:, n - nc :],
                    _utils.basis(_utils.transpose(Z[: n - nc, n - nc :])),
                ),
            )
np = P.shape[-1]
# check convergence
","E_, Z = _utils.symeig(_utils.qform(self.A, S_), largest)
# Update E, X, P
            self.X[:, nc:] = mm(S_, Z[:, :n - nc])
            self.E[nc:] = E_[:n - nc]
            P = mm(S_, mm(Z[:, n - nc:], _utils.basis(_utils.transpose(Z[:n - nc, n - nc:]))))
np = P.shape[-1]
# check convergence
"
216,"return Q
def svd_lowrank(
    A: Tensor,
    q: Optional[int] = 6,
    niter: Optional[int] = 2,
    M: Optional[Tensor] = None,
) -> Tuple[Tensor, Tensor, Tensor]:
r""""""Return the singular value decomposition ``(U, S, V)`` of a matrix,
batches of matrices, or a sparse matrix :math:`A` such that
:math:`A \approx U diag(S) V^T`. In case :math:`M` is given, then
","return Q
def svd_lowrank(A: Tensor, q: Optional[int] = 6, niter: Optional[int] = 2,
                M: Optional[Tensor] = None) -> Tuple[Tensor, Tensor, Tensor]:
r""""""Return the singular value decomposition ``(U, S, V)`` of a matrix,
batches of matrices, or a sparse matrix :math:`A` such that
:math:`A \approx U diag(S) V^T`. In case :math:`M` is given, then
"
217,"if _utils.is_sparse(A):
if len(A.shape) != 2:
            raise ValueError(""pca_lowrank input is expected to be 2-dimensional tensor"")
c = torch.sparse.sum(A, dim=(-2,)) / m
# reshape c
column_indices = c.indices()[0]
        indices = torch.zeros(
            2,
            len(column_indices),
            dtype=column_indices.dtype,
            device=column_indices.device,
        )
indices[0] = column_indices
C_t = torch.sparse_coo_tensor(
            indices, c.values(), (n, 1), dtype=dtype, device=A.device
        )
ones_m1_t = torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
M = _utils.transpose(torch.sparse.mm(C_t, ones_m1_t))
","if _utils.is_sparse(A):
if len(A.shape) != 2:
            raise ValueError('pca_lowrank input is expected to be 2-dimensional tensor')
c = torch.sparse.sum(A, dim=(-2,)) / m
# reshape c
column_indices = c.indices()[0]
        indices = torch.zeros(2, len(column_indices),
                              dtype=column_indices.dtype,
                              device=column_indices.device)
indices[0] = column_indices
C_t = torch.sparse_coo_tensor(
            indices, c.values(), (n, 1), dtype=dtype, device=A.device)
ones_m1_t = torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
M = _utils.transpose(torch.sparse.mm(C_t, ones_m1_t))
"
218,"if old_dim in dim_map.keys():
dim_map[old_dim] = new_dim
else:
            raise RuntimeError(
                (
                    ""{api_name}: Tried to rename dim '{old_dim}' to dim ""
                    ""{new_dim} in Tensor[{dims}] but dim '{old_dim}' does not exist""
                ).format(
                    old_dim=old_dim,
                    new_dim=new_dim,
                    dims=tensor.names,
                    api_name=namer_api_name(inplace),
                )
            )
return tensor._update_names(tuple(dim_map.values()), inplace)
","if old_dim in dim_map.keys():
dim_map[old_dim] = new_dim
else:
            raise RuntimeError(('{api_name}: Tried to rename dim \'{old_dim}\' to dim '
                                '{new_dim} in Tensor[{dims}] but dim \'{old_dim}\' does not exist')
                               .format(old_dim=old_dim, new_dim=new_dim, dims=tensor.names,
                                       api_name=namer_api_name(inplace)))
return tensor._update_names(tuple(dim_map.values()), inplace)
"
219,"and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""

def __init__(self, name):
        super(_OpNamespace, self).__init__(""torch.ops."" + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == ""__file__"":
            return ""torch.ops""
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = ""{}::{}"".format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
","and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""
def __init__(self, name):
        super(_OpNamespace, self).__init__('torch.ops.' + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == '__file__':
            return 'torch.ops'
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = '{}::{}'.format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
"
220,"op.__module__ = self.__module__ + ""."" + self.name
return op

class _Ops(types.ModuleType):
    __file__ = ""_ops.py""
def __init__(self):
        super(_Ops, self).__init__(""torch.ops"")
self.loaded_libraries = set()
def __getattr__(self, name):
","op.__module__ = self.__module__ + ""."" + self.name
return op
class _Ops(types.ModuleType):
    __file__ = '_ops.py'
def __init__(self):
        super(_Ops, self).__init__('torch.ops')
self.loaded_libraries = set()
def __getattr__(self, name):
"
221,"def __prepare__(cls, name, this_bases):
return meta.__prepare__(name, bases)
    return type.__new__(metaclass, ""temporary_class"", (), {})
","def __prepare__(cls, name, this_bases):
return meta.__prepare__(name, bases)
    return type.__new__(metaclass, 'temporary_class', (), {})
"
222,"whitespace = fn_def.split(""def"")[0]
# Add this leading whitespace to all lines before and after the `def`
    aligned_prefix = [
        whitespace + remove_prefix(s, whitespace) for s in sourcelines[:idx]
    ]
    aligned_suffix = [
        whitespace + remove_prefix(s, whitespace) for s in sourcelines[idx + 1 :]
    ]
# Put it together again
aligned_prefix.append(fn_def)
","whitespace = fn_def.split(""def"")[0]
# Add this leading whitespace to all lines before and after the `def`
    aligned_prefix = [whitespace + remove_prefix(s, whitespace) for s in sourcelines[:idx]]
    aligned_suffix = [whitespace + remove_prefix(s, whitespace) for s in sourcelines[idx + 1:]]
# Put it together again
aligned_prefix.append(fn_def)
"
223,"# 2. Python list is not a good fit due to performance reason.
#    `tolist()` converts every single element in the tensor into python objects
#    and serialize them one by one.
        if self.device.type == ""xla"":
            arg_xla = (
                self.cpu().numpy(),
                self.dtype,
                str(self.device),
                self.requires_grad,
            )
return (torch._utils._rebuild_xla_tensor, arg_xla)
        if self.device.type == ""mlc"":
            arg_mlc = (
                self.cpu().numpy(),
                self.dtype,
                str(self.device),
                self.requires_grad,
            )
return (torch._utils._rebuild_mlc_tensor, arg_mlc)
        if self.device.type == ""meta"":
# NB: This implementation BREAKS storage sharing.  Current
# hypothesis is that no one cares for meta tensors.
arg_meta = (
","# 2. Python list is not a good fit due to performance reason.
#    `tolist()` converts every single element in the tensor into python objects
#    and serialize them one by one.
        if self.device.type == 'xla':
            arg_xla = (self.cpu().numpy(),
                       self.dtype,
                       str(self.device),
                       self.requires_grad)
return (torch._utils._rebuild_xla_tensor, arg_xla)
        if self.device.type == 'mlc':
            arg_mlc = (self.cpu().numpy(),
                       self.dtype,
                       str(self.device),
                       self.requires_grad)
return (torch._utils._rebuild_mlc_tensor, arg_mlc)
        if self.device.type == 'meta':
# NB: This implementation BREAKS storage sharing.  Current
# hypothesis is that no one cares for meta tensors.
arg_meta = (
"
224,"# All strings are unicode in Python 3.
return torch._tensor_str._str(self)
    def backward(
        self, gradient=None, retain_graph=None, create_graph=False, inputs=None
    ):
r""""""Computes the gradient of current tensor w.r.t. graph leaves.
The graph is differentiated using the chain rule. If the tensor is
","# All strings are unicode in Python 3.
return torch._tensor_str._str(self)
    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None):
r""""""Computes the gradient of current tensor w.r.t. graph leaves.
The graph is differentiated using the chain rule. If the tensor is
"
225,"In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.
    """""",
    )
    detach_ = _C._add_docstr(
        _C._TensorBase.detach_,
        r""""""
Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.
This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.
    """""",
    )
def is_shared(self):
r""""""Checks if tensor is in shared memory.
","In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.
    """""")
    detach_ = _C._add_docstr(_C._TensorBase.detach_, r""""""
Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.
This method also affects forward mode AD gradients and the result will never
have forward mode AD gradients.
    """""")
def is_shared(self):
r""""""Checks if tensor is in shared memory.
"
226,"""""""
if has_torch_function_unary(self):
return handle_torch_function(
                Tensor.stft,
                (self,),
                self,
                n_fft,
                hop_length=hop_length,
                win_length=win_length,
                window=window,
                center=center,
                pad_mode=pad_mode,
                normalized=normalized,
                onesided=onesided,
                return_complex=return_complex,
)
        return torch.stft(
            self,
            n_fft,
            hop_length,
            win_length,
            window,
            center,
            pad_mode,
            normalized,
            onesided,
            return_complex=return_complex,
        )

    def istft(
        self,
        n_fft: int,
        hop_length: Optional[int] = None,
        win_length: Optional[int] = None,
        window: ""Optional[Tensor]"" = None,
        center: bool = True,
        normalized: bool = False,
        onesided: Optional[bool] = None,
        length: Optional[int] = None,
        return_complex: bool = False,
    ):
r""""""See :func:`torch.istft`""""""
if has_torch_function_unary(self):
return handle_torch_function(
                Tensor.istft,
                (self,),
                self,
                n_fft,
                hop_length=hop_length,
                win_length=win_length,
                window=window,
                center=center,
                normalized=normalized,
                onesided=onesided,
                length=length,
                return_complex=return_complex,
)
        return torch.istft(
            self,
            n_fft,
            hop_length,
            win_length,
            window,
            center,
            normalized,
            onesided,
            length,
            return_complex=return_complex,
        )
def resize(self, *sizes):
if has_torch_function_unary(self):
return handle_torch_function(Tensor.resize, (self,), self, *sizes)
warnings.warn(""non-inplace resize is deprecated"")
from torch.autograd._functions import Resize

return Resize.apply(self, sizes)
def resize_as(self, tensor):
","""""""
if has_torch_function_unary(self):
return handle_torch_function(
                Tensor.stft, (self,), self, n_fft, hop_length=hop_length,
                win_length=win_length, window=window, center=center, pad_mode=pad_mode, normalized=normalized,
                onesided=onesided, return_complex=return_complex
)
        return torch.stft(self, n_fft, hop_length, win_length, window, center,
                          pad_mode, normalized, onesided, return_complex=return_complex)

    def istft(self, n_fft: int, hop_length: Optional[int] = None,
              win_length: Optional[int] = None, window: 'Optional[Tensor]' = None,
              center: bool = True, normalized: bool = False,
              onesided: Optional[bool] = None, length: Optional[int] = None,
              return_complex: bool = False):
r""""""See :func:`torch.istft`""""""
if has_torch_function_unary(self):
return handle_torch_function(
                Tensor.istft, (self,), self, n_fft, hop_length=hop_length, win_length=win_length,
                window=window, center=center, normalized=normalized, onesided=onesided, length=length,
                return_complex=return_complex
)
        return torch.istft(self, n_fft, hop_length, win_length, window, center,
                           normalized, onesided, length, return_complex=return_complex)
def resize(self, *sizes):
if has_torch_function_unary(self):
return handle_torch_function(Tensor.resize, (self,), self, *sizes)
warnings.warn(""non-inplace resize is deprecated"")
from torch.autograd._functions import Resize
return Resize.apply(self, sizes)
def resize_as(self, tensor):
"
227,"def add_docstr_all(method, docstr):
add_docstr(getattr(torch._C._TensorBase, method), docstr)

common_args = parse_kwargs(
    """"""
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.preserve_format``.
""""""
)
new_common_args = parse_kwargs(
    """"""
size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
shape of the output tensor.
dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.
","def add_docstr_all(method, docstr):
add_docstr(getattr(torch._C._TensorBase, method), docstr)
common_args = parse_kwargs(""""""
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.preserve_format``.
"""""")
new_common_args = parse_kwargs(""""""
size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
shape of the output tensor.
dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.
"
228,"tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],
[ 3.0949e-41,  4.4842e-44,  0.0000e+00]])
"""""".format(
        **new_common_args
    ),
)
add_docstr_all(
    ""new_ones"",
    r""""""
new_ones(size, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with ``1``.
","tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],
[ 3.0949e-41,  4.4842e-44,  0.0000e+00]])
"""""".format(**new_common_args))
add_docstr_all('new_ones',
               r""""""
new_ones(size, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with ``1``.
"
229,".. warning::
The named tensor API is experimental and subject to change.
"""""",
)
add_docstr_all(
    ""all"",
    r""""""
all(dim=None, keepdim=False) -> Tensor
See :func:`torch.all`
"""""",
)
add_docstr_all(
    ""allclose"",
    r""""""
allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor
See :func:`torch.allclose`
"""""",
)
add_docstr_all(
    ""angle"",
    r""""""
angle() -> Tensor
See :func:`torch.angle`
"""""",
)
add_docstr_all(
    ""any"",
    r""""""
any(dim=None, keepdim=False) -> Tensor
See :func:`torch.any`
"""""",
)
add_docstr_all(
    ""apply_"",
    r""""""
apply_(callable) -> Tensor
Applies the function :attr:`callable` to each element in the tensor, replacing
",".. warning::
The named tensor API is experimental and subject to change.
"""""")
add_docstr_all('all',
               r""""""
all(dim=None, keepdim=False) -> Tensor
See :func:`torch.all`
"""""")
add_docstr_all('allclose',
               r""""""
allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor
See :func:`torch.allclose`
"""""")
add_docstr_all('angle',
               r""""""
angle() -> Tensor
See :func:`torch.angle`
"""""")
add_docstr_all('any',
               r""""""
any(dim=None, keepdim=False) -> Tensor
See :func:`torch.any`
"""""")
add_docstr_all('apply_',
               r""""""
apply_(callable) -> Tensor
Applies the function :attr:`callable` to each element in the tensor, replacing
"
230,"[  1.,   1.,   1.],
[  1.,   1.,   1.],
[  1.,   1.,   1.]])
"""""".format(
        **reproducibility_notes
    ),
)

add_docstr_all(
    ""index_copy_"",
    r""""""
index_copy_(dim, index, tensor) -> Tensor
Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting
","[  1.,   1.,   1.],
[  1.,   1.,   1.],
[  1.,   1.,   1.]])
"""""".format(**reproducibility_notes))

add_docstr_all('index_copy_',
               r""""""
index_copy_(dim, index, tensor) -> Tensor
Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting
"
231,"The :attr:`callable` should have the signature::
def callable(a, b) -> number
"""""",
)
add_docstr_all(
    ""masked_scatter_"",
    r""""""
masked_scatter_(mask, source)
Copies elements from :attr:`source` into :attr:`self` tensor at positions where
","The :attr:`callable` should have the signature::
def callable(a, b) -> number
"""""")
add_docstr_all('masked_scatter_',
               r""""""
masked_scatter_(mask, source)
Copies elements from :attr:`source` into :attr:`self` tensor at positions where
"
232,"do not have a shared-storage narrow method.  Calling ``narrow_copy``
with ``dimemsion > self.sparse_dim()`` will return a copy with the
relevant dense dimension narrowed, and ``self.shape`` updated accordingly.
"""""",
)
add_docstr_all(
    ""ndimension"",
    r""""""
ndimension() -> int
Alias for :meth:`~Tensor.dim()`
"""""",
)
add_docstr_all(
    ""nan_to_num"",
    r""""""
nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor
See :func:`torch.nan_to_num`.
"""""",
)
add_docstr_all(
    ""nan_to_num_"",
    r""""""
nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor
In-place version of :meth:`~Tensor.nan_to_num`.
"""""",
)
add_docstr_all(
    ""ne"",
    r""""""
ne(other) -> Tensor
See :func:`torch.ne`.
"""""",
)
add_docstr_all(
    ""ne_"",
    r""""""
ne_(other) -> Tensor
In-place version of :meth:`~Tensor.ne`.
"""""",
)
add_docstr_all(
    ""not_equal"",
    r""""""
not_equal(other) -> Tensor
See :func:`torch.not_equal`.
"""""",
)
add_docstr_all(
    ""not_equal_"",
    r""""""
not_equal_(other) -> Tensor
In-place version of :meth:`~Tensor.not_equal`.
"""""",
)
add_docstr_all(
    ""neg"",
    r""""""
neg() -> Tensor
See :func:`torch.neg`
"""""",
)
add_docstr_all(
    ""negative"",
    r""""""
negative() -> Tensor
See :func:`torch.negative`
"""""",
)
add_docstr_all(
    ""neg_"",
    r""""""
neg_() -> Tensor
In-place version of :meth:`~Tensor.neg`
"""""",
)
add_docstr_all(
    ""negative_"",
    r""""""
negative_() -> Tensor
In-place version of :meth:`~Tensor.negative`
"""""",
)
add_docstr_all(
    ""nelement"",
    r""""""
nelement() -> int
Alias for :meth:`~Tensor.numel`
"""""",
)
add_docstr_all(
    ""nextafter"",
    r""""""
nextafter(other) -> Tensor
See :func:`torch.nextafter`
"""""",
)
add_docstr_all(
    ""nextafter_"",
    r""""""
nextafter_(other) -> Tensor
In-place version of :meth:`~Tensor.nextafter`
"""""",
)
add_docstr_all(
    ""nonzero"",
    r""""""
nonzero() -> LongTensor
See :func:`torch.nonzero`
"""""",
)
add_docstr_all(
    ""norm"",
    r""""""
norm(p=2, dim=None, keepdim=False) -> Tensor
See :func:`torch.norm`
"""""",
)
add_docstr_all(
    ""normal_"",
    r""""""
normal_(mean=0, std=1, *, generator=None) -> Tensor
Fills :attr:`self` tensor with elements samples from the normal distribution
parameterized by :attr:`mean` and :attr:`std`.
"""""",
)
add_docstr_all(
    ""numel"",
    r""""""
numel() -> int
See :func:`torch.numel`
"""""",
)
add_docstr_all(
    ""numpy"",
    r""""""
numpy() -> numpy.ndarray
Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the
returned :class:`ndarray` share the same underlying storage. Changes to
:attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.
"""""",
)
add_docstr_all(
    ""orgqr"",
    r""""""
orgqr(input2) -> Tensor
See :func:`torch.orgqr`
"""""",
)
add_docstr_all(
    ""ormqr"",
    r""""""
ormqr(input2, input3, left=True, transpose=False) -> Tensor
See :func:`torch.ormqr`
"""""",
)
add_docstr_all(
    ""permute"",
    r""""""
permute(*dims) -> Tensor
See :func:`torch.permute`
"""""",
)
add_docstr_all(
    ""polygamma"",
    r""""""
polygamma(n) -> Tensor
See :func:`torch.polygamma`
"""""",
)
add_docstr_all(
    ""polygamma_"",
    r""""""
polygamma_(n) -> Tensor
In-place version of :meth:`~Tensor.polygamma`
"""""",
)
add_docstr_all(
    ""positive"",
    r""""""
positive() -> Tensor
See :func:`torch.positive`
"""""",
)
add_docstr_all(
    ""pow"",
    r""""""
pow(exponent) -> Tensor
See :func:`torch.pow`
"""""",
)
add_docstr_all(
    ""pow_"",
    r""""""
pow_(exponent) -> Tensor
In-place version of :meth:`~Tensor.pow`
"""""",
)
add_docstr_all(
    ""float_power"",
    r""""""
float_power(exponent) -> Tensor
See :func:`torch.float_power`
"""""",
)
add_docstr_all(
    ""float_power_"",
    r""""""
float_power_(exponent) -> Tensor
In-place version of :meth:`~Tensor.float_power`
"""""",
)
add_docstr_all(
    ""prod"",
    r""""""
prod(dim=None, keepdim=False, dtype=None) -> Tensor
See :func:`torch.prod`
"""""",
)
add_docstr_all(
    ""put_"",
    r""""""
put_(index, source, accumulate=False) -> Tensor
Copies the elements from :attr:`source` into the positions specified by
","do not have a shared-storage narrow method.  Calling ``narrow_copy``
with ``dimemsion > self.sparse_dim()`` will return a copy with the
relevant dense dimension narrowed, and ``self.shape`` updated accordingly.
"""""")
add_docstr_all('ndimension',
               r""""""
ndimension() -> int
Alias for :meth:`~Tensor.dim()`
"""""")
add_docstr_all('nan_to_num', r""""""
nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor
See :func:`torch.nan_to_num`.
"""""")
add_docstr_all('nan_to_num_', r""""""
nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor
In-place version of :meth:`~Tensor.nan_to_num`.
"""""")
add_docstr_all('ne', r""""""
ne(other) -> Tensor
See :func:`torch.ne`.
"""""")
add_docstr_all('ne_', r""""""
ne_(other) -> Tensor
In-place version of :meth:`~Tensor.ne`.
"""""")
add_docstr_all('not_equal', r""""""
not_equal(other) -> Tensor
See :func:`torch.not_equal`.
"""""")
add_docstr_all('not_equal_', r""""""
not_equal_(other) -> Tensor
In-place version of :meth:`~Tensor.not_equal`.
"""""")
add_docstr_all('neg',
               r""""""
neg() -> Tensor
See :func:`torch.neg`
"""""")
add_docstr_all('negative',
               r""""""
negative() -> Tensor
See :func:`torch.negative`
"""""")
add_docstr_all('neg_',
               r""""""
neg_() -> Tensor
In-place version of :meth:`~Tensor.neg`
"""""")
add_docstr_all('negative_',
               r""""""
negative_() -> Tensor
In-place version of :meth:`~Tensor.negative`
"""""")
add_docstr_all('nelement',
               r""""""
nelement() -> int
Alias for :meth:`~Tensor.numel`
"""""")
add_docstr_all('nextafter',
               r""""""
nextafter(other) -> Tensor
See :func:`torch.nextafter`
"""""")
add_docstr_all('nextafter_',
               r""""""
nextafter_(other) -> Tensor
In-place version of :meth:`~Tensor.nextafter`
"""""")
add_docstr_all('nonzero',
               r""""""
nonzero() -> LongTensor
See :func:`torch.nonzero`
"""""")
add_docstr_all('norm',
               r""""""
norm(p=2, dim=None, keepdim=False) -> Tensor
See :func:`torch.norm`
"""""")
add_docstr_all('normal_',
               r""""""
normal_(mean=0, std=1, *, generator=None) -> Tensor
Fills :attr:`self` tensor with elements samples from the normal distribution
parameterized by :attr:`mean` and :attr:`std`.
"""""")
add_docstr_all('numel',
               r""""""
numel() -> int
See :func:`torch.numel`
"""""")
add_docstr_all('numpy',
               r""""""
numpy() -> numpy.ndarray
Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the
returned :class:`ndarray` share the same underlying storage. Changes to
:attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.
"""""")
add_docstr_all('orgqr',
               r""""""
orgqr(input2) -> Tensor
See :func:`torch.orgqr`
"""""")
add_docstr_all('ormqr',
               r""""""
ormqr(input2, input3, left=True, transpose=False) -> Tensor
See :func:`torch.ormqr`
"""""")
add_docstr_all('permute',
               r""""""
permute(*dims) -> Tensor
See :func:`torch.permute`
"""""")
add_docstr_all('polygamma',
               r""""""
polygamma(n) -> Tensor
See :func:`torch.polygamma`
"""""")
add_docstr_all('polygamma_',
               r""""""
polygamma_(n) -> Tensor
In-place version of :meth:`~Tensor.polygamma`
"""""")
add_docstr_all('positive',
               r""""""
positive() -> Tensor
See :func:`torch.positive`
"""""")
add_docstr_all('pow',
               r""""""
pow(exponent) -> Tensor
See :func:`torch.pow`
"""""")
add_docstr_all('pow_',
               r""""""
pow_(exponent) -> Tensor
In-place version of :meth:`~Tensor.pow`
"""""")
add_docstr_all('float_power',
               r""""""
float_power(exponent) -> Tensor
See :func:`torch.float_power`
"""""")
add_docstr_all('float_power_',
               r""""""
float_power_(exponent) -> Tensor
In-place version of :meth:`~Tensor.float_power`
"""""")
add_docstr_all('prod',
               r""""""
prod(dim=None, keepdim=False, dtype=None) -> Tensor
See :func:`torch.prod`
"""""")
add_docstr_all('put_',
               r""""""
put_(index, source, accumulate=False) -> Tensor
Copies the elements from :attr:`source` into the positions specified by
"
233,"[0., 2., 0., 0., 0.],
[0., 0., 2., 1., 1.]])
"""""".format(
        **reproducibility_notes
    ),
)
add_docstr_all(
    ""select"",
    r""""""
select(dim, index) -> Tensor
Slices the :attr:`self` tensor along the selected dimension at the given index.
","[0., 2., 0., 0., 0.],
[0., 0., 2., 1., 1.]])
"""""".format(**reproducibility_notes))
add_docstr_all('select',
               r""""""
select(dim, index) -> Tensor
Slices the :attr:`self` tensor along the selected dimension at the given index.
"
234,"original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""",
)
add_docstr_all(
    ""sparse_resize_and_clear_"",
    r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
","original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""")
add_docstr_all('sparse_resize_and_clear_',
               r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
"
235,"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""",
)
add_docstr_all(
    ""view_as"",
    r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
","Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""")
add_docstr_all('view_as',
               r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
"
236,">>> x.imag
tensor([ 0.3553, -0.7896, -0.0633, -0.8119])
"""""",
)
add_docstr_all(
    ""as_subclass"",
    r""""""
as_subclass(cls) -> Tensor
Makes a ``cls`` instance with the same data pointer as ``self``. Changes
in the output mirror changes in ``self``, and the output stays attached
to the autograd graph. ``cls`` must be a subclass of ``Tensor``.
"""""",
)
add_docstr_all(
    ""crow_indices"",
    r""""""
crow_indices() -> IntTensor
Returns the tensor containing the compressed row indices of the :attr:`self`
",">>> x.imag
tensor([ 0.3553, -0.7896, -0.0633, -0.8119])
"""""")
add_docstr_all('as_subclass',
               r""""""
as_subclass(cls) -> Tensor
Makes a ``cls`` instance with the same data pointer as ``self``. Changes
in the output mirror changes in ``self``, and the output stays attached
to the autograd graph. ``cls`` must be a subclass of ``Tensor``.
"""""")
add_docstr_all('crow_indices',
               r""""""
crow_indices() -> IntTensor
Returns the tensor containing the compressed row indices of the :attr:`self`
"
237,">>> csr.col_indices()
tensor([0, 1, 2, 3, 4], dtype=torch.int32)
"""""",
)
",">>> csr.col_indices()
tensor([0, 1, 2, 3, 4], dtype=torch.int32)
"""""")
"
238,"real_str = _scalar_str(self.real, formatter1)
imag_str = (_scalar_str(self.imag, formatter2) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
        if imag_str[0] == ""+"" or imag_str[0] == ""-"":
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
else:
return formatter1.format(self.item())

def _vector_str(self, indent, summarize, formatter1, formatter2=None):
# length includes spaces and comma between elements
element_length = formatter1.width() + 2
","real_str = _scalar_str(self.real, formatter1)
imag_str = (_scalar_str(self.imag, formatter2) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
        if imag_str[0] == '+' or imag_str[0] == '-':
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
else:
return formatter1.format(self.item())
def _vector_str(self, indent, summarize, formatter1, formatter2=None):
# length includes spaces and comma between elements
element_length = formatter1.width() + 2
"
239,"real_str = formatter1.format(val.real)
imag_str = (formatter2.format(val.imag) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
            if imag_str[0] == ""+"" or imag_str[0] == ""-"":
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
","real_str = formatter1.format(val.real)
imag_str = (formatter2.format(val.imag) + ""j"").lstrip()
# handles negative numbers, +0.0, -0.0
            if imag_str[0] == '+' or imag_str[0] == '-':
return real_str + imag_str
else:
return real_str + ""+"" + imag_str
"
240,"return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = (
            [_val_formatter(val) for val in self[: PRINT_OPTS.edgeitems].tolist()]
            + ["" ...""]
            + [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems :].tolist()]
        )
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [
        data[i : i + elements_per_line] for i in range(0, len(data), elements_per_line)
    ]
    lines = ["", "".join(line) for line in data_lines]
    return ""["" + ("","" + ""\n"" + "" "" * (indent + 1)).join(lines) + ""]""

# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
","return formatter1.format(val)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        data = ([_val_formatter(val) for val in self[:PRINT_OPTS.edgeitems].tolist()] +
                [' ...'] +
                [_val_formatter(val) for val in self[-PRINT_OPTS.edgeitems:].tolist()])
else:
data = [_val_formatter(val) for val in self.tolist()]
    data_lines = [data[i:i + elements_per_line] for i in range(0, len(data), elements_per_line)]
    lines = [', '.join(line) for line in data_lines]
    return '[' + (',' + '\n' + ' ' * (indent + 1)).join(lines) + ']'
# formatter2 is only used for printing complex tensors.
# For complex tensors, formatter1 and formatter2 are the formatters for tensor.real
"
241,">>> torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.absolute,
    r""""""
absolute(input, *, out=None) -> Tensor
Alias for :func:`torch.abs`
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.acos,
    r""""""
acos(input, *, out=None) -> Tensor
Computes the inverse cosine of each element in :attr:`input`.
.. math::
\text{out}_{i} = \cos^{-1}(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",">>> torch.abs(torch.tensor([-1, -2, 3]))
tensor([ 1,  2,  3])
"""""".format(**common_args))

add_docstr(torch.absolute,
           r""""""
absolute(input, *, out=None) -> Tensor
Alias for :func:`torch.abs`
"""""".format(**common_args))

add_docstr(torch.acos, r""""""
acos(input, *, out=None) -> Tensor
Computes the inverse cosine of each element in :attr:`input`.
.. math::
\text{out}_{i} = \cos^{-1}(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
242,">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.sspaddmm,
    r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
",">>> torch.addmm(M, mat1, mat2)
tensor([[-4.8716,  1.4671, -1.3746],
[ 0.7573, -3.9555, -2.8681]])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.sspaddmm,
           r""""""
sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
Matrix multiplies a sparse tensor :attr:`mat1` with a dense tensor
"
243,"Args:
input (Tensor): a sparse matrix to be matrix multiplied
mat (Tensor): a dense matrix to be matrix multiplied
"""""",
)
add_docstr(
    torch.addmv,
    r""""""
addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`mat` and
","Args:
input (Tensor): a sparse matrix to be matrix multiplied
mat (Tensor): a dense matrix to be matrix multiplied
"""""")
add_docstr(torch.addmv,
           r""""""
addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`mat` and
"
244,"If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
""""""
    + r""""""
If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
of size `m`, then :attr:`input` must be
:ref:`broadcastable <broadcasting-semantics>` with a matrix of size
","If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
"""""" + r""""""
If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector
of size `m`, then :attr:`input` must be
:ref:`broadcastable <broadcasting-semantics>` with a matrix of size
"
245,"responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
"""""",
)
add_docstr(
    torch.unsafe_split,
    r""""""
unsafe_split(tensor, split_size_or_sections, dim=0) -> List of Tensors
Works like :func:`torch.split` but without enforcing the autograd restrictions
","responsibility to ensure that is the case. If both the input and one or more
of the outputs are modified inplace, gradients computed by autograd will be
silently incorrect.
"""""")
add_docstr(torch.unsafe_split,
           r""""""
unsafe_split(tensor, split_size_or_sections, dim=0) -> List of Tensors
Works like :func:`torch.split` but without enforcing the autograd restrictions
"
246,"tensor([[12., 13., 14., 15.]]),
tensor([], size=(0, 4)))
"""""",
)
add_docstr(
    torch.dsplit,
    r""""""
dsplit(input, indices_or_sections) -> List of Tensors
Splits :attr:`input`, a tensor with three or more dimensions, into multiple tensors
","tensor([[12., 13., 14., 15.]]),
tensor([], size=(0, 4)))
"""""")
add_docstr(torch.dsplit,
           r""""""
dsplit(input, indices_or_sections) -> List of Tensors
Splits :attr:`input`, a tensor with three or more dimensions, into multiple tensors
"
247,"[15.]]]),
tensor([], size=(2, 2, 0)))
"""""",
)
add_docstr(
    torch.can_cast,
    r""""""
can_cast(from, to) -> bool
Determines if a type conversion is allowed under PyTorch casting rules
","[15.]]]),
tensor([], size=(2, 2, 0)))
"""""")
add_docstr(torch.can_cast,
           r""""""
can_cast(from, to) -> bool
Determines if a type conversion is allowed under PyTorch casting rules
"
248,"[0.3582, 1.0000]])
>>> torch.corrcoef(x[0])
tensor(1.)
"""""",
)
add_docstr(
    torch.cov,
    r""""""
cov(input, *, correction=1, fweights=None, aweights=None) -> Tensor
Estimates the covariance matrix of the variables given by the :attr:`input` matrix, where rows are
","[0.3582, 1.0000]])
>>> torch.corrcoef(x[0])
tensor(1.)
"""""")
add_docstr(torch.cov, r""""""
cov(input, *, correction=1, fweights=None, aweights=None) -> Tensor
Estimates the covariance matrix of the variables given by the :attr:`input` matrix, where rows are
"
249,"tensor([[ -8.1626,  19.6097],
[ -5.8398,  14.2387],
[ -4.3771,  10.4173]])
"""""",
)
add_docstr(
    torch.cholesky_inverse,
    r""""""
cholesky_inverse(input, upper=False, *, out=None) -> Tensor
Computes the inverse of a symmetric positive-definite matrix :math:`A` using its
","tensor([[ -8.1626,  19.6097],
[ -5.8398,  14.2387],
[ -4.3771,  10.4173]])
"""""")
add_docstr(torch.cholesky_inverse, r""""""
cholesky_inverse(input, upper=False, *, out=None) -> Tensor
Computes the inverse of a symmetric positive-definite matrix :math:`A` using its
"
250,">>> z = torch.polar(abs, angle)
>>> z
tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)
"""""",
)
add_docstr(
    torch.conj_physical,
    r""""""
conj_physical(input, *, out=None) -> Tensor
Computes the element-wise conjugate of the given :attr:`input` tensor.
",">>> z = torch.polar(abs, angle)
>>> z
tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)
"""""")
add_docstr(torch.conj_physical,
           r""""""
conj_physical(input, *, out=None) -> Tensor
Computes the element-wise conjugate of the given :attr:`input` tensor.
"
251,".. math::
\text{out}_{i} = conj(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = conj(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
252,"values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,
1.3298, -1.3298]),
indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))
"""""".format(
        **reduceops_common_args
    ),
)

add_docstr(
    torch.cumprod,
    r""""""
cumprod(input, dim, *, dtype=None, out=None) -> Tensor
Returns the cumulative product of elements of :attr:`input` in the dimension
","values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,
indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))
"""""".format(**reduceops_common_args))

add_docstr(torch.cumprod,
           r""""""
cumprod(input, dim, *, dtype=None, out=None) -> Tensor
Returns the cumulative product of elements of :attr:`input` in the dimension
"
253,">>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ True, False],
[False, True]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.equal,
    r""""""
equal(input, other) -> bool
``True`` if two tensors have the same size and elements, ``False`` otherwise.
",">>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[ True, False],
[False, True]])
"""""".format(**common_args))

add_docstr(torch.equal,
           r""""""
equal(input, other) -> bool
``True`` if two tensors have the same size and elements, ``False`` otherwise.
"
254,"tensor([-0.8166,  1.5308, -0.2530, -0.2091])
>>> torch.floor(a)
tensor([-1.,  1., -1., -1.])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.floor_divide,
    r""""""
floor_divide(input, other, *, out=None) -> Tensor
.. warning::
","tensor([-0.8166,  1.5308, -0.2530, -0.2091])
>>> torch.floor(a)
tensor([-1.,  1., -1., -1.])
"""""".format(**common_args))

add_docstr(torch.floor_divide, r""""""
floor_divide(input, other, *, out=None) -> Tensor
.. warning::
"
255,"tensor(1.00000e-06 *
3.6386)
"""""",
)
add_docstr(
    torch.get_default_dtype,
    r""""""
get_default_dtype() -> torch.dtype
Get the current default floating point :class:`torch.dtype`.
","tensor(1.00000e-06 *
3.6386)
"""""")
add_docstr(torch.get_default_dtype,
           r""""""
get_default_dtype() -> torch.dtype
Get the current default floating point :class:`torch.dtype`.
"
256,"The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
""""""
    + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
","The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
"""""" + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
"
257,">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
>>> torch.isposinf(a)
tensor([False,  True, False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isneginf,
    r""""""
isneginf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is negative infinity or not.
",">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
>>> torch.isposinf(a)
tensor([False,  True, False])
"""""".format(**common_args))

add_docstr(torch.isneginf,
           r""""""
isneginf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is negative infinity or not.
"
258,".. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
""""""
    + r""""""
where :attr:`input` and :attr:`other` are finite. Where :attr:`input`
and/or :attr:`other` are nonfinite they are close if and only if
",".. math::
\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert
"""""" + r""""""
where :attr:`input` and :attr:`other` are finite. Where :attr:`input`
and/or :attr:`other` are nonfinite they are close if and only if
"
259,">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.linspace,
    r""""""
linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
",">>> a = torch.arange(0.5, 2, 0.5)
>>> torch.lgamma(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(**common_args))

add_docstr(torch.linspace, r""""""
linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
"
260,">>> b = torch.tensor((3, 0, 4))
>>> torch.maximum(a, b)
tensor([3, 2, 4])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.fmax,
    r""""""
fmax(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
",">>> b = torch.tensor((3, 0, 4))
>>> torch.maximum(a, b)
tensor([3, 2, 4])
"""""".format(**common_args))

add_docstr(torch.fmax, r""""""
fmax(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
"
261,">>> a = torch.arange(4.)
>>> a
tensor([0., 1., 2., 3.])
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.nanquantile,
    r""""""
nanquantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
This is a variant of :func:`torch.quantile` that ""ignores"" ``NaN`` values,
",">>> a = torch.arange(4.)
>>> a
tensor([0., 1., 2., 3.])
"""""".format(**single_dim_common))

add_docstr(torch.nanquantile, r""""""
nanquantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
This is a variant of :func:`torch.quantile` that ""ignores"" ``NaN`` values,
"
262,"tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""",
)
add_docstr(
    torch.nan_to_num,
    r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
","tensor([[ 2,  3],
[ 5,  6],
[ 8,  9]])
"""""")
add_docstr(torch.nan_to_num,
           r""""""
nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) -> Tensor
Replaces :literal:`NaN`, positive infinity, and negative infinity values in :attr:`input`
"
263,"{requires_grad}
{memory_format}
"""""".format(
        **factory_like_common_args
    ),
)
add_docstr(
    torch.randn,
    r""""""
randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with random numbers from a normal distribution
","{requires_grad}
{memory_format}
"""""".format(**factory_like_common_args))
add_docstr(torch.randn,
           r""""""
randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with random numbers from a normal distribution
"
264,"tensor([ 1,  2,  3])
>>> torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.ravel,
    r""""""
ravel(input) -> Tensor
Return a contiguous flattened tensor. A copy is made only if needed.
","tensor([ 1,  2,  3])
>>> torch.arange(1, 2.5, 0.5)
tensor([ 1.0000,  1.5000,  2.0000])
"""""".format(**factory_common_args))

add_docstr(torch.ravel,
           r""""""
ravel(input) -> Tensor
Return a contiguous flattened tensor. A copy is made only if needed.
"
265,"...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.remainder,
    r""""""
remainder(input, other, *, out=None) -> Tensor
Like :func:`torch.fmod` this applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
","...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
"""""".format(**common_args))

add_docstr(torch.remainder,
           r""""""
remainder(input, other, *, out=None) -> Tensor
Like :func:`torch.fmod` this applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_
"
266,"torch.float32
>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
torch.uint8
"""""",
)
add_docstr(
    torch.row_stack,
    r""""""
row_stack(tensors, *, out=None) -> Tensor
Alias of :func:`torch.vstack`.
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.round,
    r""""""
round(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input` rounded
","torch.float32
>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))
torch.uint8
"""""")
add_docstr(torch.row_stack,
           r""""""
row_stack(tensors, *, out=None) -> Tensor
Alias of :func:`torch.vstack`.
"""""".format(**common_args))

add_docstr(torch.round,
           r""""""
round(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input` rounded
"
267,"tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
>>> torch.round(a)
tensor([ 1.,  1.,  1., -1.])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.rsqrt,
    r""""""
rsqrt(input, *, out=None) -> Tensor
Returns a new tensor with the reciprocal of the square-root of each of
","tensor([ 0.9920,  0.6077,  0.9734, -1.0362])
>>> torch.round(a)
tensor([ 1.,  1.,  1., -1.])
"""""".format(**common_args))

add_docstr(torch.rsqrt,
           r""""""
rsqrt(input, *, out=None) -> Tensor
Returns a new tensor with the reciprocal of the square-root of each of
"
268,"[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.msort,
    r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
","[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(**common_args))

add_docstr(torch.msort,
           r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
"
269,"col_indices=tensor([0, 1, 0, 1]),
values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
dtype=torch.float64, layout=torch.sparse_csr)
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.sparse_coo_tensor,
    r""""""
sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False) -> Tensor
Constructs a :ref:`sparse tensor in COO(rdinate) format
","col_indices=tensor([0, 1, 0, 1]),
values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
dtype=torch.float64, layout=torch.sparse_csr)
"""""".format(**factory_common_args))

add_docstr(torch.sparse_coo_tensor,
           r""""""
sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False) -> Tensor
Constructs a :ref:`sparse tensor in COO(rdinate) format
"
270,">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.roll,
    r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
",">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(**common_args))

add_docstr(torch.roll,
           r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
"
271,">>> torch.take_along_dim(t, sorted_idx, dim=1)
tensor([[10, 20, 30],
[40, 50, 60]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.tan,
    r""""""
tan(input, *, out=None) -> Tensor
Returns a new tensor with the tangent of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \tan(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",">>> torch.take_along_dim(t, sorted_idx, dim=1)
tensor([[10, 20, 30],
[40, 50, 60]])
"""""".format(**common_args))

add_docstr(torch.tan,
           r""""""
tan(input, *, out=None) -> Tensor
Returns a new tensor with the tangent of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \tan(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
272,"[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(
        **common_args
    ),
)
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(
    torch.triu_indices,
    r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
","[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(**common_args))
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(torch.triu_indices,
           r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
"
273,"def _validate_loaded_sparse_tensors():
try:
for t in _sparse_tensors_to_validate:
            torch._validate_sparse_coo_tensor_args(t._indices(), t._values(), t.size())
finally:
_sparse_tensors_to_validate.clear()

def _rebuild_sparse_tensor(layout, data):
if layout == torch.sparse_coo:
indices, values, size = data
","def _validate_loaded_sparse_tensors():
try:
for t in _sparse_tensors_to_validate:
            torch._validate_sparse_coo_tensor_args(t._indices(), t._values(),
                                                   t.size())
finally:
_sparse_tensors_to_validate.clear()
def _rebuild_sparse_tensor(layout, data):
if layout == torch.sparse_coo:
indices, values, size = data
"
274,"flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(
        flat_indices, [torch.Tensor._indices(t) for t in tensors]
    )
    values = torch._C._nn.unflatten_dense_tensors(
        flat_values, [torch.Tensor._values(t) for t in tensors]
    )
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
","flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(flat_indices, [torch.Tensor._indices(t) for t in tensors])
    values = torch._C._nn.unflatten_dense_tensors(flat_values, [torch.Tensor._values(t) for t in tensors])
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
"
275,"def annotate(ret, **kwargs):
def dec(fun):
fun.__annotations__ = dict(kwargs)
        fun.__annotations__[""return""] = ret
return fun

return dec
","def annotate(ret, **kwargs):
def dec(fun):
fun.__annotations__ = dict(kwargs)
        fun.__annotations__['return'] = ret
return fun
return dec
"
276,"def decorate_autocast(*args, **kwargs):
with self:
return func(*args, **kwargs)

return decorate_autocast
","def decorate_autocast(*args, **kwargs):
with self:
return func(*args, **kwargs)
return decorate_autocast
"
277,">>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)
>>> torch.testing.assert_close(fft2, two_ffts, check_stride=False)
"""""".format(
        **common_args
    ),
)
ifft2 = _add_docstr(
    _fft.fft_ifft2,
    r""""""
ifft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor
Computes the 2 dimensional inverse discrete Fourier transform of :attr:`input`.
",">>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)
>>> torch.testing.assert_close(fft2, two_ffts, check_stride=False)
"""""".format(**common_args))
ifft2 = _add_docstr(_fft.fft_ifft2, r""""""
ifft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor
Computes the 2 dimensional inverse discrete Fourier transform of :attr:`input`.
"
278,">>> torch.fft.ifftshift(shifted)
tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])
"""""",
)
",">>> torch.fft.ifftshift(shifted)
tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])
"""""")
"
279,"from typing import Tuple, Optional, Union, Any, Sequence, TYPE_CHECKING
import torch
import torch.nn.functional as F
from torch._autograd_functions import _LU

from ._jit_internal import boolean_dispatch, List
from ._jit_internal import _overload as overload
from ._lowrank import svd_lowrank, pca_lowrank
from .overrides import (
    has_torch_function,
    has_torch_function_unary,
    has_torch_function_variadic,
    handle_torch_function,
)
Tensor = torch.Tensor
from torch import _VF
__all__ = [
    ""atleast_1d"",
    ""atleast_2d"",
    ""atleast_3d"",
    ""align_tensors"",
    ""broadcast_shapes"",
    ""broadcast_tensors"",
    ""cartesian_prod"",
    ""block_diag"",
    ""cdist"",
    ""chain_matmul"",
    ""einsum"",
    ""istft"",
    ""lu"",
    ""norm"",
    ""meshgrid"",
    ""pca_lowrank"",
    ""split"",
    ""stft"",
    ""svd_lowrank"",
    ""tensordot"",
    ""unique"",
    ""unique_consecutive"",
]
","from typing import (
    Tuple, Optional, Union, Any, Sequence, TYPE_CHECKING
)
import torch
import torch.nn.functional as F
from ._lowrank import svd_lowrank, pca_lowrank
from .overrides import (
    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    handle_torch_function)
from ._jit_internal import boolean_dispatch, List
from ._jit_internal import _overload as overload
from torch._autograd_functions import _LU
Tensor = torch.Tensor
from torch import _VF
__all__ = [
    'atleast_1d',
    'atleast_2d',
    'atleast_3d',
    'align_tensors',
    'broadcast_shapes',
    'broadcast_tensors',
    'cartesian_prod',
    'block_diag',
    'cdist',
    'chain_matmul',
    'einsum',
    'istft',
    'lu',
    'norm',
    'meshgrid',
    'pca_lowrank',
    'split',
    'stft',
    'svd_lowrank',
    'tensordot',
    'unique',
    'unique_consecutive',
]
"
280,"""""""
if has_torch_function_unary(input):
return handle_torch_function(
            unique_consecutive,
            (input,),
            input,
            return_inverse=return_inverse,
            return_counts=return_counts,
            dim=dim,
        )
output, inverse_indices, counts = _VF.unique_consecutive(  # type: ignore[attr-defined]
        input, return_inverse=return_inverse, return_counts=return_counts, dim=dim
    )
return output, inverse_indices, counts
def _return_counts(
    input, sorted=True, return_inverse=False, return_counts=False, dim=None
):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
","""""""
if has_torch_function_unary(input):
return handle_torch_function(
            unique_consecutive, (input,), input, return_inverse=return_inverse,
            return_counts=return_counts, dim=dim)
output, inverse_indices, counts = _VF.unique_consecutive(  # type: ignore[attr-defined]
        input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
return output, inverse_indices, counts
def _return_counts(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
"
281,"return output
def _return_inverse(
    input, sorted=True, return_inverse=False, return_counts=False, dim=None
):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
return _unique_impl(input, sorted, return_inverse, return_counts, dim)
    output, inverse_indices, _ = _unique_impl(
        input, sorted, return_inverse, return_counts, dim
    )
return output, inverse_indices
_return_inverse_false = boolean_dispatch(
    arg_name=""return_counts"",
arg_index=3,
default=False,
if_true=_return_counts,
if_false=_return_output,
module_name=__name__,
    func_name=""unique"",
)
_return_inverse_true = boolean_dispatch(
    arg_name=""return_counts"",
arg_index=3,
default=False,
if_true=_unique_impl,
if_false=_return_inverse,
module_name=__name__,
    func_name=""unique"",
)
# The return type of unique depends on `return_inverse`, and `return_counts` so in order to
# resolve the output type in TorchScript we need to statically know the value of both parameters
unique = boolean_dispatch(
    arg_name=""return_inverse"",
arg_index=2,
default=False,
if_true=_return_inverse_true,
if_false=_return_inverse_false,
module_name=__name__,
    func_name=""unique"",
)
unique.__doc__ = _unique_impl.__doc__
def _consecutive_return_counts(
    input, return_inverse=False, return_counts=False, dim=None
):
# type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
return _unique_consecutive_impl(input, return_inverse, return_counts, dim)
    output, _, counts = _unique_consecutive_impl(
        input, return_inverse, return_counts, dim
    )
return output, counts
def _consecutive_return_output(
    input, return_inverse=False, return_counts=False, dim=None
):
# type: (Tensor, bool, bool, Optional[int]) -> Tensor
if has_torch_function_unary(input):
","return output
def _return_inverse(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
return _unique_impl(input, sorted, return_inverse, return_counts, dim)
    output, inverse_indices, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
return output, inverse_indices
_return_inverse_false = boolean_dispatch(
    arg_name='return_counts',
arg_index=3,
default=False,
if_true=_return_counts,
if_false=_return_output,
module_name=__name__,
    func_name='unique')
_return_inverse_true = boolean_dispatch(
    arg_name='return_counts',
arg_index=3,
default=False,
if_true=_unique_impl,
if_false=_return_inverse,
module_name=__name__,
    func_name='unique')
# The return type of unique depends on `return_inverse`, and `return_counts` so in order to
# resolve the output type in TorchScript we need to statically know the value of both parameters
unique = boolean_dispatch(
    arg_name='return_inverse',
arg_index=2,
default=False,
if_true=_return_inverse_true,
if_false=_return_inverse_false,
module_name=__name__,
    func_name='unique')
unique.__doc__ = _unique_impl.__doc__
def _consecutive_return_counts(input, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor]
if has_torch_function_unary(input):
return _unique_consecutive_impl(input, return_inverse, return_counts, dim)
    output, _, counts = _unique_consecutive_impl(input, return_inverse, return_counts, dim)
return output, counts
def _consecutive_return_output(input, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, Optional[int]) -> Tensor
if has_torch_function_unary(input):
"
282,"else:
return _VF.tensordot(a, b, dims_a, dims_b, out=out)  # type: ignore[attr-defined]

def cartesian_prod(*tensors):
""""""Do cartesian product of the given sequence of tensors. The behavior is similar to
python's `itertools.product`.
","else:
return _VF.tensordot(a, b, dims_a, dims_b, out=out)  # type: ignore[attr-defined]
def cartesian_prod(*tensors):
""""""Do cartesian product of the given sequence of tensors. The behavior is similar to
python's `itertools.product`.
"
283,"pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(
        input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None
    ):  # noqa: F811
# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
","pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: F811
# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
"
284,"else:
return result[0], result[1]  # A_LU, pivots

# The return type of lu depends on `get_infos`, so in order to resolve the output type
# of lu in TorchScript we need to statically know the value of `get_infos`
lu = boolean_dispatch(
    arg_name=""get_infos"",
arg_index=2,
default=False,
if_true=_lu_with_infos,
if_false=_lu_no_infos,
module_name=__name__,
    func_name=""lu"",
)
lu.__doc__ = _lu_impl.__doc__

def align_tensors(*tensors):
    raise RuntimeError(""`align_tensors` not yet implemented."")
","else:
return result[0], result[1]  # A_LU, pivots
# The return type of lu depends on `get_infos`, so in order to resolve the output type
# of lu in TorchScript we need to statically know the value of `get_infos`
lu = boolean_dispatch(
    arg_name='get_infos',
arg_index=2,
default=False,
if_true=_lu_with_infos,
if_false=_lu_no_infos,
module_name=__name__,
    func_name='lu')
lu.__doc__ = _lu_impl.__doc__
def align_tensors(*tensors):
    raise RuntimeError('`align_tensors` not yet implemented.')
"
285,"def import_module(name, path):
import importlib.util
from importlib.abc import Loader

spec = importlib.util.spec_from_file_location(name, path)
module = importlib.util.module_from_spec(spec)
assert isinstance(spec.loader, Loader)
","def import_module(name, path):
import importlib.util
from importlib.abc import Loader
spec = importlib.util.spec_from_file_location(name, path)
module = importlib.util.module_from_spec(spec)
assert isinstance(spec.loader, Loader)
"
286,"def _git_archive_link(repo_owner, repo_name, branch):
    return ""https://github.com/{}/{}/archive/{}.zip"".format(
        repo_owner, repo_name, branch
    )
def _load_attr_from_module(module, func_name):
","def _git_archive_link(repo_owner, repo_name, branch):
    return 'https://github.com/{}/{}/archive/{}.zip'.format(repo_owner, repo_name, branch)
def _load_attr_from_module(module, func_name):
"
287,"def _check_module_exists(name):
import importlib.util

return importlib.util.find_spec(name) is not None
","def _check_module_exists(name):
import importlib.util
return importlib.util.find_spec(name) is not None
"
288,"if dependencies is not None:
missing_deps = [pkg for pkg in dependencies if not _check_module_exists(pkg)]
if len(missing_deps):
            raise RuntimeError(
                ""Missing dependencies: {}"".format("", "".join(missing_deps))
            )
def _load_entry_from_hubconf(m, model):
if not isinstance(model, str):
        raise ValueError(""Invalid input: model should be a string of function name"")
# Note that if a missing dependency is imported at top level of hubconf, it will
# throw before this function. It's a chicken and egg situation where we have to
","if dependencies is not None:
missing_deps = [pkg for pkg in dependencies if not _check_module_exists(pkg)]
if len(missing_deps):
            raise RuntimeError('Missing dependencies: {}'.format(', '.join(missing_deps)))
def _load_entry_from_hubconf(m, model):
if not isinstance(model, str):
        raise ValueError('Invalid input: model should be a string of function name')
# Note that if a missing dependency is imported at top level of hubconf, it will
# throw before this function. It's a chicken and egg situation where we have to
"
289,"try:
if hash_prefix is not None:
sha256 = hashlib.sha256()
        with tqdm(
            total=file_size,
            disable=not progress,
            unit=""B"",
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
while True:
buffer = u.read(8192)
if len(buffer) == 0:
","try:
if hash_prefix is not None:
sha256 = hashlib.sha256()
        with tqdm(total=file_size, disable=not progress,
                  unit='B', unit_scale=True, unit_divisor=1024) as pbar:
while True:
buffer = u.read(8192)
if len(buffer) == 0:
"
290,">>> L = torch.linalg.cholesky(A)
>>> torch.dist(L @ L.transpose(-2, -1), A)
tensor(5.8747e-16, dtype=torch.float64)
"""""",
)
cholesky_ex = _add_docstr(
    _linalg.linalg_cholesky_ex,
    r""""""
linalg.cholesky_ex(A, *, upper=False, check_errors=False, out=None) -> (Tensor, Tensor)
Computes the Cholesky decomposition of a complex Hermitian or real
",">>> L = torch.linalg.cholesky(A)
>>> torch.dist(L @ L.transpose(-2, -1), A)
tensor(5.8747e-16, dtype=torch.float64)
"""""")
cholesky_ex = _add_docstr(_linalg.linalg_cholesky_ex, r""""""
linalg.cholesky_ex(A, *, upper=False, check_errors=False, out=None) -> (Tensor, Tensor)
Computes the Cholesky decomposition of a complex Hermitian or real
"
291,">>> torch.dist(S, torch.linalg.svd(A, full_matrices=False).S)
tensor(2.4576e-07)
"""""",
)
cond = _add_docstr(
    _linalg.linalg_cond,
    r""""""
linalg.cond(A, p=None, *, out=None) -> Tensor
Computes the condition number of a matrix with respect to a matrix norm.
",">>> torch.dist(S, torch.linalg.svd(A, full_matrices=False).S)
tensor(2.4576e-07)
"""""")
cond = _add_docstr(_linalg.linalg_cond, r""""""
linalg.cond(A, p=None, *, out=None) -> Tensor
Computes the condition number of a matrix with respect to a matrix norm.
"
292,">>> torch.linalg.cond(A)
tensor([[4.6245],
[4.5671]])
"""""",
)
pinv = _add_docstr(
    _linalg.linalg_pinv,
    r""""""
linalg.pinv(A, rcond=1e-15, hermitian=False, *, out=None) -> Tensor
Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.
",">>> torch.linalg.cond(A)
tensor([[4.6245],
[4.5671]])
"""""")
pinv = _add_docstr(_linalg.linalg_pinv, r""""""
linalg.pinv(A, rcond=1e-15, hermitian=False, *, out=None) -> Tensor
Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.
"
293,"torch.isreal: lambda tensor: -1,
torch.isposinf: lambda input, out=None: -1,
torch.isneginf: lambda input, out=None: -1,
        torch.instance_norm: (
            lambda input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps, cudnn_enabled: -1
        ),
torch.int_repr: lambda input: -1,
torch.inverse: lambda input, out=None: -1,
torch.linalg.inv: lambda input, out=None: -1,
","torch.isreal: lambda tensor: -1,
torch.isposinf: lambda input, out=None: -1,
torch.isneginf: lambda input, out=None: -1,
        torch.instance_norm: (lambda input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps,
                              cudnn_enabled: -1),
torch.int_repr: lambda input: -1,
torch.inverse: lambda input, out=None: -1,
torch.linalg.inv: lambda input, out=None: -1,
"
294,"torch.is_signed: lambda input: -1,
torch.isclose: lambda input, other, rtol=1e-05, atol=1e-08, equal_nan=False: -1,
torch.isnan: lambda input: -1,
        torch.istft: (
            lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False: -1
        ),
        torch.kl_div: lambda input, target, size_average=None, reduce=None, reduction=""mean"", log_target=False: -1,
torch.kron: lambda input, other: -1,
torch.kthvalue: lambda input, k, dim=None, keepdim=False, out=None: -1,
torch.layer_norm: lambda input, normalized_shape, weight=None, bias=None, esp=1e-05, cudnn_enabled=True: -1,
","torch.is_signed: lambda input: -1,
torch.isclose: lambda input, other, rtol=1e-05, atol=1e-08, equal_nan=False: -1,
torch.isnan: lambda input: -1,
        torch.istft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
                      normalized=False, onesided=None, length=None, return_complex=False: -1),
        torch.kl_div: lambda input, target, size_average=None, reduce=None, reduction='mean', log_target=False: -1,
torch.kron: lambda input, other: -1,
torch.kthvalue: lambda input, k, dim=None, keepdim=False, out=None: -1,
torch.layer_norm: lambda input, normalized_shape, weight=None, bias=None, esp=1e-05, cudnn_enabled=True: -1,
"
295,"torch.trapezoid: lambda y, x=None, dim=-1: -1,
torch.triangular_solve: lambda input, A, upper=True, transpose=False, unitriangular=False: -1,
torch.tril: lambda input, diagonal=0, out=None: -1,
        torch.triplet_margin_loss: (
            lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=""mean"": -1
        ),
torch.triu: lambda input, diagonal=0, out=None: -1,
torch.true_divide: lambda input, other: -1,
torch.trunc: lambda input, out=None: -1,
","torch.trapezoid: lambda y, x=None, dim=-1: -1,
torch.triangular_solve: lambda input, A, upper=True, transpose=False, unitriangular=False: -1,
torch.tril: lambda input, diagonal=0, out=None: -1,
        torch.triplet_margin_loss: (lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False,

                                    size_average=None, reduce=None, reduction='mean': -1),
torch.triu: lambda input, diagonal=0, out=None: -1,
torch.true_divide: lambda input, other: -1,
torch.trunc: lambda input, out=None: -1,
"
296,"if k.__name__.startswith(""bitwise_""):
# bitwise_<op> have dunder methods of the form __<op>__
# And so on.
            subname = k.__name__[len(""bitwise_"") :]
            names.extend(
                [""__"" + subname + ""__"", ""__i"" + subname + ""__"", ""__r"" + subname + ""__""]
            )
for name in names:
func = getattr(Tensor, name, None)
","if k.__name__.startswith(""bitwise_""):
# bitwise_<op> have dunder methods of the form __<op>__
# And so on.
            subname = k.__name__[len(""bitwise_""):]
            names.extend([
                ""__"" + subname + ""__"",
                ""__i"" + subname + ""__"",
                ""__r"" + subname + ""__""
            ])
for name in names:
func = getattr(Tensor, name, None)
"
297,"worker_name = ""{}_{}"".format(socket.gethostname(), str(os.getpid()))
file_name = ""{}.{}.pt.trace.json"".format(worker_name, int(time.time() * 1000))
if use_gzip:
            file_name = file_name + "".gz""
prof.export_chrome_trace(os.path.join(dir_name, file_name))

return handler_fn

def supported_activities():
""""""
Returns a set of supported profiler tracing activities.
","worker_name = ""{}_{}"".format(socket.gethostname(), str(os.getpid()))
file_name = ""{}.{}.pt.trace.json"".format(worker_name, int(time.time() * 1000))
if use_gzip:
            file_name = file_name + '.gz'
prof.export_chrome_trace(os.path.join(dir_name, file_name))
return handler_fn
def supported_activities():
""""""
Returns a set of supported profiler tracing activities.
"
298,"byte = f.read(1)
f.seek(start)
    local_header_magic_number = [b""P"", b""K"", b""\x03"", b""\x04""]
return read_bytes == local_header_magic_number
","byte = f.read(1)
f.seek(start)
    local_header_magic_number = [b'P', b'K', b'\x03', b'\x04']
return read_bytes == local_header_magic_number
"
299,"source_file = source = None
try:
source_lines, _, source_file = get_source_lines_and_file(obj)
                source = """".join(source_lines)
except Exception:  # saving the source is optional, so we can ignore any errors
                warnings.warn(
                    ""Couldn't retrieve source code for container of ""
                    ""type "" + obj.__name__ + "". It won't be checked ""
                    ""for correctness upon loading.""
                )
            return (""module"", obj, source_file, source)
elif torch.is_storage(obj):
view_metadata: Optional[Tuple[str, int, int]]
","source_file = source = None
try:
source_lines, _, source_file = get_source_lines_and_file(obj)
                source = ''.join(source_lines)
except Exception:  # saving the source is optional, so we can ignore any errors
                warnings.warn(""Couldn't retrieve source code for container of ""
                              ""type "" + obj.__name__ + "". It won't be checked ""
                              ""for correctness upon loading."")
            return ('module', obj, source_file, source)
elif torch.is_storage(obj):
view_metadata: Optional[Tuple[str, int, int]]
"
300,"typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        if typename == ""module"":
# Ignore containers that don't have any sources saved
if all(data[1:]):
_check_container_source(*data)
return data[0]
        elif typename == ""storage"":
data_type, root_key, location, size, view_metadata = data
location = _maybe_decode_ascii(location)
if root_key not in deserialized_objects:
","typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        if typename == 'module':
# Ignore containers that don't have any sources saved
if all(data[1:]):
_check_container_source(*data)
return data[0]
        elif typename == 'storage':
data_type, root_key, location, size, view_metadata = data
location = _maybe_decode_ascii(location)
if root_key not in deserialized_objects:
"
301,"# NOTE: This should only be used on internal keys (e.g., `typename` and
#       `location` in `persistent_load` below!
if isinstance(bytes_str, bytes):
        return bytes_str.decode(""ascii"")
return bytes_str
","# NOTE: This should only be used on internal keys (e.g., `typename` and
#       `location` in `persistent_load` below!
if isinstance(bytes_str, bytes):
        return bytes_str.decode('ascii')
return bytes_str
"
302,"\infty & x < 0
\end{cases}
\end{align}
""""""
    + """"""
Args:
input (Tensor): the input tensor.
","\end{cases}
\end{align}
"""""" + """"""
Args:
input (Tensor): the input tensor.
"
303,"tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""",
)
psi = _add_docstr(
    _special.special_psi,
    r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""",
)
digamma = _add_docstr(
    _special.special_digamma,
    r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
""""""
    + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
","tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""")
psi = _add_docstr(_special.special_psi,
                  r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""")
digamma = _add_docstr(_special.special_digamma,
                      r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
"""""" + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
"
304,">>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
"""""".format(
        **common_args
    ),
)

erfcx = _add_docstr(
    _special.special_erfcx,
    r""""""
erfcx(input, *, out=None) -> Tensor
Computes the scaled complementary error function for each element of :attr:`input`.
",">>> torch.special.erfc(torch.tensor([0, -1., 10.]))
tensor([ 1.0000, 1.8427,  0.0000])
"""""".format(**common_args))

erfcx = _add_docstr(_special.special_erfcx,
                    r""""""
erfcx(input, *, out=None) -> Tensor
Computes the scaled complementary error function for each element of :attr:`input`.
"
305,"Similar to SciPy's `scipy.special.xlog1py`.
""""""
    + r""""""
Args:
input (Number or Tensor) : Multiplier
","Similar to SciPy's `scipy.special.xlog1py`.
"""""" + r""""""
Args:
input (Number or Tensor) : Multiplier
"
306,"import io
from typing import Any, TypeVar, Type
import torch

from ._utils import _type, _cuda
T = TypeVar(""T"", bound=""_StorageBase"")


class _StorageBase(object):
_cdata: Any
is_cuda: bool = False
is_sparse: bool = False
    def __init__(self, *args, **kwargs):
        ...  # noqa: E704

    def __len__(self) -> int:
        ...  # noqa: E704

    def __getitem__(self, idx):
        ...  # noqa: E704

    def copy_(self, source: T) -> T:
        ...  # noqa: E704

    def size(self) -> int:
        ...  # noqa: E704

    def type(self, dtype: str = None, non_blocking: bool = False) -> T:
        ...  # noqa: E704

    def cuda(self, device=None, non_blocking=False, **kwargs) -> T:
        ...  # noqa: E704

    def element_size(self) -> int:
        ...  # noqa: E704

    def get_device(self) -> int:
        ...  # noqa: E704
# Defined in torch/csrc/generic/StorageSharing.cpp
    def _share_filename_(self):
        ...  # noqa: E704

    def _share_fd_(self):
        ...  # noqa: E704

@classmethod
    def _new_using_filename(cls: Type[T], size: int) -> T:
        ...  # noqa: E704

@classmethod
    def _new_using_fd(cls: Type[T], size: int) -> T:
        ...  # noqa: E704
def __str__(self):
        content = "" "" + ""\n "".join(str(self[i]) for i in range(len(self)))
        return content + f""\n[{torch.typename(self)} of size {len(self)}]""
def __repr__(self):
return str(self)
","import io
import torch
from ._utils import _type, _cuda
from typing import Any, TypeVar, Type
T = TypeVar('T', bound='_StorageBase')
class _StorageBase(object):
_cdata: Any
is_cuda: bool = False
is_sparse: bool = False
    def __init__(self, *args, **kwargs): ...  # noqa: E704
    def __len__(self) -> int: ...  # noqa: E704
    def __getitem__(self, idx): ...  # noqa: E704
    def copy_(self, source: T) -> T: ...  # noqa: E704
    def size(self) -> int: ...  # noqa: E704
    def type(self, dtype: str = None, non_blocking: bool = False) -> T: ...  # noqa: E704
    def cuda(self, device=None, non_blocking=False, **kwargs) -> T: ...  # noqa: E704
    def element_size(self) -> int: ...  # noqa: E704
    def get_device(self) -> int: ...  # noqa: E704
# Defined in torch/csrc/generic/StorageSharing.cpp
    def _share_filename_(self): ...  # noqa: E704
    def _share_fd_(self): ...  # noqa: E704
@classmethod
    def _new_using_filename(cls: Type[T], size: int) -> T: ...  # noqa: E704
@classmethod
    def _new_using_fd(cls: Type[T], size: int) -> T: ...  # noqa: E704
def __str__(self):
        content = ' ' + '\n '.join(str(self[i]) for i in range(len(self)))
        return content + f'\n[{torch.typename(self)} of size {len(self)}]'
def __repr__(self):
return str(self)
"
307,"return self.clone()
def __deepcopy__(self, memo):
        memo = memo.setdefault(""torch"", {})
if self._cdata in memo:
return memo[self._cdata]
new_storage = self.clone()
","return self.clone()
def __deepcopy__(self, memo):
        memo = memo.setdefault('torch', {})
if self._cdata in memo:
return memo[self._cdata]
new_storage = self.clone()
"
308,"Returns: self
""""""
from torch.multiprocessing import get_sharing_strategy

if self.is_cuda:
pass  # CUDA doesn't use POSIX shared memory
        elif get_sharing_strategy() == ""file_system"":
self._share_filename_()
else:
self._share_fd_()
","Returns: self
""""""
from torch.multiprocessing import get_sharing_strategy
if self.is_cuda:
pass  # CUDA doesn't use POSIX shared memory
        elif get_sharing_strategy() == 'file_system':
self._share_filename_()
else:
self._share_fd_()
"
309,"workspace.RunNetOnce(model.param_init_net)
    init_net, predict_net = mobile_exporter.Export(
        workspace, model.net, model.params
    )
if args.debug:
print(""init_net:"")
","workspace.RunNetOnce(model.param_init_net)
    init_net, predict_net = mobile_exporter.Export(workspace, model.net, model.params)
if args.debug:
print(""init_net:"")
"
310,"type=t, lower_name=lower_name + default_arg
)
attr_init = ""{private_name}({lower_name})"".format(
            private_name=private_name, lower_name=lower_name)
attr_declare = ""{type} {private_name};"".format(
            type=t, private_name=private_name)
attr_get = dedent(
""""""
{type} get{name}() const {{
","type=t, lower_name=lower_name + default_arg
)
attr_init = ""{private_name}({lower_name})"".format(
            private_name=private_name, lower_name=lower_name
        )
attr_declare = ""{type} {private_name};"".format(
            type=t, private_name=private_name
        )
attr_get = dedent(
""""""
{type} get{name}() const {{
"
311,"if use_offsets:
code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);"")
else:
        code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);"")
for i in range(0, uf):
j = 8 * i
code.append(
","if use_offsets:
code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);"")
else:
        code.append(
            ""        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);""
        )
for i in range(0, uf):
j = 8 * i
code.append(
"
312,"############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(u'''
Welcome to the developer reference for the PyTorch C++ API.
    '''),
}
# Tell sphinx what the primary language being documented is.
primary_domain = 'cpp'
# Tell sphinx what the pygments highlight language should be.
highlight_language = 'cpp'
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
","############################################################################
# Main library page layout example configuration.                          #
############################################################################
    ""afterTitleDescription"": textwrap.dedent(
        u""""""
Welcome to the developer reference for the PyTorch C++ API.
    """"""
    ),
}
# Tell sphinx what the primary language being documented is.
primary_domain = ""cpp""
# Tell sphinx what the pygments highlight language should be.
highlight_language = ""cpp""
# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']
"
313,"# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'
# The master toctree document.
master_doc = 'index'
# General information about the project.
project = 'PyTorch'
copyright = '2019, Torch Contributors'
author = 'Torch Contributors'
# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
","# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = "".rst""
# The master toctree document.
master_doc = ""index""
# General information about the project.
project = ""PyTorch""
copyright = ""2019, Torch Contributors""
author = ""Torch Contributors""
# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
"
314,"#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = 'master'
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = 'master'
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
","#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = ""master""
# The full version, including alpha/beta/rc tags.
# TODO: verify this works as expected
release = ""master""
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
"
315,"#
#
html_theme = 'pytorch_sphinx_theme'
html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]
# Theme options are theme-specific and customize the look and feel of a theme
","#
#
html_theme = ""pytorch_sphinx_theme""
html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]
# Theme options are theme-specific and customize the look and feel of a theme
"
316,"# to autogenerated content
anchor = ref_anchor[1]
txt = node.parent.astext()
                if txt == anchor or txt == anchor.split('.')[-1]:
self.body.append('<p id=""{}""/>'.format(ref_anchor[1]))
return old_call(self, node)
Klass.visit_reference = visit_reference
replace(html.HTMLTranslator)
replace(html5.HTML5Translator)
# -- Options for HTMLHelp output ------------------------------------------
# Output file base name for HTML help builder.
htmlhelp_basename = 'PyTorchdoc'
# -- Options for LaTeX output ---------------------------------------------
","# to autogenerated content
anchor = ref_anchor[1]
txt = node.parent.astext()
                if txt == anchor or txt == anchor.split(""."")[-1]:
self.body.append('<p id=""{}""/>'.format(ref_anchor[1]))
return old_call(self, node)

Klass.visit_reference = visit_reference

replace(html.HTMLTranslator)
replace(html5.HTML5Translator)
# -- Options for HTMLHelp output ------------------------------------------
# Output file base name for HTML help builder.
htmlhelp_basename = ""PyTorchdoc""
# -- Options for LaTeX output ---------------------------------------------
"
317,"# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'PyTorch', 'PyTorch Documentation',
     author, 'PyTorch', 'One line description of project.',
     'Miscellaneous'),
]
# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {
    'python': ('https://docs.python.org/3', None),
    'numpy': ('https://numpy.org/doc/stable', None),
}
# -- A patch that prevents Sphinx from cross-referencing ivar tags -------
# See http://stackoverflow.com/a/41184353/3343043
from docutils import nodes
from sphinx.util.docfields import TypedField
from sphinx import addnodes
import sphinx.ext.doctest
# Without this, doctest adds any example with a `>>>` as a test
doctest_test_doctest_blocks = ''
doctest_default_flags = sphinx.ext.doctest.doctest.ELLIPSIS
doctest_global_setup = '''
import torch
try:
import torchvision
except ImportError:
torchvision = None
'''
def patched_make_field(self, types, domain, items, **kw):
","# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        ""PyTorch"",
        ""PyTorch Documentation"",
        author,
        ""PyTorch"",
        ""One line description of project."",
        ""Miscellaneous"",
    ),
]
# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {
    ""python"": (""https://docs.python.org/3"", None),
    ""numpy"": (""https://numpy.org/doc/stable"", None),
}
import sphinx.ext.doctest

# -- A patch that prevents Sphinx from cross-referencing ivar tags -------
# See http://stackoverflow.com/a/41184353/3343043
from docutils import nodes
from sphinx import addnodes
from sphinx.util.docfields import TypedField
# Without this, doctest adds any example with a `>>>` as a test
doctest_test_doctest_blocks = """"
doctest_default_flags = sphinx.ext.doctest.doctest.ELLIPSIS
doctest_global_setup = """"""
import torch
try:
import torchvision
except ImportError:
torchvision = None
""""""
def patched_make_field(self, types, domain, items, **kw):
"
318,"""""""
import os.path
import torch.nn.modules.activation
import torch.autograd
import matplotlib
matplotlib.use('Agg')
import pylab
# Create a directory for the images, if it doesn't exist
ACTIVATION_IMAGE_PATH = os.path.join(
    os.path.realpath(os.path.join(__file__, "".."")),
    ""activation_images""
)
if not os.path.exists(ACTIVATION_IMAGE_PATH):
","""""""
import os.path

import matplotlib
import torch.autograd
import torch.nn.modules.activation
matplotlib.use(""Agg"")
import pylab
# Create a directory for the images, if it doesn't exist
ACTIVATION_IMAGE_PATH = os.path.join(
    os.path.realpath(os.path.join(__file__, "".."")), ""activation_images""
)
if not os.path.exists(ACTIVATION_IMAGE_PATH):
"
319,"# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    'ELU',
    'Hardshrink',
    'Hardtanh',
    'LeakyReLU',  # Perhaps we should add text explaining slight slope?
    'LogSigmoid',
    'PReLU',
    'ReLU',
    'ReLU6',
    'RReLU',
    'SELU',
    'SiLU',
    'Mish',
    'CELU',
    'GELU',
    'Sigmoid',
    'Softplus',
    'Softshrink',
    'Softsign',
    'Tanh',
    'Tanhshrink'
# 'Threshold'  Omit, pending cleanup. See PR5457
]
","# In a refactor, these ought to go into their own module or entry
# points so we can generate this list programmaticly
functions = [
    ""ELU"",
    ""Hardshrink"",
    ""Hardtanh"",
    ""LeakyReLU"",  # Perhaps we should add text explaining slight slope?
    ""LogSigmoid"",
    ""PReLU"",
    ""ReLU"",
    ""ReLU6"",
    ""RReLU"",
    ""SELU"",
    ""SiLU"",
    ""Mish"",
    ""CELU"",
    ""GELU"",
    ""Sigmoid"",
    ""Softplus"",
    ""Softshrink"",
    ""Softsign"",
    ""Tanh"",
    ""Tanhshrink""
# 'Threshold'  Omit, pending cleanup. See PR5457
]
"
320,"""""""
xrange = torch.arange(-7.0, 7.0, 0.01)  # We need to go beyond 6 for ReLU6
pylab.plot(
        xrange.numpy(),
        function(torch.autograd.Variable(xrange)).data.numpy(),
        **args
)
","""""""
xrange = torch.arange(-7.0, 7.0, 0.01)  # We need to go beyond 6 for ReLU6
pylab.plot(
        xrange.numpy(), function(torch.autograd.Variable(xrange)).data.numpy(), **args
)
"
321,"example = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.trace(model, example)
ops = torch.jit.export_opnames(traced_script_module)
with open('mobilenetv2.yaml', 'w') as output:
yaml.dump(ops, output)
","example = torch.rand(1, 3, 224, 224)
traced_script_module = torch.jit.trace(model, example)
ops = torch.jit.export_opnames(traced_script_module)
with open(""mobilenetv2.yaml"", ""w"") as output:
yaml.dump(ops, output)
"
322,"if IS_WINDOWS:
# /NODEFAULTLIB makes sure we only link to DLL runtime
# and matches the flags set for protobuf and ONNX
        extra_link_args = ['/NODEFAULTLIB:LIBCMT.LIB']
# /MD links against DLL runtime
# and matches the flags set for protobuf and ONNX
# /EHsc is about standard C++ exception handling
# /DNOMINMAX removes builtin min/max functions
# /wdXXXX disables warning no. XXXX
        extra_compile_args = ['/MD', '/EHsc', '/DNOMINMAX',
                              '/wd4267', '/wd4251', '/wd4522', '/wd4522', '/wd4838',
                              '/wd4305', '/wd4244', '/wd4190', '/wd4101', '/wd4996',
                              '/wd4275']
else:
extra_link_args = []
extra_compile_args = [
            '-Wall',
            '-Wextra',
            '-Wno-strict-overflow',
            '-Wno-unused-parameter',
            '-Wno-missing-field-initializers',
            '-Wno-write-strings',
            '-Wno-unknown-pragmas',
# This is required for Python 2 declarations that are deprecated in 3.
            '-Wno-deprecated-declarations',
# Python 2.6 requires -fno-strict-aliasing, see
# http://legacy.python.org/dev/peps/pep-3123/
# We also depend on it in our code (even Python 3).
            '-fno-strict-aliasing',
# Clang has an unfixed bug leading to spurious missing
# braces warnings, see
# https://bugs.llvm.org/show_bug.cgi?id=21629
            '-Wno-missing-braces',
]
library_dirs.append(lib_path)
main_compile_args = []
    main_libraries = ['torch_python']
main_link_args = []
main_sources = [""torch/csrc/stub.c""]
    if cmake_cache_vars['USE_CUDA']:
        library_dirs.append(
            os.path.dirname(cmake_cache_vars['CUDA_CUDA_LIB']))
if build_type.is_debug():
if IS_WINDOWS:
            extra_compile_args.append('/Z7')
            extra_link_args.append('/DEBUG:FULL')
else:
            extra_compile_args += ['-O0', '-g']
            extra_link_args += ['-O0', '-g']
if build_type.is_rel_with_deb_info():
if IS_WINDOWS:
            extra_compile_args.append('/Z7')
            extra_link_args.append('/DEBUG:FULL')
else:
            extra_compile_args += ['-g']
            extra_link_args += ['-g']
# Cross-compile for M1
if IS_DARWIN:
        macos_target_arch = os.getenv('CMAKE_OSX_ARCHITECTURES', '')
        if macos_target_arch in ['arm64', 'x86_64']:
            macos_sysroot_path = os.getenv('CMAKE_OSX_SYSROOT')
if macos_sysroot_path is None:
                macos_sysroot_path = subprocess.check_output([
                    'xcrun', '--show-sdk-path', '--sdk', 'macosx'
                ]).decode('utf-8').strip()
            extra_compile_args += ['-arch', macos_target_arch, '-isysroot', macos_sysroot_path]
            extra_link_args += ['-arch', macos_target_arch]

def make_relative_rpath_args(path):
if IS_DARWIN:
            return ['-Wl,-rpath,@loader_path/' + path]
elif IS_WINDOWS:
return []
else:
            return ['-Wl,-rpath,$ORIGIN/' + path]
################################################################################
# Declare extensions and package
################################################################################
extensions = []
    packages = find_packages(exclude=('tools', 'tools.*'))
    C = Extension(""torch._C"",
                  libraries=main_libraries,
                  sources=main_sources,
                  language='c',
                  extra_compile_args=main_compile_args + extra_compile_args,
                  include_dirs=[],
                  library_dirs=library_dirs,
                  extra_link_args=extra_link_args + main_link_args + make_relative_rpath_args('lib'))
extensions.append(C)
if not IS_WINDOWS:
        DL = Extension(""torch._dl"",
                       sources=[""torch/csrc/dl.c""],
                       language='c')
extensions.append(DL)
# These extensions are built by cmake and copied manually in build_extensions()
# inside the build_ext implementation
extensions.append(
        Extension(
            name=str('caffe2.python.caffe2_pybind11_state'),
            sources=[]),
)
    if cmake_cache_vars['USE_CUDA']:
extensions.append(
            Extension(
                name=str('caffe2.python.caffe2_pybind11_state_gpu'),
                sources=[]),
)
    if cmake_cache_vars['USE_ROCM']:
extensions.append(
            Extension(
                name=str('caffe2.python.caffe2_pybind11_state_hip'),
                sources=[]),
)
cmdclass = {
        'bdist_wheel': wheel_concatenate,
        'build_ext': build_ext,
        'clean': clean,
        'install': install,
        'sdist': sdist,
}
entry_points = {
        'console_scripts': [
            'convert-caffe2-to-onnx = caffe2.python.onnx.bin.conversion:caffe2_to_onnx',
            'convert-onnx-to-caffe2 = caffe2.python.onnx.bin.conversion:onnx_to_caffe2',
]
}
return extensions, cmdclass, packages, entry_points, extra_install_requires
# post run, warnings, printed at the end to make them more visible
build_update_message = """"""
It is no longer necessary to use the 'build' or 'rebuild' targets
","if IS_WINDOWS:
# /NODEFAULTLIB makes sure we only link to DLL runtime
# and matches the flags set for protobuf and ONNX
        extra_link_args = [""/NODEFAULTLIB:LIBCMT.LIB""]
# /MD links against DLL runtime
# and matches the flags set for protobuf and ONNX
# /EHsc is about standard C++ exception handling
# /DNOMINMAX removes builtin min/max functions
# /wdXXXX disables warning no. XXXX
        extra_compile_args = [
            ""/MD"",
            ""/EHsc"",
            ""/DNOMINMAX"",
            ""/wd4267"",
            ""/wd4251"",
            ""/wd4522"",
            ""/wd4522"",
            ""/wd4838"",
            ""/wd4305"",
            ""/wd4244"",
            ""/wd4190"",
            ""/wd4101"",
            ""/wd4996"",
            ""/wd4275"",
        ]
else:
extra_link_args = []
extra_compile_args = [
            ""-Wall"",
            ""-Wextra"",
            ""-Wno-strict-overflow"",
            ""-Wno-unused-parameter"",
            ""-Wno-missing-field-initializers"",
            ""-Wno-write-strings"",
            ""-Wno-unknown-pragmas"",
# This is required for Python 2 declarations that are deprecated in 3.
            ""-Wno-deprecated-declarations"",
# Python 2.6 requires -fno-strict-aliasing, see
# http://legacy.python.org/dev/peps/pep-3123/
# We also depend on it in our code (even Python 3).
            ""-fno-strict-aliasing"",
# Clang has an unfixed bug leading to spurious missing
# braces warnings, see
# https://bugs.llvm.org/show_bug.cgi?id=21629
            ""-Wno-missing-braces"",
]
library_dirs.append(lib_path)
main_compile_args = []
    main_libraries = [""torch_python""]
main_link_args = []
main_sources = [""torch/csrc/stub.c""]
    if cmake_cache_vars[""USE_CUDA""]:
        library_dirs.append(os.path.dirname(cmake_cache_vars[""CUDA_CUDA_LIB""]))
if build_type.is_debug():
if IS_WINDOWS:
            extra_compile_args.append(""/Z7"")
            extra_link_args.append(""/DEBUG:FULL"")
else:
            extra_compile_args += [""-O0"", ""-g""]
            extra_link_args += [""-O0"", ""-g""]
if build_type.is_rel_with_deb_info():
if IS_WINDOWS:
            extra_compile_args.append(""/Z7"")
            extra_link_args.append(""/DEBUG:FULL"")
else:
            extra_compile_args += [""-g""]
            extra_link_args += [""-g""]
# Cross-compile for M1
if IS_DARWIN:
        macos_target_arch = os.getenv(""CMAKE_OSX_ARCHITECTURES"", """")
        if macos_target_arch in [""arm64"", ""x86_64""]:
            macos_sysroot_path = os.getenv(""CMAKE_OSX_SYSROOT"")
if macos_sysroot_path is None:
                macos_sysroot_path = (
                    subprocess.check_output(
                        [""xcrun"", ""--show-sdk-path"", ""--sdk"", ""macosx""]
                    )
                    .decode(""utf-8"")
                    .strip()
                )
            extra_compile_args += [
                ""-arch"",
                macos_target_arch,
                ""-isysroot"",
                macos_sysroot_path,
            ]
            extra_link_args += [""-arch"", macos_target_arch]
def make_relative_rpath_args(path):
if IS_DARWIN:
            return [""-Wl,-rpath,@loader_path/"" + path]
elif IS_WINDOWS:
return []
else:
            return [""-Wl,-rpath,$ORIGIN/"" + path]
################################################################################
# Declare extensions and package
################################################################################
extensions = []
    packages = find_packages(exclude=(""tools"", ""tools.*""))
    C = Extension(
        ""torch._C"",
        libraries=main_libraries,
        sources=main_sources,
        language=""c"",
        extra_compile_args=main_compile_args + extra_compile_args,
        include_dirs=[],
        library_dirs=library_dirs,
        extra_link_args=extra_link_args
        + main_link_args
        + make_relative_rpath_args(""lib""),
    )
extensions.append(C)
if not IS_WINDOWS:
        DL = Extension(""torch._dl"", sources=[""torch/csrc/dl.c""], language=""c"")
extensions.append(DL)
# These extensions are built by cmake and copied manually in build_extensions()
# inside the build_ext implementation
extensions.append(
        Extension(name=str(""caffe2.python.caffe2_pybind11_state""), sources=[]),
)
    if cmake_cache_vars[""USE_CUDA""]:
extensions.append(
            Extension(name=str(""caffe2.python.caffe2_pybind11_state_gpu""), sources=[]),
)
    if cmake_cache_vars[""USE_ROCM""]:
extensions.append(
            Extension(name=str(""caffe2.python.caffe2_pybind11_state_hip""), sources=[]),
)
cmdclass = {
        ""bdist_wheel"": wheel_concatenate,
        ""build_ext"": build_ext,
        ""clean"": clean,
        ""install"": install,
        ""sdist"": sdist,
}
entry_points = {
        ""console_scripts"": [
            ""convert-caffe2-to-onnx = caffe2.python.onnx.bin.conversion:caffe2_to_onnx"",
            ""convert-onnx-to-caffe2 = caffe2.python.onnx.bin.conversion:onnx_to_caffe2"",
]
}
return extensions, cmdclass, packages, entry_points, extra_install_requires

# post run, warnings, printed at the end to make them more visible
build_update_message = """"""
It is no longer necessary to use the 'build' or 'rebuild' targets
"
323,"
r""""""
The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
","r""""""
The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
"
324,"# The __file__ check only works for Python 3.7 and above.
if sys.version_info >= (3, 7) and _C_for_compiled_check.__file__ is None:
        raise ImportError(textwrap.dedent('''
Failed to load PyTorch C extensions:
It appears that PyTorch has loaded the `torch/_C` folder
of the PyTorch repository rather than the C extensions which
","# The __file__ check only works for Python 3.7 and above.
if sys.version_info >= (3, 7) and _C_for_compiled_check.__file__ is None:
        raise ImportError(
            textwrap.dedent(
                """"""
Failed to load PyTorch C extensions:
It appears that PyTorch has loaded the `torch/_C` folder
of the PyTorch repository rather than the C extensions which
"
325,"device_type = torch.device(device_type).type
m = sys.modules[__name__]
if hasattr(m, device_type):
        raise RuntimeError(""The runtime module of '{}' has already ""
                           ""been registered with '{}'"".format(device_type, getattr(m, device_type)))
setattr(m, device_type, module)
","device_type = torch.device(device_type).type
m = sys.modules[__name__]
if hasattr(m, device_type):
        raise RuntimeError(
            ""The runtime module of '{}' has already ""
            ""been registered with '{}'"".format(device_type, getattr(m, device_type))
        )
setattr(m, device_type, module)
"
326,"if has_high_char:
try:
import win32api
dir = win32api.GetShortPathName(dir)
except ImportError:
pass
","if has_high_char:
try:
import win32api

dir = win32api.GetShortPathName(dir)
except ImportError:
pass
"
327,"return fn
if not isinstance(drop, bool):
        raise RuntimeError(""Argument to @torch.jit.ignore must be a bool or ""
                           f""a function but got {drop}"")
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
drop = drop_on_export
elif drop:
        warnings.warn(""ignore(True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
def decorator(fn):
if drop:
","return fn
if not isinstance(drop, bool):
        raise RuntimeError(
            ""Argument to @torch.jit.ignore must be a bool or ""
            f""a function but got {drop}""
        )
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(
            ""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
drop = drop_on_export
elif drop:
        warnings.warn(
            ""ignore(True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
def decorator(fn):
if drop:
"
328,"def get_torchscript_modifier(fn):
if not callable(fn):
return None
    if hasattr(fn, '__func__'):
fn = fn.__func__
    return getattr(fn, '_torchscript_modifier', FunctionModifiers.DEFAULT)
def copy_torchscript_modifier(orig, new) -> None:
attr = get_torchscript_modifier(orig)
","def get_torchscript_modifier(fn):
if not callable(fn):
return None
    if hasattr(fn, ""__func__""):
fn = fn.__func__
    return getattr(fn, ""_torchscript_modifier"", FunctionModifiers.DEFAULT)

def copy_torchscript_modifier(orig, new) -> None:
attr = get_torchscript_modifier(orig)
"
329,"else:
existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]
if existing_lineno != line_no:
            raise RuntimeError(""Cannot currently overload the same method name in two different""
                               "" classes with the same name in the same module"")
method_overloads.append(func)
return func
def _get_overloaded_methods(method, mod_class):
# TODO: __name__ not set for submodules in recursive script
if not hasattr(method, ""__name__""):
","else:
existing_lineno = _overloaded_method_class_fileno[(qual_name, class_name)]
if existing_lineno != line_no:
            raise RuntimeError(
                ""Cannot currently overload the same method name in two different""
                "" classes with the same name in the same module""
            )
method_overloads.append(func)
return func

def _get_overloaded_methods(method, mod_class):
# TODO: __name__ not set for submodules in recursive script
if not hasattr(method, ""__name__""):
"
330,"raise_error_container_parameter_missing(""Tuple"")
# For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is Tuple or
            getattr(ann, '__origin__', None) is tuple)
def is_list(ann) -> bool:
if ann is List:
raise_error_container_parameter_missing(""List"")
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is List or
            getattr(ann, '__origin__', None) is list)
def is_dict(ann) -> bool:
if ann is Dict:
raise_error_container_parameter_missing(""Dict"")
    if not hasattr(ann, '__module__'):
return False
    return ann.__module__ == 'typing' and \
        (getattr(ann, '__origin__', None) is Dict or
            getattr(ann, '__origin__', None) is dict)
def is_optional(ann) -> bool:
if ann is Optional:
","raise_error_container_parameter_missing(""Tuple"")
# For some reason Python 3.7 violates the Type[A, B].__origin__ == Type rule
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is Tuple
        or getattr(ann, ""__origin__"", None) is tuple
    )

def is_list(ann) -> bool:
if ann is List:
raise_error_container_parameter_missing(""List"")
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is List
        or getattr(ann, ""__origin__"", None) is list
    )

def is_dict(ann) -> bool:
if ann is Dict:
raise_error_container_parameter_missing(""Dict"")
    if not hasattr(ann, ""__module__""):
return False
    return ann.__module__ == ""typing"" and (
        getattr(ann, ""__origin__"", None) is Dict
        or getattr(ann, ""__origin__"", None) is dict
    )

def is_optional(ann) -> bool:
if ann is Optional:
"
331,"def transpose(A):
    """"""Return transpose of a matrix or batches of matrices.
    """"""
ndim = len(A.shape)
return A.transpose(ndim - 1, ndim - 2)
def transjugate(A):
    """"""Return transpose conjugate of a matrix or batches of matrices.
    """"""
return conjugate(transpose(A))
def bform(X: Tensor, A: Optional[Tensor], Y: Tensor) -> Tensor:
    """"""Return bilinear form of matrices: :math:`X^T A Y`.
    """"""
return matmul(transpose(X), matmul(A, Y))
def qform(A: Optional[Tensor], S: Tensor):
    """"""Return quadratic form :math:`S^T A S`.
    """"""
return bform(S, A, S)
def basis(A):
    """"""Return orthogonal basis of A columns.
    """"""
if A.is_cuda:
# torch.orgqr is not available in CUDA
Q = torch.linalg.qr(A).Q
","def transpose(A):
    """"""Return transpose of a matrix or batches of matrices.""""""
ndim = len(A.shape)
return A.transpose(ndim - 1, ndim - 2)
def transjugate(A):
    """"""Return transpose conjugate of a matrix or batches of matrices.""""""
return conjugate(transpose(A))
def bform(X: Tensor, A: Optional[Tensor], Y: Tensor) -> Tensor:
    """"""Return bilinear form of matrices: :math:`X^T A Y`.""""""
return matmul(transpose(X), matmul(A, Y))
def qform(A: Optional[Tensor], S: Tensor):
    """"""Return quadratic form :math:`S^T A S`.""""""
return bform(S, A, S)
def basis(A):
    """"""Return orthogonal basis of A columns.""""""
if A.is_cuda:
# torch.orgqr is not available in CUDA
Q = torch.linalg.qr(A).Q
"
332,"and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""
def __init__(self, name):
        super(_OpNamespace, self).__init__('torch.ops.' + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == '__file__':
            return 'torch.ops'
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = '{}::{}'.format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
","and subsequent accesses will incur no further lookup (the namespace and
operation will already exist).
""""""

def __init__(self, name):
        super(_OpNamespace, self).__init__(""torch.ops."" + name)
self.name = name
def __getattr__(self, op_name):
# It is not a valid op_name when __file__ is passed in
        if op_name == ""__file__"":
            return ""torch.ops""
# Get the op `my_namespace::my_op` if available. This will also check
# for overloads and raise an exception if there are more than one.
        qualified_op_name = ""{}::{}"".format(self.name, op_name)
op = torch._C._jit_get_operation(qualified_op_name)
# let the script frontend know that op is identical to the builtin op
# with qualified_op_name
"
333,"from collections import OrderedDict
import functools
from numbers import Number
from typing import Any, Dict, Optional, Tuple, Union
import warnings
import copyreg
import torch
import torch._C as _C
from torch._namedtensor_internals import (
    update_names, check_serializing_named_tensor, resolve_ellipsis,
    unzip_namedshape, single_ellipsis_index, is_ellipsis)
from torch.overrides import (
    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    handle_torch_function, get_default_nowrap_functions)
import torch.utils.hooks as hooks
def _wrap_type_error_to_not_implemented(f):
# functools.wraps doesn't work well with methods in python 2
    method_assignments = ('__name__', '__doc__')
assigned = functools.WRAPPER_ASSIGNMENTS
@functools.wraps(f, assigned=assigned)
","import copyreg
import functools
import warnings
from collections import OrderedDict
from numbers import Number
from typing import Any, Dict, Optional, Tuple, Union
import torch
import torch._C as _C
import torch.utils.hooks as hooks
from torch._namedtensor_internals import (
    update_names,
    check_serializing_named_tensor,
    resolve_ellipsis,
    unzip_namedshape,
    single_ellipsis_index,
    is_ellipsis,
)
from torch.overrides import (
    has_torch_function,
    has_torch_function_unary,
    has_torch_function_variadic,
    handle_torch_function,
    get_default_nowrap_functions,
)
def _wrap_type_error_to_not_implemented(f):
# functools.wraps doesn't work well with methods in python 2
    method_assignments = (""__name__"", ""__doc__"")
assigned = functools.WRAPPER_ASSIGNMENTS
@functools.wraps(f, assigned=assigned)
"
334,"# does accurate alias tracking; however, the code below
# doesn't work because of
# https://github.com/pytorch/pytorch/issues/47442
            if self.is_sparse or self.device.type in ['xla', 'mlc', 'meta']:
new_tensor = self.clone()
else:
new_storage = self.storage().__deepcopy__(memo)
if self.is_quantized:
# quantizer_params can be different type based on torch attribute
                    quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[torch.qscheme, Tensor, Tensor, int]]
if self.qscheme() == torch.per_tensor_affine:
                        quantizer_params = self.qscheme(), self.q_scale(), self.q_zero_point()
                    elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):
                        quantizer_params = self.qscheme(), \
                            self.q_per_channel_scales(), \
                            self.q_per_channel_zero_points(), \
                            self.q_per_channel_axis()
else:
                        raise RuntimeError(f""Unsupported qscheme {self.qscheme()} in deepcopy"")
new_tensor = torch._utils._rebuild_qtensor(
new_storage,
self.storage_offset(),
","# does accurate alias tracking; however, the code below
# doesn't work because of
# https://github.com/pytorch/pytorch/issues/47442
            if self.is_sparse or self.device.type in [""xla"", ""mlc"", ""meta""]:
new_tensor = self.clone()
else:
new_storage = self.storage().__deepcopy__(memo)
if self.is_quantized:
# quantizer_params can be different type based on torch attribute
                    quantizer_params: Union[
                        Tuple[torch.qscheme, float, int],
                        Tuple[torch.qscheme, Tensor, Tensor, int],
                    ]
if self.qscheme() == torch.per_tensor_affine:
                        quantizer_params = (
                            self.qscheme(),
                            self.q_scale(),
                            self.q_zero_point(),
                        )
                    elif self.qscheme() in (
                        torch.per_channel_affine,
                        torch.per_channel_affine_float_qparams,
                    ):
                        quantizer_params = (
                            self.qscheme(),
                            self.q_per_channel_scales(),
                            self.q_per_channel_zero_points(),
                            self.q_per_channel_axis(),
                        )
else:
                        raise RuntimeError(
                            f""Unsupported qscheme {self.qscheme()} in deepcopy""
                        )
new_tensor = torch._utils._rebuild_qtensor(
new_storage,
self.storage_offset(),
"
335,"# 2. Python list is not a good fit due to performance reason.
#    `tolist()` converts every single element in the tensor into python objects
#    and serialize them one by one.
        if self.device.type == 'xla':
            arg_xla = (self.cpu().numpy(),
                       self.dtype,
                       str(self.device),
                       self.requires_grad)
return (torch._utils._rebuild_xla_tensor, arg_xla)
        if self.device.type == 'mlc':
            arg_mlc = (self.cpu().numpy(),
                       self.dtype,
                       str(self.device),
                       self.requires_grad)
return (torch._utils._rebuild_mlc_tensor, arg_mlc)
        if self.device.type == 'meta':
# NB: This implementation BREAKS storage sharing.  Current
# hypothesis is that no one cares for meta tensors.
arg_meta = (
","# 2. Python list is not a good fit due to performance reason.
#    `tolist()` converts every single element in the tensor into python objects
#    and serialize them one by one.
        if self.device.type == ""xla"":
            arg_xla = (
                self.cpu().numpy(),
                self.dtype,
                str(self.device),
                self.requires_grad,
            )
return (torch._utils._rebuild_xla_tensor, arg_xla)
        if self.device.type == ""mlc"":
            arg_mlc = (
                self.cpu().numpy(),
                self.dtype,
                str(self.device),
                self.requires_grad,
            )
return (torch._utils._rebuild_mlc_tensor, arg_mlc)
        if self.device.type == ""meta"":
# NB: This implementation BREAKS storage sharing.  Current
# hypothesis is that no one cares for meta tensors.
arg_meta = (
"
336,"return (torch._utils._rebuild_meta_tensor_no_storage, arg_meta)
if self.is_quantized:
# quantizer_params can be different type based on torch attribute
            quantizer_params: Union[Tuple[torch.qscheme, float, int], Tuple[Any, Tensor, Tensor, int]]
if self.qscheme() == torch.per_tensor_affine:
                quantizer_params = (torch.per_tensor_affine,
                                    self.q_scale(),
                                    self.q_zero_point())
            elif self.qscheme() in (torch.per_channel_affine, torch.per_channel_affine_float_qparams):
# convert scales and zero points to tuple to avoid recursive calls
# when/if we get multi-axis quantized tensors in the future, the shape
# is recoverable from the main tensor shape
                quantizer_params = (torch.per_channel_affine,
                                    self.q_per_channel_scales(),
                                    self.q_per_channel_zero_points(),
                                    self.q_per_channel_axis())
else:
                raise RuntimeError(f""Serialization is not supported for tensors of type {self.qscheme()}"")
            args_qtensor = (self.storage(),
                            self.storage_offset(),
                            tuple(self.size()),
                            self.stride(),
                            quantizer_params,
                            self.requires_grad,
                            backward_hooks)
return (torch._utils._rebuild_qtensor, args_qtensor)
elif self.is_sparse:
if self.layout == torch.sparse_coo:
                args_sparse = (self.layout,
                               (self._indices(),
                                self._values(),
                                self.size()))
else:
raise NotImplementedError(
                    'sparse tensor __reduce_ex__ for layout `%s`' % (self.layout))
return (torch._utils._rebuild_sparse_tensor, args_sparse)
else:
            args = (self.storage(),
                    self.storage_offset(),
                    tuple(self.size()),
                    self.stride(),
                    self.requires_grad,
                    backward_hooks)  # previously was self._backward_hooks
return (torch._utils._rebuild_tensor_v2, args)
def __setstate__(self, state):
","return (torch._utils._rebuild_meta_tensor_no_storage, arg_meta)
if self.is_quantized:
# quantizer_params can be different type based on torch attribute
            quantizer_params: Union[
                Tuple[torch.qscheme, float, int], Tuple[Any, Tensor, Tensor, int]
            ]
if self.qscheme() == torch.per_tensor_affine:
                quantizer_params = (
                    torch.per_tensor_affine,
                    self.q_scale(),
                    self.q_zero_point(),
                )
            elif self.qscheme() in (
                torch.per_channel_affine,
                torch.per_channel_affine_float_qparams,
            ):
# convert scales and zero points to tuple to avoid recursive calls
# when/if we get multi-axis quantized tensors in the future, the shape
# is recoverable from the main tensor shape
                quantizer_params = (
                    torch.per_channel_affine,
                    self.q_per_channel_scales(),
                    self.q_per_channel_zero_points(),
                    self.q_per_channel_axis(),
                )
else:
                raise RuntimeError(
                    f""Serialization is not supported for tensors of type {self.qscheme()}""
                )
            args_qtensor = (
                self.storage(),
                self.storage_offset(),
                tuple(self.size()),
                self.stride(),
                quantizer_params,
                self.requires_grad,
                backward_hooks,
            )
return (torch._utils._rebuild_qtensor, args_qtensor)
elif self.is_sparse:
if self.layout == torch.sparse_coo:
                args_sparse = (
                    self.layout,
                    (self._indices(), self._values(), self.size()),
                )
else:
raise NotImplementedError(
                    ""sparse tensor __reduce_ex__ for layout `%s`"" % (self.layout)
                )
return (torch._utils._rebuild_sparse_tensor, args_sparse)
else:
            args = (
                self.storage(),
                self.storage_offset(),
                tuple(self.size()),
                self.stride(),
                self.requires_grad,
                backward_hooks,
            )  # previously was self._backward_hooks
return (torch._utils._rebuild_tensor_v2, args)
def __setstate__(self, state):
"
337,"raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)

def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.rename_, (self,), self, *names, **rename_map)
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
","raise RuntimeError(""unflatten: sizes must be non-empty"")
names = None
        if isinstance(sizes, OrderedDict) or (
            isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))
        ):
names, sizes = unzip_namedshape(sizes)
return super(Tensor, self).unflatten(dim, sizes, names)
def rename_(self, *names, **rename_map):
""""""In-place version of :meth:`~Tensor.rename`.""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.rename_, (self,), self, *names, **rename_map
            )
# Note [rename_ / rename API]
# The Python API for these is different from the C++ API. In Python:
"
338,"shape = self.size()
fill_value = 0
if len(shape) != 2:
            raise RuntimeError(""Only 2D tensors can be converted to the CSR format but got shape: "", shape)
if self.is_sparse:
coalesced_self = self.coalesce()
row_indices = coalesced_self.indices()[0]
device = coalesced_self.values().device
crow_indices = torch._convert_indices_from_coo_to_csr(
                row_indices, self.shape[0], out_int32=row_indices.dtype == torch.int32)
            return torch.sparse_csr_tensor(crow_indices,
                                           coalesced_self.indices()[1].contiguous(),
                                           coalesced_self.values(),
                                           size=coalesced_self.shape,
                                           dtype=coalesced_self.dtype,
                                           device=device)
elif self.is_sparse_csr:
return self
else:
","shape = self.size()
fill_value = 0
if len(shape) != 2:
            raise RuntimeError(
                ""Only 2D tensors can be converted to the CSR format but got shape: "",
                shape,
            )
if self.is_sparse:
coalesced_self = self.coalesce()
row_indices = coalesced_self.indices()[0]
device = coalesced_self.values().device
crow_indices = torch._convert_indices_from_coo_to_csr(
                row_indices, self.shape[0], out_int32=row_indices.dtype == torch.int32
            )
            return torch.sparse_csr_tensor(
                crow_indices,
                coalesced_self.indices()[1].contiguous(),
                coalesced_self.values(),
                size=coalesced_self.shape,
                dtype=coalesced_self.dtype,
                device=device,
            )
elif self.is_sparse_csr:
return self
else:
"
339,"def add_docstr_all(method, docstr):
add_docstr(getattr(torch._C._TensorBase, method), docstr)
common_args = parse_kwargs(""""""
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.preserve_format``.
"""""")
new_common_args = parse_kwargs(""""""
size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
shape of the output tensor.
dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.
","def add_docstr_all(method, docstr):
add_docstr(getattr(torch._C._TensorBase, method), docstr)

common_args = parse_kwargs(
    """"""
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.preserve_format``.
""""""
)
new_common_args = parse_kwargs(
    """"""
size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
shape of the output tensor.
dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.
"
340,"This function only works with CPU tensors and should not be used in code
sections that require high performance.
"""""")
add_docstr_all('asin', r""""""
asin() -> Tensor
See :func:`torch.asin`
"""""")
add_docstr_all('asin_',
               r""""""
asin_() -> Tensor
In-place version of :meth:`~Tensor.asin`
"""""")
add_docstr_all('arcsin', r""""""
arcsin() -> Tensor
See :func:`torch.arcsin`
"""""")
add_docstr_all('arcsin_', r""""""
arcsin_() -> Tensor
In-place version of :meth:`~Tensor.arcsin`
"""""")
add_docstr_all('asinh', r""""""
asinh() -> Tensor
See :func:`torch.asinh`
"""""")
add_docstr_all('asinh_',
               r""""""
asinh_() -> Tensor
In-place version of :meth:`~Tensor.asinh`
"""""")
add_docstr_all('arcsinh', r""""""
arcsinh() -> Tensor
See :func:`torch.arcsinh`
"""""")
add_docstr_all('arcsinh_', r""""""
arcsinh_() -> Tensor
In-place version of :meth:`~Tensor.arcsinh`
"""""")
add_docstr_all('as_strided', r""""""
as_strided(size, stride, storage_offset=0) -> Tensor
See :func:`torch.as_strided`
"""""")
add_docstr_all('atan', r""""""
atan() -> Tensor
See :func:`torch.atan`
"""""")
add_docstr_all('atan_', r""""""
atan_() -> Tensor
In-place version of :meth:`~Tensor.atan`
"""""")
add_docstr_all('arctan', r""""""
arctan() -> Tensor
See :func:`torch.arctan`
"""""")
add_docstr_all('arctan_', r""""""
arctan_() -> Tensor
In-place version of :meth:`~Tensor.arctan`
"""""")
add_docstr_all('atan2', r""""""
atan2(other) -> Tensor
See :func:`torch.atan2`
"""""")
add_docstr_all('atan2_', r""""""
atan2_(other) -> Tensor
In-place version of :meth:`~Tensor.atan2`
"""""")
add_docstr_all('atanh', r""""""
atanh() -> Tensor
See :func:`torch.atanh`
"""""")
add_docstr_all('atanh_', r""""""
atanh_(other) -> Tensor
In-place version of :meth:`~Tensor.atanh`
"""""")
add_docstr_all('arctanh', r""""""
arctanh() -> Tensor
See :func:`torch.arctanh`
"""""")
add_docstr_all('arctanh_', r""""""
arctanh_(other) -> Tensor
In-place version of :meth:`~Tensor.arctanh`
"""""")
add_docstr_all('baddbmm',
               r""""""
baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.baddbmm`
"""""")
add_docstr_all('baddbmm_',
               r""""""
baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.baddbmm`
"""""")
add_docstr_all('bernoulli',
               r""""""
bernoulli(*, generator=None) -> Tensor
Returns a result tensor where each :math:`\texttt{result[i]}` is independently
","This function only works with CPU tensors and should not be used in code
sections that require high performance.
"""""",
)
add_docstr_all(
    ""asin"",
    r""""""
asin() -> Tensor
See :func:`torch.asin`
"""""",
)
add_docstr_all(
    ""asin_"",
    r""""""
asin_() -> Tensor
In-place version of :meth:`~Tensor.asin`
"""""",
)
add_docstr_all(
    ""arcsin"",
    r""""""
arcsin() -> Tensor
See :func:`torch.arcsin`
"""""",
)
add_docstr_all(
    ""arcsin_"",
    r""""""
arcsin_() -> Tensor
In-place version of :meth:`~Tensor.arcsin`
"""""",
)
add_docstr_all(
    ""asinh"",
    r""""""
asinh() -> Tensor
See :func:`torch.asinh`
"""""",
)
add_docstr_all(
    ""asinh_"",
    r""""""
asinh_() -> Tensor
In-place version of :meth:`~Tensor.asinh`
"""""",
)
add_docstr_all(
    ""arcsinh"",
    r""""""
arcsinh() -> Tensor
See :func:`torch.arcsinh`
"""""",
)
add_docstr_all(
    ""arcsinh_"",
    r""""""
arcsinh_() -> Tensor
In-place version of :meth:`~Tensor.arcsinh`
"""""",
)
add_docstr_all(
    ""as_strided"",
    r""""""
as_strided(size, stride, storage_offset=0) -> Tensor
See :func:`torch.as_strided`
"""""",
)
add_docstr_all(
    ""atan"",
    r""""""
atan() -> Tensor
See :func:`torch.atan`
"""""",
)
add_docstr_all(
    ""atan_"",
    r""""""
atan_() -> Tensor
In-place version of :meth:`~Tensor.atan`
"""""",
)
add_docstr_all(
    ""arctan"",
    r""""""
arctan() -> Tensor
See :func:`torch.arctan`
"""""",
)
add_docstr_all(
    ""arctan_"",
    r""""""
arctan_() -> Tensor
In-place version of :meth:`~Tensor.arctan`
"""""",
)
add_docstr_all(
    ""atan2"",
    r""""""
atan2(other) -> Tensor
See :func:`torch.atan2`
"""""",
)
add_docstr_all(
    ""atan2_"",
    r""""""
atan2_(other) -> Tensor
In-place version of :meth:`~Tensor.atan2`
"""""",
)
add_docstr_all(
    ""atanh"",
    r""""""
atanh() -> Tensor
See :func:`torch.atanh`
"""""",
)
add_docstr_all(
    ""atanh_"",
    r""""""
atanh_(other) -> Tensor
In-place version of :meth:`~Tensor.atanh`
"""""",
)
add_docstr_all(
    ""arctanh"",
    r""""""
arctanh() -> Tensor
See :func:`torch.arctanh`
"""""",
)
add_docstr_all(
    ""arctanh_"",
    r""""""
arctanh_(other) -> Tensor
In-place version of :meth:`~Tensor.arctanh`
"""""",
)
add_docstr_all(
    ""baddbmm"",
    r""""""
baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor
See :func:`torch.baddbmm`
"""""",
)
add_docstr_all(
    ""baddbmm_"",
    r""""""
baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor
In-place version of :meth:`~Tensor.baddbmm`
"""""",
)
add_docstr_all(
    ""bernoulli"",
    r""""""
bernoulli(*, generator=None) -> Tensor
Returns a result tensor where each :math:`\texttt{result[i]}` is independently
"
341,"f(X=k) = p^{k - 1} (1 - p)
"""""")
add_docstr_all('geqrf',
               r""""""
geqrf() -> (Tensor, Tensor)
See :func:`torch.geqrf`
"""""")
add_docstr_all('ger',
               r""""""
ger(vec2) -> Tensor
See :func:`torch.ger`
"""""")
add_docstr_all('inner', r""""""
inner(other) -> Tensor
See :func:`torch.inner`.
"""""")
add_docstr_all('outer', r""""""
outer(vec2) -> Tensor
See :func:`torch.outer`.
"""""")
add_docstr_all('hypot',
               r""""""
hypot(other) -> Tensor
See :func:`torch.hypot`
"""""")
add_docstr_all('hypot_',
               r""""""
hypot_(other) -> Tensor
In-place version of :meth:`~Tensor.hypot`
"""""")
add_docstr_all('i0',
               r""""""
i0() -> Tensor
See :func:`torch.i0`
"""""")
add_docstr_all('i0_',
               r""""""
i0_() -> Tensor
In-place version of :meth:`~Tensor.i0`
"""""")
add_docstr_all('igamma',
               r""""""
igamma(other) -> Tensor
See :func:`torch.igamma`
"""""")
add_docstr_all('igamma_',
               r""""""
igamma_(other) -> Tensor
In-place version of :meth:`~Tensor.igamma`
"""""")
add_docstr_all('igammac',
               r""""""
igammac(other) -> Tensor
See :func:`torch.igammac`
"""""")
add_docstr_all('igammac_',
               r""""""
igammac_(other) -> Tensor
In-place version of :meth:`~Tensor.igammac`
"""""")
add_docstr_all('indices',
               r""""""
indices() -> Tensor
Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.
","f(X=k) = p^{k - 1} (1 - p)
"""""",
)
add_docstr_all(
    ""geqrf"",
    r""""""
geqrf() -> (Tensor, Tensor)
See :func:`torch.geqrf`
"""""",
)
add_docstr_all(
    ""ger"",
    r""""""
ger(vec2) -> Tensor
See :func:`torch.ger`
"""""",
)
add_docstr_all(
    ""inner"",
    r""""""
inner(other) -> Tensor
See :func:`torch.inner`.
"""""",
)
add_docstr_all(
    ""outer"",
    r""""""
outer(vec2) -> Tensor
See :func:`torch.outer`.
"""""",
)
add_docstr_all(
    ""hypot"",
    r""""""
hypot(other) -> Tensor
See :func:`torch.hypot`
"""""",
)
add_docstr_all(
    ""hypot_"",
    r""""""
hypot_(other) -> Tensor
In-place version of :meth:`~Tensor.hypot`
"""""",
)
add_docstr_all(
    ""i0"",
    r""""""
i0() -> Tensor
See :func:`torch.i0`
"""""",
)
add_docstr_all(
    ""i0_"",
    r""""""
i0_() -> Tensor
In-place version of :meth:`~Tensor.i0`
"""""",
)
add_docstr_all(
    ""igamma"",
    r""""""
igamma(other) -> Tensor
See :func:`torch.igamma`
"""""",
)
add_docstr_all(
    ""igamma_"",
    r""""""
igamma_(other) -> Tensor
In-place version of :meth:`~Tensor.igamma`
"""""",
)
add_docstr_all(
    ""igammac"",
    r""""""
igammac(other) -> Tensor
See :func:`torch.igammac`
"""""",
)
add_docstr_all(
    ""igammac_"",
    r""""""
igammac_(other) -> Tensor
In-place version of :meth:`~Tensor.igammac`
"""""",
)
add_docstr_all(
    ""indices"",
    r""""""
indices() -> Tensor
Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.
"
342,"[  1.,   1.,   1.],
[  1.,   1.,   1.],
[  1.,   1.,   1.]])
"""""".format(**reproducibility_notes))

add_docstr_all('index_copy_',
               r""""""
index_copy_(dim, index, tensor) -> Tensor
Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting
","[  1.,   1.,   1.],
[  1.,   1.,   1.],
[  1.,   1.,   1.]])
"""""".format(
        **reproducibility_notes
    ),
)

add_docstr_all(
    ""index_copy_"",
    r""""""
index_copy_(dim, index, tensor) -> Tensor
Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting
"
343,"indices (tuple of LongTensor): tensors used to index into `self`.
values (Tensor): tensor of same dtype as `self`.
accumulate (bool): whether to accumulate into self
"""""")
add_docstr_all('index_put',
               r""""""
index_put(indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
"""""")
add_docstr_all('index_select',
               r""""""
index_select(dim, index) -> Tensor
See :func:`torch.index_select`
"""""")
add_docstr_all('sparse_mask',
               r""""""
sparse_mask(mask) -> Tensor
Returns a new :ref:`sparse tensor <sparse-docs>` with values from a
","indices (tuple of LongTensor): tensors used to index into `self`.
values (Tensor): tensor of same dtype as `self`.
accumulate (bool): whether to accumulate into self
"""""",
)
add_docstr_all(
    ""index_put"",
    r""""""
index_put(indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
"""""",
)
add_docstr_all(
    ""index_select"",
    r""""""
index_select(dim, index) -> Tensor
See :func:`torch.index_select`
"""""",
)
add_docstr_all(
    ""sparse_mask"",
    r""""""
sparse_mask(mask) -> Tensor
Returns a new :ref:`sparse tensor <sparse-docs>` with values from a
"
344,":meth:`select` is equivalent to slicing. For example,
``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and
``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.
"""""")
add_docstr_all('set_',
               r""""""
set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor
Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,
",":meth:`select` is equivalent to slicing. For example,
``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and
``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.
"""""",
)
add_docstr_all(
    ""set_"",
    r""""""
set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor
Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,
"
345,"original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""")
add_docstr_all('sparse_resize_and_clear_',
               r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
","original size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""",
)
add_docstr_all(
    ""sparse_resize_and_clear_"",
    r""""""
sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor
Removes all specified elements from a :ref:`sparse tensor
"
346,":noindex:
See :func:`torch.std`
"""""")
add_docstr_all('storage',
               r""""""
storage() -> torch.Storage
Returns the underlying storage.
"""""")
add_docstr_all('storage_offset',
               r""""""
storage_offset() -> int
Returns :attr:`self` tensor's offset in the underlying storage in terms of
",":noindex:
See :func:`torch.std`
"""""",
)
add_docstr_all(
    ""storage"",
    r""""""
storage() -> torch.Storage
Returns the underlying storage.
"""""",
)
add_docstr_all(
    ""storage_offset"",
    r""""""
storage_offset() -> int
Returns :attr:`self` tensor's offset in the underlying storage in terms of
"
347,">>> x[3:].storage_offset()
3
"""""")
add_docstr_all('storage_type',
               r""""""
storage_type() -> type
Returns the type of the underlying storage.
"""""")
add_docstr_all('stride',
               r""""""
stride(dim) -> tuple or int
Returns the stride of :attr:`self` tensor.
",">>> x[3:].storage_offset()
3
"""""",
)
add_docstr_all(
    ""storage_type"",
    r""""""
storage_type() -> type
Returns the type of the underlying storage.
"""""",
)
add_docstr_all(
    ""stride"",
    r""""""
stride(dim) -> tuple or int
Returns the stride of :attr:`self` tensor.
"
348,"tensor([[ 0,  0,  0],
[ 9,  0, 10],
[ 0,  0,  0]])
"""""")
add_docstr_all('to_sparse',
               r""""""
to_sparse(sparseDims) -> Tensor
Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
:ref:`coordinate format <sparse-coo-docs>`.
","tensor([[ 0,  0,  0],
[ 9,  0, 10],
[ 0,  0,  0]])
"""""",
)
add_docstr_all(
    ""to_sparse"",
    r""""""
to_sparse(sparseDims) -> Tensor
Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
:ref:`coordinate format <sparse-coo-docs>`.
"
349,"tensor([[ 1.,  2.],
[ 3.,  4.],
[ 5.,  6.]])
"""""")
add_docstr_all('uniform_',
               r""""""
uniform_(from=0, to=1) -> Tensor
Fills :attr:`self` tensor with numbers sampled from the continuous uniform
","tensor([[ 1.,  2.],
[ 3.,  4.],
[ 5.,  6.]])
"""""",
)
add_docstr_all(
    ""uniform_"",
    r""""""
uniform_(from=0, to=1) -> Tensor
Fills :attr:`self` tensor with numbers sampled from the continuous uniform
"
350,".. math::
P(x) = \dfrac{1}{\text{to} - \text{from}}
"""""")
add_docstr_all('unsqueeze',
               r""""""
unsqueeze(dim) -> Tensor
See :func:`torch.unsqueeze`
"""""")
add_docstr_all('unsqueeze_',
               r""""""
unsqueeze_(dim) -> Tensor
In-place version of :meth:`~Tensor.unsqueeze`
"""""")
add_docstr_all('var',
               r""""""
var(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.var`
",".. math::
P(x) = \dfrac{1}{\text{to} - \text{from}}
"""""",
)
add_docstr_all(
    ""unsqueeze"",
    r""""""
unsqueeze(dim) -> Tensor
See :func:`torch.unsqueeze`
"""""",
)
add_docstr_all(
    ""unsqueeze_"",
    r""""""
unsqueeze_(dim) -> Tensor
In-place version of :meth:`~Tensor.unsqueeze`
"""""",
)
add_docstr_all(
    ""var"",
    r""""""
var(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.var`
"
351,"tensor([[ 1,  1,  1,  1],
[ 2,  2,  2,  2],
[ 3,  3,  3,  3]])
"""""")
add_docstr_all('expand_as',
               r""""""
expand_as(other) -> Tensor
Expand this tensor to the same size as :attr:`other`.
","tensor([[ 1,  1,  1,  1],
[ 2,  2,  2,  2],
[ 3,  3,  3,  3]])
"""""",
)
add_docstr_all(
    ""expand_as"",
    r""""""
expand_as(other) -> Tensor
Expand this tensor to the same size as :attr:`other`.
"
352,">>> x.real
tensor([ 0.3100, -0.5445, -1.6492, -0.0638])
"""""")
add_docstr_all('imag',
               r""""""
Returns a new tensor containing imaginary values of the :attr:`self` tensor.
The returned tensor and :attr:`self` share the same underlying storage.
",">>> x.real
tensor([ 0.3100, -0.5445, -1.6492, -0.0638])
"""""",
)
add_docstr_all(
    ""imag"",
    r""""""
Returns a new tensor containing imaginary values of the :attr:`self` tensor.
The returned tensor and :attr:`self` share the same underlying storage.
"
353,"regx = re.compile(r""\n\s{4}(?!\s)"")
kwargs = [section.strip() for section in regx.split(desc)]
kwargs = [section for section in kwargs if len(section) > 0]
    return {desc.split(' ')[0]: desc for desc in kwargs}
def merge_dicts(*dicts):
return {x: d[x] for d in dicts for x in d}
common_args = parse_kwargs(""""""
input (Tensor): the input tensor.
generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
out (Tensor, optional): the output tensor.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned tensor. Default: ``torch.preserve_format``.
"""""")
reduceops_common_args = merge_dicts(common_args, parse_kwargs(""""""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
If specified, the input tensor is casted to :attr:`dtype` before the operation
is performed. This is useful for preventing data type overflows. Default: None.
keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
""""""))

multi_dim_common = merge_dicts(reduceops_common_args, parse_kwargs(""""""
dim (int or tuple of ints): the dimension or dimensions to reduce.
""""""), {'keepdim_details': """"""
If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
output tensor having 1 (or ``len(dim)``) fewer dimension(s).
""""""})

single_dim_common = merge_dicts(reduceops_common_args, parse_kwargs(""""""
dim (int): the dimension to reduce.
""""""), {'keepdim_details': """"""If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensor having 1 fewer dimension than :attr:`input`.""""""})

factory_common_args = merge_dicts(common_args, parse_kwargs(""""""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
","regx = re.compile(r""\n\s{4}(?!\s)"")
kwargs = [section.strip() for section in regx.split(desc)]
kwargs = [section for section in kwargs if len(section) > 0]
    return {desc.split("" "")[0]: desc for desc in kwargs}
def merge_dicts(*dicts):
return {x: d[x] for d in dicts for x in d}
common_args = parse_kwargs(
    """"""
input (Tensor): the input tensor.
generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
out (Tensor, optional): the output tensor.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned tensor. Default: ``torch.preserve_format``.
""""""
)
reduceops_common_args = merge_dicts(
    common_args,
    parse_kwargs(
        """"""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
If specified, the input tensor is casted to :attr:`dtype` before the operation
is performed. This is useful for preventing data type overflows. Default: None.
keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
""""""
    ),
)

multi_dim_common = merge_dicts(
    reduceops_common_args,
    parse_kwargs(
        """"""
dim (int or tuple of ints): the dimension or dimensions to reduce.
""""""
    ),
    {
        ""keepdim_details"": """"""
If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
output tensor having 1 (or ``len(dim)``) fewer dimension(s).
""""""
    },
)

single_dim_common = merge_dicts(
    reduceops_common_args,
    parse_kwargs(
        """"""
dim (int): the dimension to reduce.
""""""
    ),
    {
        ""keepdim_details"": """"""If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensor having 1 fewer dimension than :attr:`input`.""""""
    },
)

factory_common_args = merge_dicts(
    common_args,
    parse_kwargs(
        """"""
dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
"
354,"returned tensor. Default: ``False``.
pin_memory (bool, optional): If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: ``False``.
"""""")
tf32_notes = {
""tf32_note"": """"""This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.""""""
","returned tensor. Default: ``False``.
pin_memory (bool, optional): If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: ``False``.
""""""
)
tf32_notes = {
""tf32_note"": """"""This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.""""""
"
355,"and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is \
undesirable, you can try to make the operation deterministic (potentially at \
a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. \
See :doc:`/notes/randomness` for more information.""""""
}
add_docstr(torch.abs, r""""""
abs(input, *, out=None) -> Tensor
Computes the absolute value of each element in :attr:`input`.
.. math::
\text{out}_{i} = |\text{input}_{i}|
"""""" + r""""""
Args:
{input}
","and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is \
undesirable, you can try to make the operation deterministic (potentially at \
a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. \
See :doc:`/notes/randomness` for more information."""""",
}
add_docstr(
    torch.abs,
    r""""""
abs(input, *, out=None) -> Tensor
Computes the absolute value of each element in :attr:`input`.
.. math::
\text{out}_{i} = |\text{input}_{i}|
""""""
    + r""""""
Args:
{input}
"
356,"tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
>>> torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
"""""".format(**common_args))

add_docstr(torch.arccos, r""""""
arccos(input, *, out=None) -> Tensor
Alias for :func:`torch.acos`.
"""""")
add_docstr(torch.acosh, r""""""
acosh(input, *, out=None) -> Tensor
Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`.
","tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
>>> torch.acos(a)
tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.arccos,
    r""""""
arccos(input, *, out=None) -> Tensor
Alias for :func:`torch.acos`.
"""""",
)
add_docstr(
    torch.acosh,
    r""""""
acosh(input, *, out=None) -> Tensor
Returns a new tensor with the inverse hyperbolic cosine of the elements of :attr:`input`.
"
357,".. math::
\text{out}_{i} = \tanh^{-1}(\text{input}_{i})
"""""" + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \tanh^{-1}(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
"
358,">>> batch2 = torch.randn(10, 4, 5)
>>> torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.bernoulli,
           r""""""
bernoulli(input, *, generator=None, out=None) -> Tensor
Draws binary random numbers (0 or 1) from a Bernoulli distribution.
",">>> batch2 = torch.randn(10, 4, 5)
>>> torch.baddbmm(M, batch1, batch2).size()
torch.Size([10, 3, 5])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.bernoulli,
    r""""""
bernoulli(input, *, generator=None, out=None) -> Tensor
Draws binary random numbers (0 or 1) from a Bernoulli distribution.
"
359,">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)
"""""".format(**common_args))

add_docstr(torch.bmm,
           r""""""
bmm(input, mat2, *, out=None) -> Tensor
Performs a batch matrix-matrix product of matrices stored in :attr:`input`
",">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))
tensor([ 0,  1, -4], dtype=torch.int8)
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.bmm,
    r""""""
bmm(input, mat2, *, out=None) -> Tensor
Performs a batch matrix-matrix product of matrices stored in :attr:`input`
"
360,".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
"""""" + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
""""""
    + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
"
361,"[[3, 6]]])
"""""".format(**common_args))
add_docstr(torch.tensor_split,
           r""""""
tensor_split(input, indices_or_sections, dim=0) -> List of Tensors
Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`,
","[[3, 6]]])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.tensor_split,
    r""""""
tensor_split(input, indices_or_sections, dim=0) -> List of Tensors
Splits a tensor into multiple sub-tensors, all of which are views of :attr:`input`,
"
362,"[15.]]]),
tensor([], size=(2, 2, 0)))
"""""")
add_docstr(torch.can_cast,
           r""""""
can_cast(from, to) -> bool
Determines if a type conversion is allowed under PyTorch casting rules
","[15.]]]),
tensor([], size=(2, 2, 0)))
"""""",
)
add_docstr(
    torch.can_cast,
    r""""""
can_cast(from, to) -> bool
Determines if a type conversion is allowed under PyTorch casting rules
"
363,"[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""".format(**common_args))

add_docstr(torch.reciprocal,
           r""""""
reciprocal(input, *, out=None) -> Tensor
Returns a new tensor with the reciprocal of the elements of :attr:`input`
","[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.reciprocal,
    r""""""
reciprocal(input, *, out=None) -> Tensor
Returns a new tensor with the reciprocal of the elements of :attr:`input`
"
364,"Unlike NumPy's reciprocal, torch.reciprocal supports integral inputs. Integral
inputs to reciprocal are automatically :ref:`promoted <type-promotion-doc>` to
the default scalar type.
"""""" + r""""""
Args:
{input}
","Unlike NumPy's reciprocal, torch.reciprocal supports integral inputs. Integral
inputs to reciprocal are automatically :ref:`promoted <type-promotion-doc>` to
the default scalar type.
""""""
    + r""""""
Args:
{input}
"
365,"values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,
1.9946,  1.9946]),
indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))
"""""".format(**reduceops_common_args))

add_docstr(torch.cummin,
           r""""""
cummin(input, dim, *, out=None) -> (Tensor, LongTensor)
Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of
elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
","values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,
1.9946,  1.9946]),
indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))
"""""".format(
        **reduceops_common_args
    ),
)

add_docstr(
    torch.cummin,
    r""""""
cummin(input, dim, *, out=None) -> (Tensor, LongTensor)
Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of
elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
"
366,"Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""")
add_docstr(torch.diag,
           r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
 If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
","Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""",
)
add_docstr(
    torch.diag,
    r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
"
367,".. _LAPACK documentation for geqrf:
http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html
"""""")
add_docstr(torch.inner, r""""""
inner(input, other, *, out=None) -> Tensor
Computes the dot product for 1D tensors. For higher dimensions, sums the product
",".. _LAPACK documentation for geqrf:
http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html
"""""",
)
add_docstr(
    torch.inner,
    r""""""
inner(input, other, *, out=None) -> Tensor
Computes the dot product for 1D tensors. For higher dimensions, sums the product
"
368,">>> torch.inner(a, torch.tensor(2))
tensor([[1.6347, 2.1748, 2.3567],
[0.6558, 0.2469, 5.5787]])
"""""")
add_docstr(torch.outer, r""""""
outer(input, vec2, *, out=None) -> Tensor
Outer product of :attr:`input` and :attr:`vec2`.
",">>> torch.inner(a, torch.tensor(2))
tensor([[1.6347, 2.1748, 2.3567],
[0.6558, 0.2469, 5.5787]])
"""""",
)
add_docstr(
    torch.outer,
    r""""""
outer(input, vec2, *, out=None) -> Tensor
Outer product of :attr:`input` and :attr:`vec2`.
"
369,">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
"""""".format(**common_args))

add_docstr(torch.greater, r""""""
greater(input, other, *, out=None) -> Tensor
Alias for :func:`torch.gt`.
"""""")
add_docstr(torch.histc,
           r""""""
histc(input, bins=100, min=0, max=0, *, out=None) -> Tensor
Computes the histogram of a tensor.
",">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, True], [False, False]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.greater,
    r""""""
greater(input, other, *, out=None) -> Tensor
Alias for :func:`torch.gt`.
"""""",
)
add_docstr(
    torch.histc,
    r""""""
histc(input, bins=100, min=0, max=0, *, out=None) -> Tensor
Computes the histogram of a tensor.
"
370,">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])
"""""".format(**common_args))

add_docstr(torch.isnan, r""""""
isnan(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input`
",">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([True,  False,  True,  False,  False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isnan,
    r""""""
isnan(input) -> Tensor
Returns a new tensor with boolean elements representing if each element of :attr:`input`
"
371,">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))
tensor([True, False, True])
"""""".format(**common_args))

add_docstr(torch.is_floating_point, r""""""
is_floating_point(input) -> (bool)
Returns True if the data type of :attr:`input` is a floating point data type i.e.,
",">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))
tensor([True, False, True])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.is_floating_point,
    r""""""
is_floating_point(input) -> (bool)
Returns True if the data type of :attr:`input` is a floating point data type i.e.,
"
372,".. math::
y_{i} = \log_{10} (x_{i})
"""""" + r""""""
Args:
{input}
",".. math::
y_{i} = \log_{10} (x_{i})
""""""
    + r""""""
Args:
{input}
"
373,".. math::
y_{i} = \log_{2} (x_{i})
"""""" + r""""""
Args:
{input}
",".. math::
y_{i} = \log_{2} (x_{i})
""""""
    + r""""""
Args:
{input}
"
374,"tensor([ True,  True, False, False])
>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True, False, False])
"""""".format(**common_args))
add_docstr(torch.logspace, """"""
logspace(start, end, steps, base=10.0, *, \
out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
"""""" + r""""""
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to
","tensor([ True,  True, False, False])
>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))
tensor([ True,  True, False, False])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.logspace,
    """"""
logspace(start, end, steps, base=10.0, *, \
out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
""""""
    + r""""""
Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly
spaced from :math:`{{\text{{base}}}}^{{\text{{start}}}}` to
"
375,">>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
>>> torch.norm(A_ - A)
tensor(2.9802e-08)
"""""".format(**common_args))

add_docstr(torch.less, r""""""
less(input, other, *, out=None) -> Tensor
Alias for :func:`torch.lt`.
"""""")
add_docstr(torch.lu_solve,
           r""""""
lu_solve(b, LU_data, LU_pivots, *, out=None) -> Tensor
Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted
",">>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
>>> torch.norm(A_ - A)
tensor(2.9802e-08)
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.less,
    r""""""
less(input, other, *, out=None) -> Tensor
Alias for :func:`torch.lt`.
"""""",
)
add_docstr(
    torch.lu_solve,
    r""""""
lu_solve(b, LU_data, LU_pivots, *, out=None) -> Tensor
Returns the LU solve of the linear system :math:`Ax = b` using the partially pivoted
"
376,"See :func:`torch.maximum`.
"""""".format(**single_dim_common))
add_docstr(torch.maximum, r""""""
maximum(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
","See :func:`torch.maximum`.
"""""".format(
        **single_dim_common
    ),
)
add_docstr(
    torch.maximum,
    r""""""
maximum(input, other, *, out=None) -> Tensor
Computes the element-wise maximum of :attr:`input` and :attr:`other`.
"
377,">>> b = a + (torch.randn(50, 1) * 5).long()
>>> torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
"""""".format(**single_dim_common))

add_docstr(torch.mul, r""""""
mul(input, other, *, out=None) -> Tensor
Multiplies each element of the input :attr:`input` with the scalar
",">>> b = a + (torch.randn(50, 1) * 5).long()
>>> torch.mode(b, 0)
torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.mul,
    r""""""
mul(input, other, *, out=None) -> Tensor
Multiplies each element of the input :attr:`input` with the scalar
"
378,"[[1, 5],
[3, 7]]])
"""""".format(**common_args))

add_docstr(torch.swapaxes, r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
","[[1, 5],
[3, 7]]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.swapaxes,
    r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
"
379,"[8., 6., 6., 0.],
[0., 4., 5., 3.],
[2., 1., 4., 2.]])
"""""".format(**common_args))

add_docstr(torch.polygamma,
           r""""""
polygamma(n, input, *, out=None) -> Tensor
Alias for :func:`torch.special.polygamma`.
"""""".format(**common_args))

add_docstr(torch.positive,
           r""""""
positive(input) -> Tensor
Returns :attr:`input`.
Throws a runtime error if :attr:`input` is a bool tensor.
"""""" + r""""""
Args:
{input}
","[8., 6., 6., 0.],
[0., 4., 5., 3.],
[2., 1., 4., 2.]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.polygamma,
    r""""""
polygamma(n, input, *, out=None) -> Tensor
Alias for :func:`torch.special.polygamma`.
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.positive,
    r""""""
positive(input) -> Tensor
Returns :attr:`input`.
Throws a runtime error if :attr:`input` is a bool tensor.
""""""
    + r""""""
Args:
{input}
"
380,">>> torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
[ 0.3816,  0.7249,  0.0998]])
"""""".format(**factory_common_args))

add_docstr(torch.rand_like,
           r""""""
rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor with the same size as :attr:`input` that is filled with
",">>> torch.rand(2, 3)
tensor([[ 0.8237,  0.5781,  0.6879],
[ 0.3816,  0.7249,  0.0998]])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.rand_like,
    r""""""
rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor with the same size as :attr:`input` that is filled with
"
381,"{requires_grad}
{memory_format}
"""""".format(**factory_like_common_args))
add_docstr(torch.randint,
           """"""
randint(low=0, high, size, \\*, generator=None, out=None, \
dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
","{requires_grad}
{memory_format}
"""""".format(
        **factory_like_common_args
    ),
)
add_docstr(
    torch.randint,
    """"""
randint(low=0, high, size, \\*, generator=None, out=None, \
dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
"
382,"[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(**common_args))

add_docstr(torch.msort,
           r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
","[3, 2, 1, 0],
[2, 1, 0, 3],
[3, 2, 1, 0]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.msort,
    r""""""
msort(input, *, out=None) -> Tensor
Sorts the elements of the :attr:`input` tensor along its first dimension
"
383,"tensor([[-2.0527, -1.1250, -1.2631, -1.1289],
[-0.1321, -0.1259, -0.5495,  0.3077],
[-0.0881,  0.4370,  0.2275,  1.0284]])
"""""".format(**common_args))

add_docstr(torch.sparse_csr_tensor,
           r""""""
sparse_csr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, requires_grad=False) -> Tensor
Constructs a :ref:`sparse tensor in CSR (Compressed Sparse Row) <sparse-csr-docs>` with specified
","tensor([[-2.0527, -1.1250, -1.2631, -1.1289],
[-0.1321, -0.1259, -0.5495,  0.3077],
[-0.0881,  0.4370,  0.2275,  1.0284]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.sparse_csr_tensor,
    r""""""
sparse_csr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, requires_grad=False) -> Tensor
Constructs a :ref:`sparse tensor in CSR (Compressed Sparse Row) <sparse-csr-docs>` with specified
"
384,"tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(**common_args))

add_docstr(torch.flip,
           r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
","tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.flip,
    r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
"
385,"[[ 2,  3],
[ 0,  1]]])
"""""".format(**common_args))

add_docstr(torch.fliplr,
           r""""""
fliplr(input) -> Tensor
Flip tensor in the left/right direction, returning a new tensor.
","[[ 2,  3],
[ 0,  1]]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.fliplr,
    r""""""
fliplr(input) -> Tensor
Flip tensor in the left/right direction, returning a new tensor.
"
386,">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(**common_args))

add_docstr(torch.roll,
           r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
",">>> torch.flipud(x)
tensor([[2, 3],
[0, 1]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.roll,
    r""""""
roll(input, shifts, dims=None) -> Tensor
Roll the tensor along the given dimension(s). Elements that are shifted beyond the
"
387,">>> torch.take_along_dim(t, sorted_idx, dim=1)
tensor([[10, 20, 30],
[40, 50, 60]])
"""""".format(**common_args))

add_docstr(torch.tan,
           r""""""
tan(input, *, out=None) -> Tensor
Returns a new tensor with the tangent of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \tan(\text{input}_{i})
"""""" + r""""""
Args:
{input}
",">>> torch.take_along_dim(t, sorted_idx, dim=1)
tensor([[10, 20, 30],
[40, 50, 60]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.tan,
    r""""""
tan(input, *, out=None) -> Tensor
Returns a new tensor with the tangent of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \tan(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
"
388,"[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(**common_args))
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(torch.triu_indices,
           r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
","[-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],
[ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],
[ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])
"""""".format(
        **common_args
    ),
)
# docstr is split in two parts to avoid format mis-capturing :math: braces '{}'
# as common args.
add_docstr(
    torch.triu_indices,
    r""""""
triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor
Returns the indices of the upper triangular part of a :attr:`row` by
"
389,">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var(a, unbiased=False)
tensor(0.1754)
"""""".format(**multi_dim_common))

add_docstr(torch.var_mean,
           r""""""
var_mean(input, dim, unbiased, keepdim=False, *, out=None) -> (Tensor, Tensor)
If :attr:`unbiased` is ``True``, Bessel's correction will be used to calculate
",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.var(a, unbiased=False)
tensor(0.1754)
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.var_mean,
    r""""""
var_mean(input, dim, unbiased, keepdim=False, *, out=None) -> (Tensor, Tensor)
If :attr:`unbiased` is ``True``, Bessel's correction will be used to calculate
"
390,".. note::
See also :func:`torch.nonzero`.
"""""")
add_docstr(torch.logdet,
           r""""""
logdet(input) -> Tensor
Calculates log determinant of a square matrix or batches of square matrices.
",".. note::
See also :func:`torch.nonzero`.
"""""",
)
add_docstr(
    torch.logdet,
    r""""""
logdet(input) -> Tensor
Calculates log determinant of a square matrix or batches of square matrices.
"
391,">>> g_cpu = torch.Generator()
>>> g_cpu.manual_seed(2147483647)
"""""")
add_docstr(torch.Generator.initial_seed,
           r""""""
Generator.initial_seed() -> int
Returns the initial seed for generating random numbers.
",">>> g_cpu = torch.Generator()
>>> g_cpu.manual_seed(2147483647)
"""""",
)
add_docstr(
    torch.Generator.initial_seed,
    r""""""
Generator.initial_seed() -> int
Returns the initial seed for generating random numbers.
"
392,"**kwargs: For compatibility, may contain the key ``async`` in place of
the ``non_blocking`` argument.
""""""
    non_blocking = _get_async_or_non_blocking('cuda', non_blocking, kwargs)
if self.is_cuda:
if device is None:
device = torch.cuda.current_device()
","**kwargs: For compatibility, may contain the key ``async`` in place of
the ``non_blocking`` argument.
""""""
    non_blocking = _get_async_or_non_blocking(""cuda"", non_blocking, kwargs)
if self.is_cuda:
if device is None:
device = torch.cuda.current_device()
"
393,"if isinstance(batched_outputs, Tensor):
out_dim = out_dims_as_tuple[0]
return torch._remove_batch_dim(batched_outputs, vmap_level, batch_size, out_dim)  # type: ignore[return-value]
    return tuple(torch._remove_batch_dim(out, vmap_level, batch_size, out_dim)
                 for out, out_dim in zip(batched_outputs, out_dims_as_tuple))
# Checks that `fn` returned one or more Tensors and nothing else.
# NB: A python function that return multiple arguments returns a single tuple,
","if isinstance(batched_outputs, Tensor):
out_dim = out_dims_as_tuple[0]
return torch._remove_batch_dim(batched_outputs, vmap_level, batch_size, out_dim)  # type: ignore[return-value]
    return tuple(
        torch._remove_batch_dim(out, vmap_level, batch_size, out_dim)
        for out, out_dim in zip(batched_outputs, out_dims_as_tuple)
    )

# Checks that `fn` returned one or more Tensors and nothing else.
# NB: A python function that return multiple arguments returns a single tuple,
"
394,"# examples, don't have a __name__.
return repr(func)
# vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,
# sends those into func, and then unwraps the output BatchedTensors. Operations
# on BatchedTensors perform the batched operations that the user is asking for.
","# examples, don't have a __name__.
return repr(func)

# vmap(func)(inputs) wraps all Tensor inputs to be batched in BatchedTensors,
# sends those into func, and then unwraps the output BatchedTensors. Operations
# on BatchedTensors perform the batched operations that the user is asking for.
"
395,"import torch
import functools
import warnings
class autocast(object):
r""""""
Instances of :class:`autocast` serve as context managers or decorators that
","import functools
import warnings
import torch


class autocast(object):
r""""""
Instances of :class:`autocast` serve as context managers or decorators that
"
396,"enabled(bool, optional, default=True)"":  Whether autocasting should be enabled in the region.
fast_dtype(torch_dtype, optional):  Whether to use torch.float16 or torch.bfloat16
""""""
def __init__(self, device_type, enabled=True, **kwargs):
self.device = device_type
        if self.device == 'cuda':
self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == 'cpu':
self.fast_dtype = torch.get_autocast_cpu_dtype()
else:
            raise RuntimeError('User specified autocast device_type must be \'cuda\' or \'cpu\'')
        if not torch.cuda.is_available() and self.device == 'cuda':
            warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
enabled = False
for key, value in kwargs.items():
            if key == 'fast_dtype':
self.fast_dtype = value
            if not (key == 'fast_dtype'):
                raise RuntimeError('Unrecognized optional argument supplied to autocast context manager: ' + str(key))
        if self.device == 'cpu':
supported_dtype = [torch.bfloat16]
if self.fast_dtype not in supported_dtype:
                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\n'
                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'
warnings.warn(error_message)
enabled = False
        if self.device == 'cuda':
            if self.fast_dtype == torch.bfloat16 and torch.cuda.get_device_properties(torch.cuda.current_device()).major < 8:
                raise RuntimeError('Current CUDA Device does not support bfloat16. Switching fast_dtype to float16.')
self._enabled = enabled
def __enter__(self):
        if self.device == 'cpu':
self.prev = torch.is_autocast_cpu_enabled()
self.prev_fastdtype = torch.get_autocast_cpu_dtype()
torch.set_autocast_cpu_enabled(self._enabled)
","enabled(bool, optional, default=True)"":  Whether autocasting should be enabled in the region.
fast_dtype(torch_dtype, optional):  Whether to use torch.float16 or torch.bfloat16
""""""

def __init__(self, device_type, enabled=True, **kwargs):
self.device = device_type
        if self.device == ""cuda"":
self.fast_dtype = torch.get_autocast_gpu_dtype()
        elif self.device == ""cpu"":
self.fast_dtype = torch.get_autocast_cpu_dtype()
else:
            raise RuntimeError(
                ""User specified autocast device_type must be 'cuda' or 'cpu'""
            )
        if not torch.cuda.is_available() and self.device == ""cuda"":
            warnings.warn(
                ""User provided device_type of 'cuda', but CUDA is not available. Disabling""
            )
enabled = False
for key, value in kwargs.items():
            if key == ""fast_dtype"":
self.fast_dtype = value
            if not (key == ""fast_dtype""):
                raise RuntimeError(
                    ""Unrecognized optional argument supplied to autocast context manager: ""
                    + str(key)
                )
        if self.device == ""cpu"":
supported_dtype = [torch.bfloat16]
if self.fast_dtype not in supported_dtype:
                error_message = ""In CPU autocast, but the target dtype is not supported. Disabling autocast.\n""
                error_message += (
                    ""CPU Autocast only supports dtype of torch.bfloat16 currently.""
                )
warnings.warn(error_message)
enabled = False
        if self.device == ""cuda"":
            if (
                self.fast_dtype == torch.bfloat16
                and torch.cuda.get_device_properties(torch.cuda.current_device()).major
                < 8
            ):
                raise RuntimeError(
                    ""Current CUDA Device does not support bfloat16. Switching fast_dtype to float16.""
                )
self._enabled = enabled
def __enter__(self):
        if self.device == ""cpu"":
self.prev = torch.is_autocast_cpu_enabled()
self.prev_fastdtype = torch.get_autocast_cpu_dtype()
torch.set_autocast_cpu_enabled(self._enabled)
"
397,"import torch
class autocast(torch.autocast_mode.autocast):
r""""""
See :class:`torch.autocast`.
``torch.cpu.amp.autocast(args...)`` is equivalent to ``torch.autocast(""cpu"", args...)``
""""""
def __init__(self, enabled=True, fast_dtype=torch.float16):
super().__init__(""cpu"", enabled=enabled, fast_dtype=fast_dtype)
","import torch

class autocast(torch.autocast_mode.autocast):
r""""""
See :class:`torch.autocast`.
``torch.cpu.amp.autocast(args...)`` is equivalent to ``torch.autocast(""cpu"", args...)``
""""""

def __init__(self, enabled=True, fast_dtype=torch.float16):
super().__init__(""cpu"", enabled=enabled, fast_dtype=fast_dtype)
"
398,"tensors = tensors[0]
return _VF.atleast_1d(tensors)  # type: ignore[attr-defined]
def atleast_2d(*tensors):
r""""""
Returns a 2-dimensional view of each input tensor with zero dimensions.
","tensors = tensors[0]
return _VF.atleast_1d(tensors)  # type: ignore[attr-defined]

def atleast_2d(*tensors):
r""""""
Returns a 2-dimensional view of each input tensor with zero dimensions.
"
399,"if has_torch_function_unary(input):
return handle_torch_function(
            norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype)
ndim = input.dim()
","if has_torch_function_unary(input):
return handle_torch_function(
            norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype
        )
ndim = input.dim()
"
400,"""""""
if not torch._jit_internal.is_scripting():
if A.requires_grad:
            if not (A.size(-2) == A.size(-1) and (A.dtype.is_floating_point or A.is_complex)):
raise ValueError(
                    'lu.backward works only with batches of squared full-rank matrices'
                    ' of floating or complex types.'
)
return _LU.apply(A, pivot, get_infos)
else:
if A.requires_grad:
raise RuntimeError(
                'Script and require gradients is not supported at the moment.'
                'If you just want to do the forward, use .detach()'
                'on the input before calling the function.'
)
# If get_infos is True, then we don't need to check for errors and vice versa
return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
if TYPE_CHECKING:
_ListOrSeq = Sequence[Tensor]
else:
_ListOrSeq = List[Tensor]
def _check_list_size(out_len: int, get_infos: bool, out: _ListOrSeq) -> None:
get_infos_int = 1 if get_infos else 0
if out_len - get_infos_int != 2:
        raise TypeError(f""expected tuple of {2 + int(get_infos)} elements but got {out_len}"")
if not isinstance(out, (tuple, list)):
        raise TypeError(f""argument 'out' must be tuple of Tensors, not {type(out).__name__}"")
def _lu_with_infos(A, pivot=True, get_infos=False, out=None):
# type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor, Tensor]]) -> Tuple[Tensor, Tensor, Tensor]
if has_torch_function_unary(A):
return handle_torch_function(
            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out)
result = _lu_impl(A, pivot, get_infos, out)
if out is not None:
_check_list_size(len(out), get_infos, out)
","""""""
if not torch._jit_internal.is_scripting():
if A.requires_grad:
            if not (
                A.size(-2) == A.size(-1) and (A.dtype.is_floating_point or A.is_complex)
            ):
raise ValueError(
                    ""lu.backward works only with batches of squared full-rank matrices""
                    "" of floating or complex types.""
)
return _LU.apply(A, pivot, get_infos)
else:
if A.requires_grad:
raise RuntimeError(
                ""Script and require gradients is not supported at the moment.""
                ""If you just want to do the forward, use .detach()""
                ""on the input before calling the function.""
)
# If get_infos is True, then we don't need to check for errors and vice versa
return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))

if TYPE_CHECKING:
_ListOrSeq = Sequence[Tensor]
else:
_ListOrSeq = List[Tensor]

def _check_list_size(out_len: int, get_infos: bool, out: _ListOrSeq) -> None:
get_infos_int = 1 if get_infos else 0
if out_len - get_infos_int != 2:
        raise TypeError(
            f""expected tuple of {2 + int(get_infos)} elements but got {out_len}""
        )
if not isinstance(out, (tuple, list)):
        raise TypeError(
            f""argument 'out' must be tuple of Tensors, not {type(out).__name__}""
        )

def _lu_with_infos(A, pivot=True, get_infos=False, out=None):
# type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor, Tensor]]) -> Tuple[Tensor, Tensor, Tensor]
if has_torch_function_unary(A):
return handle_torch_function(
            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out
        )
result = _lu_impl(A, pivot, get_infos, out)
if out is not None:
_check_list_size(len(out), get_infos, out)
"
401,"Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
"""""" + fr""""""
.. note:: {common_notes[""sync_note""]}
"""""" + r""""""
.. seealso::
","Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
""""""
    + fr""""""
.. note:: {common_notes[""sync_note""]}
""""""
    + r""""""
.. seealso::
"
402,"""wrap_torch_function"",
]
@functools.lru_cache(None)
def get_ignored_functions() -> Set[Callable]:
""""""
","""wrap_torch_function"",
]

@functools.lru_cache(None)
def get_ignored_functions() -> Set[Callable]:
""""""
"
403,"torch.linalg.eigh: lambda input, UPLO=""L"", out=None: -1,
torch.linalg.eigvalsh: lambda input, UPLO=""L"", out=None: -1,
torch.einsum: lambda equation, *operands: -1,
        torch.embedding: (lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,
                          sparse=False: -1),
        torch.embedding_bag: (lambda input, weight, offsets, max_norm=None, norm_type=2, scale_grad_by_freq=False,
                              mode='mean', sparse=False, per_sample_weights=None, padding_idx=None: -1),
torch.empty_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
torch.eq: lambda input, other, out=None: -1,
torch.equal: lambda input, other: -1,
","torch.linalg.eigh: lambda input, UPLO=""L"", out=None: -1,
torch.linalg.eigvalsh: lambda input, UPLO=""L"", out=None: -1,
torch.einsum: lambda equation, *operands: -1,
        torch.embedding: (
            lambda input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False: -1
        ),
        torch.embedding_bag: (
            lambda input, weight, offsets, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode=""mean"", sparse=False, per_sample_weights=None, padding_idx=None: -1
        ),
torch.empty_like: lambda input, dtype=None, layout=None, device=None, requires_grad=False: -1,
torch.eq: lambda input, other, out=None: -1,
torch.equal: lambda input, other: -1,
"
404,"torch.stack: lambda tensors, dim=0, out=None: -1,
torch.std: lambda input, dim=None: -1,
torch.std_mean: lambda input, dim=None: -1,
        torch.stft: (lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True,
                     pad_mode='reflect', normalized=False, onesided=True, return_complex=None: -1),
torch.sub: lambda input, other, out=None: -1,
torch.subtract: lambda input, other, out=None: -1,
torch.sum: lambda input, dim=None: -1,
","torch.stack: lambda tensors, dim=0, out=None: -1,
torch.std: lambda input, dim=None: -1,
torch.std_mean: lambda input, dim=None: -1,
        torch.stft: (
            lambda input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode=""reflect"", normalized=False, onesided=True, return_complex=None: -1
        ),
torch.sub: lambda input, other, out=None: -1,
torch.subtract: lambda input, other, out=None: -1,
torch.sum: lambda input, dim=None: -1,
"
405,"if result is not NotImplemented:
return result
    func_name = '{}.{}'.format(public_api.__module__, public_api.__name__)
    raise TypeError(""no implementation found for '{}' on types that implement ""
                    '__torch_function__: {}'
                    .format(func_name, [type(arg) for arg in overloaded_args]))
has_torch_function = _add_docstr(
_has_torch_function,
","if result is not NotImplemented:
return result
    func_name = ""{}.{}"".format(public_api.__module__, public_api.__name__)
    raise TypeError(
        ""no implementation found for '{}' on types that implement ""
        ""__torch_function__: {}"".format(
            func_name, [type(arg) for arg in overloaded_args]
        )
    )

has_torch_function = _add_docstr(
_has_torch_function,
"
406,"def start(self):
self._enter_actions()
if self.record_steps:
            self.step_rec_fn = prof.record_function(""ProfilerStep#"" + str(self.step_num))
self.step_rec_fn.__enter__()
def stop(self):
","def start(self):
self._enter_actions()
if self.record_steps:
            self.step_rec_fn = prof.record_function(
                ""ProfilerStep#"" + str(self.step_num)
            )
self.step_rec_fn.__enter__()
def stop(self):
"
407,"location = tagger(storage)
if location:
return location
    raise RuntimeError(""don't know how to determine data location of ""
                       + torch.typename(storage))
def default_restore_location(storage, location):
","location = tagger(storage)
if location:
return location
    raise RuntimeError(
        ""don't know how to determine data location of "" + torch.typename(storage)
    )
def default_restore_location(storage, location):
"
408,"source_file = source = None
try:
source_lines, _, source_file = get_source_lines_and_file(obj)
                source = ''.join(source_lines)
except Exception:  # saving the source is optional, so we can ignore any errors
                warnings.warn(""Couldn't retrieve source code for container of ""
                              ""type "" + obj.__name__ + "". It won't be checked ""
                              ""for correctness upon loading."")
            return ('module', obj, source_file, source)
elif torch.is_storage(obj):
view_metadata: Optional[Tuple[str, int, int]]
","source_file = source = None
try:
source_lines, _, source_file = get_source_lines_and_file(obj)
                source = """".join(source_lines)
except Exception:  # saving the source is optional, so we can ignore any errors
                warnings.warn(
                    ""Couldn't retrieve source code for container of ""
                    ""type "" + obj.__name__ + "". It won't be checked ""
                    ""for correctness upon loading.""
                )
            return (""module"", obj, source_file, source)
elif torch.is_storage(obj):
view_metadata: Optional[Tuple[str, int, int]]
"
409,"typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        assert typename == 'storage', \
            f""Unknown typename for persistent_load, expected 'storage' but got '{typename}'""
data_type, key, location, size = data
if key not in loaded_storages:
load_tensor(data_type, size, key, _maybe_decode_ascii(location))
","typename = _maybe_decode_ascii(saved_id[0])
data = saved_id[1:]
        assert (
            typename == ""storage""
        ), f""Unknown typename for persistent_load, expected 'storage' but got '{typename}'""
data_type, key, location, size = data
if key not in loaded_storages:
load_tensor(data_type, size, key, _maybe_decode_ascii(location))
"
410,"tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""")
psi = _add_docstr(_special.special_psi,
                  r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""")
digamma = _add_docstr(_special.special_digamma,
                      r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
"""""" + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
","tensor([-0.5000,  0.0000,  0.5000])
>>> torch.special.entr(a)
tensor([  -inf, 0.0000, 0.3466])
"""""",
)
psi = _add_docstr(
    _special.special_psi,
    r""""""
psi(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""",
)
digamma = _add_docstr(
    _special.special_digamma,
    r""""""
digamma(input, *, out=None) -> Tensor
Computes the logarithmic derivative of the gamma function on `input`.
.. math::
\digamma(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}
""""""
    + r""""""
Args:
input (Tensor): the tensor to compute the digamma function on
"
411,">>> torch.special.gammaln(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(**common_args))
polygamma = _add_docstr(_special.special_polygamma,
                        r""""""
polygamma(n, input, *, out=None) -> Tensor
Computes the :math:`n^{th}` derivative of the digamma function on :attr:`input`.
",">>> torch.special.gammaln(a)
tensor([ 0.5724,  0.0000, -0.1208])
"""""".format(
        **common_args
    ),
)
polygamma = _add_docstr(
    _special.special_polygamma,
    r""""""
polygamma(n, input, *, out=None) -> Tensor
Computes the :math:`n^{th}` derivative of the digamma function on :attr:`input`.
"
412,"tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742])
"""""".format(**common_args))

erf = _add_docstr(_special.special_erf,
                  r""""""
erf(input, *, out=None) -> Tensor
Computes the error function of :attr:`input`. The error function is defined as follows:
.. math::
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
"""""" + r""""""
Args:
{input}
","tensor([ 6.4939, 97.4091])
>>> torch.special.polygamma(4, a)
tensor([ -24.8863, -771.4742])
"""""".format(
        **common_args
    ),
)

erf = _add_docstr(
    _special.special_erf,
    r""""""
erf(input, *, out=None) -> Tensor
Computes the error function of :attr:`input`. The error function is defined as follows:
.. math::
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
""""""
    + r""""""
Args:
{input}
"
413,">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(**common_args))
i0e = _add_docstr(_special.special_i0e,
                  r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
",">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(
        **common_args
    ),
)
i0e = _add_docstr(
    _special.special_i0e,
    r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
"
414,"def _new_shared(cls, size):
""""""Creates a new storage in shared memory with the same data type""""""
from torch.multiprocessing import get_sharing_strategy
if cls.is_cuda:
return cls(size)
        elif get_sharing_strategy() == 'file_system':
return cls._new_using_filename(size)
else:
return cls._new_using_fd(size)
","def _new_shared(cls, size):
""""""Creates a new storage in shared memory with the same data type""""""
from torch.multiprocessing import get_sharing_strategy

if cls.is_cuda:
return cls(size)
        elif get_sharing_strategy() == ""file_system"":
return cls._new_using_filename(size)
else:
return cls._new_using_fd(size)
"
415,"# Storage protocol implemented by ${Type}StorageBase classes
class Storage(object):
_cdata: int
    def __deepcopy__(self, memo) -> 'Storage':
...
    def _new_shared(self, int) -> 'Storage':
...
def _write_file(self, f: Any, is_real_file: _bool, save_size: _bool) -> None:
","# Storage protocol implemented by ${Type}StorageBase classes

class Storage(object):
_cdata: int
    def __deepcopy__(self, memo) -> ""Storage"":
...
    def _new_shared(self, int) -> ""Storage"":
...
def _write_file(self, f: Any, is_real_file: _bool, save_size: _bool) -> None:
"
416,"return proxy
class _Classes(types.ModuleType):
def __init__(self):
super(_Classes, self).__init__('torch.classes')
","return proxy
class _Classes(types.ModuleType):
    __file__ = '_classes.py'

def __init__(self):
super(_Classes, self).__init__('torch.classes')
"
417,"""""""
# Metadata about each shard of the Tensor
    shards_metadata: List[ShardMetadata]
# Size of each dim of the overall Tensor.
    size: torch.Size
# Regular tensor fields
    dtype: torch.dtype
    layout: torch.layout
    requires_grad: bool
    memory_format: torch.memory_format
    pin_memory: bool
def __getstate__(self):
# Since torch.memory_format cannot be pickled!
","""""""
# Metadata about each shard of the Tensor
    shards_metadata: List[ShardMetadata] = field(default_factory=list)
# Size of each dim of the overall Tensor.
    size: torch.Size = field(default=torch.Size([]))
# Regular tensor fields
    dtype: torch.dtype = field(default=torch.get_default_dtype())
    layout: torch.layout = field(default=torch.strided)
    requires_grad: bool = False
    memory_format: torch.memory_format = field(default=torch.contiguous_format)
    pin_memory: bool = False
def __getstate__(self):
# Since torch.memory_format cannot be pickled!
"
418,"qscheme=torch.per_tensor_affine,
reduce_range=True)
def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
","qscheme=torch.per_tensor_affine,
reduce_range=True)
default_fused_act_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                       quant_min=0,
                                                                       quant_max=255,
                                                                       dtype=torch.quint8,)


default_fused_wt_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                                      quant_min=-128,
                                                                      quant_max=127,
                                                                      dtype=torch.qint8,
                                                                      qscheme=torch.per_tensor_symmetric)

default_fused_per_channel_wt_fake_quant = FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,
                                                                                  quant_min=-128,
                                                                                  quant_max=127,
                                                                                  dtype=torch.qint8,
                                                                                  qscheme=torch.per_channel_symmetric)

def _is_fake_quant_script_module(mod):
''' Returns true if given mod is an instance of FakeQuantize script module.
'''
"
419,"default_activation_only_qconfig = QConfig(activation=default_fake_quant,
weight=torch.nn.Identity)
def get_default_qconfig(backend='fbgemm'):
if backend == 'fbgemm':
qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
","default_activation_only_qconfig = QConfig(activation=default_fake_quant,
weight=torch.nn.Identity)
# QAT config that uses a fused observer + fake quant modules for optimized training performance.
# to modify the activation/weight observers, the default entries in fake_quantize.py can be modified.
default_qat_qconfig_v2 = QConfig(activation=default_fused_act_fake_quant, weight=default_fused_wt_fake_quant)

def get_default_qconfig(backend='fbgemm'):
if backend == 'fbgemm':
qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=True),
"
420,"default_generator = torch.cuda.default_generators[idx]
default_generator.manual_seed(seed)
    _lazy_call(cb)
def manual_seed_all(seed: int) -> None:
","default_generator = torch.cuda.default_generators[idx]
default_generator.manual_seed(seed)
    _lazy_call(cb, seed=True)
def manual_seed_all(seed: int) -> None:
"
421,"""call_method"", ""to"", (op_out, torch.float16), {})
else:
assert is_reference
            if dtypes in [(torch.quint8, torch.qint8, None)]:
                load_arg(quantized=[0])(node.args)
args = load_arg(quantized=torch.float)(node.args)
kwargs = load_arg(quantized=torch.float)(node.kwargs)
op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
                activation_post_process = \
                    self._maybe_get_last_node_only_observer(modules)
                assert activation_post_process is not None
return quantize_node(
op_out, activation_post_process,
node, modules, quantized_graph, node_name_to_scope, is_input=False)
            else:
                assert dtypes in [(torch.float16, torch.float16, None)]
                op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
                return quantized_graph.create_node(
                    ""call_method"", ""to"", (op_out, torch.float16), {})
# TODO: elu is using scale/zero_point instead of output_scale, output_zero_point
","""call_method"", ""to"", (op_out, torch.float16), {})
else:
assert is_reference
            # We can produce reference for a dtypes including
            # (torch.quint8, torch.qint8, torch.qint32, torch.float16)
            activation_post_process = \
                self._maybe_get_last_node_only_observer(modules)
            if activation_post_process is None:
                op_out = quantized_graph.node_copy(
                    node,
                    load_arg(quantized=torch.float))
                return op_out
            else:
                act_dtype = activation_dtype(qconfig)
                load_arg(quantized={0: act_dtype})(node.args)
args = load_arg(quantized=torch.float)(node.args)
kwargs = load_arg(quantized=torch.float)(node.kwargs)
op_out = quantized_graph.node_copy(node, load_arg(quantized=torch.float))
return quantize_node(
op_out, activation_post_process,
node, modules, quantized_graph, node_name_to_scope, is_input=False)
# TODO: elu is using scale/zero_point instead of output_scale, output_zero_point
"
422,"state['step'] += 1
with torch.no_grad():
            F.rmsprop(params,
grads,
square_avgs,
grad_avgs,
","state['step'] += 1
with torch.no_grad():
            F.rmsprop(params_with_grad,
grads,
square_avgs,
grad_avgs,
"
423,"def new_fn(inp):
new_inputs = list(tupled_inputs)
new_inputs[input_idx] = inp
            return func(*new_inputs)[output_idx]
slow_analytical = _get_analytical_jacobian_forward_ad(new_fn, (tupled_inputs[input_idx],), (outputs[output_idx],))[0][0]
else:
slow_analytical = _get_analytical_jacobian(tupled_inputs, outputs, input_idx, output_idx)
","def new_fn(inp):
new_inputs = list(tupled_inputs)
new_inputs[input_idx] = inp
            return _as_tuple(func(*new_inputs))[output_idx]
slow_analytical = _get_analytical_jacobian_forward_ad(new_fn, (tupled_inputs[input_idx],), (outputs[output_idx],))[0][0]
else:
slow_analytical = _get_analytical_jacobian(tupled_inputs, outputs, input_idx, output_idx)
"
424,"import nvd3
import os
import sys
import tornado.httpserver
# pyre-fixme[21]: Could not find a module corresponding to import `tornado.wsgi`
import tornado.wsgi
","import nvd3
import os
import sys
# pyre-fixme[21]: Could not find module `tornado.httpserver`.
import tornado.httpserver
# pyre-fixme[21]: Could not find a module corresponding to import `tornado.wsgi`
import tornado.wsgi
"
425,"Examples::
    >>> a = torch.randn(2, 2, dtype=torch.complex128)
    >>> a = a @ a.t().conj()  + torch.eye(2) # creates a Hermitian positive-definite matrix
    >>> l = torch.linalg.cholesky(a)
    >>> a
tensor([[2.5266+0.0000j, 1.9586-2.0626j],
[1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)
    >>> l
tensor([[1.5895+0.0000j, 0.0000+0.0000j],
[1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)
    >>> l @ l.t().conj()
    tensor([[2.5266+0.0000j, 1.9586-2.0626j],
            [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)

    >>> a = torch.randn(3, 2, 2, dtype=torch.float64)
    >>> a = a @ a.transpose(-2, -1) + torch.eye(2).squeeze(0)  # symmetric positive definite  matrices
    >>> l = torch.linalg.cholesky(a)
    >>> a
    tensor([[[ 1.1629,  2.0237],
            [ 2.0237,  6.6593]],

            [[ 0.4187,  0.1830],
            [ 0.1830,  0.1018]],

            [[ 1.9348, -2.5744],
            [-2.5744,  4.6386]]], dtype=torch.float64)
    >>> l
    tensor([[[ 1.0784,  0.0000],
            [ 1.8766,  1.7713]],

            [[ 0.6471,  0.0000],
            [ 0.2829,  0.1477]],

            [[ 1.3910,  0.0000],
            [-1.8509,  1.1014]]], dtype=torch.float64)
    >>> torch.allclose(l @ l.transpose(-2, -1), a)
    True
"""""")
cholesky_ex = _add_docstr(_linalg.linalg_cholesky_ex, r""""""
","Examples::
    >>> A = torch.randn(2, 2, dtype=torch.complex128)
    >>> A = A @ A.T.conj() + torch.eye(2) # creates a Hermitian positive-definite matrix
    >>> A
tensor([[2.5266+0.0000j, 1.9586-2.0626j],
[1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)
    >>> L = torch.linalg.cholesky(A)
    >>> L
tensor([[1.5895+0.0000j, 0.0000+0.0000j],
[1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)
    >>> torch.dist(L @ L.T.conj(), A)
    tensor(4.4692e-16, dtype=torch.float64)

    >>> A = torch.randn(3, 2, 2, dtype=torch.float64)
    >>> A = A @ A.transpose(-2, -1) + torch.eye(2)  # batch of symmetric positive-definite matrices
    >>> L = torch.linalg.cholesky(A)
    >>> torch.dist(L @ L.transpose(-2, -1), A)
    tensor(5.8747e-16, dtype=torch.float64)
"""""")
cholesky_ex = _add_docstr(_linalg.linalg_cholesky_ex, r""""""
"
426,"Examples::
    >>> a = torch.randn(2, 2, dtype=torch.complex128)
    >>> a
    tensor([[ 0.9828+0.3889j, -0.4617+0.3010j],
            [ 0.1662-0.7435j, -0.6139+0.0562j]], dtype=torch.complex128)
    >>> w = torch.linalg.eigvals(a)
    >>> w
tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)
"""""")
eigh = _add_docstr(_linalg.linalg_eigh, r""""""
","Examples::
    >>> A = torch.randn(2, 2, dtype=torch.complex128)
    >>> L = torch.linalg.eigvals(A)
    >>> L
tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)

    >>> torch.dist(L, torch.linalg.eig(A).eigenvalues)
    tensor(2.4576e-07)
"""""")
eigh = _add_docstr(_linalg.linalg_eigh, r""""""
"
427,"Examples::
    >>> a = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))
    >>> b = torch.randn(2 * 3, 4)
    >>> x = torch.linalg.tensorsolve(a, b)
    >>> x.shape
torch.Size([2, 3, 4])
    >>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b)
True
    >>> a = torch.randn(6, 4, 4, 3, 2)
    >>> b = torch.randn(4, 3, 2)
    >>> x = torch.linalg.tensorsolve(a, b, dims=(0, 2))
    >>> x.shape
torch.Size([6, 4])
    >>> a = a.permute(1, 3, 4, 0, 2)
    >>> a.shape[b.ndim:]
torch.Size([6, 4])
    >>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b, atol=1e-6)
True
"""""")
","Examples::
    >>> A = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))
    >>> B = torch.randn(2 * 3, 4)
    >>> X = torch.linalg.tensorsolve(A, B)
    >>> X.shape
torch.Size([2, 3, 4])
    >>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B)
True
    >>> A = torch.randn(6, 4, 4, 3, 2)
    >>> B = torch.randn(4, 3, 2)
    >>> X = torch.linalg.tensorsolve(A, B, dims=(0, 2))
    >>> X.shape
torch.Size([6, 4])
    >>> A = A.permute(1, 3, 4, 0, 2)
    >>> A.shape[B.ndim:]
torch.Size([6, 4])
    >>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B, atol=1e-6)
True
"""""")
"
428,"""paths"": [""torch/csrc/""],
""include-dir"": [""/usr/lib/llvm-11/include/openmp""] + clang_search_dirs(),
""clang-tidy-exe"": INSTALLATION_PATH,
    ""parallel"": True,
""compile-commands-dir"": ""build"",
    ""config-file"": "".clang-tidy"",
}
","""paths"": [""torch/csrc/""],
""include-dir"": [""/usr/lib/llvm-11/include/openmp""] + clang_search_dirs(),
""clang-tidy-exe"": INSTALLATION_PATH,
""compile-commands-dir"": ""build"",
    ""config-file"": "".clang-tidy-oss"",
    ""disable-progress-bar"": False,
}
"
429,"action=""store_true"",
help=""Add NOLINT to suppress clang-tidy violations"",
)
parser.add_argument(
""extra_args"", nargs=""*"", help=""Extra arguments to forward to clang-tidy""
)
","action=""store_true"",
help=""Add NOLINT to suppress clang-tidy violations"",
)
    parser.add_argument(
        ""--disable-progress-bar"",
        action=""store_true"",
        default=DEFAULTS[""disable-progress-bar""],
        help=""Disable the progress bar"",
    )
parser.add_argument(
""extra_args"", nargs=""*"", help=""Extra arguments to forward to clang-tidy""
)
"
430,"return dict(files)
ninja_template = """"""
rule do_cmd
  command = $cmd
  description = Running clang-tidy

{build_rules}
""""""

build_template = """"""
build {i}: do_cmd
  cmd = {cmd}
""""""


def run_shell_commands_in_parallel(commands: Iterable[List[str]]) -> str:
    """"""runs all the commands in parallel with ninja, commands is a List[List[str]]""""""

    async def run_command(cmd: List[str]) -> str:
        proc = await asyncio.create_subprocess_shell(
            "" "".join(shlex.quote(x) for x in cmd),  # type: ignore[attr-defined]
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate()
        return f"">>>\nstdout:\n{stdout.decode()}\nstderr:\n{stderr.decode()}\n<<<""

    async def gather_with_concurrency(n: int, tasks: List[Any]) -> Any:
        semaphore = asyncio.Semaphore(n)

        async def sem_task(task: Any) -> Any:
            async with semaphore:
                return await task

        return await asyncio.gather(
            *(sem_task(task) for task in tasks), return_exceptions=True
        )

    async def helper() -> Any:
        coros = [run_command(cmd) for cmd in commands]
        return await gather_with_concurrency(multiprocessing.cpu_count(), coros)

    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(helper())
    return ""\n"".join(results)


def run_clang_tidy(
    options: Any, line_filters: List[Dict[str, Any]], files: Iterable[str]
) -> str:
    """"""Executes the actual clang-tidy command in the shell.""""""
    command = [options.clang_tidy_exe, ""-p"", options.compile_commands_dir]
    if not options.config_file and os.path.exists("".clang-tidy""):
        options.config_file = "".clang-tidy""
    if options.config_file:
        import yaml

        with open(options.config_file) as config:
            # Here we convert the YAML config file to a JSON blob.
            command += [
                ""-config"",
                json.dumps(yaml.load(config, Loader=yaml.SafeLoader)),
            ]
    if options.print_include_paths:
        command += [""--extra-arg"", ""-v""]
    if options.include_dir:
        for dir in options.include_dir:
            command += [""--extra-arg"", f""-I{dir}""]

    command += options.extra_args

    if line_filters:
        command += [""-line-filter"", json.dumps(line_filters)]

    if options.parallel:
        commands = [
            list(command) + [map_filename(options.compile_commands_dir, f)]
            for f in files
        ]
        output = run_shell_commands_in_parallel(commands)
    else:
        command += map_filenames(options.compile_commands_dir, files)
        if options.dry_run:
            command = [re.sub(r""^([{[].*[]}])$"", r""'\1'"", arg) for arg in command]
            return "" "".join(command)

        output = run_shell_command(command)

    if not options.keep_going and ""[clang-diagnostic-error]"" in output:
        message = ""Found clang-diagnostic-errors in clang-tidy output: {}""
        raise RuntimeError(message.format(output))

    return output


def extract_warnings(
    output: str, base_dir: str = "".""
) -> Dict[str, Dict[int, Set[str]]]:
    rc: Dict[str, Dict[int, Set[str]]] = {}
    for line in output.split(""\n""):
        p = CLANG_WARNING_PATTERN.match(line)
        if p is None:
            continue
        if os.path.isabs(p.group(1)):
            path = os.path.abspath(p.group(1))
        else:
            path = os.path.abspath(os.path.join(base_dir, p.group(1)))
        line_no = int(p.group(2))
        warnings = set(p.group(3).split("",""))
        if path not in rc:
            rc[path] = {}
        if line_no not in rc[path]:
            rc[path][line_no] = set()
        rc[path][line_no].update(warnings)
    return rc


def apply_nolint(fname: str, warnings: Dict[int, Set[str]]) -> None:
    with open(fname, encoding=""utf-8"") as f:
        lines = f.readlines()

    line_offset = -1  # As in .cpp files lines are numbered starting from 1
    for line_no in sorted(warnings.keys()):
        nolint_diagnostics = "","".join(warnings[line_no])
        line_no += line_offset
        indent = "" "" * (len(lines[line_no]) - len(lines[line_no].lstrip("" "")))
        lines.insert(line_no, f""{indent}// NOLINTNEXTLINE({nolint_diagnostics})\n"")
        line_offset += 1

    with open(fname, mode=""w"") as f:
        f.write("""".join(lines))


def filter_from_diff(
paths: List[str], diffs: List[str]
) -> Tuple[List[str], List[Dict[Any, Any]]]:
","return dict(files)
def filter_from_diff(
paths: List[str], diffs: List[str]
) -> Tuple[List[str], List[Dict[Any, Any]]]:
"
431,")
is_first_joinable = False
    def _extract_dist_info(self):
r""""""
        Extracts the process group and device information from the join hooks.
Preconditions:
            ``self._join_hooks`` is not ``None`` and is non-empty.
Raises:
ValueError
If there are multiple conflicting ``process_group`` attributes
                among the ``_JoinHook`` objects.

        NOTE: The context manager uses the first specified device.
""""""
process_group = None
device = None
",")
is_first_joinable = False
    def _extract_dist_info(self) -> None:
r""""""
        Extracts the process group and device information from the joinables.
        If there are multiple joinables, then the context manager uses the
        first specified device.
Preconditions:
            ``self._joinables`` is not ``None`` and is non-empty.
Raises:
ValueError
If there are multiple conflicting ``process_group`` attributes
                among the ``_Joinable`` objects.
""""""
process_group = None
device = None
"
432,"param(module.mask),
unsafe=True)
            self.activation_handle = module.register_forward_hook(
ActivationReconstruction(module.parametrizations.weight[0])
            )
if module.bias is not None:
module.register_parameter('_bias', nn.Parameter(module.bias.detach()))
module.bias = None
            self.bias_handle = module.register_forward_hook(self.bias_hook)

def convert(self, use_path=False, *args, **kwargs):
for config in self.module_groups:
","param(module.mask),
unsafe=True)
            self.activation_handles.append(module.register_forward_hook(
ActivationReconstruction(module.parametrizations.weight[0])
            ))
if module.bias is not None:
module.register_parameter('_bias', nn.Parameter(module.bias.detach()))
module.bias = None
            self.bias_handles.append(module.register_forward_hook(self.bias_hook))
def convert(self, use_path=False, *args, **kwargs):
for config in self.module_groups:
"
433,"def visit_Call(self, node):
# __import__ calls aren't routed to the visit_Import/From nodes
if hasattr(node.func, ""id"") and node.func.id == ""__import__"":
            if type(node.args[0]) not in [ast.Constant, ast.Str]:
                # We don't want to parse dynamic uses of __import__
return
            name = self._grab_node_str(node.args[0])
            fromlist = []
            level = 0
            if len(node.args) > 3:
                for v in node.args[3].elts:
                    fromlist.append(self._grab_node_str(v))
            elif hasattr(node, ""keywords""):
                for keyword in node.keywords:
                    if keyword.arg == ""fromlist"":
                        for v in keyword.value.elts:
                            fromlist.append(self._grab_node_str(v))
            if len(node.args) > 4:
                level = self._grab_node_int(node.args[4])
            elif hasattr(node, ""keywords""):
                for keyword in node.keywords:
                    if keyword.arg == ""level"":
                        level = self._grab_node_int(keyword.value)
            if fromlist == []:
                # the top-level package (the name up till the first dot) is returned
                # when the fromlist argument is empty in normal import system,
                # we need to include top level package to match this behavior and last
                # level package to capture the intended dependency of user
                self.references[(name, None)] = True
                top_name = name.rsplit(""."", maxsplit=1)[0]
                if top_name != name:
                    top_name = self._absmodule(top_name, level)
                    self.references[(top_name, None)] = True
            else:
                name = self._absmodule(name, level)
                for alias in fromlist:
                    # fromlist args may be submodules, so we have to add the fromlist args
                    # to the list of potential references. If import of an arg fails we
                    # will ignore it, similar to visit_ImportFrom
                    if alias != ""*"":
                        self.references[(name, alias)] = True
                    else:
                        self.references[(name, None)] = True

find_files_source_depends_on = _ExtractModuleReferences.run
","def visit_Call(self, node):
# __import__ calls aren't routed to the visit_Import/From nodes
if hasattr(node.func, ""id"") and node.func.id == ""__import__"":
            try:
                name = self._grab_node_str(node.args[0])
                fromlist = []
                level = 0
                if len(node.args) > 3:
                    for v in node.args[3].elts:
                        fromlist.append(self._grab_node_str(v))
                elif hasattr(node, ""keywords""):
                    for keyword in node.keywords:
                        if keyword.arg == ""fromlist"":
                            for v in keyword.value.elts:
                                fromlist.append(self._grab_node_str(v))
                if len(node.args) > 4:
                    level = self._grab_node_int(node.args[4])
                elif hasattr(node, ""keywords""):
                    for keyword in node.keywords:
                        if keyword.arg == ""level"":
                            level = self._grab_node_int(keyword.value)
                if fromlist == []:
                    # the top-level package (the name up till the first dot) is returned
                    # when the fromlist argument is empty in normal import system,
                    # we need to include top level package to match this behavior and last
                    # level package to capture the intended dependency of user
                    self.references[(name, None)] = True
                    top_name = name.rsplit(""."", maxsplit=1)[0]
                    if top_name != name:
                        top_name = self._absmodule(top_name, level)
                        self.references[(top_name, None)] = True
                else:
                    name = self._absmodule(name, level)
                    for alias in fromlist:
                        # fromlist args may be submodules, so we have to add the fromlist args
                        # to the list of potential references. If import of an arg fails we
                        # will ignore it, similar to visit_ImportFrom
                        if alias != ""*"":
                            self.references[(name, alias)] = True
                        else:
                            self.references[(name, None)] = True
            except Exception as e:
return
find_files_source_depends_on = _ExtractModuleReferences.run
"
434,"FUNCTIONAL_OPS_WITH_BIAS,
)
from ..quantization_mappings import (
get_default_qat_module_mappings,
)
","FUNCTIONAL_OPS_WITH_BIAS,
)
from ..fuser_method_mappings import DEFAULT_OP_LIST_TO_FUSER_METHOD

from ..quantization_mappings import (
get_default_qat_module_mappings,
)
"
435,"# NOTE: Clang-tidy cannot lint headers directly, because headers are not
# compiled -- translation units are, of which there is one per implementation
# (c/cc/cpp) file.
DEFAULT_FILE_PATTERN = re.compile(r"".*\.c(c|pp)?"")
# Search for:
#    diff --git ...
","# NOTE: Clang-tidy cannot lint headers directly, because headers are not
# compiled -- translation units are, of which there is one per implementation
# (c/cc/cpp) file.
DEFAULT_FILE_PATTERN = re.compile(r""^.*\.c(c|pp)?$"")
# Search for:
#    diff --git ...
"
436,"Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]
>>> tensor_list
[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
","Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]
>>> tensor_list
[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank
"
437,"Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> output_tensor = torch.zero(2, dtype=torch.int64)
>>> output_tensor
[tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(1, dtype=torch.int64) + 1 + rank
","Examples:
>>> # All tensors below are of torch.int64 dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> output_tensor = torch.zeros(2, dtype=torch.int64)
>>> output_tensor
[tensor([0, 0])] # Rank 0 and 1
>>> tensor = torch.arange(1, dtype=torch.int64) + 1 + rank
"
438,"assert isinstance(e.get('__line__'), int), e
loc = Location(path, e['__line__'])
funcs = e.get('func')
            with context(f'in {loc}:\n  {funcs}'):
func, m = NativeFunction.from_yaml(e, loc)
rs.append(func)
BackendIndex.grow_index(bs, m)
","assert isinstance(e.get('__line__'), int), e
loc = Location(path, e['__line__'])
funcs = e.get('func')
            with context(lambda: f'in {loc}:\n  {funcs}'):
func, m = NativeFunction.from_yaml(e, loc)
rs.append(func)
BackendIndex.grow_index(bs, m)
"
439,"if not isinstance(mod, _FusedModule) and \
type(mod) not in custom_module_class_mapping:
_convert(mod, mapping, True,  # inplace
                     custom_module_class_mapping)
reassign[name] = swap_module(mod, mapping, custom_module_class_mapping)
for key, value in reassign.items():
","if not isinstance(mod, _FusedModule) and \
type(mod) not in custom_module_class_mapping:
_convert(mod, mapping, True,  # inplace
                     convert_custom_config_dict)
reassign[name] = swap_module(mod, mapping, custom_module_class_mapping)
for key, value in reassign.items():
"
440,"\text{stride[1]} \times w + n)
\end{aligned}
    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
","\text{stride[1]} \times w + n)
\end{aligned}
    If :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides
for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
"
441,"DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS : Dict[Callable, Any] = {
nn.GRUCell: nnqd.GRUCell,
nn.Linear: nnqd.Linear,
    nn.modules.linear._LinearWithBias: nnqd.Linear,
nn.LSTM: nnqd.LSTM,
nn.GRU: nnqd.GRU,
nn.LSTMCell: nnqd.LSTMCell,
","DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS : Dict[Callable, Any] = {
nn.GRUCell: nnqd.GRUCell,
nn.Linear: nnqd.Linear,
    nn.modules.linear.NonDynamicallyQuantizableLinear: nnqd.Linear,
nn.LSTM: nnqd.LSTM,
nn.GRU: nnqd.GRU,
nn.LSTMCell: nnqd.LSTMCell,
"
442,"if right is Any or left == right:
return True
if right == type(None):
return False
","if right is Any or left == right:
return True
    if isinstance(right, _GenericAlias):
        if getattr(right, '__origin__', None) is Generic:
            return True

if right == type(None):
return False
"
443,"name_column_width = max([len(evt.key) for evt in events]) + 4
name_column_width = min(name_column_width, MAX_NAME_COLUMN_WIDTH)
    DEFAULT_COLUMN_WIDTH = 12
shapes_column_width = max([len(str(evt.input_shapes)) for evt in events]) + 4
    shapes_column_width = min(shapes_column_width, 45)
flops_column_width = DEFAULT_COLUMN_WIDTH
src_column_width = None
","name_column_width = max([len(evt.key) for evt in events]) + 4
name_column_width = min(name_column_width, MAX_NAME_COLUMN_WIDTH)
    MAX_SHAPES_COLUMN_WIDTH = 80
shapes_column_width = max([len(str(evt.input_shapes)) for evt in events]) + 4
    shapes_column_width = min(shapes_column_width, MAX_SHAPES_COLUMN_WIDTH)
    DEFAULT_COLUMN_WIDTH = 12
flops_column_width = DEFAULT_COLUMN_WIDTH
src_column_width = None
"
444,"[DOCKER_REQUIREMENT_NDK],
is_master_only=False,
is_pr_only=True),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build"",
""pytorch_android_gradle_build"",
","[DOCKER_REQUIREMENT_NDK],
is_master_only=False,
is_pr_only=True),
    AndroidGradleJob(
        ""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single_lite_interpreter"",
        ""pytorch_android_gradle_custom_build_single"",
        [DOCKER_REQUIREMENT_NDK],
        is_master_only=False,
        is_pr_only=True,
        extra_props=tuple({
            ""lite_interpreter"": miniutils.quote(str(int(True)))
        }.items())),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build"",
""pytorch_android_gradle_build"",
"
445,"True
"""""")
inv = _add_docstr(_linalg.linalg_inv, r""""""
linalg.inv(A, *, out=None) -> Tensor
","True
"""""")
cholesky_ex = _add_docstr(_linalg.linalg_cholesky_ex, r""""""
linalg.cholesky_ex(input, *, check_errors=False, out=None) -> (Tensor, Tensor)

Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.

Supports inputs of float, double, cfloat and cdouble dtypes.
Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions.

Returns a namedtuple ``(L,info)``. ``L`` contains the result of the Cholesky decomposition.
``info`` stores the LAPACK error codes.

If :attr:`input` is not a Hermitian positive-definite matrix, or if it's a batch of matrices
and one or more of them is not a Hermitian positive-definite matrix,
then ``info`` stores a positive integer for the corresponding matrix.
The positive integer indicates the order of the leading minor that is not positive-definite,
and the decomposition could not be completed.
``info`` filled with zeros indicates that the decomposition was successful.
If ``check_errors=True`` and ``info`` contains positive integers, then a RuntimeError is thrown.

.. note:: Given inputs on a CUDA device, this function may synchronize that device with the CPU.

.. warning:: This function is ""experimental"" and it may change in a future PyTorch release.

.. seealso::
        :func:`torch.linalg.cholesky` is a NumPy compatible variant that always checks for errors.

Args:
    input (Tensor): the Hermitian `n \times n` matrix or the batch of such matrices of size
                    `(*, n, n)` where `*` is one or more batch dimensions.
    check_errors (bool, optional): controls whether to check the content of ``infos``. Default: `False`.

Keyword args:
    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.

Examples::

    >>> a = torch.randn(2, 2, dtype=torch.complex128)
    >>> a = a @ a.t().conj()  # creates a Hermitian positive-definite matrix
    >>> l, info = torch.linalg.cholesky_ex(a)
    >>> a
    tensor([[ 2.3792+0.0000j, -0.9023+0.9831j],
            [-0.9023-0.9831j,  0.8757+0.0000j]], dtype=torch.complex128)
    >>> l
    tensor([[ 1.5425+0.0000j,  0.0000+0.0000j],
            [-0.5850-0.6374j,  0.3567+0.0000j]], dtype=torch.complex128)
    >>> info
    tensor(0, dtype=torch.int32)

"""""")

inv = _add_docstr(_linalg.linalg_inv, r""""""
linalg.inv(A, *, out=None) -> Tensor
"
446,"def _rm(self, my_error_file):
if os.path.isfile(my_error_file):
with open(my_error_file, ""r"") as fp:
                original = json.dumps(json.load(fp), indent=2)
                log.warning(
                    f""{my_error_file} already exists""
                    f"" and will be overwritten.""
                    f"" Original contents:\n{original}""
                )
os.remove(my_error_file)
","def _rm(self, my_error_file):
if os.path.isfile(my_error_file):
            # Log the contents of the original file.
with open(my_error_file, ""r"") as fp:
                try:
                    original = json.dumps(json.load(fp), indent=2)
                    log.warning(
                        f""{my_error_file} already exists""
                        f"" and will be overwritten.""
                        f"" Original contents:\n{original}""
                    )
                except json.decoder.JSONDecodeError as err:
                    log.warning(
                        f""{my_error_file} already exists""
                        f"" and will be overwritten.""
                        f"" Unable to load original contents:\n""
                    )
os.remove(my_error_file)
"
447,"if getattr(signature_type, '__origin__', None) in {list, List}:
sig_el_type = signature_type.__args__[0]
if not inspect.isclass(sig_el_type):
            raise RuntimeError(
                f""Does not support nested parametric types, got {sig_el_type}. Please file a bug."")
if getattr(argument_type, '__origin__', None) in {list, List}:
return issubclass(argument_type.__args__[0], sig_el_type)
","if getattr(signature_type, '__origin__', None) in {list, List}:
sig_el_type = signature_type.__args__[0]
if not inspect.isclass(sig_el_type):
            warnings.warn(
                f""Does not support nested parametric types, got {signature_type}. Please file a bug."")
            return False
if getattr(argument_type, '__origin__', None) in {list, List}:
return issubclass(argument_type.__args__[0], sig_el_type)
"
448,"log.info(
f""User process failed with error data: {json.dumps(self.error_file_data, indent=2)}""
)
                    self.message = self.error_file_data[""message""][""message""]
                    self.timestamp = int(
                        self.error_file_data[""message""][""extraInfo""][""timestamp""]
)
except Exception:
log.exception(f""Failed to parse reply file: {self.error_file}"")
","log.info(
f""User process failed with error data: {json.dumps(self.error_file_data, indent=2)}""
)
                    self.message, self.timestamp = self._get_error_data(
                        self.error_file_data
)
except Exception:
log.exception(f""Failed to parse reply file: {self.error_file}"")
"
449,"else:
raise AssertionError(f""type f{type(prev_node_c)} is not handled"")
def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
input_node_c_2: Optional[Union[Node, List[Node]]],
","else:
raise AssertionError(f""type f{type(prev_node_c)} is not handled"")
# TODO(future PR): look into using copy_node API instead
def _copy_node_from_a_to_c(
    node_a: Node,
    gm_a: GraphModule,
    gm_b: GraphModule,
    graph_c: Graph,
) -> Node:
    """"""
    Simple copy of node_a to graph_c.
    """"""
    if node_a.op == 'get_attr':
        node_a_copy_name = \
            get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
        node_a_obj = getattr_from_fqn(gm_a, node_a.target)  # type: ignore[arg-type]
        setattr(gm_b, node_a_copy_name, node_a_obj.detach())
        node_a_copy = graph_c.create_node(
            node_a.op, node_a_copy_name, (), {}, node_a_copy_name)
        return node_a_copy
    elif node_a.op == 'call_method':
        assert node_a.target in ('dequantize', 'to'), \
            f""target {node_a.target} is not implemented""
        if node_a.target == 'dequantize':
            arg_copy = _copy_node_from_a_to_c(node_a.args[0], gm_a, gm_b, graph_c)  # type: ignore[arg-type]
            node_a_copy_name = \
                get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
            node_a_copy = graph_c.create_node(
                node_a.op, node_a.target, (arg_copy,), {}, node_a_copy_name)
            return node_a_copy
        else:  # to
            arg_copy = _copy_node_from_a_to_c(node_a.args[0], gm_a, gm_b, graph_c)  # type: ignore[arg-type]
            node_a_copy_name = \
                get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
            node_a_copy = graph_c.create_node(
                node_a.op, node_a.target, (arg_copy, node_a.args[1]), {},
                node_a_copy_name)
            return node_a_copy

    else:
        raise AssertionError(
            f""handling of node with op {node_a.op} is not implemented"")

def _insert_copy_of_subgraph_a_after_input_node_c(
input_node_c: Union[Node, List[Node]],
input_node_c_2: Optional[Union[Node, List[Node]]],
"
450,"parser.add_argument(""--file-filter"", help=""only pass through files with this extension"", default='')
parser.add_argument(""--changed-only"", help=""only run on changed files"", action='store_true', default=False)
parser.add_argument(""--job"", help=""job name"", required=True)
parser.add_argument(""--step"", action=""append"", help=""steps to run (in order)"")
parser.add_argument(
""--all-steps-after"", help=""include every step after this one (non inclusive)""
","parser.add_argument(""--file-filter"", help=""only pass through files with this extension"", default='')
parser.add_argument(""--changed-only"", help=""only run on changed files"", action='store_true', default=False)
parser.add_argument(""--job"", help=""job name"", required=True)
    parser.add_argument(""--no-quiet"", help=""output commands"", action='store_true', default=False)
parser.add_argument(""--step"", action=""append"", help=""steps to run (in order)"")
parser.add_argument(
""--all-steps-after"", help=""include every step after this one (non inclusive)""
"
451,"with torch.no_grad():
var.clamp_(min=eps)
    # Calculate loss (without constant)
    loss = 0.5 * (torch.log(var) + (input - target)**2 / var).view(input.size(0), -1).sum(dim=1)

    # Add constant to loss term if required
if full:
        D = input.size(1)
        loss = loss + 0.5 * D * math.log(2 * math.pi)
    # Apply reduction
if reduction == 'mean':
return loss.mean()
elif reduction == 'sum':
","with torch.no_grad():
var.clamp_(min=eps)
    # Calculate the loss
    loss = 0.5 * (torch.log(var) + (input - target)**2 / var)
if full:
        loss += 0.5 * math.log(2 * math.pi)
if reduction == 'mean':
return loss.mean()
elif reduction == 'sum':
"
452,">>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')
""""""
fan = _calculate_correct_fan(tensor, mode)
gain = calculate_gain(nonlinearity, a)
std = gain / math.sqrt(fan)
",">>> w = torch.empty(3, 5)
>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')
""""""
    if 0 in tensor.shape:
        warnings.warn(""Initializing zero-element tensors is a no-op"")
        return tensor
fan = _calculate_correct_fan(tensor, mode)
gain = calculate_gain(nonlinearity, a)
std = gain / math.sqrt(fan)
"
453,"def modifies_arguments(f: NativeFunction) -> bool:
return f.func.kind() in [SchemaKind.inplace, SchemaKind.out]
def emit_inplace_or_view_body(fn: NativeFunctionWithDifferentiabilityInfo) -> List[str]:
f = fn.func
inplace_view_body: List[str] = []
","def modifies_arguments(f: NativeFunction) -> bool:
return f.func.kind() in [SchemaKind.inplace, SchemaKind.out]
@with_native_function_with_differentiability_info
def emit_inplace_or_view_body(fn: NativeFunctionWithDifferentiabilityInfo) -> List[str]:
f = fn.func
inplace_view_body: List[str] = []
"
454,"raise AssertionError(f""base type should have been value type {t}"")
elif isinstance(t, OptionalType):
if str(t.elem) == 'Tensor':
            if mutable:
return NamedCType(binds, MutRefCType(BaseCType(tensorT)))  # TODO: fix this discrepancy
else:
return NamedCType(binds, ConstRefCType(OptionalCType(BaseCType(tensorT))))
","raise AssertionError(f""base type should have been value type {t}"")
elif isinstance(t, OptionalType):
if str(t.elem) == 'Tensor':
            if mutable and not local.use_const_ref_for_mutable_tensors():
return NamedCType(binds, MutRefCType(BaseCType(tensorT)))  # TODO: fix this discrepancy
else:
return NamedCType(binds, ConstRefCType(OptionalCType(BaseCType(tensorT))))
"
455,"return config, cmd
@record
def run(args):
if args.standalone:
etcd_server = EtcdServer()
","return config, cmd
def run(args):
if args.standalone:
etcd_server = EtcdServer()
"
456,"rdzv_endpoint: str = """"
rdzv_backend: str = ""etcd""
rdzv_configs: Dict[str, Any] = field(default_factory=dict)
    rdzv_timeout: int = 300
max_restarts: int = 3
monitor_interval: float = 30
start_method: str = ""spawn""
","rdzv_endpoint: str = """"
rdzv_backend: str = ""etcd""
rdzv_configs: Dict[str, Any] = field(default_factory=dict)
    rdzv_timeout: int = 900
max_restarts: int = 3
monitor_interval: float = 30
start_method: str = ""spawn""
"
457,"WEIGHT_PREPACK_OPS = {
torch._ops.ops.quantized.linear_prepack,
torch._ops.ops.quantized.linear_prepack_fp16,
torch._ops.ops.quantized.conv2d_prepack,
torch._ops.ops.quantized.conv3d_prepack,
}
","WEIGHT_PREPACK_OPS = {
torch._ops.ops.quantized.linear_prepack,
torch._ops.ops.quantized.linear_prepack_fp16,
    torch._ops.ops.quantized.conv1d_prepack,
torch._ops.ops.quantized.conv2d_prepack,
torch._ops.ops.quantized.conv3d_prepack,
}
"
458,"from datetime import datetime
from functools import wraps
from string import Template
from typing import Callable, Dict, List, Optional, Tuple
from .error_handler import ErrorHandler  # noqa F401
from .handlers import get_error_handler  # noqa F401
","from datetime import datetime
from functools import wraps
from string import Template
from typing import Callable, Dict, List, Optional, Tuple, TypeVar
from .error_handler import ErrorHandler  # noqa F401
from .handlers import get_error_handler  # noqa F401
"
459,"allow_empty: bool = True,
):
""""""Include `module` in the list of external modules the package can import.
        This will prevent dependency discover from saving
it in the package. The importer will load an external module directly from the standard import system.
Code for extern modules must also exist in the process loading the package.
","allow_empty: bool = True,
):
""""""Include `module` in the list of external modules the package can import.
        This will prevent dependency discovery from saving
it in the package. The importer will load an external module directly from the standard import system.
Code for extern modules must also exist in the process loading the package.
"
460,"return self.profiler.export_stacks(path, metric)
def key_averages(self, group_by_input_shape: bool = False, group_by_stack_n: int = 0):
        """"""
        Averages events, grouping them by operator name and (optionally) input shapes and
stack.
        Note: to use shape/stack functionality make sure to set record_shapes/with_stack
        when creating profiler context manager.
""""""
assert self.profiler
return self.profiler.key_averages(group_by_input_shape, group_by_stack_n)
","return self.profiler.export_stacks(path, metric)
def key_averages(self, group_by_input_shape: bool = False, group_by_stack_n: int = 0):
        """"""Averages events, grouping them by operator name and (optionally) input shapes and
stack.

        .. note::
            To use shape/stack functionality make sure to set record_shapes/with_stack
            when creating profiler context manager.
""""""
assert self.profiler
return self.profiler.key_averages(group_by_input_shape, group_by_stack_n)
"
461,"This is the second value returned by :meth:`torch.max`. See its
documentation for the exact semantics of this method.
.. note:: If there are multiple minimal values then the indices of the first minimal value are returned.
Args:
{input}
","This is the second value returned by :meth:`torch.max`. See its
documentation for the exact semantics of this method.
.. note:: If there are multiple maximal values then the indices of the first maximal value are returned.
Args:
{input}
"
462,":attr:`offsets`, if those are not None.
include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
        The last element is the size of the input, or the ending index position of the last bag (sequence).
Shape:

 :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)
          - If :attr:`input` is 2D of shape `(B, N)`,

            it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and
            this will return ``B`` values aggregated in a way depending on the :attr:`mode`.
            :attr:`offsets` is ignored and required to be ``None`` in this case.

          - If :attr:`input` is 1D of shape `(N)`,
            it will be treated as a concatenation of multiple bags (sequences).
            :attr:`offsets` is required to be a 1D tensor containing the
            starting index positions of each bag in :attr:`input`. Therefore,
            for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as
            having ``B`` bags. Empty bags (i.e., having 0-length) will have
            returned vectors filled by zeros.
        - :attr:`weight` (Tensor): the learnable weights of the module of
          shape `(num_embeddings, embedding_dim)`
        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as
          :attr:`input`.
 :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`
",":attr:`offsets`, if those are not None.
include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
            The last element is the size of the input, or the ending index position of the last bag (sequence).
Shape:
          - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)
            each of fixed length ``N``, and this will return ``B`` values aggregated in a way
            depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.
          - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of
            multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing
            the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`
            of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.
            Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
        - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.
"
463,"weight (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
initialized from :math:`\mathcal{N}(0, 1)`.
    Inputs: :attr:`input` (IntTensor or LongTensor), :attr:`offsets` (IntTensor or LongTensor, optional), and
        :attr:`per_index_weights` (Tensor, optional)

        - :attr:`input` and :attr:`offsets` have to be of the same type, either int or long

        - If :attr:`input` is 2D of shape `(B, N)`,

          it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and
          this will return ``B`` values aggregated in a way depending on the :attr:`mode`.
          :attr:`offsets` is ignored and required to be ``None`` in this case.

        - If :attr:`input` is 1D of shape `(N)`,

          it will be treated as a concatenation of multiple bags (sequences).
          :attr:`offsets` is required to be a 1D tensor containing the
          starting index positions of each bag in :attr:`input`. Therefore,
          for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as
          having ``B`` bags. Empty bags (i.e., having 0-length) will have
          returned vectors filled by zeros.

        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
            to indicate all weights should be taken to be ``1``. If specified, :attr:`per_sample_weights`
            must have exactly the same shape as input and is treated as having the same
            :attr:`offsets`, if those are not ``None``. Only supported for ``mode='sum'``.


    Output shape: `(B, embedding_dim)`

Examples::
>>> # an Embedding module containing 10 tensors of size 3
","weight (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
initialized from :math:`\mathcal{N}(0, 1)`.
Examples::
>>> # an Embedding module containing 10 tensors of size 3
"
464,"assert node.inputsAt(1).type().kind() == ""TensorType""
# TODO: Should support constant as either operand.
        in0_id, in0_oper = self.get_tensor_operand_by_jitval(node.inputsAt(0))
        in1_id, in1_oper = self.get_tensor_operand_by_jitval(node.inputsAt(1))
assert in0_oper.op_type == in1_oper.op_type
in0_id, in0_oper, in1_id, in1_oper = self.transpose_for_broadcast(
","assert node.inputsAt(1).type().kind() == ""TensorType""
# TODO: Should support constant as either operand.
        in0_id, in0_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
        in1_id, in1_oper = self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(1))
assert in0_oper.op_type == in1_oper.op_type
in0_id, in0_oper, in1_id, in1_oper = self.transpose_for_broadcast(
"
465,"total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
else:
total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
    if error_if_nonfinite and (total_norm.isnan() or total_norm.isinf()):
        raise RuntimeError(
            f'The total norm of order {norm_type} for gradients from '
            '`parameters` is non-finite, so it cannot be clipped. To disable '
            'this error and scale the gradients by the non-finite norm anyway, '
            'set `error_if_nonfinite=False`')
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
","total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
else:
total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
    if total_norm.isnan() or total_norm.isinf():
        if error_if_nonfinite:
            raise RuntimeError(
                f'The total norm of order {norm_type} for gradients from '
                '`parameters` is non-finite, so it cannot be clipped. To disable '
                'this error and scale the gradients by the non-finite norm anyway, '
                'set `error_if_nonfinite=False`')
        else:
            warnings.warn(""Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. ""
                          ""Note that the default behavior will change in a future release to error out ""
                          ""if a non-finite total norm is encountered. At that point, setting ""
                          ""error_if_nonfinite=false will be required to retain the old behavior."",
                          FutureWarning, stacklevel=2)
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
"
466,"def clip_grad_norm(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.,
        error_if_nonfinite: bool = True) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
.. warning::
","def clip_grad_norm(
parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.,
        error_if_nonfinite: bool = False) -> torch.Tensor:
r""""""Clips gradient norm of an iterable of parameters.
.. warning::
"
467,"else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('at::GradMode::is_enabled() ? '
                             'CreationMeta::DEFAULT : '
                             'CreationMeta::NO_GRAD_MODE')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
","else:
_, unpacked_bindings = unpack_args(f)
call += emit_view_lambda(f, unpacked_bindings)
            creation_meta = ('InferenceMode::is_enabled() ? '
                             'CreationMeta::INFERENCE_MODE : '
                             '(at::GradMode::is_enabled() ? CreationMeta::DEFAULT : CreationMeta::NO_GRAD_MODE)')
rhs_value = (f'as_view(/* base */ {view_info}, /* output */ {var}, /* is_bw_differentiable */ true, '
'/* is_fw_differentiable */ true, '
f'/* view_func */ func, /* creation_meta */ {creation_meta})')
"
468,"_python_cu = torch._C.CompilationUnit()
# qualified_name => ScriptClass mapping
_script_classes = {}
def _add_script_class(cls, name):
    global _script_classes
    _script_classes[name] = cls
def _get_script_class(name):
    global _script_classes
    if name not in _script_classes:
        return None
    return _script_classes[name]
# Caching: we currently cache compilation of free functions and overloaded functions.
","_python_cu = torch._C.CompilationUnit()
# python class => ScriptClass mapping
_script_classes = {}
_name_to_pyclass = {}
def _add_script_class(python_class, script_class):
    _script_classes[python_class] = script_class
    _name_to_pyclass[script_class.qualified_name()] = python_class

def _get_script_class(python_class):
    override = getattr(python_class, ""_jit_override_qualname"", None)
    if override is not None:
        python_class = _get_python_class(override)
    return _script_classes.get(python_class, None)


def _get_python_class(qualified_name):
    return _name_to_pyclass.get(qualified_name, None)


def _clear_class_state():
    _script_classes.clear()
    _name_to_pyclass.clear()
# Caching: we currently cache compilation of free functions and overloaded functions.
"
469,"if ann is type(None):
return NoneType.get()
if inspect.isclass(ann) and hasattr(ann, ""__torch_script_interface__""):
        return InterfaceType(_qualified_name(ann))
if ann is torch.device:
return DeviceObjType.get()
if ann is torch.Stream:
","if ann is type(None):
return NoneType.get()
if inspect.isclass(ann) and hasattr(ann, ""__torch_script_interface__""):
        return InterfaceType(ann.__torch_script_interface__)
if ann is torch.device:
return DeviceObjType.get()
if ann is torch.Stream:
"
470,"body.extend(unpack_args_stats)
if requires_derivative:
body.extend(emit_any_requires_grad())
body.extend(emit_check_inplace())
body.extend(setup_derivative(differentiable_inputs))
body.append(declare_returned_variables(f))
","body.extend(unpack_args_stats)
if requires_derivative:
body.extend(emit_any_requires_grad())
        body.extend(emit_assert_no_inference_tensor())
body.extend(emit_check_inplace())
body.extend(setup_derivative(differentiable_inputs))
body.append(declare_returned_variables(f))
"
471,"""XLA"", ""AutogradXLA"",
]
alias_keys = [
        ""DefaultBackend"",
""Autograd"",
""CompositeImplicitAutograd"",
]
","""XLA"", ""AutogradXLA"",
]
alias_keys = [
        ""CompositeExplicitAutograd"",
""Autograd"",
""CompositeImplicitAutograd"",
]
"
472,"if torch.allclose(res, exp):
continue
return fail_test(get_failed_batched_grad_test_msg(output_idx, input_idx, res, exp))
def test_backward_mul_by_grad_output(fail_test, outputs, inputs, check_sparse_nnz) -> bool:
","if torch.allclose(res, exp):
continue
return fail_test(get_failed_batched_grad_test_msg(output_idx, input_idx, res, exp))
    return True
def test_backward_mul_by_grad_output(fail_test, outputs, inputs, check_sparse_nnz) -> bool:
"
473,"assert list(_weight.shape) == [num_embeddings, embedding_dim], \
'Shape of weight does not match num_embeddings and embedding_dim'
self.weight = Parameter(_weight)
            self._fill_padding_idx_with_zero()
self.sparse = sparse
def reset_parameters(self) -> None:
","assert list(_weight.shape) == [num_embeddings, embedding_dim], \
'Shape of weight does not match num_embeddings and embedding_dim'
self.weight = Parameter(_weight)

self.sparse = sparse
def reset_parameters(self) -> None:
"
474,"_verify_model_across_ranks,
_verify_replicas_within_process,
_test_python_store,
)
if sys.platform != 'win32':
from torch._C._distributed_c10d import (
","_verify_model_across_ranks,
_verify_replicas_within_process,
_test_python_store,
        _DistributedDebugLevel,
        _get_debug_mode
)
if sys.platform != 'win32':
from torch._C._distributed_c10d import (
"
475,"# TODO: we don't need to replicate params in here. they're always going to
# be broadcasted using larger blocks in broadcast_coalesced, so it might be
# better to not pollute the caches with these small blocks
            self._module_copies = replicate(self.module, self.device_ids, detach=True)
            self._module_copies[0] = self.module
            for module_copy in self._module_copies[1:]:
                for param, copy_param in zip(self.module.parameters(), parameters(module_copy)):
# Reducer requires param copies have the same strides across replicas.
# Fixes up copy_param strides in case replicate didn't match param strides.
if param.layout is torch.strided and param.stride() != copy_param.stride():
","# TODO: we don't need to replicate params in here. they're always going to
# be broadcasted using larger blocks in broadcast_coalesced, so it might be
# better to not pollute the caches with these small blocks
            module_copies = replicate(self.module, self.device_ids, detach=True)
            module_copies[0] = self.module
            for module_copy in module_copies[1:]:
                for param, copy_param in zip(self.module.parameters(), self._get_parameters(module_copy)):
# Reducer requires param copies have the same strides across replicas.
# Fixes up copy_param strides in case replicate didn't match param strides.
if param.layout is torch.strided and param.stride() != copy_param.stride():
"
476,"elif self.dispatch_key == DispatchKey.CUDA:
empty_impl = ""at::native::empty_cuda""
empty_strided_impl = ""at::native::empty_strided_cuda""
else:
raise AssertionError(""unsupported dispatch key"")
return f""""""
","elif self.dispatch_key == DispatchKey.CUDA:
empty_impl = ""at::native::empty_cuda""
empty_strided_impl = ""at::native::empty_strided_cuda""
                elif self.dispatch_key == DispatchKey.DefaultBackend:
                    empty_impl = ""at::empty""
                    empty_strided_impl = ""at::empty_strided""
else:
raise AssertionError(""unsupported dispatch key"")
return f""""""
"
477,"# After running meta, op.outputs_ is guaranteed to be valid;
# add it to the context
# TODO: handle multi-return
context.append(Expr(
expr=""op.outputs_[0]"",
                type=structured.out_arguments(self.g)[0].ctype,
))
# With the expanded context, do the impl call (if not a meta
# function)
            if self.dispatch_key != DispatchKey.Meta:
impl_exprs = ', '.join(
e.expr for e in translate(
context,
","# After running meta, op.outputs_ is guaranteed to be valid;
# add it to the context
# TODO: handle multi-return
            assert ConstRefCType(BaseCType(""Tensor"", structured.out_arguments(self.g)[0].ctype.name)) == \
                structured.out_arguments(self.g)[0].ctype
context.append(Expr(
expr=""op.outputs_[0]"",
                # TODO: Stop hardcoding that the output type is a Tensor.  Note
                # that for the codegen here this is fine because outputs_ is
                # hardcoded to be tensor already
                type=MutRefCType(BaseCType(""Tensor"", structured.out_arguments(self.g)[0].ctype.name)),
))
# With the expanded context, do the impl call (if not a meta
# function)
            if self.dispatch_key == DispatchKey.DefaultBackend:
                # TODO: https://github.com/pytorch/pytorch/issues/53023
                out_sig_group = CppSignatureGroup.from_native_function(
                    self.g.out, method=False, fallback_binding=f.manual_cpp_binding)
                out_sig = out_sig_group.most_faithful_signature()
                api_name = out_sig.name()
                out_exprs = ', '.join(
                    e.expr for e in translate(
                        context,
                        out_sig.arguments(),
                        method=False
                    )
                )
                # TODO: I think this means structured won't work with method
                # only functions (but maybe you're saved by faithful? iunno.)
                # NB: Originally I wrote this as an at::redispatch call, but
                # I got in trouble because that meant I needed a DispatchKeySet
                # in the wrapper function, which meant I needed a DispatchKeySet
                # in the DispatchKeyFunctions declarations, but the defined API
                # there does NOT permit a dispatch key set.  I think you can
                # probably unwind this by calling some function to do the TLS
                # fetch and get the DispatchKeySet when you don't have it, but
                # I didn't do it for this version
                sig_body.append(f""at::{api_name}({out_exprs});"")
            elif self.dispatch_key != DispatchKey.Meta:
impl_exprs = ', '.join(
e.expr for e in translate(
context,
"
478,"from .dataset import (Dataset, IterableDataset, TensorDataset, ConcatDataset, ChainDataset, BufferedShuffleDataset,
Subset, random_split)
from .dataset import IterableDataset as IterDataPipe
from .distributed import DistributedSampler
from .dataloader import DataLoader, _DatasetKind, get_worker_info
from . import datapipes
","from .dataset import (Dataset, IterableDataset, TensorDataset, ConcatDataset, ChainDataset, BufferedShuffleDataset,
Subset, random_split)
from .dataset import IterableDataset as IterDataPipe
from .dataset import functional_datapipe
from .distributed import DistributedSampler
from .dataloader import DataLoader, _DatasetKind, get_worker_info
from . import datapipes
"
479,"except ImportError:
_GLOO_AVAILABLE = False

logger = logging.getLogger(__name__)


# Some reduce ops are not supported by complex numbers and will result in an error.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
","except ImportError:
_GLOO_AVAILABLE = False
# Some reduce ops are not supported by complex numbers and will result in an error.
# We currently provide complex support to the distributed API by viewing
# complex tensors as real (torch.view_as_real), meaning that calling
"
480,"import pickletools
from .find_file_dependencies import find_files_source_depends_on
from ._custom_import_pickler import create_custom_import_pickler
from ._importlib import _normalize_path
from ._mangling import is_mangled
from ._stdlib import is_stdlib_module
from .importer import Importer, OrderedImporter, sys_importer
import types
from typing import List, Any, Callable, Dict, Sequence, Tuple, Union, Iterable, BinaryIO, Optional
from pathlib import Path
import linecache
from urllib.parse import quote
import re
class PackageExporter:
","import pickletools
from .find_file_dependencies import find_files_source_depends_on
from ._custom_import_pickler import create_custom_import_pickler
from ._file_structure_representation import _create_folder_from_file_list, Folder
from ._glob_group import GlobPattern, _GlobGroup
from ._importlib import _normalize_path
from ._mangling import is_mangled
from ._stdlib import is_stdlib_module
from .importer import Importer, OrderedImporter, sys_importer
import types
from typing import List, Any, Callable, Dict, Sequence, Tuple, Union, BinaryIO, Optional
from pathlib import Path
import linecache
from urllib.parse import quote
class PackageExporter:
"
481,"(load_arg(quantized=activation_statically_quantized)(self.linear_node.args[0]),), {})
else:  # call_function
assert self.linear_node.op == 'call_function'
            if debug:
quantized_input_idxs = []
if activation_statically_quantized:
quantized_input_idxs.append(0)
","(load_arg(quantized=activation_statically_quantized)(self.linear_node.args[0]),), {})
else:  # call_function
assert self.linear_node.op == 'call_function'
            if is_reference:
quantized_input_idxs = []
if activation_statically_quantized:
quantized_input_idxs.append(0)
"
482,"def gather(outputs, target_device, dim=0):
r""""""
    Gathers tensors from different GPUs on a specified device
      (-1 means the CPU).
""""""
def gather_map(outputs):
out = outputs[0]
","def gather(outputs, target_device, dim=0):
r""""""
    Gathers tensors from different GPUs on a specified device.
    Use 'cpu' for CPU to avoid a deprecation warning.
""""""
def gather_map(outputs):
out = outputs[0]
"
483,"""""""Represents an abstract field type in a dataset.
""""""
    __slots__ = (""_parent"", ""_field_offsets"")
def __init__(self, children):
""""""Derived classes must call this after their initialization.""""""
","""""""Represents an abstract field type in a dataset.
""""""
    __slots__: Sequence[str] = (""_parent"", ""_field_offsets"")
def __init__(self, children):
""""""Derived classes must call this after their initialization.""""""
"
484,"the parent domain.
""""""
    __slots__ = (""lengths"", ""_items"")
def __init__(self, values, lengths_blob=None):
if isinstance(lengths_blob, Field):
","the parent domain.
""""""
    __slots__: Sequence[str] = (""lengths"", ""_items"")
def __init__(self, values, lengths_blob=None):
if isinstance(lengths_blob, Field):
"
485,"class _SchemaNode(object):
""""""This is a private class used to represent a Schema Node""""""
    __slots__ = (""name"", ""children"", ""type_str"", ""field"")
def __init__(self, name, type_str=''):
self.name = name
","class _SchemaNode(object):
""""""This is a private class used to represent a Schema Node""""""
    __slots__: Sequence[str] = (""name"", ""children"", ""type_str"", ""field"")
def __init__(self, name, type_str=''):
self.name = name
"
486,"return sym_opset9.min(g, self, dim_or_y, keepdim)
for block_listed_op in block_listed_operators:
vars()[block_listed_op] = _block_list_in_opset(block_listed_op)
","return sym_opset9.min(g, self, dim_or_y, keepdim)
def div(g, self, other, *args):
    if len(args) == 0:
        return sym_opset9.true_divide(g, self, other)
    else:
        return _div_rounding_mode(g, self, other, *args)


@parse_args('v', 'v', 's')
def _div_rounding_mode(g, self, other, rounding_mode):
    if rounding_mode == 'floor':
        return _floor_divide(g, self, other)
    else:
        return sym_opset9._div_rounding_mode(g, self, other, rounding_mode)


def _floor_divide(g, self, other):
    if sym_help._is_fp(self) or sym_help._is_fp(other):
        out = sym_opset9.true_divide(g, self, other)
        return g.op('Floor', out)
    else:
        raise RuntimeError('Integer floor division requires ONNX opset 9 or greater')


for block_listed_op in block_listed_operators:
vars()[block_listed_op] = _block_list_in_opset(block_listed_op)
"
487,"from .node import Node, Argument, Target, map_arg
from typing import Callable, Any, List, Dict, Optional, Tuple, Set
import builtins
import torch
import types
import keyword
import re
def _shadows_builtin_name(name: str) -> bool:
return name in builtins.__dict__ or name in keyword.kwlist or name in {'inf', 'nan', 'NoneType'}
","from .node import Node, Argument, Target, map_arg, _type_repr, _get_qualified_name
from typing import Callable, Any, List, Dict, Optional, Tuple, Set
import torch
import keyword
import re
import builtins
def _shadows_builtin_name(name: str) -> bool:
return name in builtins.__dict__ or name in keyword.kwlist or name in {'inf', 'nan', 'NoneType'}
"
488,"elif reduction == 2:
return sym_help._reducesum_helper(g, output, keepdims_i=0)
else:
        return sym_help._onnx_unsupported(""kl_div with reduction other than none, mean, or sum. Please open a bug to ""
                                          ""request ONNX export support for the missing reduction type."")
@parse_args('v', 'v', 'is', 'i')
","elif reduction == 2:
return sym_help._reducesum_helper(g, output, keepdims_i=0)
else:
        return sym_help._onnx_unsupported(""kl_div with reduction other than none, mean, or sum."")
@parse_args('v', 'v', 'is', 'i')
"
489,"@parse_args('v', 'i', 'v', 'v')
def unfold(g, input, dimension, size, step):
    size = sym_help._maybe_get_const(size, 'i')
    step = sym_help._maybe_get_const(step, 'i')
    if not sym_help._is_value(size) and not sym_help._is_value(step):
from torch.onnx.symbolic_opset9 import unfold as _unfold
        return _unfold(g, input, dimension, size, step)
if sym_help._operator_export_type == torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK:
return g.op(""ATen"", input, operator_s=""unfold"", dimension_i=dimension, size_i=size, step_i=step)
","@parse_args('v', 'i', 'v', 'v')
def unfold(g, input, dimension, size, step):
    const_size = sym_help._maybe_get_const(size, 'i')
    const_step = sym_help._maybe_get_const(step, 'i')
    if not sym_help._is_value(const_size) and not sym_help._is_value(const_step):
from torch.onnx.symbolic_opset9 import unfold as _unfold
        return _unfold(g, input, dimension, const_size, const_step)
if sym_help._operator_export_type == torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK:
return g.op(""ATen"", input, operator_s=""unfold"", dimension_i=dimension, size_i=size, step_i=step)
"
490,"# Run vanilla allreduce in the first `start_powerSGD_iter` iterations.
if state.iter < state.start_powerSGD_iter:
state.maybe_increase_iter(bucket)
        return default.allreduce_fut(group_to_use, input_tensor)
# Apply PowerSGD after `start_powerSGD_iter` iterations.
device = input_tensor.device
","# Run vanilla allreduce in the first `start_powerSGD_iter` iterations.
if state.iter < state.start_powerSGD_iter:
        fut = dist.all_reduce(
            input_tensor, group=group_to_use, async_op=True
        ).get_future()

        def div_callback(fut):
            return [fut.value()[0].div_(world_size)]

state.maybe_increase_iter(bucket)
        return fut.then(div_callback)
# Apply PowerSGD after `start_powerSGD_iter` iterations.
device = input_tensor.device
"
491,"q_idx += m * matrix_approximation_rank
# If warm-start is enabled, reuse Qs from the previous iteration if possible and skip filling random values.
    # The exceptions are the first time and when the buckets are rebuilt.
if not need_randomize_qs:
for q in qs:
_orthogonalize(q)
","q_idx += m * matrix_approximation_rank
# If warm-start is enabled, reuse Qs from the previous iteration if possible and skip filling random values.
    # The exception is the first iteration when PowerSGD is applied.
if not need_randomize_qs:
for q in qs:
_orthogonalize(q)
"
492,"def _site_packages(dirname, platform):
if platform.startswith(""win""):
        os.path.join(pytdir.name, ""Lib"", ""site-packages"")
else:
template = os.path.join(dirname, ""lib"", ""python*.*"", ""site-packages"")
        spdir = glob.glob(template)[0]
return spdir
","def _site_packages(dirname, platform):
if platform.startswith(""win""):
        template = os.path.join(dirname, ""Lib"", ""site-packages"")
else:
template = os.path.join(dirname, ""lib"", ""python*.*"", ""site-packages"")
    spdir = glob.glob(template)[0]
return spdir
"
493,"dependent = _Dependent()
dependent_property = _DependentProperty
boolean = _Boolean()
nonnegative_integer = _IntegerGreaterThan(0)
positive_integer = _IntegerGreaterThan(1)
integer_interval = _IntegerInterval
","dependent = _Dependent()
dependent_property = _DependentProperty
boolean = _Boolean()
one_hot = _OneHot()
nonnegative_integer = _IntegerGreaterThan(0)
positive_integer = _IntegerGreaterThan(1)
integer_interval = _IntegerInterval
"
494,"assert isinstance(funcs, str), f'not a str: {funcs}'
func = FunctionSchema.parse(funcs)
use_c10_dispatcher_s = e.pop('use_c10_dispatcher', None)
if use_c10_dispatcher_s is None:
use_c10_dispatcher = UseC10Dispatcher.full
","assert isinstance(funcs, str), f'not a str: {funcs}'
func = FunctionSchema.parse(funcs)
        cpp_no_default_args_list = e.pop('cpp_no_default_args', [])
        assert isinstance(cpp_no_default_args_list, list)
        cpp_no_default_args = set(cpp_no_default_args_list)

use_c10_dispatcher_s = e.pop('use_c10_dispatcher', None)
if use_c10_dispatcher_s is None:
use_c10_dispatcher = UseC10Dispatcher.full
"
495,"self.profiler_kind,
self.record_shapes,
self.profile_memory,
            self.with_stack)
def __enter__(self):
if not self.enabled:
","self.profiler_kind,
self.record_shapes,
self.profile_memory,
            self.with_stack,
            self.with_flops)
def __enter__(self):
if not self.enabled:
"
496,"profiler_kind,
self.record_shapes,
self.profile_memory,
False)
_enable_server_process_global_profiler(profiler_config)
return self
","profiler_kind,
self.record_shapes,
self.profile_memory,
            False,
False)
_enable_server_process_global_profiler(profiler_config)
return self
"
497,"Args:
size (int...): a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.
{out}
{dtype}
{layout}
","Args:
size (int...): a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.

Keyword arguments:
{out}
{dtype}
{layout}
"
498,""""""" Adds an activation post process module and register
a post hook that calls the module
""""""
        if needs_observation(m):
# observer and hook will be gone after we swap the module
m.add_module('activation_post_process', get_activation_post_process(m.qconfig, device, special_act_post_process))
# Register observer as the first entry in the hook list
",""""""" Adds an activation post process module and register
a post hook that calls the module
""""""
        # We don't insert observer/fake_quantize for DeQuantStub
        if needs_observation(m) and not isinstance(m, DeQuantStub):
# observer and hook will be gone after we swap the module
m.add_module('activation_post_process', get_activation_post_process(m.qconfig, device, special_act_post_process))
# Register observer as the first entry in the hook list
"
499,"r""""""Short-time Fourier transform (STFT).
.. warning::
        From version 1.8.0, :attr:`return_complex` must always be given
        explicitly for real inputs and `return_complex=False` has been
        deprecated. Strongly prefer `return_complex=True` as in a future
        pytorch release, this function will only return complex tensors.

        Note that :func:`torch.view_as_real` can be used to recover a real
        tensor with an extra last dimension for real and imaginary components.
The STFT computes the Fourier transform of short overlapping windows of the
input. This giving frequency components of the signal as they change over
","r""""""Short-time Fourier transform (STFT).
.. warning::
        Setting :attr:`return_complex` explicitly will be required in a future
        PyTorch release. Set it to False to preserve the current behavior or
        True to return a complex output.
The STFT computes the Fourier transform of short overlapping windows of the
input. This giving frequency components of the signal as they change over
"
500,"r""""""Short-time Fourier transform (STFT).
.. warning::
        Setting :attr:`return_complex` explicitly will be required in a future
        PyTorch release. Set it to False to preserve the current behavior or
        True to return a complex output.
The STFT computes the Fourier transform of short overlapping windows of the
input. This giving frequency components of the signal as they change over
","r""""""Short-time Fourier transform (STFT).
.. warning::
        From version 1.8.0, :attr:`return_complex` must always be given
        explicitly for real inputs and `return_complex=False` has been
        deprecated. Strongly prefer `return_complex=True` as in a future
        pytorch release, this function will only return complex tensors.

        Note that :func:`torch.view_as_real` can be used to recover a real
        tensor with an extra last dimension for real and imaginary components.
The STFT computes the Fourier transform of short overlapping windows of the
input. This giving frequency components of the signal as they change over
"
501,"import itertools
from dataclasses import dataclass
from typing import Optional, Union, Sequence, Set, List, Tuple, Dict
","from dataclasses import dataclass
from typing import Optional, Union, Sequence, Set, List, Tuple, Dict
"
502,"returns: Tuple['Return', ...]
def schema_order_arguments(self) -> Iterator['Argument']:
        return itertools.chain(self.arguments.positional, self.arguments.kwarg_only, self.arguments.out)
@staticmethod
def parse(func: str) -> 'FunctionSchema':
","returns: Tuple['Return', ...]
def schema_order_arguments(self) -> Iterator['Argument']:
        return itertools.chain(
            self.arguments.flat_positional,
            self.arguments.flat_kwarg_only,
            self.arguments.out
        )
@staticmethod
def parse(func: str) -> 'FunctionSchema':
"
503,"partitions.append(partition)
partitions.remove(partition_0)
partitions.remove(partition_1)
    # Reorganize partitions
reorganize_partitions(partitions)
return
","partitions.append(partition)
partitions.remove(partition_0)
partitions.remove(partition_1)
reorganize_partitions(partitions)
return
"
504,"# For each node in the current partition, find its users
users = node.users
for n in users:
                # Find which the partition the user belongs to.
# Note that if the node itself is also belongs to that partition,
# that partition is not the child of the current partition
for p in partitions:
","# For each node in the current partition, find its users
users = node.users
for n in users:
                # Find which the partition the user node belongs to.
# Note that if the node itself is also belongs to that partition,
# that partition is not the child of the current partition
for p in partitions:
"
505,"def reorganize_partitions(partitions: List[Partition]) -> None:
""""""Given a list of partitions, reorganzie partiton id,
    its parents and its children for each partition
""""""
# Rearrange partition ids
for i, partition in enumerate(partitions):
","def reorganize_partitions(partitions: List[Partition]) -> None:
""""""Given a list of partitions, reorganzie partiton id,
       its parents and its children for each partition
""""""
# Rearrange partition ids
for i, partition in enumerate(partitions):
"
506,"device_to_left_mem_bytes[device] = d.available_mem_bytes - partition.used_mem_bytes
else:
no_device_partitions.append(partition)
    # Find device for each no device partition
found_device = True
for partition in no_device_partitions:
device_to_left_mem_bytes = {
","device_to_left_mem_bytes[device] = d.available_mem_bytes - partition.used_mem_bytes
else:
no_device_partitions.append(partition)
    # Find devices for all the partitions without a device
found_device = True
for partition in no_device_partitions:
device_to_left_mem_bytes = {
"
507,"model = copy.deepcopy(model)
model.eval()
prepare(model, inplace=True)
    run_fn(model, run_args)
convert(model, mapping, inplace=True)
return model
","model = copy.deepcopy(model)
model.eval()
prepare(model, inplace=True)
    run_fn(model, *run_args)
convert(model, mapping, inplace=True)
return model
"
508,"powerSGD_state = powerSGD.PowerSGDState(
process_group=state,
matrix_approximation_rank=matrix_approximation_rank,
random_seed=random_seed,
)
model.register_comm_hook(powerSGD_state, comm_hook)
","powerSGD_state = powerSGD.PowerSGDState(
process_group=state,
matrix_approximation_rank=matrix_approximation_rank,
        use_error_feedback=use_error_feedback,
random_seed=random_seed,
)
model.register_comm_hook(powerSGD_state, comm_hook)
"
509,"class PowerSGDState(object):
    __slots__ = [""process_group"", ""matrix_approximation_rank"", ""rng""]

    def __init__(self, process_group, matrix_approximation_rank=1, random_seed=0):
self.process_group = process_group
self.matrix_approximation_rank = matrix_approximation_rank
# The purpose of this RNG is to generate different random seeds for initializing Q across iterations,
# but in the same order for all the DDP replicas.
# Different random seeds across iterations indicate different 'projections' of the gradients at different SGD steps.
# If the same random projection is used,
# there will be differences between the gradients that are never synchronized.
self.rng = np.random.RandomState(random_seed)
def powerSGD_hook(
","class PowerSGDState(object):
    __slots__ = [
        ""process_group"",
        ""matrix_approximation_rank"",
        ""use_error_feedback"",
        ""rng"",
        ""error_dict"",
    ]

    def __init__(
        self,
        process_group,
        matrix_approximation_rank=1,
        use_error_feedback=True,
        random_seed=0,
    ):
self.process_group = process_group
self.matrix_approximation_rank = matrix_approximation_rank
        # Error feedback is usually crucial for both for convergence and generalization,
        # because PowerSGD is a biased compressor,
        # i.e., compressing and decompressing a random gradient does not yield the original in expectation.
        # This mechanism requires a temporary copy of the input gradients,
        # so it increases the peak memory consumption by the size of gradient tensor.
        # However, if the target matrices are known to be exactly low-ranked (instead of just low stable rank),
        # sometimes it is possible to converge to the optima without error feedback.
        # See: http://proceedings.mlr.press/v54/yurtsever17a/yurtsever17a.pdf
        self.use_error_feedback = use_error_feedback
# The purpose of this RNG is to generate different random seeds for initializing Q across iterations,
# but in the same order for all the DDP replicas.
# Different random seeds across iterations indicate different 'projections' of the gradients at different SGD steps.
# If the same random projection is used,
# there will be differences between the gradients that are never synchronized.
self.rng = np.random.RandomState(random_seed)
        # Since there is only a single state instance for all the input buckets,
        # need to maintain a dictionary that maps each bucket to the local error.
        # TODO(wayi): Currently the key is the (hashcode of) input tensor, which may change across steps,
        # since the bucket can be rebuilt in the forward pass (to save peak memory usage).
        # Need to add an index field to the input bucket of comm hook.
        self.error_dict = {}
def powerSGD_hook(
"
510,"self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self._conv_forward(input, self.weight_fake_quant(self.weight), self.bias)
@classmethod
def from_float(cls, mod):
","self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self._conv_forward(input, self.weight_fake_quant(self.weight))
@classmethod
def from_float(cls, mod):
"
511,"# TODO: support regex as well
propagate_qconfig_(model, flattened_qconfig_dict)
if model.training:
            additional_qat_module_mapping = prepare_custom_config_dict.get(""additioanl_qat_module_mapping"", {})
self._qat_swap_modules(model, additional_qat_module_mapping)
self.modules = dict(model.named_modules())
","# TODO: support regex as well
propagate_qconfig_(model, flattened_qconfig_dict)
if model.training:
            additional_qat_module_mapping = prepare_custom_config_dict.get(""additional_qat_module_mapping"", {})
self._qat_swap_modules(model, additional_qat_module_mapping)
self.modules = dict(model.named_modules())
"
512,"""""""
def __init__(self, scale: float, zero_point: int, negative_slope: float = 1e-2, inplace: bool = False):
super().__init__(negative_slope, inplace)
        self.register_buffer('scale', torch.tensor([scale]))
        self.register_buffer('zero_point', torch.tensor([zero_point]))
def forward(self, input):
return torch.ops.quantized.leaky_relu(
","""""""
def __init__(self, scale: float, zero_point: int, negative_slope: float = 1e-2, inplace: bool = False):
super().__init__(negative_slope, inplace)
        self.register_buffer('scale', torch.tensor(scale))
        self.register_buffer('zero_point', torch.tensor(zero_point))
def forward(self, input):
return torch.ops.quantized.leaky_relu(
"
513,"if weight.dtype == torch.float16:
if activation.dtype == torch.float:
            return QuantType.WEIGHT_ONLY
raise Exception(""Unrecognized dtype combination in get_quant_type: activation({}),""
""weight({})"".format(activation.dtype, weight.dtype))
","if weight.dtype == torch.float16:
if activation.dtype == torch.float:
            return QuantType.DYNAMIC
raise Exception(""Unrecognized dtype combination in get_quant_type: activation({}),""
""weight({})"".format(activation.dtype, weight.dtype))
"
514,"custom_op_name: (temporary) specify this observer for an operator that doesn't require any observation
(Can be used in Graph Mode Passes for special case ops).
""""""
    def __init__(self, dtype=torch.float16, custom_op_name="""", compute_dtype=None):
super(PlaceholderObserver, self).__init__(dtype=dtype)
# dtype of input of the target operator, e.g. for dynamic quantization
# ops, the dtype will be float32
","custom_op_name: (temporary) specify this observer for an operator that doesn't require any observation
(Can be used in Graph Mode Passes for special case ops).
""""""
    def __init__(self, dtype=torch.float32, custom_op_name="""", compute_dtype=None):
super(PlaceholderObserver, self).__init__(dtype=dtype)
# dtype of input of the target operator, e.g. for dynamic quantization
# ops, the dtype will be float32
"
515,"per_channel_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer,
weight=default_per_channel_weight_observer)
float_qparams_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer,
                                               weight=default_float_qparams_observer)
default_qat_qconfig = QConfig(activation=default_fake_quant,
weight=default_weight_fake_quant)
","per_channel_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer,
weight=default_per_channel_weight_observer)
# TODO: this is weight only quant, change this to QConfigWeightOnly
# or remove the QConfigDynamic later
float_qparams_weight_only_qconfig = QConfigDynamic(
    activation=default_placeholder_observer,
    weight=default_float_qparams_observer)
default_qat_qconfig = QConfig(activation=default_fake_quant,
weight=default_weight_fake_quant)
"
516,"set_parents_and_children(self.partitions)
# Get bfs level for each partition
get_bfs_level_partition(self.partitions)

find_combination = True
while find_combination:
# Search for a pair partition to generate the minimum new cost,
","set_parents_and_children(self.partitions)
# Get bfs level for each partition
get_bfs_level_partition(self.partitions)
find_combination = True
while find_combination:
# Search for a pair partition to generate the minimum new cost,
"
517,"sum: Tensor
sum_squares: Tensor
def __init__(
self,
","sum: Tensor
sum_squares: Tensor
    running_mean: Tensor
    running_var: Tensor
def __init__(
self,
"
518,"def quantization_pertensor_hook(
    process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_tensor`` logic to DDP using ``allgather``
","def quantization_pertensor_hook(
    process_group: dist.ProcessGroup, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_tensor`` logic to DDP using ``allgather``
"
519,"try:
from urllib.parse import urlparse, urlunparse
except ImportError:
    from urlparse import urlparse, urlunparse
import torch._six as six
import numbers
import os
import sys
from torch._C._distributed_c10d import FileStore
from .constants import default_pg_timeout
","try:
from urllib.parse import urlparse, urlunparse
except ImportError:
    raise ImportError(""urllib cannot be found, urlparse from python2 is no longer supported."")
import torch._six as six
import numbers
import os
import sys
from datetime import timedelta
from typing import Optional, Dict, Union
from torch._C._distributed_c10d import FileStore
from .constants import default_pg_timeout
"
520,"import tempfile
import torch
from torch.distributed.nn.jit.templates.remote_module_template import (
REMOTE_MODULE_TEMPLATE,
)
","import tempfile
import torch
from typing import Optional
from torch.distributed.nn.jit.templates.remote_module_template import (
REMOTE_MODULE_TEMPLATE,
)
"
521,"MPI = ""mpi""
TCP = ""tcp""
    def __new__(cls, name):
if not isinstance(name, string_classes):
raise ValueError(""Backend name must be a string, but got: {}"".format(name))
value = getattr(Backend, name.upper(), Backend.UNDEFINED)
","MPI = ""mpi""
TCP = ""tcp""
    def __new__(cls, name: str):
if not isinstance(name, string_classes):
raise ValueError(""Backend name must be a string, but got: {}"".format(name))
value = getattr(Backend, name.upper(), Backend.UNDEFINED)
"
522,"""torch.distributed.ReduceOp instead"")
return object.__getattribute__(self, key)
reduce_op = reduce_op()
class group(object):
","""torch.distributed.ReduceOp instead"")
return object.__getattribute__(self, key)
reduce_op = _reduce_op()
class group(object):
"
523,"all_gather(output_tensors, input_tensor, group=group)
# Deserialize outputs back to object.
for i, tensor in enumerate(output_tensors):
        tensor = tensor.type(torch.ByteTensor)
tensor_size = object_size_list[i]
object_list[i] = _tensor_to_object(tensor, tensor_size)
","all_gather(output_tensors, input_tensor, group=group)
# Deserialize outputs back to object.
for i, tensor in enumerate(output_tensors):
        tensor = tensor.type(torch.ByteTensor)  # type:ignore[call-overload]
tensor_size = object_size_list[i]
object_list[i] = _tensor_to_object(tensor, tensor_size)
"
524,"opts.rootRank = dst
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.gather(output_tensors, input_tensors, opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
","opts.rootRank = dst
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.gather(output_tensors, input_tensors, opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
"
525,"opts.reduceOp = op
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.reduce_scatter([output], [input_list], opts)
else:
work = group.reduce_scatter([output], [input_list], opts)
","opts.reduceOp = op
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.reduce_scatter([output], [input_list], opts)
else:
work = group.reduce_scatter([output], [input_list], opts)
"
526,"def remainder(g, input, other):
div = g.op(""Div"", input, other)
if sym_help._is_fp(input):
div = g.op(""Floor"", div)
","def remainder(g, input, other):
    # torch.remainder does not follow regular type promotion logic.
    # Instead, it is implicitly casting `other` to the same type of `input`.
    input_scalar_type = input.type().scalarType()
    if input_scalar_type:
        other = g.op(""Cast"", other, to_i=sym_help.cast_pytorch_to_onnx[input_scalar_type])
div = g.op(""Div"", input, other)
if sym_help._is_fp(input):
div = g.op(""Floor"", div)
"
527,"parser.add_argument('--profiling_tensor_size', default=1, type=int)
parser.add_argument('--workload', default='loop', type=str)
parser.add_argument('--internal_iter', default=256, type=int)
    parser.add_argument('--n', default=100, type=int)
    parser.add_argument('--use_timer', action='store_true')
parser.add_argument('--timer_min_run_time', default=100, type=int)
args = parser.parse_args()
","parser.add_argument('--profiling_tensor_size', default=1, type=int)
parser.add_argument('--workload', default='loop', type=str)
parser.add_argument('--internal_iter', default=256, type=int)
parser.add_argument('--timer_min_run_time', default=100, type=int)
args = parser.parse_args()
"
528,"print(""No CUDA available"")
sys.exit()
    print(""Payload: {}; {} iterations, N = {}\n"".format(
        args.workload, args.internal_iter, args.n))
INTERNAL_ITER = args.internal_iter
for profiling_enabled in [False, True]:
","print(""No CUDA available"")
sys.exit()
    print(""Payload: {}, {} iterations; timer min. runtime = {}\n"".format(
        args.workload, args.internal_iter, args.timer_min_run_time))
INTERNAL_ITER = args.internal_iter
for profiling_enabled in [False, True]:
"
529,"next(names_iter)  # skip self
args.append(self.root)
def proxy_placeholder(name: str):
            return self.create_proxy('placeholder', name, (), {},
type_expr=fn_for_analysis.__annotations__.get(name, None))
args.extend(proxy_placeholder(next(names_iter)) for _ in range(skip_arg_idx, total_args))
","next(names_iter)  # skip self
args.append(self.root)
        sig = inspect.signature(fn_for_analysis)

def proxy_placeholder(name: str):
            if name[0] == '*':
                default = ()    # type: ignore
            else:
                param = sig.parameters[name]
                default = () if param.default is inspect.Parameter.empty else (param.default,)  # type: ignore
            return self.create_proxy('placeholder', name, default, {},
type_expr=fn_for_analysis.__annotations__.get(name, None))
args.extend(proxy_placeholder(next(names_iter)) for _ in range(skip_arg_idx, total_args))
"
530,"``allreduce`` protocol. It works only with flattened grads.
Example::
            >>> ddp_model._register_comm_hook(process_group, quantization_pertensor_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
","``allreduce`` protocol. It works only with flattened grads.
Example::
            >>> ddp_model.register_comm_hook(process_group, quantization_pertensor_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
"
531,">>>     fut.set_result(bucket.get_tensors())
>>>     return fut
            >>> ddp._register_comm_hook(state = None, hook = noop)
Example::
Below is an example of a Parallel SGD algorithm where gradients are encoded before
",">>>     fut.set_result(bucket.get_tensors())
>>>     return fut
            >>> ddp.register_comm_hook(state = None, hook = noop)
Example::
Below is an example of a Parallel SGD algorithm where gradients are encoded before
"
532,"cls = type(self)
cls.forward = _forward_from_src(self.code)
def __reduce__(self):
dict_without_graph = self.__dict__.copy()
del dict_without_graph['_graph']
","cls = type(self)
cls.forward = _forward_from_src(self.code)
        cls_call = cls.__call__

        def print_full_traceback(exctype, value, tb):
            traceback.print_exception(exctype, value, tb)

        def wrapped_call(self, *args, **kwargs):
            old_excepthook = sys.excepthook
            try:
                sys.excepthook = print_full_traceback
                return cls_call(self, *args, **kwargs)
            finally:
                sys.excepthook = old_excepthook
        cls.__call__ = wrapped_call

def __reduce__(self):
dict_without_graph = self.__dict__.copy()
del dict_without_graph['_graph']
"
533,"If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
for :attr:`padding` number of points.
The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:
 a single ``int`` -- in which case the same value is used for the height and width dimension
","If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
for :attr:`padding` number of points.
    Note:
        When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
        or the input. Sliding windows that would start in the right padded region are ignored.

The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:
"
534,"total_size_of_input_nodes = get_extra_size_of(node, partition.nodes)
partition_to_left_mem_bytes[partition] = device.available_mem_bytes
partition.logical_device_ids.append(device.logical_id)
                    partition.nodes.add(node)
partition_to_left_mem_bytes[partition] -= total_size_of_input_nodes
partition.used_mem_bytes += total_size_of_input_nodes
# No device left, create single node partitions
","total_size_of_input_nodes = get_extra_size_of(node, partition.nodes)
partition_to_left_mem_bytes[partition] = device.available_mem_bytes
partition.logical_device_ids.append(device.logical_id)
                    partition.add_node(node)
partition_to_left_mem_bytes[partition] -= total_size_of_input_nodes
partition.used_mem_bytes += total_size_of_input_nodes
# No device left, create single node partitions
"
535,"class NativeArgument:
type: str
name: str
    # Native function arguments have defaults for some reasons (e.g.,
    # the function prototypes in CPUType.h are defaulted).  There isn't
    # really any good reason to do this, as these functions are only
    # ever called from a context where all defaulted arguments are
    # guaranteed to be given explicitly.
    # TODO: Remove this
default: Optional[str]
argument: Union[Argument, TensorOptionsArguments]
","class NativeArgument:
type: str
name: str
    # Native function arguments have defaults to make it a little
    # easier to call them directly to bypass dispatch.
default: Optional[str]
argument: Union[Argument, TensorOptionsArguments]
"
536,"#     (as would be the case if you directly registered native::
#     functions).
#
# {dispatch}Type.h
#   - In principle, this file shouldn't exist at all; historically,
#     it existed so that we could directly access these functions
#     outside of the registration API for the implementation of
#     static dispatch.  Should be deleted now!
#
# This function is also used for a secondary purpose: the registration
# logic is also reused to implement per-operator registration.
def compute_type_method(
dispatch: Optional[str], *,
target: Target,
# Selector object to determine which operators to generate
# registration code for.
","#     (as would be the case if you directly registered native::
#     functions).
#
# This function is also used for a secondary purpose: the registration
# logic is also reused to implement per-operator registration.
def compute_type_method(
dispatch: Optional[str], *,
    # TODO: Give more precise type Union[Literal[Target.DEFINITION,
    # Target.REGISTRATION]]; requires Literal from typing_extensions
    # which we don't have a dep for yet.
target: Target,
# Selector object to determine which operators to generate
# registration code for.
"
537,"args_str = ', '.join(map(str, args))
dispatch_to_all_backends = dispatch is not None and dispatch in KEYWORD_ALL_BACKENDS
        if target is Target.DECLARATION:
            assert dispatch is not None
            return f""{returns_type} {name}({args_str});""
        elif target is Target.DEFINITION:
assert dispatch is not None
impl_name = f""at::native::{f.dispatch[dispatch]}""
","args_str = ', '.join(map(str, args))
dispatch_to_all_backends = dispatch is not None and dispatch in KEYWORD_ALL_BACKENDS
        if target is Target.DEFINITION:
assert dispatch is not None
impl_name = f""at::native::{f.dispatch[dispatch]}""
"
538,"elif target is Target.REGISTRATION:
if dispatch is None:
return f'm.def({cpp_string(str(f.func))});\n'
            elif def_only or f.manual_kernel_registration:
return None
else:
if dispatch_to_all_backends:
","elif target is Target.REGISTRATION:
if dispatch is None:
return f'm.def({cpp_string(str(f.func))});\n'
            elif f.manual_kernel_registration:
return None
else:
if dispatch_to_all_backends:
"
539,"return quantizer.quantized_graph.create_node(
'call_function', op, load_arg(quantized=True)(self.add_node.args), kwargs)
@register_quant_pattern(operator.mul)
@register_quant_pattern(torch.mul)
@register_quant_pattern((torch.nn.ReLU, operator.mul))
","return quantizer.quantized_graph.create_node(
'call_function', op, load_arg(quantized=True)(self.add_node.args), kwargs)
# TODO: merge with Add
@register_quant_pattern(operator.mul)
@register_quant_pattern(torch.mul)
@register_quant_pattern((torch.nn.ReLU, operator.mul))
"
540,"# of quantizable objects (e.g. modules and functionals)
class DefaultQuantizeHandler(QuantizeHandler):
def convert(self, quantizer, node):
        assert self.all_nodes
root_module = quantizer.modules['']
return quantize_node(
root_module,
","# of quantizable objects (e.g. modules and functionals)
class DefaultQuantizeHandler(QuantizeHandler):
def convert(self, quantizer, node):
        assert self.all_node_args
root_module = quantizer.modules['']
return quantize_node(
root_module,
"
541,"op._set_backward_test(run_backward)
op.init(**init_dict)
input_name = None
# _num_inputs_require_grads is used to track the number of tensors
","op._set_backward_test(run_backward)
op.init(**init_dict)
        if not run_backward:
            for _, attr in vars(op).items():
                if isinstance(attr, torch.nn.Module):
                    for param in attr.parameters():
                        param.requires_grad = False

input_name = None
# _num_inputs_require_grads is used to track the number of tensors
"
542,"else:
post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
                post_cflags += COMMON_HIPCC_FLAGS
append_std14_if_no_std_present(post_cflags)
cuda_post_cflags = None
","else:
post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
                post_cflags = COMMON_HIP_FLAGS + post_cflags
append_std14_if_no_std_present(post_cflags)
cuda_post_cflags = None
"
543,"else:
raise NotImplementedError(""Cannot fuse eval modules: {}"".format((conv, bn, relu)))
OP_LIST_TO_FUSER_METHOD : Dict[Tuple, Union[nn.Sequential, Callable]] = {
(nn.Conv1d, nn.BatchNorm1d): fuse_conv_bn,
(nn.Conv1d, nn.BatchNorm1d, nn.ReLU): fuse_conv_bn_relu,
(nn.Conv2d, nn.BatchNorm2d): fuse_conv_bn,
","else:
raise NotImplementedError(""Cannot fuse eval modules: {}"".format((conv, bn, relu)))
DEFAULT_OP_LIST_TO_FUSER_METHOD : Dict[Tuple, Union[nn.Sequential, Callable]] = {
(nn.Conv1d, nn.BatchNorm1d): fuse_conv_bn,
(nn.Conv1d, nn.BatchNorm1d, nn.ReLU): fuse_conv_bn_relu,
(nn.Conv2d, nn.BatchNorm2d): fuse_conv_bn,
"
544,"If :attr:`indices_or_sections` is a list of ints, :attr:`input` is split along
dimension :attr:`dim` at each of the indices in the list. For instance,
        :code:`[2, 3]` and :code:`dim=0` would result in the following tensors:

            - :code:`input[:2]`
            - :code:`input[2:3]`
            - :code:`input[3:]`
dim (int, optional): dimension along which to split the tensor. Default: ``0``
","If :attr:`indices_or_sections` is a list of ints, :attr:`input` is split along
dimension :attr:`dim` at each of the indices in the list. For instance,
        :code:`indices_or_sections=[2, 3]` and :code:`dim=0` would result in the tensors
        :code:`input[:2]`, :code:`input[2:3]`, and :code:`input[3:]`.
dim (int, optional): dimension along which to split the tensor. Default: ``0``
"
545,"import argparse
import re
from itertools import groupby
from ..autograd.gen_autograd import load_aten_declarations
from ..autograd.gen_autograd import RETURNS_VIEWS_OF_INPUT
from ..autograd.utils import CodeTemplate, write, is_out_variant, op_name_without_overload
","import argparse
import re
from itertools import groupby
from functools import reduce
from ..autograd.gen_autograd import load_aten_declarations
from ..autograd.gen_autograd import RETURNS_VIEWS_OF_INPUT
from ..autograd.utils import CodeTemplate, write, is_out_variant, op_name_without_overload
"
546,"raise NotImplementedError
@property
    def support(self):
""""""
Returns a :class:`~torch.distributions.constraints.Constraint` object
representing this distribution's support.
","raise NotImplementedError
@property
    def support(self) -> Optional[Any]:
""""""
Returns a :class:`~torch.distributions.constraints.Constraint` object
representing this distribution's support.
"
547,"from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import _sum_rightmost

class Independent(Distribution):
r""""""
","from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import _sum_rightmost
from typing import Dict
class Independent(Distribution):
r""""""
"
548,"from torch._six import inf
from torch.distributions.distribution import Distribution
from torch.distributions import Categorical
from numbers import Number
from torch.distributions import constraints
from torch.distributions.utils import broadcast_all
","from torch._six import inf
from torch.distributions.distribution import Distribution
from torch.distributions import Categorical
from torch.distributions import constraints
from torch.distributions.utils import broadcast_all
"
549,"import math
from numbers import Number
import torch
","import math
from numbers import Real
from numbers import Number
import torch
"
550,"@constraints.dependent_property
def domain(self):
return self._inv.codomain
@constraints.dependent_property
def codomain(self):
return self._inv.domain
@property
def bijective(self):
return self._inv.bijective
@property
def sign(self):
return self._inv.sign
@property
def event_dim(self):
return self._inv.event_dim
@property
","@constraints.dependent_property
def domain(self):
        assert self._inv is not None
return self._inv.codomain
@constraints.dependent_property
def codomain(self):
        assert self._inv is not None
return self._inv.domain
@property
def bijective(self):
        assert self._inv is not None
return self._inv.bijective
@property
def sign(self):
        assert self._inv is not None
return self._inv.sign
@property
def event_dim(self):
        assert self._inv is not None
return self._inv.event_dim
@property
"
551,".. math::
(\text{base}^{\text{start}},
    \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps}})},
\ldots,
    \text{base}^{(\text{start} + (\text{steps} - 1) * \frac{\text{end} - \text{start}}{ \text{steps}})},
\text{base}^{\text{end}})
"""""" + """"""
",".. math::
(\text{base}^{\text{start}},
    \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})},
\ldots,
    \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})},
\text{base}^{\text{end}})
"""""" + """"""
"
552,"if should_drop(fn):
unused_fn_def = ast.parse(""def unused_fn(self: Any):\n\traise RuntimeError(\""Cannot call @unused methods\"")"")
if len(unused_fn_def.body) != 1 or not isinstance(unused_fn_def.body[0], ast.FunctionDef):
            raise RuntimeError(""Expected a single top-level function"")
unused_def = unused_fn_def.body[0]
fn_def.body = unused_def.body
# kwarg/vararg not supported by `build_def`
","if should_drop(fn):
unused_fn_def = ast.parse(""def unused_fn(self: Any):\n\traise RuntimeError(\""Cannot call @unused methods\"")"")
if len(unused_fn_def.body) != 1 or not isinstance(unused_fn_def.body[0], ast.FunctionDef):
            raise RuntimeError(f""Expected a single top-level function: {filename}:{file_lineno}"")
unused_def = unused_fn_def.body[0]
fn_def.body = unused_def.body
# kwarg/vararg not supported by `build_def`
"
553,"i += 2
return f'""{s}""'
return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
","i += 2
return f'""{s}""'

    if isinstance(t, OptionalType):
        if d == 'None':
            return 'c10::nullopt'

        return default_expr(d, t.elem)

    if isinstance(t, ListType):
        if (d.startswith('[') and d.endswith(']')):
            return '{' + d[1:-1] + '}'
        elif t.size is None:
            # NOTE: Sized lists can have scalar defaults
            raise ValueError(f""Expected a list default '[...]' but found: '{d}'"")

return JIT_TO_CPP_DEFAULT.get(d, d)
# Convert an argument into its C++ API form
"
554,"def _adaptive_pool(name, type, tuple_fn, fn=None):
    @parse_args('v', 'is')
def symbolic_fn(g, input, output_size):
# _adaptive_pool is supported for cases where output_size is 1 for all dimensions,
# by executing a GlobalPool.
","def _adaptive_pool(name, type, tuple_fn, fn=None):
def symbolic_fn(g, input, output_size):
# _adaptive_pool is supported for cases where output_size is 1 for all dimensions,
# by executing a GlobalPool.
"
555,"if mod != [0] * len(mod):
if output_size == [1] * len(output_size):
return g.op(""GlobalMaxPool"", input), None
            if sym_help._operator_export_type == torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK:
                return _unimplemented(name, 'output size that are not factor of input size')
            else:
                return sym_help._onnx_unsupported(name, ', since output size is not factor of input size')
k = [int(dim[i] / output_size[i]) for i in range(0, len(dim))]
# call max_poolxd_with_indices to get indices in the output
if type == ""MaxPool"":
","if mod != [0] * len(mod):
if output_size == [1] * len(output_size):
return g.op(""GlobalMaxPool"", input), None
            return _unimplemented(name, 'output size that are not factor of input size')
k = [int(dim[i] / output_size[i]) for i in range(0, len(dim))]
# call max_poolxd_with_indices to get indices in the output
if type == ""MaxPool"":
"
556,"beta is an optional parameter that defaults to 1.
The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.
Args:
","beta is an optional parameter that defaults to 1.
    Note: When beta is set to 0, this is equivalent to we call out directly to :class:`L1Loss`.
    Passing a negative value in for beta will result in an exception.

The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.
Args:
"
557,"env = {}
def load_arg(a):
        return map_arg(a, lambda node: env[node.name])
for producer_node in producer_nodes:
        env[producer_node.name] = graph.node_copy(producer_node, load_arg)
    graph.output(load_arg(producer_nodes[-1].name))
graph_module = GraphModule(root, graph)
return graph_module
","env = {}
def load_arg(a):
        return map_arg(a, lambda node: env[node])
for producer_node in producer_nodes:
        env[producer_node] = graph.node_copy(producer_node, load_arg)
    graph.output(load_arg(producer_nodes[-1]))
graph_module = GraphModule(root, graph)
return graph_module
"
558,"import torch
def set_fuser(fuser_name, executor_name):
    assert fuser_name in ['te', 'old', 'none']
if fuser_name == 'te':
torch._C._jit_set_profiling_executor(True)
torch._C._jit_set_profiling_mode(True)
","import torch
def set_fuser(fuser_name, executor_name):
    assert fuser_name in ['te', 'old', 'none', 'default']
if fuser_name == 'te':
torch._C._jit_set_profiling_executor(True)
torch._C._jit_set_profiling_mode(True)
"
559,"# Only tensor like arguments are eligible
device_of = next((f'{a.name}' for a in candidate_args if a.type.is_tensor_like()), None)
                # See Note [Byte-for-byte compatibility]
                # I wasn't able to figure out the internal logic for
                # these device guards
                if str(f.func.name) == ""_thnn_fused_lstm_cell_backward"":
                    device_of = ""cx""
                elif str(f.func.name) == ""_thnn_differentiable_lstm_cell_backward"":
                    device_of = ""input_gates""

has_tensor_options = any(isinstance(a.argument, TensorOptionsArguments) for a in args)
# TODO: There is probably a simpler version of this that
","# Only tensor like arguments are eligible
device_of = next((f'{a.name}' for a in candidate_args if a.type.is_tensor_like()), None)
has_tensor_options = any(isinstance(a.argument, TensorOptionsArguments) for a in args)
# TODO: There is probably a simpler version of this that
"
560,"if not def_only and not f.manual_kernel_registration and (dispatch is not None or f.dispatch is None):
# Figure out which signature the function is
if local.use_c10_dispatcher() is UseC10Dispatcher.full:
                    # See Note [Byte-for-byte compatibility]
                    if dispatch is not None:
                        nl = ""\n""
                    else:
                        nl = """"
payload = ""c10::impl::hacky_wrapper_for_legacy_signatures<"" \
                        f""{returns_type} ({dispatcher_args_types_str})>({nl}TORCH_FN({type_name}))""
else:
payload = f""torch::CppFunction::makeUnboxedOnly(&{type_name})""
","if not def_only and not f.manual_kernel_registration and (dispatch is not None or f.dispatch is None):
# Figure out which signature the function is
if local.use_c10_dispatcher() is UseC10Dispatcher.full:
payload = ""c10::impl::hacky_wrapper_for_legacy_signatures<"" \
                        f""{returns_type} ({dispatcher_args_types_str})>(TORCH_FN({type_name}))""
else:
payload = f""torch::CppFunction::makeUnboxedOnly(&{type_name})""
"
561,"WORKFLOW_DATA = BUILD_CONFIGS + [
    IOSNightlyJob(""binary"", is_upload=True),
]
","WORKFLOW_DATA = BUILD_CONFIGS + [
    # IOSNightlyJob(""binary"", is_upload=True),
]
"
562,"create_folder(JSON_FOLDER_BASE_DIR)
def print_init_info() -> None:
print_log(""pytorch folder: "", get_pytorch_folder())
print_log(""cpp test binaries folder: "", get_oss_binary_folder(TestType.CPP))
","create_folder(JSON_FOLDER_BASE_DIR)
def get_python_run_only(args_run_only: Optional[List[str]]) -> List[str]:
    # if user specifies run-only option
    if args_run_only:
        return args_run_only

    # if not specified, use default setting, different for gcc and clang
    if detect_compiler_type() == CompilerType.GCC:
        return [""run_test.py""]
    else:
        # for clang, some tests will result in too large intermidiate files that can't be merged by llvm, we need to skip them
        run_only: List[str] = []
        binary_folder = get_oss_binary_folder(TestType.PY)
        g = os.walk(binary_folder)
        for _, _, file_list in g:
            for file_name in file_list:
                if file_name in BLOCKED_PYTHON_TESTS or not file_name.endswith("".py""):
                    continue
                run_only.append(file_name)
            # only run tests in the first-level folder in test/
            break
        return run_only


def print_init_info() -> None:
print_log(""pytorch folder: "", get_pytorch_folder())
print_log(""cpp test binaries folder: "", get_oss_binary_folder(TestType.CPP))
"
563,"print_log(""pytorch folder: "", get_pytorch_folder())
print_log(""cpp test binaries folder: "", get_oss_binary_folder(TestType.CPP))
print_log(""python test scripts folder: "", get_oss_binary_folder(TestType.PY))
    print_log(""cov_type: "", get_cov_type())
print_log(
""llvm tool folder (only for clang, if you are using gcov please ignore it): "",
get_llvm_tool_path(),
","print_log(""pytorch folder: "", get_pytorch_folder())
print_log(""cpp test binaries folder: "", get_oss_binary_folder(TestType.CPP))
print_log(""python test scripts folder: "", get_oss_binary_folder(TestType.PY))
    print_log(""compiler type: "", detect_compiler_type().value)
print_log(
""llvm tool folder (only for clang, if you are using gcov please ignore it): "",
get_llvm_tool_path(),
"
564,"import time
from typing import Any, Dict, List, Set, Tuple
from ..util.setting import JSON_FOLDER_BASE_DIR, TestList, TestPlatform, TestStatusType
from ..util.utils import (
check_compiler_type,
    get_cov_type,
print_error,
print_time,
related_to_test_list,
","import time
from typing import Any, Dict, List, Set, Tuple
from ..util.setting import (
    JSON_FOLDER_BASE_DIR,
    CompilerType,
    TestList,
    TestPlatform,
    TestStatusType,
)
from ..util.utils import (
check_compiler_type,
    detect_compiler_type,
print_error,
print_time,
related_to_test_list,
"
565,"torch.Size([2, 2, 1, 2])
"""""".format(**common_args))
add_docstr(torch.std,
           r""""""
std(input, unbiased=True) -> Tensor
Returns the standard-deviation of all elements in the :attr:`input` tensor.
","torch.Size([2, 2, 1, 2])
"""""".format(**common_args))
add_docstr(torch.std, r""""""
std(input, unbiased=True) -> Tensor
Returns the standard-deviation of all elements in the :attr:`input` tensor.
"
566,"[ 4]])
"""""".format(**common_args))
add_docstr(torch.var,
           r""""""
var(input, unbiased=True) -> Tensor
Returns the variance of all elements in the :attr:`input` tensor.
","[ 4]])
"""""".format(**common_args))
add_docstr(torch.var, r""""""
var(input, unbiased=True) -> Tensor
Returns the variance of all elements in the :attr:`input` tensor.
"
567,"_EXCLUDE_QCONFIG_PROPAGATE_LIST
)
DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_WHITE_LIST = (
set(DEFAULT_MODULE_MAPPING.values())
| set(DEFAULT_QAT_MODULE_MAPPING.values())
| set(DEFAULT_DYNAMIC_MODULE_MAPPING.values())
","_EXCLUDE_QCONFIG_PROPAGATE_LIST
)
DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_ALLOWED_LIST = (
set(DEFAULT_MODULE_MAPPING.values())
| set(DEFAULT_QAT_MODULE_MAPPING.values())
| set(DEFAULT_DYNAMIC_MODULE_MAPPING.values())
"
568,"return DictType(key, value)
if is_optional(ann):
if issubclass(ann.__args__[1], type(None)):
            valid_type = try_ann_to_type(ann.__args__[0], loc)
else:
            valid_type = try_ann_to_type(ann.__args__[1], loc)
        assert valid_type, ""Unsupported annotation {} could not be resolved."".format(repr(ann))
return OptionalType(valid_type)
if torch.distributed.rpc.is_available() and is_rref(ann):
return RRefType(try_ann_to_type(ann.__args__[0], loc))
","return DictType(key, value)
if is_optional(ann):
if issubclass(ann.__args__[1], type(None)):
            contained = ann.__args__[0]
else:
            contained = ann.__args__[1]
        valid_type = try_ann_to_type(contained, loc)
        msg = ""Unsupported annotation {} could not be resolved because {} could not be resolved.""
        assert valid_type, msg.format(repr(ann), repr(contained))
return OptionalType(valid_type)
if torch.distributed.rpc.is_available() and is_rref(ann):
return RRefType(try_ann_to_type(ann.__args__[0], loc))
"
569,"r""""""
Generator(device='cpu') -> Generator
Creates and returns a generator object which manages the state of the algorithm that
produces pseudo random numbers. Used as a keyword argument in many :ref:`inplace-random-sampling`
functions.
","r""""""
Generator(device='cpu') -> Generator
Creates and returns a generator object that manages the state of the algorithm which
produces pseudo random numbers. Used as a keyword argument in many :ref:`inplace-random-sampling`
functions.
"
570,"r""""""
quantize_per_tensor(input, scale, zero_point, dtype) -> Tensor
Converts a float tensor to quantized tensor with given scale and zero point.
Arguments:
input (Tensor): float tensor to quantize
","r""""""
quantize_per_tensor(input, scale, zero_point, dtype) -> Tensor
Converts a float tensor to a quantized tensor with given scale and zero point.
Arguments:
input (Tensor): float tensor to quantize
"
571,"import warnings
import torch._six
","import collections
import warnings
import torch._six
"
572,"if not torch.jit.is_scripting():
if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors):
return handle_torch_function(broadcast_tensors, tensors, *tensors)
    return _VF.broadcast_tensors(tensors)
def split(tensor, split_size_or_sections, dim=0):
","if not torch.jit.is_scripting():
if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors):
return handle_torch_function(broadcast_tensors, tensors, *tensors)
    return _VF.broadcast_tensors(tensors)  # type: ignore
def split(tensor, split_size_or_sections, dim=0):
"
573,"return handle_torch_function(einsum, operands, equation, *operands)
if len(operands) == 1 and isinstance(operands[0], (list, tuple)):
# the old interface of passing the operands as one list argument
        operands = operands[0]
# recurse incase operands contains value that has torch function
# in the original implementation this line is omitted
        return einsum(equation, *operands)
    return _VF.einsum(equation, operands)
def meshgrid(*tensors):
r""""""Take :math:`N` tensors, each of which can be either scalar or 1-dimensional
vector, and create :math:`N` N-dimensional grids, where the :math:`i` :sup:`th` grid is defined by
expanding the :math:`i` :sup:`th` input over dimensions defined by other inputs.
","return handle_torch_function(einsum, operands, equation, *operands)
if len(operands) == 1 and isinstance(operands[0], (list, tuple)):
# the old interface of passing the operands as one list argument
        _operands = operands[0]
# recurse incase operands contains value that has torch function
# in the original implementation this line is omitted
        return einsum(equation, *_operands)

    return _VF.einsum(equation, operands)  # type: ignore

if TYPE_CHECKING:
    # The JIT doesn't understand Union, so only add type annotation for mypy
    def meshgrid(*tensors: Union[Tensor, List[Tensor]]) -> Tuple[Tensor, ...]:
        return _meshgrid(*tensors)
else:
    def meshgrid(*tensors):
        return _meshgrid(*tensors)
def _meshgrid(*tensors):
r""""""Take :math:`N` tensors, each of which can be either scalar or 1-dimensional
vector, and create :math:`N` N-dimensional grids, where the :math:`i` :sup:`th` grid is defined by
expanding the :math:`i` :sup:`th` input over dimensions defined by other inputs.
"
574,"window=window, center=center, normalized=normalized, onesided=onesided,
length=length)
    return _VF.istft(
        input, n_fft, hop_length, win_length, window, center, normalized, onesided, length)
del torch.unique_dim
def _unique_impl(input, sorted=True, return_inverse=False, return_counts=False, dim=None):
    # type: (Tensor, bool, bool, bool, Optional[int]) -> Tuple[Tensor, Tensor, Tensor]
r""""""Returns the unique elements of the input tensor.
.. note:: This function is different from :func:`torch.unique_consecutive` in the sense that
","window=window, center=center, normalized=normalized, onesided=onesided,
length=length)
    return _VF.istft(input, n_fft, hop_length, win_length, window, center, normalized, onesided, length)  # type: ignore
del torch.unique_dim
if TYPE_CHECKING:
    # These _impl functions return a variable number of tensors as output with
    # __torch_function__; tuple unpacking is done already rather than being
    # done by the caller of the _impl function
    _unique_impl_out = Any
else:
    _unique_impl_out = Tuple[Tensor, Tensor, Tensor]


def _unique_impl(input: Tensor, sorted: bool = True,
                 return_inverse: bool = False, return_counts: bool = False,
                 dim: Optional[int] = None) -> _unique_impl_out:
r""""""Returns the unique elements of the input tensor.
.. note:: This function is different from :func:`torch.unique_consecutive` in the sense that
"
575,"return handle_torch_function(
unique_consecutive, (input,), input, return_inverse=return_inverse,
return_counts=return_counts, dim=dim)
    output, inverse_indices, counts = _VF.unique_consecutive(
input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
return output, inverse_indices, counts
","return handle_torch_function(
unique_consecutive, (input,), input, return_inverse=return_inverse,
return_counts=return_counts, dim=dim)
    output, inverse_indices, counts = _VF.unique_consecutive(  # type: ignore
input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
return output, inverse_indices, counts
"
576,"""""""
if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors):
return handle_torch_function(block_diag, tensors, *tensors)
    return torch._C._VariableFunctions.block_diag(tensors)
def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):
","""""""
if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors):
return handle_torch_function(block_diag, tensors, *tensors)
    return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore
def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):
"
577,"torch.fbgemm_pack_quantized_matrix: lambda input, a, b: -1,
torch.feature_alpha_dropout: lambda input, p, train: -1,
torch.feature_dropout: lambda input, p, train: -1,
torch.fft: lambda input, signal_ndim, normalized=False: -1,
torch.flatten: lambda input, start_dim=0, end_dim=-1: -1,
torch.flip: lambda input, dims: -1,
","torch.fbgemm_pack_quantized_matrix: lambda input, a, b: -1,
torch.feature_alpha_dropout: lambda input, p, train: -1,
torch.feature_dropout: lambda input, p, train: -1,
        torch.fix: lambda input, out=None: -1,
torch.fft: lambda input, signal_ndim, normalized=False: -1,
torch.flatten: lambda input, start_dim=0, end_dim=-1: -1,
torch.flip: lambda input, dims: -1,
"
578,"Arguments:
backend (BackendType, optional): The type of RPC backend
implementation. Supported values include
                ``BackendType.PROCESS_GROUP`` (the default) and
                ``BackendType.TENSORPIPE``. See :ref:`rpc-backends` for more
information.
name (str): a globally unique name of this node. (e.g.,
``Trainer3``, ``ParameterServer2``, ``Master``, ``Worker1``)
","Arguments:
backend (BackendType, optional): The type of RPC backend
implementation. Supported values include
                ``BackendType.TENSORPIPE`` (the default) and
                ``BackendType.PROCESS_GROUP``. See :ref:`rpc-backends` for more
information.
name (str): a globally unique name of this node. (e.g.,
``Trainer3``, ``ParameterServer2``, ``Master``, ``Worker1``)
"
579,"Attributes:
freeze_bn:
        activation_post_process: fake quant module for output activation
weight_fake_quant: fake quant module for weight
""""""
","Attributes:
freeze_bn:
weight_fake_quant: fake quant module for weight
""""""
"
580,"default.
Attributes:
        activation_post_process: fake quant module for output activation
weight: fake quant module for weight
Examples::
","default.
Attributes:
weight: fake quant module for weight
Examples::
"
581,"class ConvBn2d(_ConvBnNd, nn.Conv2d):
r""""""
A ConvBn2d module is a module fused from Conv2d and BatchNorm2d,
    attached with FakeQuantize modules for weight,
used in quantization aware training.
We combined the interface of :class:`torch.nn.Conv2d` and
","class ConvBn2d(_ConvBnNd, nn.Conv2d):
r""""""
A ConvBn2d module is a module fused from Conv2d and BatchNorm2d,
    attached with FakeQuantize modules for both output activation and weight,
used in quantization aware training.
We combined the interface of :class:`torch.nn.Conv2d` and
"
582,"class ConvReLU2d(nnqat.Conv2d):
r""""""
A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with
    FakeQuantize modules for both output activation and weight for
quantization aware training.
We combined the interface of :class:`~torch.nn.Conv2d` and
:class:`~torch.nn.BatchNorm2d`.
Attributes:
        activation_post_process: fake quant module for output activation
weight_fake_quant: fake quant module for weight
""""""
","class ConvReLU2d(nnqat.Conv2d):
r""""""
A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with
    FakeQuantize modules for weight for
quantization aware training.
We combined the interface of :class:`~torch.nn.Conv2d` and
:class:`~torch.nn.BatchNorm2d`.
Attributes:
weight_fake_quant: fake quant module for weight
""""""
"
583,"super(Linear, self).__init__(in_features, out_features, bias)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
        self.activation_post_process = qconfig.activation()
self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self.activation_post_process(
            F.linear(input, self.weight_fake_quant(self.weight), self.bias))
@classmethod
def from_float(cls, mod, qconfig=None):
","super(Linear, self).__init__(in_features, out_features, bias)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return F.linear(input, self.weight_fake_quant(self.weight), self.bias)
@classmethod
def from_float(cls, mod, qconfig=None):
"
584,"n (integer, optional): if :math:`X` is not specified then `n`
specifies the size of the generated random
approximation of eigenvectors. Default value for `n`
                  is `k`. If :math:`X` is specifed, the value of `n`
(when specified) must be the number of :math:`X`
columns.
","n (integer, optional): if :math:`X` is not specified then `n`
specifies the size of the generated random
approximation of eigenvectors. Default value for `n`
                  is `k`. If :math:`X` is specified, the value of `n`
(when specified) must be the number of :math:`X`
columns.
"
585,":meth:`~torch.nn.ModuleDict.update`).
Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict`` before Python verision 3.6) does not
preserve the order of the merged mapping.
Arguments:
",":meth:`~torch.nn.ModuleDict.update`).
Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict`` before Python version 3.6) does not
preserve the order of the merged mapping.
Arguments:
"
586,"corresponding_topts.append(
{'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
'kwarg_only': True, 'default': 'at::kLong'})
def check_topt_representation(topt_representation):
for idx, supported_topt in enumerate(supported_topt_arguments):
","corresponding_topts.append(
{'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
'kwarg_only': True, 'default': 'at::kLong'})
    corresponding_topts.append(
        {'type': 'TensorOptions', 'name': 'options', 'is_nullable': False, 'annotation': None,
         'kwarg_only': True})
def check_topt_representation(topt_representation):
for idx, supported_topt in enumerate(supported_topt_arguments):
"
587,"class QFunctional(torch.nn.Module):
    r""""""Wrapper class for quantized operatitons.
The instance of this class can be used instead of the
``torch.ops.quantized`` prefix. See example usage below.
","class QFunctional(torch.nn.Module):
    r""""""Wrapper class for quantized operations.
The instance of this class can be used instead of the
``torch.ops.quantized`` prefix. See example usage below.
"
588,"If :math:`y = 1` then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for :math:`y = -1`.
    The loss function for each sample in the mini-batch is:
.. math::
        \text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})
Args:
margin (float, optional): Has a default value of :math:`0`.
","If :math:`y = 1` then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for :math:`y = -1`.
    The loss function for each pair of samples in the mini-batch is:
.. math::
        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})
Args:
margin (float, optional): Has a default value of :math:`0`.
"
589,"[-0.2098, -0.6699],
[ 0.3470, -0.9451],
[-0.5174, -1.3136]])
"""""")
add_docstr(torch.view_as_complex,
r""""""
","[-0.2098, -0.6699],
[ 0.3470, -0.9451],
[-0.5174, -1.3136]])
"""""".format(**common_args))
add_docstr(torch.view_as_complex,
r""""""
"
590,"torch.half,         # 5
torch.float,        # 6
torch.double,       # 7
torch.complex64,    # 9
torch.complex128,   # 10
torch.bool,         # 11
]

def _cast_func_template(to_i, g, input, non_blocking):
return g.op(""Cast"", input, to_i=to_i)
","torch.half,         # 5
torch.float,        # 6
torch.double,       # 7
    torch.complex32,    # 8
torch.complex64,    # 9
torch.complex128,   # 10
torch.bool,         # 11
]
def _cast_func_template(to_i, g, input, non_blocking):
return g.op(""Cast"", input, to_i=to_i)
"
591,"cast_pytorch_to_onnx[""ComplexDouble""],
cast_pytorch_to_onnx[""Bool""],
]
# Global set to store the list of quantized operators in the network.
# This is currently only used in the conversion of quantized ops from PT -> C2 via ONNX.
_quantized_ops = set()
","cast_pytorch_to_onnx[""ComplexDouble""],
cast_pytorch_to_onnx[""Bool""],
]

# Global set to store the list of quantized operators in the network.
# This is currently only used in the conversion of quantized ops from PT -> C2 via ONNX.
_quantized_ops = set()
"
592,"and distributed autograd.
Initializes the local RPC agent which immediately makes the current
        process ready to send and receive RPCs. This method also properly
        initializes a default process group backend that uses Gloo for
        communication.
Arguments:
backend (Enum): type of RPC backend implementation. Currently,
","and distributed autograd.
Initializes the local RPC agent which immediately makes the current
        process ready to send and receive RPCs.
Arguments:
backend (Enum): type of RPC backend implementation. Currently,
"
593,"import enum
import torch.distributed as dist
import torch.distributed.distributed_c10d as dc10d
from . import constants as rpc_constants
","import enum
import torch.distributed as dist
from . import constants as rpc_constants
"
594,"# collective operations, for which it relies on a process group, instead of
# re-implementing this on top of RPCs.
    # Initialize ProcessGroup.
    if dist.is_initialized():
        raise RuntimeError(
            ""Default process group must not be initialized before init_rpc.""
        )
    process_group_timeout = rpc_constants.DEFAULT_PROCESS_GROUP_TIMEOUT

    dist.init_process_group(
        backend=dist.Backend.GLOO,
        store=store,
        rank=rank,
        world_size=world_size,
        timeout=process_group_timeout,
)
    try:
        group = dc10d._get_default_group()
        assert group is not None, ""Failed to initialize default ProcessGroup.""

        if (rank != -1) and (rank != group.rank()):
            raise RuntimeError(
                ""rank argument {} doesn't match pg rank {}"".format(rank, group.rank())
            )
        if (world_size != -1) and (world_size != group.size()):
            raise RuntimeError(
                ""world_size argument {} doesn't match pg size {}"".format(
                    world_size, group.size()
                )
            )
        # TODO: add try-except and destroy _agent in all processes if any fails.
        return TensorPipeAgent(
            store, name, rank, world_size, group, rpc_backend_options
        )
    except Exception as ex:
        dist.destroy_process_group()
        raise ex

register_backend(
""TENSORPIPE"",
","# collective operations, for which it relies on a process group, instead of
# re-implementing this on top of RPCs.
    group = _init_process_group(store, rank, world_size)
    # TODO: add try-except and destroy _agent in all processes if any fails.
    return TensorPipeAgent(
        store, name, rank, world_size, group, rpc_backend_options
)
register_backend(
""TENSORPIPE"",
"
595,"assert not inplace, ""The inplace support is still in development""
_check_is_script_module(model)
model.eval()
    model = wrap_cpp_module(torch._C._jit_pass_insert_quant_dequant(model._c, 'forward', inplace, debug, quant_type))
if not debug:
model = wrap_cpp_module(torch._C._jit_pass_quant_finalize(model._c, quant_type))
return model
","assert not inplace, ""The inplace support is still in development""
_check_is_script_module(model)
model.eval()
    model = wrap_cpp_module(torch._C._jit_pass_insert_quant_dequant(model._c, 'forward', inplace, quant_type))
if not debug:
model = wrap_cpp_module(torch._C._jit_pass_quant_finalize(model._c, quant_type))
return model
"
596,"needed_modules = set()
FACTORY_PARAMS = ""dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False""
# this could be more precise w.r.t list contents etc. How to do Ellipsis?
INDICES = ""indices: Union[None, _int, slice, Tensor, List, Tuple]""
","needed_modules = set()
DEVICE_PARAM = ""device: Union[_device, str, None]=None""
FACTORY_PARAMS = f""dtype: Optional[_dtype]=None, {DEVICE_PARAM}, requires_grad: _bool=False""
# this could be more precise w.r.t list contents etc. How to do Ellipsis?
INDICES = ""indices: Union[None, _int, slice, Tensor, List, Tuple]""
"
597,"super(BuildExtension, self).__init__(*args, **kwargs)
self.no_python_abi_suffix = kwargs.get(""no_python_abi_suffix"", False)
        self.use_ninja = kwargs.get('use_ninja', False if IS_HIP_EXTENSION else True)
if self.use_ninja:
# Test if we can use ninja. Fallback otherwise.
msg = ('Attempted to use ninja as the BuildExtension backend but '
","super(BuildExtension, self).__init__(*args, **kwargs)
self.no_python_abi_suffix = kwargs.get(""no_python_abi_suffix"", False)
        self.use_ninja = kwargs.get('use_ninja', True)
if self.use_ninja:
# Test if we can use ninja. Fallback otherwise.
msg = ('Attempted to use ninja as the BuildExtension backend but '
"
598,"from .throughput_benchmark import ThroughputBenchmark
# Set the module for a given object for nicer printing
def set_module(obj, mod):
if not isinstance(mod, str):
raise TypeError(""The mod argument should be a string"")
obj.__module__ = mod
","from .throughput_benchmark import ThroughputBenchmark
import os.path as _osp

# Set the module for a given object for nicer printing
def set_module(obj, mod):
if not isinstance(mod, str):
raise TypeError(""The mod argument should be a string"")
obj.__module__ = mod

#: Path to folder containing CMake definitions for Torch package
cmake_prefix_path = _osp.join(_osp.dirname(_osp.dirname(__file__)), 'share', 'cmake')
"
599,"Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
be a parent of two other intervals.
        If for any reason two intervals intersect only partialy, this function
will not record a parent child relationship between then.
""""""
if self.cpu_children_populated:
","Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
be a parent of two other intervals.
        If for any reason two intervals intersect only partially, this function
will not record a parent child relationship between then.
""""""
if self.cpu_children_populated:
"
600,"# TODO: proper overriding analysis when implementing class inheritance
methods = inspect.getmembers(
cls, predicate=lambda m: (inspect.ismethod(m) or inspect.isfunction(m)) and m.__name__ in cls.__dict__)
    print(methods)
method_defs = [get_jit_def(method[1],
method[0],
","# TODO: proper overriding analysis when implementing class inheritance
methods = inspect.getmembers(
cls, predicate=lambda m: (inspect.ismethod(m) or inspect.isfunction(m)) and m.__name__ in cls.__dict__)
method_defs = [get_jit_def(method[1],
method[0],
"
601,"continue
if isinstance(v, int) or isinstance(v, float):
ssi.hparams[k].number_value = v
continue
if isinstance(v, string_types):
ssi.hparams[k].string_value = v
continue
if isinstance(v, bool):
ssi.hparams[k].bool_value = v
continue
if isinstance(v, torch.Tensor):
v = make_np(v)[0]
ssi.hparams[k].number_value = v
continue
raise ValueError('value should be one of int, float, str, bool, or torch.Tensor')
","continue
if isinstance(v, int) or isinstance(v, float):
ssi.hparams[k].number_value = v
            hps.append(HParamInfo(name=k, type=DataType.Value(""DATA_TYPE_FLOAT64"")))
continue
if isinstance(v, string_types):
ssi.hparams[k].string_value = v
            hps.append(HParamInfo(name=k, type=DataType.Value(""DATA_TYPE_STRING"")))
continue
if isinstance(v, bool):
ssi.hparams[k].bool_value = v
            hps.append(HParamInfo(name=k, type=DataType.Value(""DATA_TYPE_BOOL"")))
continue
if isinstance(v, torch.Tensor):
v = make_np(v)[0]
ssi.hparams[k].number_value = v
            hps.append(HParamInfo(name=k, type=DataType.Value(""DATA_TYPE_FLOAT64"")))
continue
raise ValueError('value should be one of int, float, str, bool, or torch.Tensor')
"
602,"if self.rowWise:
if self.use_mask is True:
op = 'MaskedRowWiseSparseAdagrad'
                    assert weight_decay == 0, f'weight decay is not implemented for {op} yet'
input_args += [mask_blob, mask_changed_blob]
else:
op = 'RowWiseSparseAdagrad'
else:
if self.use_mask is True:
op = 'MaskedSparseAdagrad'
                    assert weight_decay == 0, f'weight decay is not implemented for {op} yet'
input_args += [mask_blob, mask_changed_blob]
else:
op = 'SparseAdagrad'
            logger.info(f""using {op} for {str(param)}"")
if self.prune_delays:
input_args += [lr_iteration, last_mask_updated_iter]
","if self.rowWise:
if self.use_mask is True:
op = 'MaskedRowWiseSparseAdagrad'
                    assert weight_decay == 0, 'weight decay is not implemented for {} yet'.format(op)
input_args += [mask_blob, mask_changed_blob]
else:
op = 'RowWiseSparseAdagrad'
else:
if self.use_mask is True:
op = 'MaskedSparseAdagrad'
                    assert weight_decay == 0, 'weight decay is not implemented for {} yet'.format(op)
input_args += [mask_blob, mask_changed_blob]
else:
op = 'SparseAdagrad'
            logger.info(""using {} for {}"".format(op, str(param)))
if self.prune_delays:
input_args += [lr_iteration, last_mask_updated_iter]
"
603,"import torch.backends.cudnn as cudnn
from torch._six import PY37
from ..nn.modules.utils import _single, _pair, _triple, _quadruple
from collections import OrderedDict
","import torch.backends.cudnn as cudnn
from torch._six import PY37
from ..nn.modules.utils import _single, _pair, _triple, _quadruple, _list_with_default
from collections import OrderedDict
"
604,"(torch._VF.frobenius_norm, ""aten::frobenius_norm""),
]
# ops in torch.functional are bound to torch
# in these cases, we want to resolve the function to their python implementation
# instead looking up a builtin ""aten::"" schema
def _gen_torch_functional_registered_ops():
    # eventually ops should encompass all of torch/functional.py, (torch.functional.__all__)
    # but we are currently only able to compile some of the functions. additionally,
    # some functions directly map to their aten:: implementations.
# TODO: add support for more ops
ops = [""stft"", ""istft"", ""lu"", ""lu_unpack"", ""cdist"", ""norm""]
return set(getattr(torch.functional, name) for name in ops)
","(torch._VF.frobenius_norm, ""aten::frobenius_norm""),
]
# ops in torch.functional are bound to torch
# in these cases, we want to resolve the function to their python implementation
# instead looking up a builtin ""aten::"" schema
def _gen_torch_functional_registered_ops():
    # eventually ops should encompass all of torch/functional.py, (torch.functional.__all__)
    # but we are currently only able to compile some of the functions. additionally,
    # some functions directly map to their aten:: implementations.
# TODO: add support for more ops
ops = [""stft"", ""istft"", ""lu"", ""lu_unpack"", ""cdist"", ""norm""]
return set(getattr(torch.functional, name) for name in ops)
"
605,"raise Exception(""unknown op"", opname)
def generate_type_hints(fname, decls, is_tensor=False):
""""""generate_type_hints(fname, decls, is_tensor=False)
Generates type hints for the declarations pertaining to the function
:attr:`fname`. attr:`decls` are the declarations from the parsed
Declarations.yaml.
The :attr:`is_tensor` flag indicates whether we are parsing
members of the Tensor class (true) or functions in the
`torch` namespace (default, false).
","raise Exception(""unknown op"", opname)
def generate_type_hints(fname, decls, namedtuples, is_tensor=False):
""""""generate_type_hints(fname, decls, is_tensor=False)
Generates type hints for the declarations pertaining to the function
:attr:`fname`. attr:`decls` are the declarations from the parsed
Declarations.yaml.
    :attr:`namedtuples` is a dictionary for accumulating NamedTuple definitions.
The :attr:`is_tensor` flag indicates whether we are parsing
members of the Tensor class (true) or functions in the
`torch` namespace (default, false).
"
606,"function_declarations = get_py_torch_functions(declarations)
for name in sorted(function_declarations.keys()):
        unsorted_function_hints[name] += generate_type_hints(name, function_declarations[name])
# Generate type signatures for deprecated functions
","function_declarations = get_py_torch_functions(declarations)
for name in sorted(function_declarations.keys()):
        unsorted_function_hints[name] += generate_type_hints(name, function_declarations[name], namedtuples)
# Generate type signatures for deprecated functions
"
607,"which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
# Use os.path.realpath to resolve any symlinks, in particular from 'c++' to e.g. 'g++'.
compiler_path = os.path.realpath(which.decode().strip())
    return any(name in compiler_path for name in _accepted_compilers_for_platform())
def check_compiler_abi_compatibility(compiler):
","which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
# Use os.path.realpath to resolve any symlinks, in particular from 'c++' to e.g. 'g++'.
compiler_path = os.path.realpath(which.decode().strip())
    # Check the compiler name
    if any(name in compiler_path for name in _accepted_compilers_for_platform()):
        return True
    # If ccache is used the compiler path is /usr/bin/ccache. Check by -v flag.
    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT).decode()
    if sys.platform.startswith('linux'):
        # Check for 'gcc' or 'g++'
        pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)
        results = re.findall(pattern, version_string)
        if len(results) != 1:
            return False
        compiler_path = os.path.realpath(results[0].strip())
        return any(name in compiler_path for name in _accepted_compilers_for_platform())
    if sys.platform.startswith('darwin'):
        # Check for 'clang' or 'clang++'
        return version_string.startswith(""Apple clang"")
    return False
def check_compiler_abi_compatibility(compiler):
"
608,"hook(module, input) -> None or modified input
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).
","hook(module, input) -> None or modified input
        The input contains only the positional arguments given to the module.
        Keyword arguments won't be passed to the hooks and only to the ``forward``.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).
"
609,"arranged_img_CHW = make_grid(make_np(label_img), ncols=nrow)
# augment images so that #images equals nrow*nrow
    arranged_augment_square_HWC = np.ndarray((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))
arranged_img_HWC = arranged_img_CHW.transpose(1, 2, 0)  # chw -> hwc
arranged_augment_square_HWC[:arranged_img_HWC.shape[0], :, :] = arranged_img_HWC
im = Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))
","arranged_img_CHW = make_grid(make_np(label_img), ncols=nrow)
# augment images so that #images equals nrow*nrow
    arranged_augment_square_HWC = np.zeros((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))
arranged_img_HWC = arranged_img_CHW.transpose(1, 2, 0)  # chw -> hwc
arranged_augment_square_HWC[:arranged_img_HWC.shape[0], :, :] = arranged_img_HWC
im = Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))
"
610,"return arange(g, stop, 4, None, None, None)
def size(g, self, dim):
return sym_help._size_helper(g, self, dim)
","return arange(g, stop, 4, None, None, None)
def size(g, self, dim=None):
    if dim is None:
        return g.op(""Shape"", self)
return sym_help._size_helper(g, self, dim)
"
611,">>> # non-square kernels and unequal stride and with padding and dilation
>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
>>> input = torch.randn(20, 16, 50, 100)
        >>> # quantize input to qint8
        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.qint32)
        >>> output = m(input)
""""""
",">>> # non-square kernels and unequal stride and with padding and dilation
>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
>>> input = torch.randn(20, 16, 50, 100)
        >>> # quantize input to quint8
        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
        >>> output = m(q_input)
""""""
"
612,"import torch
from collections import defaultdict
from torch._six import container_abcs
class _MultiDeviceReplicator(object):
","import torch
from collections import defaultdict
from torch._six import container_abcs
import warnings
class _MultiDeviceReplicator(object):
"
613,"import torch
import functools
import warnings
import numpy as np
from torch._six import container_abcs, string_classes


class autocast(object):
    r""""""
    Instances of :class:`autocast` serve as context managers or decorators that
    allow regions of your script to run in mixed precision.

    In these regions, CUDA ops run in an op-specific dtype chosen by autocast
    to improve performance while maintaining accuracy.
    See the :ref:`Autocast Op Reference<autocast-op-reference>` for details.

    When entering an autocast-enabled region, Tensors may be any type.
    You should not call ``.half()`` on your model(s) or inputs when using autocasting.

    :class:`autocast` should wrap only the forward pass(es) of your network, including the loss
    computation(s).  Backward passes under autocast are not recommended.
    Backward ops run in the same type that autocast used for corresponding forward ops.

    Example::

        # Creates model and optimizer in default precision
        model = Net().cuda()
        optimizer = optim.SGD(model.parameters(), ...)

        for input, target in data:
            optimizer.zero_grad()

            # Enables autocasting for the forward pass (model + loss)
            with autocast():
                output = model(input)
                loss = loss_fn(output, target)

            # Exits the context manager before backward()
            loss.backward()
            optimizer.step()

    See the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage (along with gradient scaling)
    in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).

    :class:`autocast` can also be used as a decorator, e.g., on the ``forward`` method of your model::

        class AutocastModel(nn.Module):
            ...
            @autocast()
            def forward(self, input):
                ...

    Floating-point Tensors produced in an autocast-enabled region may be ``float16``.
    After returning to an autocast-disabled region, using them with floating-point
    Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
    produced in the autocast region back to ``float32`` (or other dtype if desired).
    If a Tensor from the autocast region is already ``float32``, the cast is a no-op,
    and incurs no additional overhead.  Example::

        # Creates some tensors in default dtype (here assumed to be float32)
        a_float32 = torch.rand((8, 8), device=""cuda"")
        b_float32 = torch.rand((8, 8), device=""cuda"")
        c_float32 = torch.rand((8, 8), device=""cuda"")
        d_float32 = torch.rand((8, 8), device=""cuda"")

        with autocast():
            # torch.mm is on autocast's list of ops that should run in float16.
            # Inputs are float32, but the op runs in float16 and produces float16 output.
            # No manual casts are required.
            e_float16 = torch.mm(a_float32, b_float32)
            # Also handles mixed input types
            f_float16 = torch.mm(d_float32, e_float16)

        # After exiting autocast, calls f_float16.float() to use with d_float32
        g_float32 = torch.mm(d_float32, f_float16.float())

    Type mismatch errors *in* an autocast-enabled region are a bug; if this is what you observe,
    please file an issue.

    ``autocast(enabled=False)`` subregions can be nested in autocast-enabled regions.
    Locally disabling autocast can be useful, for example, if you want to force a subregion
    to run in a particular ``dtype``.  Disabling autocast gives you explicit control over
    the execution type.  In the subregion, inputs from the surrounding region
    should be cast to ``dtype`` before use::

        # Creates some tensors in default dtype (here assumed to be float32)
        a_float32 = torch.rand((8, 8), device=""cuda"")
        b_float32 = torch.rand((8, 8), device=""cuda"")
        c_float32 = torch.rand((8, 8), device=""cuda"")
        d_float32 = torch.rand((8, 8), device=""cuda"")

        with autocast():
            e_float16 = torch.mm(a_float32, b_float32)

            with autocast(enabled=False):
                # Calls e_float16.float() to ensure float32 execution
                # (necessary because e_float16 was created in an autocasted region)
                f_float32 = torch.mm(c_float32, e_float16.float())

            # No manual casts are required when re-entering the autocast-enabled region.
            # torch.mm again runs in float16 and produces float16 output, regardless of input types.
            g_float16 = torch.mm(d_float32, f_float32)

    The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator
    must be invoked in that thread.  This affects :class:`torch.nn.DataParallel` and
    :class:`torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process
    (see :ref:`Working with Multiple GPUs<amp-multigpu>`).

    Arguments:
        enabled(bool, optional, default=True):  Whether autocasting should be enabled in the region.
    """"""
    def __init__(self, enabled=True):
        if enabled and not torch.cuda.is_available():
            warnings.warn(""torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling."")
            self._enabled = False
        else:
            self._enabled = enabled

    def __enter__(self):
        self.prev = torch.is_autocast_enabled()
        torch.set_autocast_enabled(self._enabled)
        torch.autocast_increment_nesting()

    def __exit__(self, *args):
        # Drop the cache when we exit to a nesting level that's outside any instance of autocast.
        if torch.autocast_decrement_nesting() == 0:
            torch.clear_autocast_cache()
        torch.set_autocast_enabled(self.prev)
        return False

    def __call__(self, func):
        @functools.wraps(func)
        def decorate_autocast(*args, **kwargs):
            with self:
                return func(*args, **kwargs)
        return decorate_autocast


# Casts Tensors and containers of Tensors.  Special-cases passthroughs for strings and np.ndarrays, which
# may be falsely detected as ""Iterables.""
def _cast(value, dtype):
    if isinstance(value, torch.Tensor):
        return value.to(dtype) if (value.is_floating_point() and value.is_cuda) else value
    elif isinstance(value, string_classes):
        return value
    elif isinstance(value, np.ndarray):
        return value
    elif isinstance(value, container_abcs.Mapping):
        return {_cast(k, dtype): _cast(v, dtype) for k, v in value.items()}
    elif isinstance(value, container_abcs.Iterable):
        return type(value)(_cast(v, dtype) for v in value)
    else:
        return value


# custom_fwd is a decorator that may or may not be used with arguments, following
# https://github.com/dabeaz/python-cookbook/tree/master/src/9/defining_a_decorator_that_takes_an_optional_argument.
# this works:
#     @custom_fwd
#     def forward(...):
# this also works:
#     @custom_fwd(cast_inputs=torch.float)
#     def forward(...):
# TODO:  when python 2 support is dropped, change the signature to
# def custom_fwd(fwd=None, *, cast_inputs=None) with internal changes following the link above.
def custom_fwd(fwd=None, **kwargs):
    """"""
    Helper decorator for ``forward`` methods of custom autograd functions (subclasses of
    :class:`torch.autograd.Function`).  See the :ref:`example page<amp-custom-examples>` for more detail.

    Arguments:
        cast_inputs (:class:`torch.dtype` or None, optional, default=None):  If not ``None``, casts incoming
            floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),
            and causes ``forward`` to execute with autocast disabled.
            If ``None``, ``forward``'s internal ops execute with the current autocast state.
    """"""
    if fwd is None:
        if len(kwargs) == 0:
            cast_inputs = None
        else:
            assert len(kwargs) == 1
            cast_inputs = kwargs[""cast_inputs""]
        return functools.partial(custom_fwd, cast_inputs=cast_inputs)

    if len(kwargs) == 0:
        cast_inputs = None
    else:
        assert len(kwargs) == 1
        cast_inputs = kwargs[""cast_inputs""]

    @functools.wraps(fwd)
    def decorate_fwd(*args, **kwargs):
        if cast_inputs is None:
            args[0]._fwd_used_autocast = torch.is_autocast_enabled()
            return fwd(*args, **kwargs)
        else:
            args[0]._fwd_used_autocast = False
            with autocast(enabled=False):
                return fwd(*_cast(args, cast_inputs), **_cast(kwargs, cast_inputs))
    return decorate_fwd


# Autograd ensures incoming gradients are the same type as forward outputs.  Allowing a separate
# cast_inputs argument on custom_bwd is unnecessary and could cause errors if it doesn't match
# cast_inputs supplied to custom_fwd.
def custom_bwd(bwd):
    """"""
    Helper decorator for backward methods of custom autograd functions (subclasses of
    :class:`torch.autograd.Function`).
    Ensures that ``backward`` executes with the same autocast state as ``forward``.
    See the :ref:`example page<amp-custom-examples>` for more detail.
    """"""
    @functools.wraps(bwd)
    def decorate_bwd(*args, **kwargs):
        with autocast(args[0]._fwd_used_autocast):
            return bwd(*args, **kwargs)
    return decorate_bwd
","++ /dev/null
"
614,"r""""""
Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC
messages are sent and received in parallel to execution of Python code. This
    method is thread-safe. This method will immediately return a
    Future that can be awaited on.
Arguments:
to (str or WorkerInfo): id or name of the destination worker.
        func (callable): any callable function. python callable, builtin or annotated TorchScript
                         functions (like meth:`torch.add`) can be sent over RPC more efficiently.
args (tuple): the argument tuple for the ``func`` invocation.
kwargs (dict): is a dictionary of keyword arguments for the ``func``
invocation.
","r""""""
Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC
messages are sent and received in parallel to execution of Python code. This
    method is thread-safe. This method will immediately return a Future that can
    be awaited on.
Arguments:
to (str or WorkerInfo): id or name of the destination worker.
        func (callable): a callable function, such as Python callables, builtin
                         operators (e.g. :meth:`~torch.add`) and annotated
                         TorchScript functions.
args (tuple): the argument tuple for the ``func`` invocation.
kwargs (dict): is a dictionary of keyword arguments for the ``func``
invocation.
"
615,">>> rpc.init_rpc(""worker1"", rank=1, world_size=2)
>>> rpc.shutdown()
        If invoking an annotated TorchScript function, then run the following
        code in two different processes:
        >>> # On worker 0:
>>> @torch.jit.script
>>> def my_script_add(t1, t2):
>>>    return torch.add(t1, t2)
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc(""worker0"", rank=0, world_size=2)
>>> fut = rpc.rpc_async(""worker1"", my_script_add, args=(torch.ones(2), 3))
",">>> rpc.init_rpc(""worker1"", rank=1, world_size=2)
>>> rpc.shutdown()
        Below is an example of running a TorchScript function using RPC.
        >>> # On both workers:
>>> @torch.jit.script
>>> def my_script_add(t1, t2):
>>>    return torch.add(t1, t2)

        >>> # On worker 0:
>>> import torch.distributed.rpc as rpc
>>> rpc.init_rpc(""worker0"", rank=0, world_size=2)
>>> fut = rpc.rpc_async(""worker1"", my_script_add, args=(torch.ones(2), 3))
"
616,"import torch
import torch.cuda.comm as comm
from torch.cuda._utils import _get_device_index
from torch.nn import Parameter
def _is_script_module(module):
","import torch
import torch.cuda.comm as comm
from torch.cuda._utils import _get_device_index
def _is_script_module(module):
"
617,"return torch._C._VariableFunctions.meshgrid(tensors)

def stft(input, n_fft, hop_length=None, win_length=None, window=None,
center=True, pad_mode='reflect', normalized=False, onesided=True):
# type: (Tensor, int, Optional[int], Optional[int], Optional[Tensor], bool, str, bool, bool) -> Tensor
","return torch._C._VariableFunctions.meshgrid(tensors)
def stft(input, n_fft, hop_length=None, win_length=None, window=None,
center=True, pad_mode='reflect', normalized=False, onesided=True):
# type: (Tensor, int, Optional[int], Optional[int], Optional[Tensor], bool, str, bool, bool) -> Tensor
"
618,">>> output = loss(m(input), target)
>>> output.backward()
""""""
    __constants__ = ['reduction', 'weight']
def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean'):
super(BCELoss, self).__init__(weight, size_average, reduce, reduction)
",">>> output = loss(m(input), target)
>>> output.backward()
""""""
    __constants__ = ['reduction']
def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean'):
super(BCELoss, self).__init__(weight, size_average, reduce, reduction)
"
619,"from .utils import YamlLoader, split_name_params
# See NOTE [ Autograd View Variables ] in variable.h for details.
# A map: function name => name of the argument that all outputs are view of
VIEW_FUNCTIONS = {
'numpy_T': 'self',
'alias': 'self',
","from .utils import YamlLoader, split_name_params
# See NOTE [ Autograd View Variables ] in variable.h for details.
# A map: function name => two options:
#      1. name of the argument that all outputs are view of
#      2. map: output idx => name of the argument that this result is view of
VIEW_FUNCTIONS = {
'numpy_T': 'self',
'alias': 'self',
"
620,"def emit_call(env):
combined = nested_dict(env, declaration)
if strategy == 'use_derived':
# We only care about adding `at::AutoNonVariableTypeMode` guard for non-variable dispatch
# (which corresponds to 'use_derived' strategy). The purpose of this guard is to make sure
","def emit_call(env):
combined = nested_dict(env, declaration)
        extra_wrapping_stmts = []
if strategy == 'use_derived':
# We only care about adding `at::AutoNonVariableTypeMode` guard for non-variable dispatch
# (which corresponds to 'use_derived' strategy). The purpose of this guard is to make sure
"
621,"tags = attr[""tags""]
continue
            # if 'cuda' is sepcified in input shape but the testing machines doesn't
# support, we will skip this input
if 'cuda' in attr.values():
if not torch.cuda.is_available():
","tags = attr[""tags""]
continue
            # if 'cuda' is specified in input shape but the testing machines doesn't
# support, we will skip this input
if 'cuda' in attr.values():
if not torch.cuda.is_available():
"
622,"# Currently our implementation only supports
# generic type enum 1. If new types are implemented, we need to
# modify the ParseGeneric operator, the schema above,
                # and this part accordinly to parse the generic feature strings
# into input_record
ranges = net.LengthsToRanges(
","# Currently our implementation only supports
# generic type enum 1. If new types are implemented, we need to
# modify the ParseGeneric operator, the schema above,
                # and this part accordingly to parse the generic feature strings
# into input_record
ranges = net.LengthsToRanges(
"
623,"class Initializer(object):
'''
This class abstracts out parameter creation. One can come up with a new
    Initializer in order to implement more complex parameter initializaion logic
'''
def __init__(self, operator_name=None, **kwargs):
","class Initializer(object):
'''
This class abstracts out parameter creation. One can come up with a new
    Initializer in order to implement more complex parameter initialization logic
'''
def __init__(self, operator_name=None, **kwargs):
"
624,"def unpack_variable(name, unpack_expr, typename):
# optional<ArrayRef<T>> are special. The PythonArgParser returns an
            # optional<vector<T>>, which cannot be implictly converted to
# optional<ArrayRef<T>>. One needs to unwrap the optional and rewrap.
if typename == 'c10::optional<DimnameList>':
result = """"""\
","def unpack_variable(name, unpack_expr, typename):
# optional<ArrayRef<T>> are special. The PythonArgParser returns an
            # optional<vector<T>>, which cannot be implicitly converted to
# optional<ArrayRef<T>>. One needs to unwrap the optional and rewrap.
if typename == 'c10::optional<DimnameList>':
result = """"""\
"
625,"**Important Notices:**
1. This utilty and multi-process distributed (single-node or
multi-node) GPU training currently only achieves the best performance using
the NCCL distributed backend. Thus NCCL backend is the recommended backend to
use for GPU training.
","**Important Notices:**
1. This utility and multi-process distributed (single-node or
multi-node) GPU training currently only achieves the best performance using
the NCCL distributed backend. Thus NCCL backend is the recommended backend to
use for GPU training.
"
626,"have some subtle differences. :class:`InstanceNorm1d` is applied
on each channel of channeled data like multidimensional time series, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionaly, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm1d` usually don't apply affine
transform.
","have some subtle differences. :class:`InstanceNorm1d` is applied
on each channel of channeled data like multidimensional time series, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm1d` usually don't apply affine
transform.
"
627,"self.activation_post_process = observer(**observer_kwargs)
assert torch.iinfo(self.activation_post_process.dtype).min <= quant_min, 'quant_min out of bound'
assert quant_max <= torch.iinfo(self.activation_post_process.dtype).max, 'quant_max out of bound'
        self.scale = None
        self.zero_point = None
self.dtype = self.activation_post_process.dtype
self.qscheme = self.activation_post_process.qscheme
self.ch_axis = self.activation_post_process.ch_axis if hasattr(self.activation_post_process, 'ch_axis') else None
","self.activation_post_process = observer(**observer_kwargs)
assert torch.iinfo(self.activation_post_process.dtype).min <= quant_min, 'quant_min out of bound'
assert quant_max <= torch.iinfo(self.activation_post_process.dtype).max, 'quant_max out of bound'
        self.scale = torch.tensor([1.0])
        self.zero_point = torch.tensor([0])
self.dtype = self.activation_post_process.dtype
self.qscheme = self.activation_post_process.qscheme
self.ch_axis = self.activation_post_process.ch_axis if hasattr(self.activation_post_process, 'ch_axis') else None
"
628,"r""""""Pass the input through the encoder layer.
Args:
            src: the sequnce to the encoder layer (required).
src_mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).
","r""""""Pass the input through the encoder layer.
Args:
            src: the sequence to the encoder layer (required).
src_mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).
"
