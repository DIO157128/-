,source,target
0,"# inplace modifies node/inps
def _convert_node_to_placeholder(node, inps):
    if node.op == 'output':
return
node.op = 'placeholder'
node.args = ()
node.target = node.name
    concrete_val = node.meta['concrete_value']
if isinstance(concrete_val, torch.Tensor):
inps.append(concrete_val)
else:
","# inplace modifies node/inps
def _convert_node_to_placeholder(node, inps):
    if node.op == 'output' or node.op == ""placeholder"":
return
node.op = 'placeholder'
node.args = ()
node.target = node.name
    concrete_val = node.meta.get('concrete_value', None)
if isinstance(concrete_val, torch.Tensor):
inps.append(concrete_val)
else:
"
1,"###
# There are generated files that depend on this file
# To re-generate, please run:
# cd ~/pytorch && python
# torchgen/shape_functions/gen_jit_shape_functions.py
####
import torch
","###
# There are generated files that depend on this file
# To re-generate, please run from the root of the repo:
# python torchgen/shape_functions/gen_jit_shape_functions.py

# How to test:
# After regenerating files, compile PyTorch.
# Then run: ./build/bin/test_jit --gtest_filter=TestShapeGraphLinting.Basic
# If you have enabled opinfo testing for the op, also run:
# python test/test_ops_jit.py TestJitCPU::test_variant_consistency_jit_[FAILING_OP]_cpu_float32
# to reproduce errors from opinfo tests.

# Example PR: https://github.com/pytorch/pytorch/pull/80860/files
####
import torch
"
2,"ort_outs = _run_ort(ort_session, ort_inputs)
_compare_ort_pytorch_outputs(
            ort_outs, pt_outs, rtol, atol, check_shape, check_dtype
)
compare_ort_pytorch_model_with_input(input_args, input_kwargs)
","ort_outs = _run_ort(ort_session, ort_inputs)
_compare_ort_pytorch_outputs(
            ort_outs,
            pt_outs,
            rtol,
            atol,
            check_shape,
            check_dtype,
            accetable_error_persentage,
)
compare_ort_pytorch_model_with_input(input_args, input_kwargs)
"
3,"import functools
import inspect
import sys
import warnings
from typing import Any, Callable, List, Optional, Sequence, Set, Tuple, Union
import torch
import torch._C._onnx as _C_onnx
from torch import _C
# Monkey-patch graph manipulation methods on Graph, used for the ONNX symbolics
from torch.onnx import _patch_torch, _type_utils  # noqa: F401
from torch.onnx._globals import GLOBALS
# Note [Edit Symbolic Files]
","import functools
import inspect
import sys
import typing
import warnings
from typing import Any, Callable, List, Optional, Sequence, Set, Tuple, Union
from typing_extensions import Literal

import torch
import torch._C._onnx as _C_onnx
from torch import _C
# Monkey-patch graph manipulation methods on Graph, used for the ONNX symbolics
from torch.onnx import _patch_torch, _type_utils, errors  # noqa: F401
from torch.onnx._globals import GLOBALS
# Note [Edit Symbolic Files]
"
4,"_onnx_unsupported(f""{op}, {msg}"")
def _onnx_unsupported(op_name):
raise RuntimeError(
f""Unsupported: ONNX export of operator {op_name}. ""
""Please feel free to request support or submit a pull request on PyTorch GitHub.""
)
def _onnx_opset_unsupported(op_name, current_opset, supported_opset):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in opset {current_opset}. ""
f""Please try opset version {supported_opset}.""
)
def _onnx_opset_unsupported_detailed(op_name, current_opset, supported_opset, reason):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in ""
f""opset {current_opset}. {reason}. Please try opset version {supported_opset}.""
)
def _block_list_in_opset(name):
def symbolic_fn(*args, **kwargs):
raise RuntimeError(
f""ONNX export failed on {name}, which is not implemented for opset ""
","_onnx_unsupported(f""{op}, {msg}"")
def _onnx_unsupported(op_name: str):
raise RuntimeError(
f""Unsupported: ONNX export of operator {op_name}. ""
""Please feel free to request support or submit a pull request on PyTorch GitHub.""
)
def _onnx_opset_unsupported(op_name: str, current_opset: int, supported_opset: int):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in opset {current_opset}. ""
f""Please try opset version {supported_opset}.""
)
def _onnx_opset_unsupported_detailed(
    op_name: str, current_opset: int, supported_opset: int, reason: str
):
raise RuntimeError(
f""Unsupported: ONNX export of {op_name} in ""
f""opset {current_opset}. {reason}. Please try opset version {supported_opset}.""
)
def _block_list_in_opset(name: str):
def symbolic_fn(*args, **kwargs):
raise RuntimeError(
f""ONNX export failed on {name}, which is not implemented for opset ""
"
5,"return g.op(""Unsqueeze"", input, axes_i=axes_i)
# Tensor type
if GLOBALS.export_onnx_opset_version < 13:
        raise ValueError(
            f""Opset version must be >= 13 for Unsqueeze with dynamic axes. {input.node().sourceRange()}""
)
return g.op(""Unsqueeze"", input, axes_i[0])
","return g.op(""Unsqueeze"", input, axes_i=axes_i)
# Tensor type
if GLOBALS.export_onnx_opset_version < 13:
        raise errors.SymbolicValueError(
            ""Opset version must be >= 13 for Unsqueeze with dynamic axes."", input
)
return g.op(""Unsqueeze"", input, axes_i[0])
"
6,"@contextlib.contextmanager
def _batch_p2p_manager(backend):
    if backend == Backend.NCCL:
        ProcessGroupNCCL._group_start()
try:
yield
finally:
        if backend == Backend.NCCL:
            ProcessGroupNCCL._group_end()
def batch_isend_irecv(p2p_op_list):
","@contextlib.contextmanager
def _coalescing_manager(group, reqs):
    if group is None:
        group = _get_default_group()
    group._start_coalescing()
try:
yield
finally:
        group._end_coalescing(reqs)
def batch_isend_irecv(p2p_op_list):
"
7,"# Turn this into AttributeError so getattr(obj, key, default)
# works (this is called by TorchScript with __origin__)
raise AttributeError(
                f""'_OpNamespace' object has no attribute '{op_name}'""
) from e
# let the script frontend know that op is identical to the builtin op
","# Turn this into AttributeError so getattr(obj, key, default)
# works (this is called by TorchScript with __origin__)
raise AttributeError(
                f""'_OpNamespace' '{self.name}' object has no attribute '{op_name}'""
) from e
# let the script frontend know that op is identical to the builtin op
"
8,"# TODO: We should also check tensor identities
if event.name() != ""aten::to"":
return False
# Up one level
event = event.parent
if event is None:
","# TODO: We should also check tensor identities
if event.name() != ""aten::to"":
return False
        to_event = event
        if not event.children:
            return False
        event = event.children[-1]
        if event.name() != ""aten::_to_copy"":
            return False
        if not event.children:
            return False
        event = event.children[-1]
        if event.name() != ""aten::copy_"":
            return False
        # aten::copy_ should have the first 2 args dtype the same
        dtypes = input_dtypes(event)
        if len(dtypes) < 2:
            return False
        if dtypes[0] != dtypes[1]:
            return False
        event = to_event
# Up one level
event = event.parent
if event is None:
"
9,"build-in method                 |build-in method
...                         |    aten::to
aten::fill_/aten::zero_ |        aten::_to_copy
                                    |            aten::copy_
                                    |                cudaMemcpyAsync
Algorithm:
We start at node aten::to, go parent events' previous events,
","build-in method                 |build-in method
...                         |    aten::to
aten::fill_/aten::zero_ |        aten::_to_copy
Algorithm:
We start at node aten::to, go parent events' previous events,
"
10,"# TODO: We should also check tensor identities
if event.name() != ""aten::to"":
return False
# Up one level
event = event.parent
if event is None:
return False
","# TODO: We should also check tensor identities
if event.name() != ""aten::to"":
return False
        to_event = event
        if not event.children:
            return False
        event = event.children[-1]
        if event.name() != ""aten::_to_copy"":
            return False
        if not event.children:
            return False
        event = event.children[-1]
        if event.name() != ""aten::copy_"":
            return False
        if not event.children:
            return False
        event = event.children[0]
        if event.name() != ""cudaMemcpyAsync"":
            return False
# Up one level
        event = to_event
event = event.parent
if event is None:
return False
"
11,"_check_shard_metadata_pair_overlap,
)
from torch.distributed._shard.sharded_tensor.shard import Shard
from .metadata import (
    BytesStorageMetadata,
BytesWriteRequest,
TensorReadRequest,
    ShardStorageMetadata,
    ShardedTensorStorageMetadata,
    TensorStorageMetadata,
TensorWriteRequest,
)
def _trim(tensor: torch.Tensor) -> torch.Tensor:
","_check_shard_metadata_pair_overlap,
)
from torch.distributed._shard.sharded_tensor.shard import Shard
from torch.distributed._shard.sharded_tensor.metadata import TensorProperties
from .metadata import (
BytesWriteRequest,
TensorReadRequest,
TensorWriteRequest,
    ChunkStorageMetadata,
    TensorStorageMetadata,
    BytesStorageMetadata,
    MetadataIndex,
)
def _trim(tensor: torch.Tensor) -> torch.Tensor:
"
12,"from typing import List, Callable, Optional, Union, TypeVar, cast, Any
import torch.distributed as dist
from .api import CheckpointException
import torch
","from typing import List, Callable, Optional, Union, TypeVar, Dict, Any, cast
import torch.distributed as dist
from .api import (
    CheckpointException,
    _wrap_exception,
    _is_wrapped_exception,
    WRAPPED_EXCEPTION
)
import torch
"
13,"# then we don't allow recomputation.
if 'tensor_meta' not in node.meta:
return False
            input_tensors_size = sum(_size_of(i.meta['tensor_meta']) for i in node.args if isinstance(i, fx.Node))
output_size = _size_of(node.meta['tensor_meta'])
return (output_size * 4 < input_tensors_size)
","# then we don't allow recomputation.
if 'tensor_meta' not in node.meta:
return False
            input_tensors_size = sum(_maybe_size_of(i) for i in node.args if isinstance(i, fx.Node))
output_size = _size_of(node.meta['tensor_meta'])
return (output_size * 4 < input_tensors_size)
"
14,"Args:
device_type(str, required):  Whether to use 'cuda' or 'cpu' device
        enabled(bool, optional, default=True):  Whether autocasting should be enabled in the region.
dtype(torch_dtype, optional):  Whether to use torch.float16 or torch.bfloat16.
        cache_enabled(bool, optional, default=True):  Whether the weight cache inside autocast should be enabled.
""""""
def __init__(self, device_type : str,
dtype : Optional[_dtype] = None,
","Args:
device_type(str, required):  Whether to use 'cuda' or 'cpu' device
        enabled(bool, optional):  Whether autocasting should be enabled in the region.
            Default: ``True``
dtype(torch_dtype, optional):  Whether to use torch.float16 or torch.bfloat16.
        cache_enabled(bool, optional):  Whether the weight cache inside autocast should be enabled.
            Default: ``True``
""""""
def __init__(self, device_type : str,
dtype : Optional[_dtype] = None,
"
15,"Args:
tensor: a {3, 4, 5}-dimensional `torch.Tensor`
        groups (optional): number of groups in the conv layer (default: 1)
Examples:
>>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
","Args:
tensor: a {3, 4, 5}-dimensional `torch.Tensor`
        groups (int, optional): number of groups in the conv layer (default: 1)
Examples:
>>> w = torch.empty(3, 16, 5, 5)
>>> nn.init.dirac_(w)
"
16,"import textwrap
from dataclasses import dataclass
from typing import List, Optional, Tuple
from torchgen.api.translate import translate
from torchgen.api.types import DispatcherSignature
","import textwrap
from dataclasses import dataclass
from typing import List, Optional, Sequence, Tuple
from torchgen.api.translate import translate
from torchgen.api.types import DispatcherSignature
"
17,"}}""""""
def gen_returns(returns: List[Return], cur_level_var: str, results_var: str) -> str:
idx = 0
wrapped_returns = []
for ret in returns:
","}}""""""
def gen_returns(
    returns: Tuple[Return, ...], cur_level_var: str, results_var: str
) -> str:
idx = 0
wrapped_returns = []
for ret in returns:
"
18,"return result
def gen_all_vmap_plumbing(native_functions):
body = ""\n"".join(list(mapMaybe(ComputeBatchRulePlumbing(), native_functions)))
return f""""""
#pragma once
","return result
def gen_all_vmap_plumbing(native_functions: Sequence[NativeFunction]) -> str:
body = ""\n"".join(list(mapMaybe(ComputeBatchRulePlumbing(), native_functions)))
return f""""""
#pragma once
"
19,"from .module import *
\ No newline at end of file
","++ /dev/null
\ No newline at end of file
"
20,"# ""torch.ops.aten.type_as"": None,       # missing refs
""torch.ops.aten.linear"": None,
""torch.ops.aten.gelu"": None,
            ""torch.ops.aten.gelu_backward"": None,
# ""torch.ops.aten.hardtanh"": None,        # has functional ref, using unsupported aten.clamp
""torch.ops.aten.leaky_relu"": None,
""torch.ops.aten.square"": None,
","# ""torch.ops.aten.type_as"": None,       # missing refs
""torch.ops.aten.linear"": None,
""torch.ops.aten.gelu"": None,
            # ""torch.ops.aten.gelu_backward"": None,       # gelu_backward is handled at aten2aten decomp
# ""torch.ops.aten.hardtanh"": None,        # has functional ref, using unsupported aten.clamp
""torch.ops.aten.leaky_relu"": None,
""torch.ops.aten.square"": None,
"
21,"except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    strip_overloads(joint_module)
joint_module.graph.eliminate_dead_code()
joint_module.recompile()
fx_g = joint_module.graph
","except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
joint_module.graph.eliminate_dead_code()
joint_module.recompile()
fx_g = joint_module.graph
"
22,"# step 4: assign the new grads, delete the sample grads
for param, param_grad in zip(model.parameters(), grads):
        param.grad = param_grad
del param.grad_sample
","# step 4: assign the new grads, delete the sample grads
for param, param_grad in zip(model.parameters(), grads):
        param.grad = param_grad/batch_size
del param.grad_sample
"
23,"if __name__ == ""__main__"":
    main()
\ No newline at end of file
","if __name__ == ""__main__"":
\ No newline at end of file
    main()
"
24,"from .partitioners import default_partition
from .named_members_polyfill import _named_parameters, _named_buffers
from typing import Callable, List, Dict, Any, Tuple, Optional
try:
from torchdynamo import disable as disable_torchdynamo
","from .partitioners import default_partition
from .named_members_polyfill import _named_parameters, _named_buffers
from typing import Callable, List, Dict, Any, Tuple, Optional
from functools import wraps
try:
from torchdynamo import disable as disable_torchdynamo
"
25,"else:
return e
if isinstance(real_out, tuple):
            return tuple([wrap_with_proxy(e, proxy_out[idx]) for idx, e in enumerate(real_out)])
elif isinstance(real_out, list):
            return list([wrap_with_proxy(e, proxy_out[idx]) for idx, e in enumerate(real_out)])
elif isinstance(real_out, torch.Tensor):
return wrap_with_proxy(real_out, proxy_out)
else:
","else:
return e
if isinstance(real_out, tuple):
            return tuple(wrap_with_proxy(e, proxy_out[idx]) for idx, e in enumerate(real_out))
elif isinstance(real_out, list):
            return [wrap_with_proxy(e, proxy_out[idx]) for idx, e in enumerate(real_out)]
elif isinstance(real_out, torch.Tensor):
return wrap_with_proxy(real_out, proxy_out)
else:
"
26,"@register_decomposition(aten._softmax_backward_data)
def _softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: int):
new_grad = grad_output * output
    return (new_grad - output * torch.sum(new_grad, dim=dim, keepdim=True)).to(input_dtype)
@register_decomposition(aten._log_softmax_backward_data)
def _log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: int):
grad_input = grad_output - torch.exp(output) * torch.sum(grad_output, dim=dim, keepdim=True)
    return grad_input.to(input_dtype)
@register_decomposition(aten.im2col_backward)
","@register_decomposition(aten._softmax_backward_data)
def _softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: int):
new_grad = grad_output * output
    grad_output = new_grad - output * torch.sum(new_grad, dim=dim, keepdim=True)
    return aten.to(grad_output, dtype=input_dtype)
@register_decomposition(aten._log_softmax_backward_data)
def _log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: int):
grad_input = grad_output - torch.exp(output) * torch.sum(grad_output, dim=dim, keepdim=True)
    return aten.to(grad_input, dtype=input_dtype)
@register_decomposition(aten.im2col_backward)
"
27,"return self + value * tensor1 * tensor2
@register_decomposition(aten.embedding)
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor:
assert weight.dim() == 2,  ""'weight' must be 2-D""
","return self + value * tensor1 * tensor2
@register_decomposition(aten.rsub.Tensor)
def rsub(self: Tensor, other: Tensor, alpha: float = 1) -> Tensor:
    return torch.sub(other, self, alpha=alpha)


@register_decomposition(aten.rsub.Scalar)
def rsub(self: Tensor, other: float, alpha: float = 1) -> Tensor:
    return torch.sub(other, self, alpha=alpha)


@register_decomposition(aten.embedding)
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor:
assert weight.dim() == 2,  ""'weight' must be 2-D""
"
28,"return sum / n
@register_decomposition(aten.std)
def std_decomposition(x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False):
return torch.sqrt(torch.var(x, dims, correction=correction, keepdim=keepdim))
","return sum / n
@register_decomposition(aten.std.correction)
def std_decomposition(x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False):
return torch.sqrt(torch.var(x, dims, correction=correction, keepdim=keepdim))
"
29,"func_args = _wrap_all_tensors_to_functional(args, func_level)
func_kwargs = _wrap_all_tensors_to_functional(kwargs, func_level)
            flattened_unwrapped_args = tree_flatten(args)
            flattened_wrapped_args = tree_flatten(func_args)
            flattened_unwrapped_kwargs = tree_flatten(kwargs)
            flattened_wrapped_kwargs = tree_flatten(func_kwargs)
func_outputs = func(*func_args, **func_kwargs)
outputs = _unwrap_all_tensors_from_functional(func_outputs)
","func_args = _wrap_all_tensors_to_functional(args, func_level)
func_kwargs = _wrap_all_tensors_to_functional(kwargs, func_level)
            flattened_unwrapped_args, _ = tree_flatten(args)
            flattened_wrapped_args, _ = tree_flatten(func_args)
            flattened_unwrapped_kwargs, _ = tree_flatten(kwargs)
            flattened_wrapped_kwargs, _ = tree_flatten(func_kwargs)
func_outputs = func(*func_args, **func_kwargs)
outputs = _unwrap_all_tensors_from_functional(func_outputs)
"
30,"return out
return beta * self + out
@register_decomposition(aten.native_layer_norm_backward)
def native_layer_norm_backward(grad_out: Tensor, input: Tensor, normalized_shape: List[int], mean: Tensor, rstd: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], output_mask: List[bool]) -> Tuple[Tensor, Tensor, Tensor]:
input_shape = input.shape
","return out
return beta * self + out

@register_decomposition(aten.native_layer_norm_backward)
def native_layer_norm_backward(grad_out: Tensor, input: Tensor, normalized_shape: List[int], mean: Tensor, rstd: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], output_mask: List[bool]) -> Tuple[Tensor, Tensor, Tensor]:
input_shape = input.shape
"
31,"else:
d_weight = aten.mul(grad_out, x_hat)
else:
        d_weight = None
if output_mask[2] and bias is not None:
if len(outer_dim_indices) > 0:
","else:
d_weight = aten.mul(grad_out, x_hat)
else:
        d_weight = aten.new_empty(input, (0,))
if output_mask[2] and bias is not None:
if len(outer_dim_indices) > 0:
"
32,"""https://github.com/pytorch/functorch/issues/446"")
def extract_weights(mod: nn.Module) -> Tuple[Tuple[Tensor, ...], List[str]]:
""""""
This function removes all the Parameters from the model and
return them as a tuple as well as their original attribute names.
","""https://github.com/pytorch/functorch/issues/446"")
def create_names_map(named_params, tied_named_params):
    """"""
    named_params is a dictionary of tensors: {'A': A, 'B': B}
    tied_named_params is another dictionary of tensors {'A': A, 'B': B, 'B_tied': B}
    with potentially tied (or 'duplicated') tensors

    This function creates a mapping from the names in named_params to the
    names in tied_named_params: {'A': ['A'], 'B': ['B', 'B_tied']}.
    """"""
    named_params = {k: v for k, v in named_params}
    tied_named_params = {k: v for k, v in tied_named_params}

    tensors_dict_keys = set(named_params.keys())
    tied_tensors_dict_keys = set(tied_named_params.keys())
    assert tensors_dict_keys.issubset(tied_tensors_dict_keys)

    tensor_to_mapping = {}
    for key, tensor in named_params.items():
        tensor_to_mapping[tensor] = (key, [])
    for key, tensor in tied_named_params.items():
        assert tensor in tensor_to_mapping
        tensor_to_mapping[tensor][1].append(key.split('.'))
    result = {key: value for key, value in tensor_to_mapping.values()}
    return result


def _extract_members(mod: nn.Module, _named_members, named_members, subclass):
    all_named_members = tuple(_named_members(mod, remove_duplicate=False))
    named_members = tuple(named_members())
    names_map = create_names_map(named_members, all_named_members)

    # Remove all the members in the model
    memo = {}
    for name, p in all_named_members:
        if p not in memo:
            memo[p] = subclass(torch.empty_like(p, device='meta'))
        replacement = memo[p]
        _set_nested_attr(mod, name.split("".""), replacement)

    if len(named_members) == 0:
        names, params = (), ()
    else:
        names, params = zip(*named_members)
    return params, names, names_map


def extract_weights(mod: nn.Module):
""""""
This function removes all the Parameters from the model and
return them as a tuple as well as their original attribute names.
"
33,"aot_autograd_decompositions = {}
@register_decomposition(aten.rsub, aot_autograd_decompositions)
def rsub(a, b, alpha=1):
return -aten.sub(a, b)
","aot_autograd_decompositions = {}
@register_decomposition(aten.rsub.Tensor, aot_autograd_decompositions)
def rsub(a, b, alpha=1):
return -aten.sub(a, b)
"
34,"auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
{textwrap.indent(unwraps, ""  "")}
batch_rule({', '.join(unwrapped_arg_list)});
return {schema.arguments.flat_all[0].name};
","auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
{textwrap.indent(bdims_all_none_case, ""  "")}
{textwrap.indent(unwraps, ""  "")}
batch_rule({', '.join(unwrapped_arg_list)});
return {schema.arguments.flat_all[0].name};
"
35,"auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
{textwrap.indent(unwraps, ""  "")}
auto results = batch_rule({', '.join(unwrapped_arg_list)});
{wrapped_returns}
","auto maybe_layer = maybeCurrentDynamicLayer();
TORCH_INTERNAL_ASSERT(maybe_layer.has_value());
int64_t cur_level = maybe_layer->layerId();
{textwrap.indent(bdims_all_none_case, ""  "")}
{textwrap.indent(unwraps, ""  "")}
auto results = batch_rule({', '.join(unwrapped_arg_list)});
{wrapped_returns}
"
36,"@register_decomposition(aten.prelu_backward)
def prelu_backward(grad_output: Tensor, self: Tensor, weight: Tensor) -> Tuple[Tensor, Tensor]:
    spatial_dims = list(range(2, grad_output.dim()))
    for _ in range(len(spatial_dims)):
        weight = weight.unsqueeze(-1)
    input_grad = aten.where(self > 0, grad_output, weight * grad_output)
weight_grad_collector = aten.where(self > 0, aten.new_zeros(grad_output, ()), self * grad_output)
    return (input_grad, aten.sum(weight_grad_collector, [0] + spatial_dims))
@register_decomposition(aten.rrelu_with_noise_backward)
","@register_decomposition(aten.prelu_backward)
def prelu_backward(grad_output: Tensor, self: Tensor, weight: Tensor) -> Tuple[Tensor, Tensor]:
    # Logic is more complicated than I would like.  Basically, weight can either
    # be a scalar or a vector of size [C], and in the forward pass it's
    # broadcast against [N, C, ...]. So now, we need to do the corresponding
    # reduction, which is harder than we'd like...
    cur_weight = weight
    for _ in range(2, grad_output.dim()):
        cur_weight = cur_weight.unsqueeze(-1)
    input_grad = aten.where(self > 0, grad_output, cur_weight * grad_output)
weight_grad_collector = aten.where(self > 0, aten.new_zeros(grad_output, ()), self * grad_output)
    out = aten.sum_to_size(weight_grad_collector, cur_weight.shape)
    while out.dim() > weight.dim():
        out = out.squeeze(-1)
    return (input_grad, out)
@register_decomposition(aten.rrelu_with_noise_backward)
"
37,"return mem_sz * 2
for node in full_bw_graph.nodes:
        if node in tangent_closure:
nx_graph.add_edge(node.name+""_in"", ""sink"", capacity=math.inf)
continue
","return mem_sz * 2
for node in full_bw_graph.nodes:
        if node in tangent_closure and node.op != 'output':
nx_graph.add_edge(node.name+""_in"", ""sink"", capacity=math.inf)
continue
"
38,"res = bool_mask * input * float(1.0 / p)
return [res, bool_mask]
# @register_decomposition(aten._fused_dropout)
# def _fused_dropout_decomposition(input, p, generator=None):
#     mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
","res = bool_mask * input * float(1.0 / p)
return [res, bool_mask]
@register_decomposition(aten._softmax)
def _softmax(x: Tensor, dim: int, half_to_float: bool):
    return aten.exp(x) / aten.sum(aten.exp(x), dim=dim, keepdim=True)


@register_decomposition(aten.addmm)
def addmm(self: Tensor, mat1: Tensor, mat2: Tensor, beta=1, alpha=1):
    if not self.is_floating_point():
        beta = int(beta)
        alpha = int(alpha)
    out = alpha * aten.mm(mat1, mat2)
    if beta == 0:
        return out
    return beta * self + out

@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
    return aten.clamp(self, min=min)


@register_decomposition(aten.clamp_max)
def clamp_max(self: Tensor, min: float):
    return aten.clamp(self, max=max)

# @register_decomposition(aten._fused_dropout)
# def _fused_dropout_decomposition(input, p, generator=None):
#     mask = aten.to(aten.rand_like(input) < p, dtype=torch.uint8)
"
39,"break
failing_fx = fx.GraphModule(fail_f, failing_graph)
print(failing_fx.code)
    print([i.shape for i in inps])
return failing_fx, inps
def check_nvfuser_subprocess(f, inps):

f.to_folder(""temp"")
with open(""_temp.py"", 'w') as fil:
fil.write(f'''
    import torch
    from temp import FxModule
    f = FxModule().cuda()
    inps = {[(i.shape, i.dtype) for i in inps]}
    inps = [torch.randn(shape, dtype=dtype, device='cuda') for shape, dtype in inps]
    with torch.jit.fuser(""fuser2""):
nf = torch.jit.script(f)
for _ in range(5):
nf(*inps)
","break
failing_fx = fx.GraphModule(fail_f, failing_graph)
print(failing_fx.code)
    print([(i.shape, i.dtype) for i in inps])
return failing_fx, inps
def check_nvfuser_subprocess(f, inps):
f.to_folder(""temp"")
with open(""_temp.py"", 'w') as fil:
fil.write(f'''
import torch
from temp import FxModule
f = FxModule().cuda()
inps = {[(i.shape, i.dtype) for i in inps]}
inps = [torch.randn(shape, dtype=dtype, device='cuda') for shape, dtype in inps]
with torch.jit.fuser(""fuser2""):
nf = torch.jit.script(f)
for _ in range(5):
nf(*inps)
"
40,"fw_compiler,
bw_compiler,
partition_fn,
        decompose=True,
hasher_type=hasher_type,
)
","fw_compiler,
bw_compiler,
partition_fn,
        decompositions=decomposition_table,
hasher_type=hasher_type,
)
"
41,"import networkx as nx
except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
","import networkx as nx
except ImportError:
raise RuntimeError(""Need networkx installed to perform smart recomputation heuristics"")
    # draw_graph(joint_module, ""joint.svg"")
full_bw_graph = joint_module.graph
nx_graph = nx.DiGraph()
"
42,"@register_decomposition(aten.softplus_backward)
# The out argument seems to always be ignored?
def softplus_backward_decomposition(out_grad: Tensor, x: Tensor, beta: float, threshold: float, out):
z = (x * beta).exp()
return aten.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
","@register_decomposition(aten.softplus_backward)
# The out argument seems to always be ignored?
def softplus_backward_decomposition(out_grad: Tensor, x: Tensor, beta: float, threshold: float):
z = (x * beta).exp()
return aten.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
"
43,"if isinstance(x, torch.Tensor) else x, kwargs)
real_out = func(*args, **kwargs)
        if real_out.device.type == 'meta':
            # Used to infer the output device
            input_devices = list(set([i.device for i in pytree.tree_flatten(args)[0] +
                                pytree.tree_flatten(kwargs)[0] if isinstance(i, PythonTensor)]))
            output_device = get_output_device(input_devices)
        else:
            output_device = real_out.device
def wrap_with_proxy(e, proxy):
# Some ops (like native_batch_norm_backward) return undefined tensors that get
# converted into None in python.
","if isinstance(x, torch.Tensor) else x, kwargs)
real_out = func(*args, **kwargs)
def wrap_with_proxy(e, proxy):
# Some ops (like native_batch_norm_backward) return undefined tensors that get
# converted into None in python.
"
44,"aten.new_full(self, (), float('nan')))
@register_decomposition(aten.native_dropout)
def native_dropout_decomposition(input, p, generator=None):
bool_mask = aten.rand_like(input) < p
","aten.new_full(self, (), float('nan')))

@register_decomposition(aten.native_dropout)
def native_dropout_decomposition(input, p, generator=None):
bool_mask = aten.rand_like(input) < p
"
45,"return f
return decomposition_decorator
@register_decomposition(torch.ops.aten.tanh_backward)
def tanh_backward_decomposition(out_grad, y):
    return torch.sub(out_grad, out_grad * y * y)
@register_decomposition(torch.ops.aten.sigmoid_backward)
def sigmoid_backward_decomposition(out_grad, y):
return out_grad * (y * (1 - y))
@register_decomposition(torch.ops.aten._s_where)
def _s_where_decomposition(a, b, c):
    return torch.where(a, b, c)
@register_decomposition(torch.ops.aten.detach)
def noop(x):
return x
@register_decomposition(torch.ops.aten.softplus_backward)
def softplus_backward_decomposition(out_grad, x, beta, threshold, out):
    return out_grad * torch.sigmoid(x)
USE_DECOMPOSE = False
","return f
return decomposition_decorator
@register_decomposition(aten.tanh_backward)
def tanh_backward_decomposition(out_grad, y):
    return aten.sub(out_grad, out_grad * y * y)
@register_decomposition(aten.sigmoid_backward)
def sigmoid_backward_decomposition(out_grad, y):
return out_grad * (y * (1 - y))
@register_decomposition(aten._s_where)
def _s_where_decomposition(a, b, c):
    return aten.where(a, b, c)
@register_decomposition(aten.detach)
def noop(x):
return x
@register_decomposition(aten.softplus_backward)
def softplus_backward_decomposition(out_grad, x, beta, threshold, out):
    z = (x * beta).exp()
    return aten.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
USE_DECOMPOSE = False
"
46,"continue
if 'std::array' in line:
continue
        m = re.match(r'(.*) \w+\((.*)\); // {""schema"": ""aten::(.*)\(.*', line)
if m is None:
continue
return_t = m.group(1)
","continue
if 'std::array' in line:
continue
        m = re.match(r'(.*) \w+\((.*)\); // {""schema"": ""aten::(\w+\.?\w*)\(.*', line)
if m is None:
continue
return_t = m.group(1)
"
47,"_check_out_dims_is_int_or_int_pytree(out_dims, func)
batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
vmap_level = _vmap_increment_nesting(batch_size)
try:
batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
batched_outputs = func(*batched_inputs)
return _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)
finally:
_vmap_decrement_nesting()
return wrapped
","_check_out_dims_is_int_or_int_pytree(out_dims, func)
batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(in_dims, args, func)
vmap_level = _vmap_increment_nesting(batch_size)
        torch._C._vmapmode_decrement_nesting()
try:
batched_inputs = _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)
batched_outputs = func(*batched_inputs)
return _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)
finally:
_vmap_decrement_nesting()
            torch._C._vmapmode_decrement_nesting()
return wrapped
"
48,"def lower_function(node, op, nnc_args, args):
inp_shapes = fx.node.map_aggregate(args, lambda arg: (process_shape(arg.meta['tensor_meta'].shape), arg.meta['tensor_meta'].dtype) if isinstance(arg, fx.Node) and 'tensor_meta' in arg.meta else None)
if op in lowering_functions:
out = lowering_functions[op](node.name, process_shape(node.meta['tensor_meta'].shape), inp_shapes, nnc_args)
else:
","def lower_function(node, op, nnc_args, args):
inp_shapes = fx.node.map_aggregate(args, lambda arg: (process_shape(arg.meta['tensor_meta'].shape), arg.meta['tensor_meta'].dtype) if isinstance(arg, fx.Node) and 'tensor_meta' in arg.meta else None)
    nnc_args = [x.data() if isinstance(x, te.Placeholder) else x for x in nnc_args]
if op in lowering_functions:
out = lowering_functions[op](node.name, process_shape(node.meta['tensor_meta'].shape), inp_shapes, nnc_args)
else:
"
49,"child_pytrees, context = flatten_fn(pytree)
# Recursively flatten the children
    result : List[Any] = []
    children_specs : List['TreeSpec'] = []
for child in child_pytrees:
flat, child_spec = tree_flatten_hack(child)
result += flat
","child_pytrees, context = flatten_fn(pytree)
# Recursively flatten the children
    result: List[Any] = []
    children_specs: List['TreeSpec'] = []
for child in child_pytrees:
flat, child_spec = tree_flatten_hack(child)
result += flat
"
50,"def f(*inps, out_tensors=None):
inps = fx_model.graph.flatten_inps(*inps)
if out_tensors is None:
            results = alloc_results
else:
results = out_tensors
full_inps = module_stuff + list(inps) + results
","def f(*inps, out_tensors=None):
inps = fx_model.graph.flatten_inps(*inps)
if out_tensors is None:
            results = [torch.empty(shape, dtype=dtype) for shape,dtype in outs[1]]
            # results = alloc_results
else:
results = out_tensors
full_inps = module_stuff + list(inps) + results
"
51,"name=""device"",
type=BaseType(BaseTy.Device),
default=""None"",
                default_init=""self.device()"" if is_like_or_new_function else None,
)
)
tensor_options_args.append(
","name=""device"",
type=BaseType(BaseTy.Device),
default=""None"",
                default_init=(
                    ""self.device()""
                    if is_like_or_new_function
                    else topt_default_init(""device"")
                ),
)
)
tensor_options_args.append(
"
52,")
# TODO blowtorch
# note: removing this will be BC-breaking. A quick test shows that
# randperm will otherwise default its dtype to torch.float64
def _dtype_default_type_hack(name: str) -> str:
    if name.startswith(""randperm"") or name == ""tril_indices"" or name == ""triu_indices"":
        return ""torch.int64""
    else:
        return ""None""


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#
#                          Python Interface
",")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#
#                          Python Interface
"
53,"e1 = symbols[n.args[0]]
# we will propagate the runtime value here since this is regular addition
            c = Conj([BinConstraintD(my_output, BinConstraintD(e1, n.args[1], op_add), op_eq),
BinConstraintD(0, my_output, op_leq)])
return [c], counter
","e1 = symbols[n.args[0]]
# we will propagate the runtime value here since this is regular addition
            c = Conj([BinConstraintD(my_output, BinConstraintD(e1, n.args[1], op_code), op_eq),
BinConstraintD(0, my_output, op_leq)])
return [c], counter
"
54,"e2 = symbols[n.args[1]]
# we will propagate the runtime value here since this is regular addition
            c = Conj([BinConstraintD(my_output, BinConstraintD(e2, n.args[0], op_add), op_eq),
BinConstraintD(0, my_output, op_leq)])
return [c], counter
","e2 = symbols[n.args[1]]
# we will propagate the runtime value here since this is regular addition
            c = Conj([BinConstraintD(my_output, BinConstraintD(e2, n.args[0], op_code), op_eq),
BinConstraintD(0, my_output, op_leq)])
return [c], counter
"
55,"op_registration_allowlist = None
selector = get_custom_build_selector(
        options.op_registration_allowlist,
options.op_selection_yaml_path,
)
","op_registration_allowlist = None
selector = get_custom_build_selector(
        op_registration_allowlist,
options.op_selection_yaml_path,
)
"
56,"ObserverBase,
)
from ..qconfig import (
    _partial_wrapper_equals,
float16_dynamic_qconfig,
float16_static_qconfig,
is_reuse_input_qconfig,
","ObserverBase,
)
from ..qconfig import (
    obs_or_fq_ctr_equals,
float16_dynamic_qconfig,
float16_static_qconfig,
is_reuse_input_qconfig,
"
57,"logging.debug(""Collecting supported nodes..."")
supported_nodes = []
for node in self.graph_module.graph.nodes:
            if self.operator_support.is_node_supported(self.graph_module.named_modules(), node):
supported_nodes.append(node)
return supported_nodes
","logging.debug(""Collecting supported nodes..."")
supported_nodes = []
for node in self.graph_module.graph.nodes:
            if self.operator_support.is_node_supported(dict(self.graph_module.named_modules()), node):
supported_nodes.append(node)
return supported_nodes
"
58,"# softshrink(x) = x - lambd if x > lambd
#               = x + lambd if x < -lambd
#               = 0 otherwise
ge_mask = a > lambd
le_mask = a < -lambd
zero_mask = torch.logical_not(refs.logical_or(ge_mask, le_mask))
","# softshrink(x) = x - lambd if x > lambd
#               = x + lambd if x < -lambd
#               = 0 otherwise
    check(
        lambd >= 0,
        lambda: f""lambda must be greater or equal to 0, but found to be {lambd}"",
    )
ge_mask = a > lambd
le_mask = a < -lambd
zero_mask = torch.logical_not(refs.logical_or(ge_mask, le_mask))
"
59,"Argument,
BackendIndex,
BackendMetadata,
BaseTy,
BaseType,
DEFAULT_KERNEL_NAMESPACE,
","Argument,
BackendIndex,
BackendMetadata,
    BaseOperatorName,
BaseTy,
BaseType,
DEFAULT_KERNEL_NAMESPACE,
"
60,"offload_to_cpu: bool = False,
):
super().__init__()
        self.mod = mod
self.checkpoint_impl = checkpoint_impl
self.offload_to_cpu = offload_to_cpu
# state_dict post hook to remove prefix to allow loading into a
","offload_to_cpu: bool = False,
):
super().__init__()
        self._checkpoint_wrapped_module = mod
self.checkpoint_impl = checkpoint_impl
self.offload_to_cpu = offload_to_cpu
# state_dict post hook to remove prefix to allow loading into a
"
61,")
def _logical_or(a: TensorLikeType, b: TensorLikeType):
if not utils.is_boolean_dtype(a.dtype):
        a = ne(a, 0)
if not utils.is_boolean_dtype(b.dtype):
        b = ne(b, 0)
    return bitwise_or(a, b)
logical_or = _make_elementwise_binary_reference(
",")
@_make_elementwise_unary_reference(
    ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, aten_op=torch.ops.aten.logical_not
)
def logical_not(a: TensorLikeType):
    if not utils.is_boolean_dtype(a.dtype):
        return a == 0
    return ~a


def _logical_or(a: TensorLikeType, b: TensorLikeType):
if not utils.is_boolean_dtype(a.dtype):
        a = a != 0
if not utils.is_boolean_dtype(b.dtype):
        b = b != 0
    return a | b
logical_or = _make_elementwise_binary_reference(
"
62,"return exp, ssi, sei
def scalar(name, scalar, collections=None, new_style=False, double_precision=False):
""""""Outputs a `Summary` protocol buffer containing a single scalar value.
The generated Summary has a Tensor.proto containing the input Tensor.
Args:
","return exp, ssi, sei
def scalar(name, tensor, collections=None, new_style=False, double_precision=False):
""""""Outputs a `Summary` protocol buffer containing a single scalar value.
The generated Summary has a Tensor.proto containing the input Tensor.
Args:
"
63,"import tools.jit.gen_unboxing
tools.jit.gen_unboxing.main()
","import tools.jit.gen_unboxing
tools.jit.gen_unboxing.main(sys.argv[1:])
"
64,"get_arg_indices_of_inputs_to_log,
get_node_input_qparams,
op_type_supports_shadowing,
)
from .ns_types import (
","get_arg_indices_of_inputs_to_log,
get_node_input_qparams,
op_type_supports_shadowing,
    get_normalized_nth_input,
)
from .ns_types import (
"
65,"# second (x + 1 versus 1 + x).
arg_indices_to_log = get_arg_indices_of_inputs_to_log(node)
for node_arg_idx in arg_indices_to_log:
                    node_arg = _get_normalized_nth_input(node, gm, node_arg_idx)
if type(node_arg) == Node:
# create a single input logger
prev_node = env[node_arg.name]
","# second (x + 1 versus 1 + x).
arg_indices_to_log = get_arg_indices_of_inputs_to_log(node)
for node_arg_idx in arg_indices_to_log:
                    node_arg = get_normalized_nth_input(node, gm, node_arg_idx)
if type(node_arg) == Node:
# create a single input logger
prev_node = env[node_arg.name]
"
66,"# So, we look up the output type of the previous node and return that
# as the input type of this node instance. We also look up the target
# of to and return the correct output type.
            prev_node = node.args[0]
assert isinstance(prev_node, Node)
(
_prev_node_input_type,
","# So, we look up the output type of the previous node and return that
# as the input type of this node instance. We also look up the target
# of to and return the correct output type.
            prev_node = get_normalized_nth_input(node, gm, 0)
assert isinstance(prev_node, Node)
(
_prev_node_input_type,
"
67,"return (prev_node_output_type, NodeInputOrOutputType.FP16)
elif node.target in METHS_IO_TYPE_FP32_OR_INT8:
            first_arg = node.args[0]
assert isinstance(first_arg, Node)
(
_prev_node_input_type,
","return (prev_node_output_type, NodeInputOrOutputType.FP16)
elif node.target in METHS_IO_TYPE_FP32_OR_INT8:
            first_arg = get_normalized_nth_input(node, gm, 0)
assert isinstance(first_arg, Node)
(
_prev_node_input_type,
"
68,"from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
","from argparse import ArgumentParser
parser = ArgumentParser(""Merge PR into default branch"")
parser.add_argument(""--dry-run"", action=""store_true"")
    parser.add_argument(""--on-green"", action=""store_true"")
parser.add_argument(""--revert"", action=""store_true"")
parser.add_argument(""--force"", action=""store_true"")
parser.add_argument(""--comment-id"", type=int)
"
69,"raise ValueError(msg)
def check(b, s, exc_type=RuntimeError):
""""""
Helper function for raising a RuntimeError if a boolean condition fails.
Error message is a callable producing a string (to avoid wasting time
","raise ValueError(msg)
def check(
    b: bool, s: Callable[[], str], exc_type: Type[Exception] = RuntimeError
) -> None:
""""""
Helper function for raising a RuntimeError if a boolean condition fails.
Error message is a callable producing a string (to avoid wasting time
"
70,"connector = "",\n\t\t""
args_code = []
for arg in args:
            if not arg.default:
arg_cpp = ""c10::IValue(c10::nullopt)""
            elif arg.default.startswith(""{""):
                arg_cpp = f""c10::IntArrayRef({arg.default})""
else:
                arg_cpp = f""c10::IValue({arg.default})""
args_code.append(
f""""""c10::Argument(""{arg.name}"", nullptr, c10::nullopt, {arg_cpp})""""""
)
","connector = "",\n\t\t""
args_code = []
for arg in args:
            # Using method=False faithful C++ API, so we should not see SelfArgument/TensorOptionsArgument
            assert isinstance(arg.argument, Argument)
            if not arg.argument.default:
arg_cpp = ""c10::IValue(c10::nullopt)""
else:
                # The unboxing code uses the faithful C++ API to avoid the overhead
                # from wrapping/unwrapping TensorOptios.
                # However, we would look to include default args for schema parsing.
                # Default args only show up in the nonfaithful C++ API,
                arg_default = cpp.default_expr(arg.argument.default, arg.argument.type)
                if arg_default.startswith(""{""):
                    arg_cpp = f""c10::IntArrayRef({arg_default})""
                else:
                    arg_cpp = f""c10::IValue({arg_default})""
args_code.append(
f""""""c10::Argument(""{arg.name}"", nullptr, c10::nullopt, {arg_cpp})""""""
)
"
71,"Note: Due to the multi-head attention architecture in the transformer model,
the output sequence length of a transformer is same as the input sequence
            (i.e. target) length of the decode.
where S is the source sequence length, T is the target sequence length, N is the
batch size, E is the feature number
","Note: Due to the multi-head attention architecture in the transformer model,
the output sequence length of a transformer is same as the input sequence
            (i.e. target) length of the decoder.
where S is the source sequence length, T is the target sequence length, N is the
batch size, E is the feature number
"
72,"func (Callable): registered implementation for sharded op for
``__torch_function__`` dispatch.
""""""
    @sharded_op_impl(op)
@_sharded_op_common(op, early_stop_func, extra_check)
def sharded_tensor_op_on_local_shards(types, args=(), kwargs=None, pg=None):
st = args[0]
","func (Callable): registered implementation for sharded op for
``__torch_function__`` dispatch.
""""""
    @_sharded_op_impl(op)
@_sharded_op_common(op, early_stop_func, extra_check)
def sharded_tensor_op_on_local_shards(types, args=(), kwargs=None, pg=None):
st = args[0]
"
73,"from torch.distributed._shard.partial_tensor import _PartialTensor
from .api import (
    _CUSTOM_SHARDED_OPS,
_SHARDED_OPS,
Shard,
ShardedTensor,
","from torch.distributed._shard.partial_tensor import _PartialTensor
from .api import (
_SHARDED_OPS,
Shard,
ShardedTensor,
"
74,"if isinstance(state_dict[key], ShardedTensor):
setattr(submodule, attr_name, state_dict[key])
def custom_sharded_op_impl(func):
""""""
Provides a way for users to write their own custom sharded operator. This
can be used to override existing ShardedTensor operators or write a new
","if isinstance(state_dict[key], ShardedTensor):
setattr(submodule, attr_name, state_dict[key])
def sharded_op_impl(func):
""""""
Provides a way for users to write their own custom sharded operator. This
can be used to override existing ShardedTensor operators or write a new
"
75,"# tensor ops that behave the same as the default tensor
def register_tensor_creation_op(op):
    @_sharded_op_impl(op)
def tensor_creation_op(types, args=(), kwargs=None, pg=None):
""""""
Handles ``__torch_function__`` dispatch for tensor creation ops that
","# tensor ops that behave the same as the default tensor
def register_tensor_creation_op(op):
    @sharded_op_impl(op)
def tensor_creation_op(types, args=(), kwargs=None, pg=None):
""""""
Handles ``__torch_function__`` dispatch for tensor creation ops that
"
76,")
from torch.distributed._shard.common_op_utils import _register_default_op
@_sharded_op_impl(torch.Tensor.__deepcopy__)
def tensor_deepcopy(types, args=(), kwargs=None, pg=None):
# NOTE: we directly implement deepcopy magic method
# instead of using the default tensor.__deepcopy__
",")
from torch.distributed._shard.common_op_utils import _register_default_op
@sharded_op_impl(torch.Tensor.__deepcopy__)
def tensor_deepcopy(types, args=(), kwargs=None, pg=None):
# NOTE: we directly implement deepcopy magic method
# instead of using the default tensor.__deepcopy__
"
77,"@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
            # Dispatch to custom user provided op first if it exists.
            if func in _CUSTOM_SHARDED_OPS:
                return _CUSTOM_SHARDED_OPS[func](types, args, kwargs, st._process_group)

# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
","@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
def dispatch(st: ShardedTensor, func: Callable):
# Dispatch to custom sharding spec op if it has one.
if _has_custom_op(st._sharding_spec, func):
return _dispatch_custom_op(
"
78,"from torch.distributed._shard.partial_tensor import _PartialTensor
from .api import (
_SHARDED_OPS,
Shard,
ShardedTensor,
","from torch.distributed._shard.partial_tensor import _PartialTensor
from .api import (
    _CUSTOM_SHARDED_OPS,
_SHARDED_OPS,
Shard,
ShardedTensor,
"
79,"torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
","torch.nn.init.normal_(shard.tensor, mean=mean, std=std)
return sharded_tensor
@_sharded_op_impl(torch.nn.init.kaiming_uniform_)
def kaiming_uniform_(types, args=(), kwargs=None, pg=None):
r""""""
Fills the Tensors in sharded_tensor.local_shards with values according to the method
"
80,"full, unsharded, unflattened original module parameters. This requires
FSDP to load the full parameter context on each rank which could result
in GPU OOM. As a result, :func:`state_dict_type` API is available to
        configure between `load_state_dict` implementations. User can thus use
``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`` context
manager to load a local state dict checkpoint that will restore only
local shards of the module. Currently, the only supported
","full, unsharded, unflattened original module parameters. This requires
FSDP to load the full parameter context on each rank which could result
in GPU OOM. As a result, :func:`state_dict_type` API is available to
        configure between ``load_state_dict`` implementations. User can thus use
``with self.state_dict_type(self, StateDictType.LOCAL_STATE_DICT)`` context
manager to load a local state dict checkpoint that will restore only
local shards of the module. Currently, the only supported
"
81,"def load_state_dict(
state_dict: Dict[str, Any],
storage_reader: StorageReader,
) -> None:
""""""
Load a distributed state_dict in SPMD style.
","def load_state_dict(
state_dict: Dict[str, Any],
storage_reader: StorageReader,
    process_group: Optional[dist.ProcessGroup] = None,
    coordinator_rank: int = 0,
    no_dist: bool = False
) -> None:
""""""
Load a distributed state_dict in SPMD style.
"
82,"tensor_write_requests: List[TensorWriteRequest] = []
bytes_write_requests: List[BytesWriteRequest] = []
storage_key_to_fqn: Dict[str, str] = dict()
    # The assumption is that all non ShardedTensor items are replicated
    #   and we can save them from rank 0.
    write_replicated_data = not (dist.is_initialized() and dist.get_rank() != 0)
for fqn, obj in state_dict.items():
if isinstance(obj, ShardedTensor):
","tensor_write_requests: List[TensorWriteRequest] = []
bytes_write_requests: List[BytesWriteRequest] = []
storage_key_to_fqn: Dict[str, str] = dict()
for fqn, obj in state_dict.items():
if isinstance(obj, ShardedTensor):
"
83,"Default: ``False``.
.. note:: ShardedTensor uses collectives to do various operations, i.e. it
        uses all_gather to do cross rank validations. For NCCL-based processed
groups, internal tensor representations of objects must be moved to the
GPU device before communication takes place. In this case, the device
used is given by ``torch.cuda.current_device()`` and it is the user's
        responsiblity to ensure that this is set so that each rank has an
individual GPU, via ``torch.cuda.set_device()``
""""""
","Default: ``False``.
.. note:: ShardedTensor uses collectives to do various operations, i.e. it
        uses all_gather to do cross rank validations. For NCCL-based process
groups, internal tensor representations of objects must be moved to the
GPU device before communication takes place. In this case, the device
used is given by ``torch.cuda.current_device()`` and it is the user's
        responsibility to ensure that this is set so that each rank has an
individual GPU, via ``torch.cuda.set_device()``
""""""
"
84,"""""""
offload_params: bool = False
    # TODO: state dict offloading
    # https://github.com/pytorch/pytorch/issues/67224
class BackwardPrefetch(Enum):
","""""""
offload_params: bool = False
class BackwardPrefetch(Enum):
"
85,"""_slow_conv2d_forward"",
""slow_conv3d_forward"",
""channel_shuffle"",
# If an input is returned as-is in output, we cannot guarantee its storage_impl
# use count to be 1 either.
*DONT_ENFORCE_TENSOR_IMPL_USE_COUNT,
","""_slow_conv2d_forward"",
""slow_conv3d_forward"",
""channel_shuffle"",
    # lift() should never actually be called with a requires_grad=True tensor,
    ""lift"",
# If an input is returned as-is in output, we cannot guarantee its storage_impl
# use count to be 1 either.
*DONT_ENFORCE_TENSOR_IMPL_USE_COUNT,
"
86,"import torch
from torch.autograd.graph import save_on_cpu
from torch.utils.checkpoint import checkpoint
class CheckpointImpl(Enum):
","import torch
from torch.autograd.graph import save_on_cpu
from torch.utils.checkpoint import checkpoint
from torch.distributed.utils import _replace_by_prefix
import torch.nn as nn
from typing import Dict, Any

_CHECKPOINT_PREFIX = ""mod""
class CheckpointImpl(Enum):
"
87,"return view_str
else:
f = g
assert not f.is_view_op
# functionalization needs to generate and register kernals for inplace ops.
# We *also* need to directly register CompositeImplicitAUtograd kernels
","return view_str
else:
f = g
        if str(f.func.name) == ""lift"":
            # See Note [Functionalization <> torch.Tensor constructor]
            return []
assert not f.is_view_op
# functionalization needs to generate and register kernals for inplace ops.
# We *also* need to directly register CompositeImplicitAUtograd kernels
"
88,"""copy"",  # only used by the functionalization pass
""fill.Tensor"",  # only used by the functionalization pass
""fill.Scalar"",  # only used by the functionalization pass
    ""lift"",
]
SKIP_PYTHON_BINDINGS = list(
","""copy"",  # only used by the functionalization pass
""fill.Tensor"",  # only used by the functionalization pass
""fill.Scalar"",  # only used by the functionalization pass
]
SKIP_PYTHON_BINDINGS = list(
"
89,"implementation (ex: torch.nn.functional.linear)
""""""
def decorator_sharded_func(wrapped_func):
        _register_sharded_op(func, wrapped_func)
@functools.wraps(wrapped_func)
        def wrapper(*args, **kwargs):
            return wrapped_func(*args, **kwargs)
return wrapper
return decorator_sharded_func
","implementation (ex: torch.nn.functional.linear)
""""""
def decorator_sharded_func(wrapped_func):
        from torch.distributed._shard.sharded_tensor._ops._common import _basic_validation
@functools.wraps(wrapped_func)
        def wrapper(types, args, kwargs, process_group):
            _basic_validation(func, args, kwargs)
            return wrapped_func(types, args, kwargs, process_group)

        _register_sharded_op(func, wrapper)
return wrapper
return decorator_sharded_func
"
90,"# nn.Module.state_dict() will detach the parameter. Therefore, we need
# to get flat_param from the FlattenParamsWrapper to get the metadata.
flat_param = getattr(self.module, FLAT_PARAM, None)
        assert (
            flat_param is not None
        ), ""flat_param cannot be None when doing local_state_dict.""
# Construct a ShardedTensor from the flat_param.
full_numel = flat_param.full_numel
","# nn.Module.state_dict() will detach the parameter. Therefore, we need
# to get flat_param from the FlattenParamsWrapper to get the metadata.
flat_param = getattr(self.module, FLAT_PARAM, None)
        if flat_param is None:
            return state_dict
# Construct a ShardedTensor from the flat_param.
full_numel = flat_param.full_numel
"
91,"that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
","that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
            Note that if ``mixed_precision`` is enabled for FSDP model that
            contains ``BatchNorm`` with ``auto_wrap_policy``, FSDP will take
            care to disable mixed precision for ``BatchNorm`` units by wrapping
            them separately in their own FSDP unit with ``mixed_precision=None``.
            This is done because several ``BatchNorm`` kernels do not implement
            reduced type support at the moment. If individually wrapping the model,
            users must take care to set ``mixed_precision=None`` for
            ``BatchNorm`` units.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
"
92,"that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
            Note that if ``mixed_precision`` is enabled for FSDP model that
            contains ``BatchNorm`` with ``auto_wrap_policy``, FSDP will take
            care to disable mixed precision for ``BatchNorm`` units by wrapping
            them separately in their own FSDP unit with ``mixed_precision=None``.
            This is done because several ``BatchNorm`` kernels do not implement
            reduced type support at the moment. If individually wrapping the model,
            users must take care to set ``mixed_precision=None`` for
            ``BatchNorm`` units.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
","that only floating point data is cast to the reduced precision. This allows
users potential memory saving and training speedup while trading off
accuracy during model training. If ``None``, no mixed precision is applied.
(Default: ``None``)
ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose
own parameters and child modules' parameters and buffers are
"
93,"from typing import Any, Callable, Dict, List, Set, Tuple, Union
import torch
from torch.nn.utils.rnn import PackedSequence
""""""Useful functions to deal with tensor types with other python container types.""""""
def _apply_to_tensors(
fn: Callable, container: Union[torch.Tensor, Dict, List, Tuple, Set, OrderedDict, PackedSequence]
","from typing import Any, Callable, Dict, List, Set, Tuple, Union
import torch
from torch.nn.modules.batchnorm import _BatchNorm

from torch.nn.utils.rnn import PackedSequence
""""""Useful functions to deal with tensor types with other python container types.""""""
def _contains_batchnorm(module):
    return any(
        isinstance(mod, _BatchNorm) for mod in module.modules()
    )

def _override_batchnorm_mixed_precision(module):
    for mod in module.modules():
        if isinstance(mod, _BatchNorm):
            mod._wrap_overrides = {""mixed_precision"": None}  # type: ignore[assignment]
def _apply_to_tensors(
fn: Callable, container: Union[torch.Tensor, Dict, List, Tuple, Set, OrderedDict, PackedSequence]
"
94,"Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
the returned FSDP root instance.
            ``default_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is
an example of ``auto_wrap_policy`` callable, this policy wraps layers
            with parameter sizes larger than 100M. Users can supply the customized
``auto_wrap_policy`` callable that should accept following arguments:
``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``,
extra customized arguments could be added to the customized
            ``auto_wrap_policy`` callable as well.
Example::
","Note that this policy currently will only apply to child modules of
the passed in module. The remainder modules are always wrapped in
the returned FSDP root instance.
            ``size_based_auto_wrap_policy`` written in ``torch.distributed.fsdp.wrap`` is
an example of ``auto_wrap_policy`` callable, this policy wraps layers
            with the number of parameters larger than 100M. ``transformer_auto_wrap_policy``
            written in ``torch.distributed.fsdp.wrap`` is an example of ``auto_wrap_policy``
            callable for tranformer-like model architectures. Users can supply the customized
``auto_wrap_policy`` callable that should accept following arguments:
``module: nn.Module``, ``recurse: bool``, ``unwrapped_params: int``,
extra customized arguments could be added to the customized
            ``auto_wrap_policy`` callable as well. It is a good practice to print out
            the sharded model and check whether the sharded model is what
            the application wants and then adjust accordingly.
Example::
"
95,">>> module = MyModule(device=""meta"")
>>> def my_init_fn(module):
>>>     # responsible for initializing a module, such as with reset_parameters
                >>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=default_auto_wrap_policy)
>>> print(next(fsdp_model.parameters()).device) # current CUDA device
>>> # With torchdistX
>>> module = deferred_init.deferred_init(MyModule, device=""cuda"")
>>> # Will initialize via deferred_init.materialize_module().
                >>> fsdp_model = FSDP(module, auto_wrap_policy=default_auto_wrap_policy)
""""""
",">>> module = MyModule(device=""meta"")
>>> def my_init_fn(module):
>>>     # responsible for initializing a module, such as with reset_parameters
                >>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)
>>> print(next(fsdp_model.parameters()).device) # current CUDA device
>>> # With torchdistX
>>> module = deferred_init.deferred_init(MyModule, device=""cuda"")
>>> # Will initialize via deferred_init.materialize_module().
                >>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)
""""""
"
96,"store.set(my_token, ""Done"")
break
else:
            # token_name = returned.split(""-"")[0]
# Store will wait for the token to be released
            store.wait([returned])
def _update_group_membership(worker_info, my_devices, reverse_device_map, is_join):
agent = cast(TensorPipeAgent, api._get_current_rpc_agent())
","store.set(my_token, ""Done"")
break
else:
# Store will wait for the token to be released
            try:
                store.wait([returned])
            except RuntimeError:
                logger.error(f""Group membership token {my_token} timed out waiting for {returned} to be released."")
                raise
def _update_group_membership(worker_info, my_devices, reverse_device_map, is_join):
agent = cast(TensorPipeAgent, api._get_current_rpc_agent())
"
97,"@register_decomposition(aten.tanh_backward)
@cast_for_opmath
def tanh_backward(out_grad: Tensor, y: Tensor):
return out_grad * (1 - y * y).conj_physical()
@register_decomposition(aten.sigmoid_backward)
@cast_for_opmath
def sigmoid_backward(out_grad: Tensor, y: Tensor):
return out_grad * (y * (1 - y)).conj_physical()
@register_decomposition(aten.softplus_backward)
@cast_for_opmath
def softplus_backward(out_grad: Tensor, x: Tensor, beta: float, threshold: float):
z = (x * beta).exp()
return torch.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
@register_decomposition(aten.elu)
@cast_for_opmath
def elu(
self: Tensor, alpha: float = 1, scale: float = 1, input_scale: float = 1
) -> Tensor:
","@register_decomposition(aten.tanh_backward)
@pw_cast_for_opmath
def tanh_backward(out_grad: Tensor, y: Tensor):
return out_grad * (1 - y * y).conj_physical()
@register_decomposition(aten.sigmoid_backward)
@pw_cast_for_opmath
def sigmoid_backward(out_grad: Tensor, y: Tensor):
return out_grad * (y * (1 - y)).conj_physical()
@register_decomposition(aten.softplus_backward)
@pw_cast_for_opmath
def softplus_backward(out_grad: Tensor, x: Tensor, beta: float, threshold: float):
z = (x * beta).exp()
return torch.where((x * beta) > threshold, out_grad, out_grad * z / (z + 1.0))
@register_decomposition(aten.elu)
@pw_cast_for_opmath
def elu(
self: Tensor, alpha: float = 1, scale: float = 1, input_scale: float = 1
) -> Tensor:
"
98,"@register_decomposition(aten.gelu)
@cast_for_opmath
def gelu(self: Tensor, approximate: str = 'none') -> Tensor:
M_SQRT2 = 1.41421356237309504880
M_SQRT1_2 = 0.70710678118654752440
","@register_decomposition(aten.gelu)
@pw_cast_for_opmath
def gelu(self: Tensor, approximate: str = 'none') -> Tensor:
M_SQRT2 = 1.41421356237309504880
M_SQRT1_2 = 0.70710678118654752440
"
99,"@register_decomposition(aten.mse_loss_backward)
@cast_for_opmath
def mse_loss_backward(
grad_output: Tensor, input: Tensor, target: Tensor, reduction: int
):
","@register_decomposition(aten.mse_loss_backward)
@pw_cast_for_opmath
def mse_loss_backward(
grad_output: Tensor, input: Tensor, target: Tensor, reduction: int
):
"
100,"@register_decomposition(aten.isnan)
def isnan(self: Tensor) -> Tensor:
    return torch.where(
        self != self,
        self.new_ones((), dtype=torch.bool),
        self.new_zeros((), dtype=torch.bool),
    )
@register_decomposition(aten.clamp_min)
","@register_decomposition(aten.isnan)
def isnan(self: Tensor) -> Tensor:
    return self != self
@register_decomposition(aten.clamp_min)
"
101,"#                                  self * aten.log(other)))
# TODO: var and std OpInfo doesn't the next two decomps, how to get here?


@register_decomposition(aten.var.correction)
@cast_for_opmath
def var_decomposition(
    x: Tensor, dims: List[int], correction: int = 0, keepdim: bool = False
):
if dims is None:
dims = []
    if isinstance(dims, (tuple, list)) and len(dims) == 0:
        n = x.numel()
else:
n = 1
for dim in dims:
","#                                  self * aten.log(other)))
@register_decomposition(aten.var.correction)
@reduction_complex_to_real
def var_correction(
    x: Tensor,
    dims: Optional[List[int]],
    correction: Optional[int] = None,
    keepdim: bool = False,
):
if dims is None:
dims = []
    if x.is_complex():
        # For complex, calculate variance of real and imaginary components
        # separately then add to get overall variance.
        real_in = x.real
        var_real = torch.var(real_in, dims, correction=correction, keepdim=keepdim)
        imag_in = x.imag
        var_imag = torch.var(imag_in, dims, correction=correction, keepdim=keepdim)
        return var_real + var_imag

    if correction is None:
        correction = 0

    if len(dims) == 0:
        n = prod(x.shape)  # type: ignore[arg-type]
else:
n = 1
for dim in dims:
"
102,"# mapping from module to output activation post process class
DEFAULT_MODULE_TO_ACT_POST_PROCESS : Dict[Callable, Callable] = {
    nn.Hardsigmoid: default_affine_fixed_qparams_fake_quant,
    nn.Sigmoid: default_affine_fixed_qparams_fake_quant,
    nn.Softmax: default_affine_fixed_qparams_fake_quant,
    nn.Tanh: default_symmetric_fixed_qparams_fake_quant,
}
# Default map for swapping float module to static sparse quantized ones
","# mapping from module to output activation post process class
DEFAULT_MODULE_TO_ACT_POST_PROCESS : Dict[Callable, Callable] = {
    nn.Hardsigmoid: default_fixed_qparams_range_0to1_fake_quant,
    nn.Sigmoid: default_fixed_qparams_range_0to1_fake_quant,
    nn.Softmax: default_fixed_qparams_range_0to1_fake_quant,
    nn.Tanh: default_fixed_qparams_range_neg1to1_fake_quant,
}
# Default map for swapping float module to static sparse quantized ones
"
103,"""""""
Initializer script that installs stuff to pip.
""""""
import argparse
import logging
import subprocess
","""""""
Initializer script that installs stuff to pip.
""""""
import os
import argparse
import logging
import subprocess
"
104,"def _load_from_state_dict_script(
self,
        state_dict: Union[Dict[str, torch.Tensor], Dict[str, torch.Tensor]],
prefix: str,
local_metadata: Dict[str, torch.Tensor],
strict: bool,
","def _load_from_state_dict_script(
self,
        state_dict: Dict[str, Any],
prefix: str,
local_metadata: Dict[str, torch.Tensor],
strict: bool,
"
105,"if input_name not in self._modules and input_name not in local_state:
unexpected_keys.append(key)
    def load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]',
strict: bool = True):
r""""""Copies parameters and buffers from :attr:`state_dict` into
this module and its descendants. If :attr:`strict` is ``True``, then
","if input_name not in self._modules and input_name not in local_state:
unexpected_keys.append(key)
    def load_state_dict(self, state_dict: Mapping[str, Any],
strict: bool = True):
r""""""Copies parameters and buffers from :attr:`state_dict` into
this module and its descendants. If :attr:`strict` is ``True``, then
"
106,"return (d_input, d_weight, d_bias)
@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return torch.clamp(self, min=min)
","return (d_input, d_weight, d_bias)
@register_decomposition(aten.native_batch_norm)
@cast_for_opmath
def native_batch_norm(
    input: Tensor,
    weight: Optional[Tensor],
    bias: Optional[Tensor],
    running_mean: Optional[Tensor],
    running_var: Optional[Tensor],
    training: bool,
    momentum: float,
    eps: float,
) -> Tuple[Tensor, Tensor, Tensor]:
    reduction_dims = [0] + list(range(2, input.dim()))
    if training:
        # save_mean = torch.sum(input / (input.shape[0] * input.shape[2]), dim=reduction_dims)
        biased_var, save_mean = torch.var_mean(
            input, dim=reduction_dims, unbiased=False
        )
        save_invstd = 1 / (torch.sqrt(biased_var + eps))

        if running_mean is not None:
            running_mean.copy_(momentum * save_mean + (1 - momentum) * running_mean)
        if running_var is not None:
            n = input.numel() / input.shape[1]
            # This doesn't strictly match eager's numerics, which accumulates var sum and then directly applies the correction
            # But... that would require re-implementing var here, for negligible numerics gain on a tensor whose
            # numerics probably don't matter.
            unbiased_var = biased_var * (n / (n - 1))
            running_var.copy_(momentum * unbiased_var + (1 - momentum) * running_var)
        mean = save_mean
        invstd = save_invstd
    else:
        assert running_mean is not None and running_var is not None
        mean = running_mean
        invstd = 1 / (torch.sqrt(running_var + eps))
        # Very annoying inconsistency where CPU and CUDA give different shapes
        if input.device.type == ""cuda"":
            save_mean = running_mean
            save_invstd = invstd
        else:
            save_mean = input.new_zeros((0,))
            save_invstd = input.new_zeros((0,))

    if weight is None:
        weight = input.new_ones(())

    if bias is None:
        bias = input.new_zeros(())

    mean = _unsqueeze_to_dim(mean, input.dim() - 1)
    invstd = _unsqueeze_to_dim(invstd, input.dim() - 1)
    weight = _unsqueeze_to_dim(weight, input.dim() - 1)
    bias = _unsqueeze_to_dim(bias, input.dim() - 1)
    output = ((input - mean) * invstd) * weight + bias
    return output, save_mean, save_invstd


@register_decomposition(aten.isnan)
def isnan(self: Tensor) -> Tensor:
    return torch.where(
        self != self,
        self.new_ones((), dtype=torch.bool),
        self.new_zeros((), dtype=torch.bool),
    )


@register_decomposition(aten.clamp_min)
def clamp_min(self: Tensor, min: float):
return torch.clamp(self, min=min)
"
107,"orig_dirpath = dirpath
dirpath = dirpath.replace('cuda', 'hip')
dirpath = dirpath.replace('THC', 'THH')
root = root.replace('cuda', 'hip')
","orig_dirpath = dirpath
dirpath = dirpath.replace('cuda', 'hip')
    dirpath = dirpath.replace('CUDA', 'HIP')
dirpath = dirpath.replace('THC', 'THH')
root = root.replace('cuda', 'hip')
"
108,"{self.build_ir_node(func, schema)}
{self.return_aten_tensor(func, schema)}
}};\n
    """"""]
class ComputeShapeSignature:
","{self.build_ir_node(func, schema)}
{self.return_aten_tensor(func, schema)}
}};\n
    """"""
        ]
class ComputeShapeSignature:
"
109,"for dispatch_key in dispatch_keys:
fm = cuda_fm if is_cuda_dispatch_key(dispatch_key) else cpu_fm
if dispatch_key in functions_keys:
            inl_headers = f'#include <ATen/{dispatch_key}Functions_inl.h>'
fm.write_with_template(
f""{dispatch_key}Functions.h"",
","for dispatch_key in dispatch_keys:
fm = cuda_fm if is_cuda_dispatch_key(dispatch_key) else cpu_fm
if dispatch_key in functions_keys:
            inl_headers = f""#include <ATen/{dispatch_key}Functions_inl.h>""
fm.write_with_template(
f""{dispatch_key}Functions.h"",
"
110,"mutable_input_post_processing = ""\n"".join(
[
f""""""
      auto {a.name}_functional = at::functionalization::impl::unsafeGetFunctionalWrapper({a.name});
      {a.name}_functional->replace_(tmp_output);
      {a.name}_functional->commit_update();""""""
for a in f.func.arguments.flat_all
if a.annotation and a.annotation.is_write and a.type.is_tensor_like()
]
","mutable_input_post_processing = ""\n"".join(
[
f""""""
      at::functionalization::impl::replace_({a.name}, tmp_output);
      at::functionalization::impl::commit_update({a.name});""""""
for a in f.func.arguments.flat_all
if a.annotation and a.annotation.is_write and a.type.is_tensor_like()
]
"
111,"@functional_datapipe('read_from_stream')
class StreamReaderIterDataPipe(IterDataPipe[Tuple[str, bytes]]):
r""""""
    Given IO streams and their label names, yields bytes with label name in a tuple.
Args:
datapipe: Iterable DataPipe provides label/URL and byte stream
","@functional_datapipe('read_from_stream')
class StreamReaderIterDataPipe(IterDataPipe[Tuple[str, bytes]]):
r""""""
    Given IO streams and their label names, yields bytes with label
    name in a tuple (functional name: ``read_from_stream``).
Args:
datapipe: Iterable DataPipe provides label/URL and byte stream
"
112,"else:
self.__dict__[k] = v
class DataChunk(list, Generic[T]):
def __init__(self, items):
","else:
self.__dict__[k] = v
    def __repr__(self):
        if self.repr_hook is not None:
            return self.repr_hook(self)
        # Instead of showing <torch. ... .MapperMapDataPipe object at 0x.....>, return the class name
        return str(self.__class__.__qualname__)

    def __str__(self):
        if self.str_hook is not None:
            return self.str_hook(self)
        # Instead of showing <torch. ... .MapperMapDataPipe object at 0x.....>, return the class name
        return str(self.__class__.__qualname__)

class DataChunk(list, Generic[T]):
def __init__(self, items):
"
113,"self.num_workers = num_workers
self.prefetch_factor = prefetch_factor
self.pin_memory = pin_memory
self.timeout = timeout
self.worker_init_fn = worker_init_fn
self.multiprocessing_context = multiprocessing_context
","self.num_workers = num_workers
self.prefetch_factor = prefetch_factor
self.pin_memory = pin_memory
        self.pin_memory_device = pin_memory_device
self.timeout = timeout
self.worker_init_fn = worker_init_fn
self.multiprocessing_context = multiprocessing_context
"
114,"raise TypeError(""Expected return type of '__iter__' as a subtype of {}, but found {}""
"" for {}"".format(sub_cls.type, _type_repr(data_type), sub_cls.__name__))
def reinforce_type(self, expected_type):
r""""""
Reinforce the type for DataPipe instance. And the 'expected_type' is required
","raise TypeError(""Expected return type of '__iter__' as a subtype of {}, but found {}""
"" for {}"".format(sub_cls.type, _type_repr(data_type), sub_cls.__name__))

def reinforce_type(self, expected_type):
r""""""
Reinforce the type for DataPipe instance. And the 'expected_type' is required
"
115,"if contents != new_contents:
msg = LintMessage(
path=args.native_functions_yml,
            line=1,
            char=1,
code=""NATIVEFUNCTIONS"",
severity=LintSeverity.ERROR,
name=""roundtrip inconsistency"",
","if contents != new_contents:
msg = LintMessage(
path=args.native_functions_yml,
            line=None,
            char=None,
code=""NATIVEFUNCTIONS"",
severity=LintSeverity.ERROR,
name=""roundtrip inconsistency"",
"
116,"for name in dir(_C._VariableFunctions):
if name.startswith('__') or name in PRIVATE_OPS:
continue
    globals()[name] = getattr(_C._VariableFunctions, name)
__all__.append(name)
################################################################################
","for name in dir(_C._VariableFunctions):
if name.startswith('__') or name in PRIVATE_OPS:
continue
    obj = getattr(_C._VariableFunctions, name)
    obj.__module__ = 'torch'
    globals()[name] = obj
__all__.append(name)
################################################################################
"
117,"# matter
gpu_arch_type = arch_type(arch_version)
gpu_arch_version = """" if arch_version == ""cpu"" else arch_version
ret.append(
{
""gpu_arch_type"": gpu_arch_type,
","# matter
gpu_arch_type = arch_type(arch_version)
gpu_arch_version = """" if arch_version == ""cpu"" else arch_version
            # ROCm builds without-deps failed even in ROCm runners; skip for now
            if gpu_arch_type == ""rocm"" and ""without-deps"" in libtorch_variant:
                continue
ret.append(
{
""gpu_arch_type"": gpu_arch_type,
"
118,".. _NEP-0018:
https://numpy.org/neps/nep-0018-array-function-protocol.html
""""""
# Runtime is O(num_arguments * num_unique_types)
overloaded_types: Set[Type] = set()
overloaded_args: List[Any] = []
",".. _NEP-0018:
https://numpy.org/neps/nep-0018-array-function-protocol.html
""""""
    # If torch function is not enabled, there are no overloaded types
    if not torch._C._is_torch_function_enabled():
        return []
# Runtime is O(num_arguments * num_unique_types)
overloaded_types: Set[Type] = set()
overloaded_args: List[Any] = []
"
119,"import torch.nn.intrinsic.qat as nniqat
import torch.nn.qat as nnqat
import torch.nn.quantized._reference as nnqr
from ...observer import default_affine_fixed_qparams_observer
from ...fake_quantize import FixedQParamsFakeQuantize
from ...fuser_method_mappings import reverse_sequential_wrapper2
","import torch.nn.intrinsic.qat as nniqat
import torch.nn.qat as nnqat
import torch.nn.quantized._reference as nnqr
from ...observer import (
    default_affine_fixed_qparams_observer,
    default_symmetric_fixed_qparams_observer,
)
from ...fake_quantize import FixedQParamsFakeQuantize
from ...fuser_method_mappings import reverse_sequential_wrapper2
"
120,"""""""
pass
@register_quant_pattern(torch.nn.functional.hardsigmoid, default_affine_fixed_qparams_observer)
@register_quant_pattern('hardsigmoid', default_affine_fixed_qparams_observer)
@register_quant_pattern('hardsigmoid_', default_affine_fixed_qparams_observer)
@register_quant_pattern(torch.nn.Sigmoid, default_affine_fixed_qparams_observer)
@register_quant_pattern(torch.sigmoid, default_affine_fixed_qparams_observer)
@register_quant_pattern('sigmoid', default_affine_fixed_qparams_observer)
@register_quant_pattern('sigmoid_', default_affine_fixed_qparams_observer)
@register_quant_pattern(torch.nn.Tanh, default_symmetric_fixed_qparams_observer)
@register_quant_pattern(torch.tanh, default_symmetric_fixed_qparams_observer)
@register_quant_pattern('tanh', default_symmetric_fixed_qparams_observer)
@register_quant_pattern('tanh_', default_symmetric_fixed_qparams_observer)
class FixedQParamsOpQuantizeHandler(QuantizeHandler):
    # some qhandlers override the activations constructor
    def get_activation_ctr(self, qconfig, pattern, is_training) -> Optional[Callable]:
        act_dtype = activation_dtype(qconfig)
        if act_dtype == torch.quint8:
            return get_default_output_activation_post_process_map(is_training).get(
                pattern, qconfig.activation)
        else:
            return qconfig.activation
@register_quant_pattern(torch.nn.AdaptiveAvgPool1d)
@register_quant_pattern(torch.nn.AdaptiveAvgPool2d)
","""""""
pass
# TODO: remove this class
class FixedQParamsOpQuantizeHandler(QuantizeHandler):
    pass
@register_quant_pattern(torch.nn.AdaptiveAvgPool1d)
@register_quant_pattern(torch.nn.AdaptiveAvgPool2d)
"
121,"from typing import Sequence, List, Dict
from tools.autograd.gen_python_functions import should_generate_py_binding, load_signatures, group_overloads
from libfb.py import parutil
""""""
This module implements generation of type stubs for PyTorch,
","from typing import Sequence, List, Dict
from tools.autograd.gen_python_functions import should_generate_py_binding, load_signatures, group_overloads
""""""
This module implements generation of type stubs for PyTorch,
"
122,"# so, we don't export then to it
from_c.extend(['hardtanh', 'leaky_relu', 'hardsigmoid'])
dispatch_code = [""{}: Callable"".format(_) for _ in (dispatches + from_c)]
    fm.write_with_template('torch/_C/_nn.pyi', respath(""_nn.pyi.in""), lambda: {
'imported_hints': import_code,
'dispatched_hints': dispatch_code,
})
","# so, we don't export then to it
from_c.extend(['hardtanh', 'leaky_relu', 'hardsigmoid'])
dispatch_code = [""{}: Callable"".format(_) for _ in (dispatches + from_c)]
    fm.write_with_template('torch/_C/_nn.pyi', 'torch/_C/_nn.pyi.in', lambda: {
'imported_hints': import_code,
'dispatched_hints': dispatch_code,
})
"
123,"to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))
elif is_tensor_list_type(inp.type):
cond = FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=""_t"")
                res += FW_DERIVATIVE_FORBID_LIST_TEMPLATE.substitute(arg=inp.name, cond=cond, msg=get_msg())
else:
raise RuntimeError(f'Unsupported input type for ""{name}"" when forbidding forward AD usage.')
if len(to_check) > 0:
cond = "" || "".join(to_check)
            res += FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, msg=get_msg())
return res
body: List[str] = []
","to_check.append(FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name))
elif is_tensor_list_type(inp.type):
cond = FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=""_t"")
                res += FW_DERIVATIVE_FORBID_LIST_TEMPLATE.substitute(arg=inp.name, cond=cond, name=name, msg=get_msg())
else:
raise RuntimeError(f'Unsupported input type for ""{name}"" when forbidding forward AD usage.')
if len(to_check) > 0:
cond = "" || "".join(to_check)
            res += FW_DERIVATIVE_FORBID_TEMPLATE.substitute(cond=cond, name=name, msg=get_msg())
return res
body: List[str] = []
"
124,"None, torch.per_tensor_affine, torch.per_channel_affine,
torch.per_channel_affine_float_qparams], \
Exception(f""qscheme: {self.weight_qscheme} is not support in reference quantized {self._get_name()}"")
        if self.weight_dtype in [torch.quint8, torch.qint8, torch.quint4x2]:
zero_point_dtype = weight_qparams[""zero_point""].dtype if \
isinstance(weight_qparams[""zero_point""], torch.Tensor) else \
torch.int
","None, torch.per_tensor_affine, torch.per_channel_affine,
torch.per_channel_affine_float_qparams], \
Exception(f""qscheme: {self.weight_qscheme} is not support in reference quantized {self._get_name()}"")
        if self.weight_dtype in [torch.quint8, torch.qint8, torch.quint4x2, torch.qint32]:
zero_point_dtype = weight_qparams[""zero_point""].dtype if \
isinstance(weight_qparams[""zero_point""], torch.Tensor) else \
torch.int
"
125,"self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
else:
            # added for TorchScriptability, not used
self.register_buffer(""weight_scale"", torch.tensor(1.0, dtype=torch.float, device=device))
self.register_buffer(""weight_zero_point"", torch.tensor(0, dtype=torch.int, device=device))
self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))

def get_weight(self):
""""""
Fake quantize (quantize and dequantize) the weight with
","self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
else:
            # added for TorchScriptability, and for torch.float
self.register_buffer(""weight_scale"", torch.tensor(1.0, dtype=torch.float, device=device))
self.register_buffer(""weight_zero_point"", torch.tensor(0, dtype=torch.int, device=device))
self.register_buffer(
""weight_axis"", torch.tensor(0, dtype=torch.int, device=device))
def get_weight(self):
""""""
Fake quantize (quantize and dequantize) the weight with
"
126,"process_group (Optional[ProcessGroup]):
process group for sharding
sharding_strategy (Optional[ShardingStrategy]):
            Config sharding algorithm, different sharding algorithm has trade off
            between memory saving and communication overhead. 'FULL_SHARD' will
            be chose if sharding_strategy is not specified.
        cpu_offload (Optional [CPUOffload]):
CPU offloading config. Currently, only parameter and gradient CPU
offload is supported. It can be enabled via passing in
``cpu_offload=CPUOffload(offload_params=True)``. Note that this
","process_group (Optional[ProcessGroup]):
process group for sharding
sharding_strategy (Optional[ShardingStrategy]):
            Config sharding algorithm, different sharding algorithm has trade
            off between memory saving and communication overhead. ``FULL_SHARD``
            will be chosen if sharding_strategy is not specified.
        cpu_offload (Optional[CPUOffload]):
CPU offloading config. Currently, only parameter and gradient CPU
offload is supported. It can be enabled via passing in
``cpu_offload=CPUOffload(offload_params=True)``. Note that this
"
127,"@contextlib.contextmanager
def state_dict_type(module: nn.Module, state_dict_type: StateDictType) -> Generator:
""""""
        A context manager to set the state_dict_type of all the descendant FSDP
        modules of the target module. The target module does not have to be a FSDP
        module. If the target module is a FSDP module, its state_dict_type will
        also be changed.
        .. note:: This API should be called for only the top-level (root) module.
        .. note:: The default state_dict_type is StateDictTyp.FULL_STATE_DICT.
.. note:: This API enables users to transparently use the conventional
        ``state_dict`` API to take model checkpoints in cases where the root
        FSDP module is wrapped by another ``nn.Module``. For example, the
        following will ensure `state_dict`  is called on all non-FSDP instances,
        while dispatching into `local_state_dict` implementation for FSDP:
>>> model = DDP(FSDP(...))
        >> fsdp_root = model.module
        >>> with fsdp_root.state_dict_type(StateDictType.LOCAL_STATE_DICT):
>>>     checkpoint = model.state_dict()
Args:
state_dict_type (StateDictType): the desired state_dict_type to set.
""""""
prev_state_dict_type = None
","@contextlib.contextmanager
def state_dict_type(module: nn.Module, state_dict_type: StateDictType) -> Generator:
""""""
        A context manager to set the ``state_dict_type`` of all the descendant
        FSDP modules of the target module. The target module does not have to
        be a FSDP module. If the target module is a FSDP module, its
        ``state_dict_type`` will also be changed.

        .. note:: This API should be called for only the top-level (root)
            module.

.. note:: This API enables users to transparently use the conventional
            ``state_dict`` API to take model checkpoints in cases where the
            root FSDP module is wrapped by another ``nn.Module``. For example,
            the following will ensure ``state_dict``  is called on all non-FSDP
            instances, while dispatching into `local_state_dict` implementation
            for FSDP:

        Example::

>>> model = DDP(FSDP(...))
        >>> fsdp_root = model.module
        >>> with FSDP.state_dict_type(fsdp_root, StateDictType.LOCAL_STATE_DICT):
>>>     checkpoint = model.state_dict()

Args:
            module (torch.nn.Module): Root module.
state_dict_type (StateDictType): the desired state_dict_type to set.
""""""
prev_state_dict_type = None
"
128,"r""""""Functional interface""""""
from typing import Callable, List, Optional, Tuple
import math
import warnings
","r""""""Functional interface""""""
from typing import Callable, List, Optional, Tuple, Union
import math
import warnings
"
129,"Args:
params (Sequence[nn.Parameter])
The parameters to be flattend and concatened.
        requres_grad (bool):
            Set to Ture if gradients need to be computed for this parameter,
False otherwise.
""""""
","Args:
params (Sequence[nn.Parameter])
The parameters to be flattend and concatened.
        requires_grad (bool):
            Set to True if gradients need to be computed for this parameter,
False otherwise.
""""""
"
130,".. warning::
Module should be already placed on the destination device or
        device is set properly using torch.cuda.set_device(device_id).
FSDP will get compute device from module first, if module device
is CPU, FSDP will then get compute device from current device.
",".. warning::
Module should be already placed on the destination device or
        device is set properly using ``torch.cuda.set_device(device_id)``.
FSDP will get compute device from module first, if module device
is CPU, FSDP will then get compute device from current device.
"
131,"modules of the target module. The target module does not have to be a FSDP
module. If the target module is a FSDP module, its state_dict_type will
also be changed.
.. note:: This API should be called for only the top-level (root) module.
        .. note:: The default state_dict_type is StateDictTyp.FULL_STATE_DICT.
.. note:: This API enables users to transparently use the conventional
``state_dict`` API to take model checkpoints in cases where the root
FSDP module is wrapped by another ``nn.Module``. For example, the
        following will ensure `state_dict`  is called on all non-FSDP instances,
        while dispatching into `local_state_dict` implementation for FSDP:
>>> model = DDP(FSDP(...))
        >> fsdp_root = model.module
>>> with fsdp_root.state_dict_type(StateDictType.LOCAL_STATE_DICT):
>>>     checkpoint = model.state_dict()
Args:
state_dict_type (StateDictType): the desired state_dict_type to set.
""""""
","modules of the target module. The target module does not have to be a FSDP
module. If the target module is a FSDP module, its state_dict_type will
also be changed.

.. note:: This API should be called for only the top-level (root) module.

        .. note:: The default ``state_dict_type`` is ``StateDictType.FULL_STATE_DICT``.

.. note:: This API enables users to transparently use the conventional
``state_dict`` API to take model checkpoints in cases where the root
FSDP module is wrapped by another ``nn.Module``. For example, the
        following will ensure ``state_dict``  is called on all non-FSDP instances,
        while dispatching into ``local_state_dict`` implementation for FSDP:

>>> model = DDP(FSDP(...))
        >>> fsdp_root = model.module
>>> with fsdp_root.state_dict_type(StateDictType.LOCAL_STATE_DICT):
>>>     checkpoint = model.state_dict()

Args:
state_dict_type (StateDictType): the desired state_dict_type to set.
""""""
"
132,"corresponding to the local param shard will persist after the
context manager exits (unless ``writeback=False``, in which case
changes will be discarded). In the case where FSDP does not shard
            the parameters, currently only when world_size == 1, the
modification is persisted regardless of ``writeback``.
.. warning:: Note that ``rank0_only=True`` in conjunction with
","corresponding to the local param shard will persist after the
context manager exits (unless ``writeback=False``, in which case
changes will be discarded). In the case where FSDP does not shard
            the parameters, currently only when ``world_size == 1``, the
modification is persisted regardless of ``writeback``.
.. warning:: Note that ``rank0_only=True`` in conjunction with
"
133,"return [login for (login, state) in self._get_reviewers() if state == ""APPROVED""]
def get_commit_count(self) -> int:
        return int(self.info[""commits""][""totalCount""])
def get_pr_creator_login(self) -> str:
return cast(str, self.info[""author""][""login""])
def get_committer_login(self, num: int = 0) -> str:
        user = self.info[""commits""][""nodes""][num][""commit""][""author""][""user""]
# If author is not github user, user node will be null
if user is None:
return """"
return cast(str, user[""login""])
def get_committer_author(self, num: int = 0) -> str:
        node = self.info[""commits""][""nodes""][num][""commit""][""author""]
return f""{node['name']} <{node['email']}>""
","return [login for (login, state) in self._get_reviewers() if state == ""APPROVED""]
def get_commit_count(self) -> int:
        return int(self.info[""commits_with_authors""][""totalCount""])
def get_pr_creator_login(self) -> str:
return cast(str, self.info[""author""][""login""])
def get_committer_login(self, num: int = 0) -> str:
        user = self.info[""commits_with_authors""][""nodes""][num][""commit""][""author""][""user""]
# If author is not github user, user node will be null
if user is None:
return """"
return cast(str, user[""login""])
def get_committer_author(self, num: int = 0) -> str:
        node = self.info[""commits_with_authors""][""nodes""][num][""commit""][""author""]
return f""{node['name']} <{node['email']}>""
"
134,"if ""eval_with_key"" in topmost_framesummary.filename:
print(generate_error_message(topmost_framesummary),
file=sys.stderr)
                raise e.with_traceback(None)
cls.__call__ = wrapped_call
","if ""eval_with_key"" in topmost_framesummary.filename:
print(generate_error_message(topmost_framesummary),
file=sys.stderr)
                    raise e.with_traceback(None)
                else:
                    raise e
cls.__call__ = wrapped_call
"
135,"reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
The moving average min/max is computed as follows
","reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.
The moving average min/max is computed as follows
"
136,"quant_min=quant_min,
quant_max=quant_max,
factory_kwargs=factory_kwargs,
)
factory_kwargs = torch.nn.factory_kwargs(factory_kwargs)
self.ch_axis = ch_axis
","quant_min=quant_min,
quant_max=quant_max,
factory_kwargs=factory_kwargs,
            eps=eps,
)
factory_kwargs = torch.nn.factory_kwargs(factory_kwargs)
self.ch_axis = ch_axis
"
137,"reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
The quantization parameters are computed the same way as in
:class:`~torch.ao.quantization.observer.MovingAverageMinMaxObserver`, with the
","reduce_range: Reduces the range of the quantized data type by 1 bit
quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.
quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.
        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.
The quantization parameters are computed the same way as in
:class:`~torch.ao.quantization.observer.MovingAverageMinMaxObserver`, with the
"
138,"reduce_range=False,
quant_min=None,
quant_max=None,
**kwargs
) -> None:
super(MovingAveragePerChannelMinMaxObserver, self).__init__(
","reduce_range=False,
quant_min=None,
quant_max=None,
        eps=torch.finfo(torch.float32).eps,
**kwargs
) -> None:
super(MovingAveragePerChannelMinMaxObserver, self).__init__(
"
139,"return qconfig
def _get_default_qconfig_dict_helper(qconfig, qconfig_transpose):
return {
"""": qconfig,
","return qconfig
""""""
Default symmetric QAT qconfig for qnnpack. And its per channel weight variant.
""""""
default_symmetric_qnnpack_qat_qconfig = QConfig(
    activation=FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                       quant_min=-128,
                                                       quant_max=127,
                                                       dtype=torch.qint8,
                                                       reduce_range=False,
                                                       eps=2 ** -12),
    weight=fused_wt_fake_quant_range_neg_127_to_127)

default_per_channel_symmetric_qnnpack_qat_qconfig = QConfig(
    activation=FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver,
                                                       quant_min=-128,
                                                       quant_max=127,
                                                       dtype=torch.qint8,
                                                       reduce_range=False,
                                                       eps=2 ** -12),
    weight=fused_per_channel_wt_fake_quant_range_neg_127_to_127)

def _get_default_qconfig_dict_helper(qconfig, qconfig_transpose):
return {
"""": qconfig,
"
140,")
self.reduce_range = reduce_range
self.register_buffer(
            ""eps"", torch.tensor([torch.finfo(torch.float32).eps], **factory_kwargs)
)
assert self.qscheme in (
torch.per_tensor_affine,
",")
self.reduce_range = reduce_range
self.register_buffer(
            ""eps"", torch.tensor([eps], **factory_kwargs)
)
assert self.qscheme in (
torch.per_tensor_affine,
"
141,"weight quantization is supported, such as `fbgemm`.
""""""
default_dynamic_quant_observer = PlaceholderObserver.with_args(
dtype=torch.float, compute_dtype=torch.quint8
)
","weight quantization is supported, such as `fbgemm`.
""""""
per_channel_weight_observer_range_neg_127_to_127 = MinMaxObserver.with_args(
    dtype=torch.qint8, qscheme=torch.per_channel_symmetric,
    quant_min=-127, quant_max=127, eps=2 ** -12)
""""""
Per-channel, symmetric weight observer with the 8-bit values restricted to [-127, +127], excluding -128.
""""""

default_dynamic_quant_observer = PlaceholderObserver.with_args(
dtype=torch.float, compute_dtype=torch.quint8
)
"
142,"return qconfig
default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
weight=default_embedding_fake_quant)
","return qconfig
""""""
Default, symmetric PTQ qconfig for the specified backend. And a per_channel
variant of the same.

Symmetric here applies to signed weights with zero point = 0, and additional
value restrictions. The activations are also signed 8-bit integers with this
qconfig.

    * Once this change is merged [as of 3/17/22], with backend or qengine =
    'qnnpack', some quantized operators with this symmetric qconfig may use
    operators from xnnpack library.

        ** Support to use xnnpack ops with `qnnpack` backed for asymmetric
        qconfig (returned by get_default_qconfig()) is not available yet.

    * This qconfig uses signed activations and weights. Weights have added
    restrictions such as zero point is forced to be 0, making the weights
    symmetric, hence the name. And the 8-bit quantized values are
    restricting to to [-127, +127], excluding -128.

    * xnnpack has a requantization scale value restriction, 0x1p-32 <=
    requantization_scale < 256.0 where, `requantization_scale = (input_scale
    * kernel_scale) / (output_scale)`. Using this eps (w/ assumed max value
    of 256) is to prevent requantization_scale to go below xnnpack lower
    threshold.
""""""
default_symmetric_qnnpack_qconfig = QConfig(activation=HistogramObserver.with_args(dtype=torch.qint8,
                                                                                   reduce_range=False,
                                                                                   eps=2 ** -12),
                                            weight=weight_observer_range_neg_127_to_127)

default_per_channel_symmetric_qnnpack_qconfig = QConfig(activation=HistogramObserver.with_args(dtype=torch.qint8,
                                                                                               reduce_range=False,
                                                                                               eps=2 ** -12),
                                                        weight=per_channel_weight_observer_range_neg_127_to_127)

default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
weight=default_embedding_fake_quant)
"
143,"pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()

name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
","pickler.persistent_id = self._persistent_id
pickler.dump(obj)
data_value = data_buf.getvalue()
        mocked_modules = defaultdict(list)
name_in_dependency_graph = f""<{package}.{resource}>""
self.dependency_graph.add_node(
name_in_dependency_graph,
"
144,"for pattern, pattern_info in self.patterns.items():
if pattern.matches(module):
if pattern_info.action == _ModuleProviderAction.MOCK:
                        raise NotImplementedError(
                            f""Object '{field}' from module {module} was mocked out during packaging ""
                            f""but is being used in resource - {resource} in package {package}. ""
                            ""If this error is happening during 'save_pickle', please ensure that your ""
                            ""pickled object doesn't contain any mocked objects. Try interning or externing""
                            f""{module} if {field} is supposed to be in the package.""
                        )
                    else:
                        return
if dependencies:
all_dependencies = []
","for pattern, pattern_info in self.patterns.items():
if pattern.matches(module):
if pattern_info.action == _ModuleProviderAction.MOCK:
                        mocked_modules[module].append(field)
                    return
if dependencies:
all_dependencies = []
"
145,"device=None, dtype=None) -> None:
factory_kwargs = {'device': device, 'dtype': dtype}
super(GroupNorm, self).__init__()
self.num_groups = num_groups
self.num_channels = num_channels
self.eps = eps
","device=None, dtype=None) -> None:
factory_kwargs = {'device': device, 'dtype': dtype}
super(GroupNorm, self).__init__()
        if num_channels % num_groups != 0:
            raise ValueError('num_channels must be divisible by num_groups')

self.num_groups = num_groups
self.num_channels = num_channels
self.eps = eps
"
146,")
def _check_mocked_error(module: Optional[str], field: Optional[str]):
assert isinstance(module, str)
assert isinstance(field, str)
if self._can_implicitly_extern(module):
",")
def _check_mocked_error(module: Optional[str], field: Optional[str]):
            """"""
            checks if an object (field) comes from a mocked module and then adds
            the pair to mocked_modules which contains mocked modules paired with their
            list of mocked objects present in the pickle.

            We also hold the invariant that the first user defined rule that applies
            to the module is the one we use.
            """"""

assert isinstance(module, str)
assert isinstance(field, str)
if self._can_implicitly_extern(module):
"
147,"else:
label += f""|target={self._typename(node.target)}"" + r""\n""
if len(node.args) > 0:
                    label += f""|args={_get_str_for_args_kwargs(node.args)}"" + r""\l""
if len(node.kwargs) > 0:
                    label += f""|kwargs={_get_str_for_args_kwargs(node.kwargs)}"" + r""\l""
label += f""|num_users={len(node.users)}"" + r""\n""
tensor_meta = node.meta.get('tensor_meta')
","else:
label += f""|target={self._typename(node.target)}"" + r""\n""
if len(node.args) > 0:
                    label += _get_str_for_args_kwargs(node.args)
if len(node.kwargs) > 0:
                    label += _get_str_for_args_kwargs(node.kwargs)
label += f""|num_users={len(node.users)}"" + r""\n""
tensor_meta = node.meta.get('tensor_meta')
"
148,"gpu_arch_type, gpu_arch_version
),
""libtorch_variant"": libtorch_variant,
                    ""devtoolset"": abi_version,
""container_image"": LIBTORCH_CONTAINER_IMAGES[
(arch_version, abi_version)
                    ],
""package_type"": ""libtorch"",
""build_name"": f""libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{abi_version}"".replace(
""."", ""_""
","gpu_arch_type, gpu_arch_version
),
""libtorch_variant"": libtorch_variant,
                    ""libtorch_config"": abi_version if os == ""windows"" else """",
                    ""devtoolset"": abi_version if os != ""windows"" else """",
""container_image"": LIBTORCH_CONTAINER_IMAGES[
(arch_version, abi_version)
                    ] if os != ""windows"" else """",
""package_type"": ""libtorch"",
""build_name"": f""libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{abi_version}"".replace(
""."", ""_""
"
149,"# - Better name (see https://github.com/pytorch/pytorch/pull/63496#discussion_r694091694)
@contextlib.contextmanager
def enable_python_mode(cls) -> Iterator[None]:
    if not hasattr(cls, '__torch_dispatch__'):
raise ValueError('The class passed to enable_python_mode '
                         'must have a __torch_dispatch__ classmethod')
if not isinstance(cls, type) or not issubclass(cls, (torch.Tensor,)):
raise ValueError('The argument passed to enable_python_mode '
'must be the type of a Tensor subclass')
","# - Better name (see https://github.com/pytorch/pytorch/pull/63496#discussion_r694091694)
@contextlib.contextmanager
def enable_python_mode(cls) -> Iterator[None]:
    if cls.__torch_dispatch__ is torch.Tensor.__torch_dispatch__:
raise ValueError('The class passed to enable_python_mode '
                         'must have a non-default __torch_dispatch__ classmethod')
if not isinstance(cls, type) or not issubclass(cls, (torch.Tensor,)):
raise ValueError('The argument passed to enable_python_mode '
'must be the type of a Tensor subclass')
"
150,"JSON format. If the error file cannot be determined, then logs the content
that would have been written to the error file.
""""""
        _write_error(e, self._get_error_file_path())
def dump_error_file(self, rootcause_error_file: str, error_code: int = 0):
""""""
","JSON format. If the error file cannot be determined, then logs the content
that would have been written to the error file.
""""""

        file = self._get_error_file_path()
        if file:
            data = {
                ""message"": {
                    ""message"": f""{type(e).__name__}: {e}"",
                    ""extraInfo"": {
                        ""py_callstack"": traceback.format_exc(),
                        ""timestamp"": str(int(time.time())),
                    },
                }
            }
            with open(file, ""w"") as fp:
                json.dump(data, fp)
def dump_error_file(self, rootcause_error_file: str, error_code: int = 0):
""""""
"
151,"SPECIAL_PATTERN_LOWER_MODULE_MAP = {
nn.BatchNorm2d: nnq.BatchNorm2d,
nn.BatchNorm3d: nnq.BatchNorm3d,
}
# Mapping from fused module class to a 2-tuple of:
","SPECIAL_PATTERN_LOWER_MODULE_MAP = {
nn.BatchNorm2d: nnq.BatchNorm2d,
nn.BatchNorm3d: nnq.BatchNorm3d,
    nnqr.ConvTranspose1d: nnq.ConvTranspose1d,
    nnqr.ConvTranspose2d: nnq.ConvTranspose2d,
    nn.ELU: nnq.ELU,
    nn.LeakyReLU: nnq.LeakyReLU,
    nn.Hardswish: nnq.Hardswish,
    nn.InstanceNorm1d: nnq.InstanceNorm1d,
    nn.InstanceNorm2d: nnq.InstanceNorm2d,
    nn.InstanceNorm3d: nnq.InstanceNorm3d,
    nn.LayerNorm: nnq.LayerNorm,
    nn.Dropout: nnq.Dropout,
}
# Mapping from fused module class to a 2-tuple of:
"
152,"""""""
raise RuntimeError(
""Reached a code path in Module.get_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.md ""
""to report this bug."")
def set_extra_state(self, state: Any):
","""""""
raise RuntimeError(
""Reached a code path in Module.get_extra_state() that should never be called. ""
            ""Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml ""
""to report this bug."")
def set_extra_state(self, state: Any):
"
153,"Args:
datapipes: Map DataPipes being concatenated
""""""
datapipes: Tuple[MapDataPipe]
length: int
","Args:
datapipes: Map DataPipes being concatenated

    Example:
        >>> from torchdata.datapipes.map import SequenceWrapper
        >>> dp1 = SequenceWrapper(range(3))
        >>> dp2 = SequenceWrapper(range(3))
        >>> concat_dp = dp1.concat(dp2)
        >>> list(concat_dp)
        [0, 1, 2, 0, 1, 2]
""""""
datapipes: Tuple[MapDataPipe]
length: int
"
154,"import warnings
from typing import Callable, TypeVar

from torch.utils.data import MapDataPipe, functional_datapipe
try:
    import dill

    # XXX: By default, dill writes the Pickler dispatch table to inject its
    # own logic there. This globally affects the behavior of the standard library
    # pickler for any user who transitively depends on this module!
    # Undo this extension to avoid altering the behavior of the pickler globally.
    dill.extend(use_dill=False)
    DILL_AVAILABLE = True
except ImportError:
    DILL_AVAILABLE = False

T_co = TypeVar('T_co', covariant=True)
","from torch.utils.data.datapipes.utils.common import check_lambda_fn
from typing import Callable, TypeVar
from torch.utils.data import MapDataPipe, functional_datapipe
T_co = TypeVar('T_co', covariant=True)
"
155,") -> None:
super().__init__()
self.datapipe = datapipe
        # Partial object has no attribute '__name__', but can be pickled
        if hasattr(fn, '__name__') and fn.__name__ == '<lambda>' and not DILL_AVAILABLE:
            warnings.warn(
                ""Lambda function is not supported for pickle, please use ""
                ""regular python function or functools.partial instead.""
            )
self.fn = fn  # type: ignore[assignment]
def __len__(self) -> int:
",") -> None:
super().__init__()
self.datapipe = datapipe
        check_lambda_fn(fn)
self.fn = fn  # type: ignore[assignment]
def __len__(self) -> int:
"
156,"import fnmatch
import warnings
from io import IOBase
from typing import Iterable, List, Tuple, Union, Optional
try:
    import dill

    # XXX: By default, dill writes the Pickler dispatch table to inject its
    # own logic there. This globally affects the behavior of the standard library
    # pickler for any user who transitively depends on this module!
    # Undo this extension to avoid altering the behavior of the pickler globally.
    dill.extend(use_dill=False)
    DILL_AVAILABLE = True
except ImportError:
    DILL_AVAILABLE = False
def check_lambda_fn(fn):
","import fnmatch
import warnings
from enum import Enum
from io import IOBase
from typing import Iterable, List, Tuple, Union, Optional
from torch.utils.data._utils.serialization import DILL_AVAILABLE


class SerializationType(Enum):
    PICKLE = ""pickle""
    DILL = ""dill""
def check_lambda_fn(fn):
"
157,"def _check_for_nccl_backend(group):
pg = group or _get_default_group()
    # It is not expected for PG to be wrapped many times, but support it just
    # in case
    while isinstance(pg, _ProcessGroupWrapper):
        pg = pg.wrapped_pg
return (
is_nccl_available() and
","def _check_for_nccl_backend(group):
pg = group or _get_default_group()
    # Gate PG wrapper check on Gloo availability.
    if _GLOO_AVAILABLE:
        # It is not expected for PG to be wrapped many times, but support it just
        # in case
        while isinstance(pg, _ProcessGroupWrapper):
            pg = pg.wrapped_pg
return (
is_nccl_available() and
"
158,"buffer_name: buffer for (buffer, buffer_name) in named_module_buffers
}
    def _build_param_to_name_mapping(self, parameters):
param_to_param_index = {parameters[i]: i for i in range(len(parameters))}
param_set = set(parameters)
param_index_to_param_fqn = {}
","buffer_name: buffer for (buffer, buffer_name) in named_module_buffers
}
    def _build_debug_param_to_name_mapping(self, parameters):
        if dist._get_debug_mode() == dist._DistributedDebugLevel.OFF:
            return {}

param_to_param_index = {parameters[i]: i for i in range(len(parameters))}
param_set = set(parameters)
param_index_to_param_fqn = {}
"
159,"if backend_index.dispatch_key in (DispatchKey.CPU, DispatchKey.Meta):
headers.append(""#include <ATen/EmptyTensor.h>"")
elif backend_index.dispatch_key == DispatchKey.CUDA:
        headers.append(""#include <ATen/cuda/EmptyTensor.h>"")
elif per_operator_headers:
headers += [
""#include <ATen/ops/empty.h>"",
","if backend_index.dispatch_key in (DispatchKey.CPU, DispatchKey.Meta):
headers.append(""#include <ATen/EmptyTensor.h>"")
elif backend_index.dispatch_key == DispatchKey.CUDA:
        if rocm:
            headers.append(""#include <ATen/hip/EmptyTensor.h>"")
        else:
            headers.append(""#include <ATen/cuda/EmptyTensor.h>"")
elif per_operator_headers:
headers += [
""#include <ATen/ops/empty.h>"",
"
160,"${instruction_list}
}), // instructions list"""""")
ONE_CONSTANT = CodeTemplate(""""""c10::IValue(${constant}),"""""")
CONSTANT_LIST = CodeTemplate(""""""std::vector<c10::IValue>({
${constant_list}
","${instruction_list}
}), // instructions list"""""")
ONE_CONSTANT = CodeTemplate(""""""
    c10::IValue(${constant}),"""""")
CONSTANT_LIST = CodeTemplate(""""""std::vector<c10::IValue>({
${constant_list}
"
161,")
)
return OPERATOR_VERSION_MAP.substitute(
        operator_list_in_version_map="""".join(operator_list_in_version_map_part)
)
def get_upgrader_bytecode_function_to_index_map(upgrader_dict: List[Dict[str, Any]]) -> Dict[str, Any]:
",")
)
return OPERATOR_VERSION_MAP.substitute(
        operator_list_in_version_map="""".join(operator_list_in_version_map_part).lstrip(""\n"")
)
def get_upgrader_bytecode_function_to_index_map(upgrader_dict: List[Dict[str, Any]]) -> Dict[str, Any]:
"
162,"constructor input parameters, some internal states of DistributedDataParallel
and performance metrics. Simply print the dictorinary and see what
these metrics are.
        THis is a prototype interface and subject to change in the future.
""""""
ddp_logging_data = self.logger._get_ddp_logging_data()
return {**ddp_logging_data.strs_map, **ddp_logging_data.ints_map}
","constructor input parameters, some internal states of DistributedDataParallel
and performance metrics. Simply print the dictorinary and see what
these metrics are.
        This is a prototype interface and subject to change in the future.
""""""
ddp_logging_data = self.logger._get_ddp_logging_data()
return {**ddp_logging_data.strs_map, **ddp_logging_data.ints_map}
"
163,"name: str
) -> TRTTensor:
""""""
    This function adds a TensorRT elementwise layer. We only allow at most one
    operand to not be a trt tensor, otherwise, we should const fold it first.
    If any operand is not a trt tensor, we make it a trt constant layer which
    has the same type as the other trt tensor. Then we broadcast these two inputs
    to have the same number of dimensions.
Limitation:
If we are using implicit batch dim mode, the operand that is not a trt
","name: str
) -> TRTTensor:
""""""
    This function adds a TensorRT elementwise layer. We allow both operands to be
    constant (not a trt tensor) because in implicit batch dimension mode, we could
    introduce constant via .size() op. Other scenario should be const folded first.
    If any operand is not a trt tensor, we make it a trt constant layer which has
    the same type as the other trt tensor. Then we broadcast these two inputs to
    have the same number of dimensions.
Limitation:
If we are using implicit batch dim mode, the operand that is not a trt
"
164,"set([
nn.Dropout,
F.dropout,
]),
]
","set([
nn.Dropout,
F.dropout,
            nnq.Dropout,
]),
]
"
165,"_enable_record_function, _set_empty_test_observer, kineto_available,
_record_function_with_args_enter, _record_function_with_args_exit,
_supported_activities, _add_metadata_json, SavedTensor,
                                _register_saved_tensors_default_hooks, _reset_saved_tensors_default_hooks)
from torch._C._autograd import (_ProfilerResult, _KinetoEvent,
_prepare_profiler, _enable_profiler, _disable_profiler)
","_enable_record_function, _set_empty_test_observer, kineto_available,
_record_function_with_args_enter, _record_function_with_args_exit,
_supported_activities, _add_metadata_json, SavedTensor,
                                _push_saved_tensors_default_hooks, _pop_saved_tensors_default_hooks)
from torch._C._autograd import (_ProfilerResult, _KinetoEvent,
_prepare_profiler, _enable_profiler, _disable_profiler)
"
166,"add_docstr_all('ravel',
r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""")
","add_docstr_all('ravel',
r""""""
ravel() -> Tensor
see :func:`torch.ravel`
"""""")
"
167,"Default weight observer.
""""""
default_histogram_observer = HistogramObserver.with_args(reduce_range=True)
""""""
Default histogram observer, usually used for PTQ.
""""""
","Default weight observer.
""""""
default_histogram_observer = HistogramObserver.with_args(quant_min=0, quant_max=127)
""""""
Default histogram observer, usually used for PTQ.
""""""
"
168,"super(SGD, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('nesterov', False)
@torch.no_grad()
def step(self, closure=None):
","super(SGD, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('nesterov', False)
            group.setdefault('maximize', False)
@torch.no_grad()
def step(self, closure=None):
"
169,"super(Adam, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('amsgrad', False)
@torch.no_grad()
def step(self, closure=None):
","super(Adam, self).__setstate__(state)
for group in self.param_groups:
group.setdefault('amsgrad', False)
            group.setdefault('maximize', False)
@torch.no_grad()
def step(self, closure=None):
"
170,"@register_quant_pattern(torch._C._nn.avg_pool3d)
@register_quant_pattern(torch.clamp)
@register_quant_pattern(torch.flatten)
@register_quant_pattern(torch.max)
@register_quant_pattern(torch.mean)
@register_quant_pattern(torch.min)
@register_quant_pattern(operator.floordiv)
@register_quant_pattern('clamp')
@register_quant_pattern('mean')
","@register_quant_pattern(torch._C._nn.avg_pool3d)
@register_quant_pattern(torch.clamp)
@register_quant_pattern(torch.flatten)
@register_quant_pattern(torch.mean)
@register_quant_pattern(operator.floordiv)
@register_quant_pattern('clamp')
@register_quant_pattern('mean')
"
171,"factory_kwargs = {'device': device, 'dtype': dtype}
super(BatchNorm3d, self).__init__(num_features, **factory_kwargs)
self.eps = eps
        self.scale = 1.0
        self.zero_point = 0
def forward(self, input):
return torch.ops.quantized.batch_norm3d(input, self.weight, self.bias, self.running_mean,
","factory_kwargs = {'device': device, 'dtype': dtype}
super(BatchNorm3d, self).__init__(num_features, **factory_kwargs)
self.eps = eps
        self.register_buffer('scale', torch.tensor(1.0, **factory_kwargs))
        self.register_buffer('zero_point', torch.tensor(0, **factory_kwargs))
def forward(self, input):
return torch.ops.quantized.batch_norm3d(input, self.weight, self.bias, self.running_mean,
"
172,"qstate.mark_cur_op_complete(func)
elif hook_type is HookType.ARG_DEQUANTS:
                # disabling torch function to prevent infinite recursion on
                # getset
# TODO(future PR): handle more dtypes
                with torch._C.DisableTorchFunction():
                    new_args = []
                    for arg in args:
                        if isinstance(arg, torch.Tensor) and arg.is_quantized:
                            new_args.append(arg.dequantize())
                        else:
                            new_args.append(arg)
                    args = tuple(new_args)
output = super().__torch_function__(func, types, args, kwargs)
else:  # HookType.NONE
","qstate.mark_cur_op_complete(func)
elif hook_type is HookType.ARG_DEQUANTS:
# TODO(future PR): handle more dtypes
                new_args = []
                for arg in args:
                    if isinstance(arg, torch.Tensor) and arg.is_quantized:
                        new_args.append(arg.dequantize())
                    else:
                        new_args.append(arg)
                args = tuple(new_args)
output = super().__torch_function__(func, types, args, kwargs)
else:  # HookType.NONE
"
173,"# Check that the statement is valid. It doesn't guarantee success, but it's much
# simpler and quicker to raise an exception for a faulty `stmt` or `setup` in
# the parent process rather than the valgrind subprocess.
        self._timer.timeit(1)
is_python = (self._language == Language.PYTHON)
assert is_python or not self._globals
result = valgrind_timer_interface.wrapper_singleton().collect_callgrind(
","# Check that the statement is valid. It doesn't guarantee success, but it's much
# simpler and quicker to raise an exception for a faulty `stmt` or `setup` in
# the parent process rather than the valgrind subprocess.
        self._timeit(1)
is_python = (self._language == Language.PYTHON)
assert is_python or not self._globals
result = valgrind_timer_interface.wrapper_singleton().collect_callgrind(
"
174,"kwargs_to_move_to_acc_out_ty=[(""shape"", ""shape"")],
)
@register_acc_op
def reshape(*, input, acc_out_ty):
return torch.reshape(
input, tuple(acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""shape""))
)
","kwargs_to_move_to_acc_out_ty=[(""shape"", ""shape"")],
)
@register_acc_op
def reshape(*, input, acc_out_ty=None):
    assert acc_out_ty is not None
return torch.reshape(
input, tuple(acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""shape""))
)
"
175,"@register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary)
@register_acc_op
def to_dtype(input, acc_out_ty):
    assert acc_out_ty is not None, ""valid acc_out_ty needed""
return input.to(dtype=acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""dtype""))
","@register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary)
@register_acc_op
def to_dtype(input, acc_out_ty=None):
    assert acc_out_ty is not None
return input.to(dtype=acc_utils.get_field_from_acc_out_ty(acc_out_ty, ""dtype""))
"
176,">>> # Assumes backend is not NCCL
>>> device = torch.device(""cpu"")
>>> dist.broadcast_object_list(objects, src=0, device=device)
        >>> broadcast_objects
['foo', 12, {1: 2}]
""""""
if _rank_not_in_group(group):
",">>> # Assumes backend is not NCCL
>>> device = torch.device(""cpu"")
>>> dist.broadcast_object_list(objects, src=0, device=device)
        >>> objects
['foo', 12, {1: 2}]
""""""
if _rank_not_in_group(group):
"
177,"def default_convert(data):
    r""""""Converts each NumPy array data field into a tensor""""""
elem_type = type(data)
if isinstance(data, torch.Tensor):
return data
","def default_convert(data):
    r""""""
        Function that converts each NumPy array element into a :class:`torch.Tensor`. If the input is a `Sequence`,
        `Collection`, or `Mapping`, it tries to convert each element inside to a :class:`torch.Tensor`.
        If the input is not an NumPy array, it is left unchanged.
        This is used as the default function for collation when both `batch_sampler` and
        `batch_size` are NOT defined in :class:`~torch.utils.data.DataLoader`.

        The general input type to output type mapping is similar to that
        of :func:`~torch.utils.data.default_collate`. See the description there for more details.

        Args:
            data: a single data point to be converted

        Examples:
            >>> # Example with `int`
            >>> default_convert(0)
            0
            >>> # Example with NumPy array
            >>> default_convert(np.array([0, 1]))
            tensor([0, 1])
            >>> # Example with NamedTuple
            >>> Point = namedtuple('Point', ['x', 'y'])
            >>> default_convert(Point(0, 0))
            Point(x=0, y=0)
            >>> default_convert(Point(np.array(0), np.array(0)))
            Point(x=tensor(0), y=tensor(0))
            >>> # Example with List
            >>> default_convert([np.array([0, 1]), np.array([2, 3])])
            [tensor([0, 1]), tensor([2, 3])]
    """"""
elem_type = type(data)
if isinstance(data, torch.Tensor):
return data
"
178,"'isnan', 'isposinf', 'isneginf', 'isinf', 'signbit', 'isin',
# Functions return none are not differentiable
'record_stream',
    # These functions are not differentiable
    'logical_and', 'logical_xor', 'logical_not', 'logical_or',
}
# The C -> R functions at the time of adding this are still being audited and tested
","'isnan', 'isposinf', 'isneginf', 'isinf', 'signbit', 'isin',
# Functions return none are not differentiable
'record_stream',
}
# The C -> R functions at the time of adding this are still being audited and tested
"
179,"assert shard_meta in local_shard_metadatas, \
""local shard metadata not in sharded_tensor_metadata!""
if not local_shard_tensor.is_contiguous():
raise ValueError('Only torch.contiguous_format memory_format is currently supported')
            _raise_if_mismatch(tensor_properties.layout, local_shard_tensor.layout, ""layout"", current_rank, True)
_raise_if_mismatch(shard_meta.shard_sizes, list(local_shard_tensor.size()), ""size"", current_rank)
_raise_if_mismatch(tensor_properties.pin_memory, local_shard_tensor.is_pinned(), ""pin_memory"", current_rank, True)
_raise_if_mismatch(local_device, local_shard_tensor.device, ""device"", current_rank)
","assert shard_meta in local_shard_metadatas, \
""local shard metadata not in sharded_tensor_metadata!""
            _raise_if_mismatch(tensor_properties.layout, local_shard_tensor.layout, ""layout"", current_rank, True)
if not local_shard_tensor.is_contiguous():
raise ValueError('Only torch.contiguous_format memory_format is currently supported')
_raise_if_mismatch(shard_meta.shard_sizes, list(local_shard_tensor.size()), ""size"", current_rank)
_raise_if_mismatch(tensor_properties.pin_memory, local_shard_tensor.is_pinned(), ""pin_memory"", current_rank, True)
_raise_if_mismatch(local_device, local_shard_tensor.device, ""device"", current_rank)
"
180,"elif value == Backend.UNDEFINED:
raise ValueError(""Invalid backend: '{}'"".format(name))
elif value != Backend.GLOO and value != Backend.NCCL and value != Backend.MPI:
            value = name
return value
@classmethod
","elif value == Backend.UNDEFINED:
raise ValueError(""Invalid backend: '{}'"".format(name))
elif value != Backend.GLOO and value != Backend.NCCL and value != Backend.MPI:
            value = name.lower()
return value
@classmethod
"
181,"qconfig = default_qat_qconfig_v2
return qconfig
def assert_valid_qconfig(qconfig: Optional[Union[QConfig, QConfigDynamic]],
mod: torch.nn.Module) -> None:
""""""
","qconfig = default_qat_qconfig_v2
return qconfig
def get_default_qconfig_dict(backend='fbgemm', version=0):
    qconfig = get_default_qconfig(backend)
    return {
        """": qconfig,
        ""object_type"": [(""reshape"", default_reuse_input_qconfig)]
    }

def get_default_qat_qconfig_dict(backend='fbgemm', version=1):
    qconfig = get_default_qat_qconfig(backend, version=version)
    return {
        """": qconfig,
        ""object_type"": [(""reshape"", default_reuse_input_qconfig)]
    }

def assert_valid_qconfig(qconfig: Optional[Union[QConfig, QConfigDynamic]],
mod: torch.nn.Module) -> None:
""""""
"
182,"@compatibility(is_backward_compatible=True)
def format_node(self,
                    placeholder_names: List[str] = None,
                    maybe_return_typename: List[str] = None) -> Optional[str]:
""""""
Return a descriptive string representation of ``self``.
","@compatibility(is_backward_compatible=True)
def format_node(self,
                    placeholder_names: Optional[List[str]] = None,
                    maybe_return_typename: Optional[List[str]] = None) -> Optional[str]:
""""""
Return a descriptive string representation of ``self``.
"
183,"Args:
params (iterable): iterable of parameters to optimize or dicts defining
parameter groups
        lr (float, optional): learning rate (default: 2e-3)
betas (Tuple[float, float], optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))
eps (float, optional): term added to the denominator to improve
","Args:
params (iterable): iterable of parameters to optimize or dicts defining
parameter groups
        lr (float, optional): learning rate (default: 1e-3)
betas (Tuple[float, float], optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))
eps (float, optional): term added to the denominator to improve
"
184,"add_unary_layer,
add_activation_layer,
extend_attr_to_tuple,
)
","add_unary_layer,
add_activation_layer,
extend_attr_to_tuple,
    get_positive_dim,
)
"
185,"# dim, which is a very rare case. For now we just claim not supporting dim=None.
assert dim is not None, ""We don't support dim=None right now.""
    dim = dim % (len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0))
if network.has_implicit_batch_dimension:
assert dim != 0, ""We don't support squeeze batch dim when it's implicit.""
dim -= 1
","# dim, which is a very rare case. For now we just claim not supporting dim=None.
assert dim is not None, ""We don't support dim=None right now.""
    dim = get_positive_dim(dim, len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0))
if network.has_implicit_batch_dimension:
assert dim != 0, ""We don't support squeeze batch dim when it's implicit.""
dim -= 1
"
186,"reference_tensor_output = at::_ops::{api_name}::call({', '.join(meta_call_args)});
}}
// See  Note [Propagating strides in the functionalization pass]
      at::functionalization::impl::set_sizes_strides_offset(self, reference_tensor_output);
      return self;
""""""
else:
","reference_tensor_output = at::_ops::{api_name}::call({', '.join(meta_call_args)});
}}
// See  Note [Propagating strides in the functionalization pass]
      at::functionalization::impl::set_sizes_strides_offset({view_tensor_name}, reference_tensor_output);
      return {view_tensor_name};
""""""
else:
"
187,"raise TypeError(""%s is not supported by torch"" % dtype)
def to_tuple(d):
    if hasattr(d, ""to_tuple""):
        return d.to_tuple()
    else:
        return tuple(d)


class TRTModule(torch.nn.Module):
def __init__(self, engine=None, input_names=None, output_names=None, cuda_graph_batch_size=-1):
super(TRTModule, self).__init__()
","raise TypeError(""%s is not supported by torch"" % dtype)
class TRTModule(torch.nn.Module):
def __init__(self, engine=None, input_names=None, output_names=None, cuda_graph_batch_size=-1):
super(TRTModule, self).__init__()
"
188,"since it does not provide an ``async_op`` handle and thus will be a
blocking call.
.. warning::
:func:`scatter_object_list` uses ``pickle`` module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
","since it does not provide an ``async_op`` handle and thus will be a
blocking call.
    .. note:: Note that this API does not support the NCCL backend, as the
        tensor-based scatter collective is not supported by ProcessGroupNCCL.

.. warning::
:func:`scatter_object_list` uses ``pickle`` module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
"
189,"nn.ConvTranspose2d,
nnq.ConvTranspose2d,
]),
# ELU
set([
nn.ELU,
","nn.ConvTranspose2d,
nnq.ConvTranspose2d,
]),
        set([
            nn.ConvTranspose3d,
            nnq.ConvTranspose3d,
        ]),
# ELU
set([
nn.ELU,
"
190,"mod.stride, mod.padding, mod.output_padding, mod.groups,
mod.bias is not None, mod.dilation, mod.padding_mode)
qconv.set_weight_bias(qweight, mod.bias)
qconv.scale = float(act_scale)
qconv.zero_point = int(act_zp)
","mod.stride, mod.padding, mod.output_padding, mod.groups,
mod.bias is not None, mod.dilation, mod.padding_mode)
qconv.set_weight_bias(qweight, mod.bias)
        try:
            act_scale, act_zp = mod.activation_post_process.calculate_qparams()
        except Exception:
            # for things like dynamic quantization, want to assign arbitrary s+z to not break serialization
            act_scale, act_zp = 0.0, 0

qconv.scale = float(act_scale)
qconv.zero_point = int(act_zp)
"
191,"padding=0, output_padding=0, groups=1, bias=True,
dilation=1, padding_mode='zeros', device=None, dtype=None):
factory_kwargs = {'device': device, 'dtype': dtype}
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        output_padding = _pair(output_padding)
super(ConvTranspose1d, self).__init__(
in_channels, out_channels, kernel_size, stride, padding, dilation,
","padding=0, output_padding=0, groups=1, bias=True,
dilation=1, padding_mode='zeros', device=None, dtype=None):
factory_kwargs = {'device': device, 'dtype': dtype}
        kernel_size = _single(kernel_size)
        stride = _single(stride)
        padding = _single(padding)
        dilation = _single(dilation)
        output_padding = _single(output_padding)
super(ConvTranspose1d, self).__init__(
in_channels, out_channels, kernel_size, stride, padding, dilation,
"
192,"padding=0, output_padding=0, groups=1, bias=True,
dilation=1, padding_mode='zeros', device=None, dtype=None):
factory_kwargs = {'device': device, 'dtype': dtype}
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        output_padding = _pair(output_padding)
super(ConvTranspose3d, self).__init__(
in_channels, out_channels, kernel_size, stride, padding, dilation,
","padding=0, output_padding=0, groups=1, bias=True,
dilation=1, padding_mode='zeros', device=None, dtype=None):
factory_kwargs = {'device': device, 'dtype': dtype}
        kernel_size = _triple(kernel_size)
        stride = _triple(stride)
        padding = _triple(padding)
        dilation = _triple(dilation)
        output_padding = _triple(output_padding)
super(ConvTranspose3d, self).__init__(
in_channels, out_channels, kernel_size, stride, padding, dilation,
"
193,"def check_file(
filename: str,
binary: str,
    build_dir: str,
) -> List[LintMessage]:
try:
proc = run_command(
","def check_file(
filename: str,
binary: str,
    build_dir: Path,
) -> List[LintMessage]:
try:
proc = run_command(
"
194,"from ._six import string_classes as _string_classes
from typing import Set, Type, TYPE_CHECKING
__all__ = [
'typename', 'is_tensor', 'is_storage', 'set_default_tensor_type',
","from ._six import string_classes as _string_classes
from typing import Set, Type, TYPE_CHECKING, Union
import builtins
__all__ = [
'typename', 'is_tensor', 'is_storage', 'set_default_tensor_type',
"
195,"if check_forward_ad:
complex_inp_indices = [i for i, inp in enumerate(tupled_inputs) if is_tensor_like(inp) and inp.is_complex()]
if complex_inp_indices:
            real_fn, imag_fn = _real_and_imag_input(func, complex_inp_indices)
imag_inputs = [inp.imag if is_tensor_like(inp) and inp.is_complex() else inp for inp in tupled_inputs]
imag_func_out = imag_fn(*imag_inputs)
","if check_forward_ad:
complex_inp_indices = [i for i, inp in enumerate(tupled_inputs) if is_tensor_like(inp) and inp.is_complex()]
if complex_inp_indices:
            real_fn, imag_fn = _real_and_imag_input(func, complex_inp_indices, tupled_inputs)
imag_inputs = [inp.imag if is_tensor_like(inp) and inp.is_complex() else inp for inp in tupled_inputs]
imag_func_out = imag_fn(*imag_inputs)
"
196,"except StopIteration:
stop = True
self.main_datapipe_exhausted = True
def is_instance_started(self, instance_id: int) -> bool:
return self.instance_started[instance_id]
","except StopIteration:
stop = True
self.main_datapipe_exhausted = True
                    self._datapipe_iterator = None
def is_instance_started(self, instance_id: int) -> bool:
return self.instance_started[instance_id]
"
197,"import builtins
import copy
import inspect
import textwrap
import warnings
from types import FunctionType
","import builtins
import copy
import inspect
import logging
import textwrap
import warnings
from types import FunctionType
"
198,"std_values = std._values().sqrt_().add_(eps)
param.add_(_make_sparse(grad, grad_indices, grad_values / std_values), alpha=-clr)
else:
state_sum.addcmul_(grad, grad, value=1)
std = state_sum.sqrt().add_(eps)
param.addcdiv_(grad, std, value=-clr)
def adam(params: List[Tensor],
","std_values = std._values().sqrt_().add_(eps)
param.add_(_make_sparse(grad, grad_indices, grad_values / std_values), alpha=-clr)
else:
            is_complex = torch.is_complex(param)
            if is_complex:
                grad = torch.view_as_real(grad)
                state_sum = torch.view_as_real(state_sum)
                param = torch.view_as_real(param)
state_sum.addcmul_(grad, grad, value=1)
std = state_sum.sqrt().add_(eps)
param.addcdiv_(grad, std, value=-clr)
            if is_complex:
                param = torch.view_as_complex(param)
                state_sum = torch.view_as_complex(state_sum)


def adam(params: List[Tensor],
"
199,"index 29c8cc676a..145701d9a8 100644
-- a/torch/ao/quantization/fx/_convert_new.py
patterns: Dict[Pattern, QuantizeHandler] = observed._patterns  # type: ignore[assignment]
return patterns, node_name_to_scope, prepare_custom_config_dict
def _convert_new(model: GraphModule, is_reference: bool = False,
                 convert_custom_config_dict: Dict[str, Any] = None,
                 is_standalone_module: bool = False,
                 _remove_qconfig_flag: bool = True) -> QuantizedGraphModule:
"""""" standalone_module means it a submodule that is not inlined in
parent module, and will be quantized separately as one unit.
","index 29c8cc676a..145701d9a8 100644
++ b/torch/ao/quantization/fx/_convert_do_not_use.py
patterns: Dict[Pattern, QuantizeHandler] = observed._patterns  # type: ignore[assignment]
return patterns, node_name_to_scope, prepare_custom_config_dict
def _convert_do_not_use(
        model: GraphModule, is_reference: bool = False,
        convert_custom_config_dict: Dict[str, Any] = None,
        is_standalone_module: bool = False,
        _remove_qconfig_flag: bool = True) -> QuantizedGraphModule:
"""""" standalone_module means it a submodule that is not inlined in
parent module, and will be quantized separately as one unit.
"
200,"check_is_valid_prepare_custom_config_dict(prepare_custom_config_dict)
check_is_valid_qconfig_dict(equalization_qconfig_dict)
    skipped_module_names = prepare_custom_config_dict.get(""non_traceable_module_name"", [])
    skipped_module_classes = prepare_custom_config_dict.get(""non_traceable_module_class"", [])
# swap FloatFunctional with FXFloatFunctional
_swap_ff_with_fxff(model)
","check_is_valid_prepare_custom_config_dict(prepare_custom_config_dict)
check_is_valid_qconfig_dict(equalization_qconfig_dict)
    skipped_module_names = prepare_custom_config_dict.get(
        ""non_traceable_module_name"", []
    )
    skipped_module_classes = prepare_custom_config_dict.get(
        ""non_traceable_module_class"", []
    )
# swap FloatFunctional with FXFloatFunctional
_swap_ff_with_fxff(model)
"
201,"""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.prepare_fx"")
    assert not model.training, 'prepare_fx only works for models in ' + \
        'eval mode'
return _prepare_fx(
model,
qconfig_dict,
prepare_custom_config_dict,
equalization_qconfig_dict,
        backend_config_dict)
def prepare_qat_fx(
        model: torch.nn.Module, qconfig_dict: Any,
        prepare_custom_config_dict: Optional[Dict[str, Any]] = None,
        backend_config_dict: Optional[Dict[str, Any]] = None) -> ObservedGraphModule:
r"""""" Prepare a model for quantization aware training
Args:
","""""""
torch._C._log_api_usage_once(""quantization_api.quantize_fx.prepare_fx"")
    assert not model.training, ""prepare_fx only works for models in "" + ""eval mode""
return _prepare_fx(
model,
qconfig_dict,
prepare_custom_config_dict,
equalization_qconfig_dict,
        backend_config_dict,
    )

def prepare_qat_fx(
    model: torch.nn.Module,
    qconfig_dict: Any,
    prepare_custom_config_dict: Optional[Dict[str, Any]] = None,
    backend_config_dict: Optional[Dict[str, Any]] = None,
) -> ObservedGraphModule:
r"""""" Prepare a model for quantization aware training
Args:
"
202,"import operator
from typing import Dict, Set, List, Optional, Union
import torch.fx
import torch.fx.experimental.fx_acc.acc_utils as acc_utils
from torch.fx.passes.split_module import split_module
def _make_tuple(x):
    """"""
    Helper to convert x into a one item tuple if it's not a tuple already.
    """"""
    return x if isinstance(x, tuple) else (x,)


class FoldedGraphModule(torch.fx.GraphModule):
""""""
FoldedGraphModule is a GraphModule which also contains another
","from typing import Dict, Set, Optional, Union
import torch.fx
import torch.fx.experimental.fx_acc.acc_utils as acc_utils
from torch.fx.node import map_arg
from torch.fx.passes.split_module import split_module
class FoldedGraphModule(torch.fx.GraphModule):
""""""
FoldedGraphModule is a GraphModule which also contains another
"
203,"if not isinstance(node.kwargs[k], torch.fx.Node):
continue
            kwarg_tensor_meta = node.kwargs[k].meta.get(""tensor_meta"")  # type: ignore[union-attr]
            kwarg_dtype = kwarg_tensor_meta.dtype if kwarg_tensor_meta else None

if kwarg_dtype not in dtypes:
return False
return True
","if not isinstance(node.kwargs[k], torch.fx.Node):
continue
            kwarg_dtype = _get_arg_dtype(node.kwargs[k])  # type: ignore[arg-type]
if kwarg_dtype not in dtypes:
return False
return True


# ======================================================================
# Functional interfaces and utils for defining basic operator support logic
# and composing them into more complex ones
# ======================================================================

IsNodeSupported = t.Callable[[t.Mapping[str, torch.nn.Module], torch.fx.Node], bool]


@compatibility(is_backward_compatible=False)
def create_op_support(is_node_supported: IsNodeSupported) -> OperatorSupportBase:
    """"""Wraps a `IsNodeSupported` function into an `OperatorSupportBase` instance

    `IsNodeSupported` has the same call signature as
    `OperatorSupportBase.is_node_supported`
    """"""
    class FunctionalOperatorSupport(OperatorSupportBase):
        def is_node_supported(
                self, submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node
        ) -> bool:
            return is_node_supported(submodules, node)
    return FunctionalOperatorSupport()


@compatibility(is_backward_compatible=False)
def chain(*op_support: OperatorSupportBase) -> OperatorSupportBase:
    """"""Combines a sequence of `OperatorSupportBase` instances to form a single `OperatorSupportBase`
    instance by evaluating each input `OperatorSupportBase` instance, and returns False if
    any of it reports False.
    """"""
    def _chain(submods, node) -> bool:
        return all(
            x.is_node_supported(submods, node)
            for x in op_support
        )
    return create_op_support(_chain)


@compatibility(is_backward_compatible=False)
class OpSupports:
    """"""A set of atomic `OperatorSupportBase` instances that can be combined together
    to form more complex operator support logic.
    """"""
    @classmethod
    def decline_if_input_dtype(cls, dtype: torch.dtype) -> OperatorSupportBase:
        """"""Report a node as non-supported, if any of its arguments is of dtype""""""

        def _decline_if_input_dtype(
            submodules: t.Mapping[str, torch.nn.Module],
            node: torch.fx.Node,
        ) -> bool:
            for arg in node._input_nodes:
                arg_dtype = _get_arg_dtype(arg)
                if arg_dtype == dtype:
                    return False
            return True
        return create_op_support(_decline_if_input_dtype)


def _get_arg_dtype(arg: torch.fx.Node) -> t.Any:
    assert isinstance(arg, torch.fx.Node)
    tensor_meta = arg.meta.get(""tensor_meta"")  # type: ignore[union-attr]
    dtype = tensor_meta.dtype if isinstance(tensor_meta, TensorMetadata) else arg.meta[""type""]
    return dtype
"
204,"else:
raise RuntimeError(f'Invalid torch.memory_format: {memory_format}')
        # Keep old seriazation to ensure backward compatibility
return (
self.shards_metadata,
self.size,
","else:
raise RuntimeError(f'Invalid torch.memory_format: {memory_format}')
        # Keep old serialization to ensure backward compatibility
return (
self.shards_metadata,
self.size,
"
205,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path, cupti_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
"
206,"cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
else:
cuda_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
","cuda_version_1 = cuda_version.replace('.', '_')
cuda_path_var = 'CUDA_PATH_V' + cuda_version_1
default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
        cuda_base = os.getenv(cuda_path_var, default_path)
        cuda_path = os.path.join(cuda_base, 'bin')
        cupti_path = os.path.join(cuda_base, 'extras', 'CUPTI', 'lib64')
else:
cuda_path = ''
        cupti_path = ''
    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path, cupti_path]))
kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
"
207,"if len(tracers) > 1:
raise RuntimeError(f'Found multiple different tracers {list(tracers.keys())} while '
                               'trying to trace operations {orig_method}')
tracer = next(iter(tracers.keys()))
if isinstance(orig_method, torch._C.ScriptMethod):
","if len(tracers) > 1:
raise RuntimeError(f'Found multiple different tracers {list(tracers.keys())} while '
                               f'trying to trace operations {orig_method}')
tracer = next(iter(tracers.keys()))
if isinstance(orig_method, torch._C.ScriptMethod):
"
208,"combination is not supported (since fbgemm/qnnpack only support certain dtype
combinations), so the output may be float, but when is_reference is True,
we support all dtype combinations so the output will always be quantized.
""""""
return True
","combination is not supported (since fbgemm/qnnpack only support certain dtype
combinations), so the output may be float, but when is_reference is True,
we support all dtype combinations so the output will always be quantized.

        TODO: This is fragile, whether output is quantized should not depend on `is_reference` since
        we want to make sure whether a Tensor is quantized
        should be the same in prepare and convert and is_reference
        is only available in convert currently

""""""
return True
"
209,"return axes
def create_constant(network, tensor, name, squeeze_vector=True):
    """"""
    Args:
        squeeze_vector: if set to True, we'll squeeze a vector of shape (1, ..1, n) to (n,)
        and rely on broadcasting to expand the dimensions as needed
    """"""
if isinstance(tensor, int):
tensor = torch.IntTensor([tensor])
if isinstance(tensor, float):
tensor = torch.Tensor([tensor])
    shape = tuple(tensor.shape)
    if squeeze_vector:
        # Remove all preceding 1s as they can be re-inserted later during broadcasting.
        num_preceding_ones = 0
        for j in range(len(shape)):
            if int(shape[j]) == 1:
                num_preceding_ones += 1
            else:
                break
        # If shape is all 1s, we want last digit.
        shape = shape[num_preceding_ones:] if num_preceding_ones < len(shape) else (1,)
    constant = network.add_constant(shape, to_numpy(tensor))
constant.name = name
return constant.get_output(0)
def get_trt_tensor(network, input_val, name, squeeze_vector=True):
if isinstance(input_val, (torch.Tensor, int, float)):
        return create_constant(network, input_val, name, squeeze_vector)
elif not isinstance(input_val, trt.tensorrt.ITensor):
raise RuntimeError(
f""Received input {input_val} of name {name} that ""
","return axes
def create_constant(network, tensor, name, dtype):
if isinstance(tensor, int):
tensor = torch.IntTensor([tensor])
if isinstance(tensor, float):
tensor = torch.Tensor([tensor])
    if dtype:
        tensor = tensor.to(dtype)
    constant = network.add_constant(tensor.shape, to_numpy(tensor))
constant.name = name
return constant.get_output(0)
def get_trt_tensor(network, input_val, name, dtype=None):
if isinstance(input_val, (torch.Tensor, int, float)):
        return create_constant(network, input_val, name, dtype)
elif not isinstance(input_val, trt.tensorrt.ITensor):
raise RuntimeError(
f""Received input {input_val} of name {name} that ""
"
210,"def broadcast(network, a, b, a_name, b_name, preset_diff=0):
a_shape = tuple(a.shape)
b_shape = tuple(b.shape)
","def broadcast(network, a, b, a_name, b_name, preset_diff=0):
    """"""
    Broadcast two TensorRT tensors to the same number of dimensions by
    prepending 1s to the tensor with less number of dimensions.

    Args:
        network: TensorRT network object.
        a: A TensorRT tensor.
        b: A TensorRT tensor.
        a_name: Name of tensor a.
        b_name: Name of tensor b.
        preset_diff: The difference of number of dimensions after broadcast.
            A positive number means after broadcast, tensor `a` would have
            `preset_diff` more dimensions than `b`. This is used in matmul,
            since we need to broadcast tensors but not always to the same
            number of dimension. The reason is that matmul supports Matrix
            x Vector and in this case broadcasted vector should have 1 less
            number of dimensions than the matrix tensor.
    """"""
a_shape = tuple(a.shape)
b_shape = tuple(b.shape)
"
211,"if is_reference:
act_dtype = activation_dtype(qconfig)
            if act_dtype == torch.float:
return quantized_graph.node_copy(node, load_arg(quantized=torch.float))
else:
if self.num_tensor_args == 2:
","if is_reference:
act_dtype = activation_dtype(qconfig)
            dtypes = get_qconfig_dtypes(qconfig)
            if act_dtype == torch.float or \
               not (self.binary_op in binary_op_supported_dtypes and dtypes in binary_op_supported_dtypes[self.binary_op]):
return quantized_graph.node_copy(node, load_arg(quantized=torch.float))
else:
if self.num_tensor_args == 2:
"
212,"if sys.platform.startswith('linux'):
minimum_required_version = MINIMUM_GCC_VERSION
versionstr = subprocess.check_output([compiler, '-dumpfullversion', '-dumpversion'])
            version = versionstr.decode().strip().split('.')
else:
minimum_required_version = MINIMUM_MSVC_VERSION
compiler_info = subprocess.check_output(compiler, stderr=subprocess.STDOUT)
            match = re.search(r'(\d+)\.(\d+)\.(\d+)', compiler_info.decode().strip())
version = (0, 0, 0) if match is None else match.groups()
except Exception:
_, error, _ = sys.exc_info()
","if sys.platform.startswith('linux'):
minimum_required_version = MINIMUM_GCC_VERSION
versionstr = subprocess.check_output([compiler, '-dumpfullversion', '-dumpversion'])
            version = versionstr.decode(*SUBPROCESS_DECODE_ARGS).strip().split('.')
else:
minimum_required_version = MINIMUM_MSVC_VERSION
compiler_info = subprocess.check_output(compiler, stderr=subprocess.STDOUT)
            match = re.search(r'(\d+)\.(\d+)\.(\d+)', compiler_info.decode(*SUBPROCESS_DECODE_ARGS).strip())
version = (0, 0, 0) if match is None else match.groups()
except Exception:
_, error, _ = sys.exc_info()
"
213,"export_params (bool, default True): if True, all parameters will
be exported. Set this to False if you want to export an untrained model.
In this case, the exported model will first take all of its parameters
            as arguments, with the ordering as specified by ``model.state_dict().values()``
verbose (bool, default False): if True, prints a description of the
model being exported to stdout.
training (enum, default TrainingMode.EVAL):
            * ``TrainingMode.EVAL``: export the model in inference mode.
* ``TrainingMode.PRESERVE``: export the model in inference mode if model.training is
False and in training mode if model.training is True.
* ``TrainingMode.TRAINING``: export the model in training mode. Disables optimizations
","export_params (bool, default True): if True, all parameters will
be exported. Set this to False if you want to export an untrained model.
In this case, the exported model will first take all of its parameters
            as arguments, with the ordering as specified by ``model.state_dict().values()``.
            This helps in stripping parameters from the model which is useful for training.
            Besides, if this is False, any optimization that may adjust graph inputs will
            be skipped - for example, Conv and BatchNorm fusion.
verbose (bool, default False): if True, prints a description of the
model being exported to stdout.
training (enum, default TrainingMode.EVAL):
            * ``TrainingMode.EVAL``: export the model in inference mode. In this case, optimizations
              (e.g., fusing Conv and BatchNorm ops) may adjust graph inputs by modifying model params
              and model param names. Such adjustment could be skipped by setting export_params = False
              or keep_initializers_as_inputs = True.
* ``TrainingMode.PRESERVE``: export the model in inference mode if model.training is
False and in training mode if model.training is True.
* ``TrainingMode.TRAINING``: export the model in training mode. Disables optimizations
"
214,"do_constant_folding (bool, default False): Apply the constant-folding optimization.
Constant-folding will replace some of the ops that have all constant inputs
with pre-computed constant nodes.
example_outputs (T or a tuple of T, where T is Tensor or convertible to Tensor, default None):
Must be provided when exporting a ScriptModule or ScriptFunction, ignored otherwise.
Used to determine the type and shape of the outputs without tracing the execution of
","do_constant_folding (bool, default False): Apply the constant-folding optimization.
Constant-folding will replace some of the ops that have all constant inputs
with pre-computed constant nodes.
            Since this optimization adjusts model initializers, it will be disabled if
            export_params = False or keep_initializers_as_inputs = True.
example_outputs (T or a tuple of T, where T is Tensor or convertible to Tensor, default None):
Must be provided when exporting a ScriptModule or ScriptFunction, ignored otherwise.
Used to determine the type and shape of the outputs without tracing the execution of
"
215,"This contains the information needed by :class:`ZeroRedundancyOptimizer`
to overlap with :class:`DistributedDataParallel`.
status (_OverlapStatus): current status; see :class:`_OverlapStatus`
for more information.
params_per_bucket (List[List[torch.Tensor]]): ``params_per_bucket[i]``
","This contains the information needed by :class:`ZeroRedundancyOptimizer`
to overlap with :class:`DistributedDataParallel`.
    Arguments:
        world_size (int): world size of the process group being used.

    Attributes:
        shard_buckets (bool): if ``True``, then the assignment of each
            :class:`DistributedDataParallel` bucket is partitioned across
            possibly multiple :class:`ZeroRedundancyOptimizer` instances (i.e.
            across possibly multiple ranks) to approximate uniformity following
            a threshold given by the total parameter size divided by the world
            size; if ``False``, then each bucket is wholly assigned to a single
            :class:`ZeroRedundancyOptimizer` instance (i.e. to a single rank);
            this should be set to the value passed into the hook constructor.
status (_OverlapStatus): current status; see :class:`_OverlapStatus`
for more information.
params_per_bucket (List[List[torch.Tensor]]): ``params_per_bucket[i]``
"
216,"bucket_indices_seen (List[int]): :class:`list` of the bucket indices
seen on this iteration.
""""""
    def __init__(self) -> None:
self.status: _OverlapStatus = _OverlapStatus.UNINITIALIZED
# Modified per bucket reconstruction
self.params_per_bucket: List[List[torch.Tensor]] = []
self.params_per_rank: List[List[torch.Tensor]] = \
            [[] for _ in range(dist.get_world_size())]
self.offsets: Dict[int, int] = {}
# Modified per iteration
self.broadcast_handles: List[Any] = []
","bucket_indices_seen (List[int]): :class:`list` of the bucket indices
seen on this iteration.
""""""
    def __init__(self, world_size) -> None:
self.status: _OverlapStatus = _OverlapStatus.UNINITIALIZED
        self.shard_buckets: bool = False
# Modified per bucket reconstruction
self.params_per_bucket: List[List[torch.Tensor]] = []
self.params_per_rank: List[List[torch.Tensor]] = \
            [[] for _ in range(world_size)]
self.offsets: Dict[int, int] = {}
        self.assigned_ranks_per_bucket: List[Set[int]] = []
        self.num_bucket_assignments: int = 0
        self.total_size: Optional[int] = None
# Modified per iteration
self.broadcast_handles: List[Any] = []
"
217,"self._device_to_params_per_rank_cache[device][rank].append(param)
return self._device_to_params_per_rank_cache
    @property
    def _device_to_buckets(
        self
    ) -> Dict[torch.device, List[List[_DDPBucket]]]:
r""""""
        :class:`dict` mapping each device to a :class:`list` of :class:`list`
        of :class:`_DDPBucket` s.
        ``_device_to_buckets[d][r][i]`` gives the ``i``th bucket
        assigned to rank ``r`` stored on device ``d``, where each bucket
        contains a list of the model parameters associated with the
        corresponding logical :class:`DistributedDataParallel` gradient bucket.
        This is used for constructing the parameter buckets if
        ``overlap_with_ddp=True``.
""""""
        assert self._overlap_with_ddp, \
            ""`_device_to_buckets()` should only be used if "" \
            ""`overlap_with_ddp=True`""
        if len(self._device_to_buckets_cache) > 0:
            return self._device_to_buckets_cache
overlap_info = self._overlap_info
        assert overlap_info.status == _OverlapStatus.INITIALIZED, \
            ""Accessing `_device_to_buckets` before the necessary "" \
            ""information has been collected""
params_per_bucket = overlap_info.params_per_bucket
        for bucket_idx, bucket_params in enumerate(params_per_bucket):
            assert len(bucket_params) > 0, ""Empty bucket""
            rank = self._ddp_bucket_index_to_rank(bucket_idx)
            bucket = _DDPBucket(bucket_idx, bucket_params)
            device = bucket_params[0].device  # assume same device per bucket
            if device not in self._device_to_buckets_cache:
                self._device_to_buckets_cache[device] = [[] for _ in range(self.world_size)]
            self._device_to_buckets_cache[device][rank].append(bucket)
        return self._device_to_buckets_cache
def _local_step(
self,
","self._device_to_params_per_rank_cache[device][rank].append(param)
return self._device_to_params_per_rank_cache
    def _get_min_index(
        self,
        values: List[int],
        disallowed_indices: Optional[Set[int]] = None,
    ) -> int:
r""""""
        Returns ``values.index(min(values))``, except only uses one pass. It
        also excludes any indices in ``disallowed_indices`` if provided.
        Arguments:
            values: (List[int]): :class:`list` of values.
            disallowed_indices (Optional[Set[int]]): indices that are
                disallowed from being the returned min index.
        """"""
        min_index = -1
        min_value = float(""inf"")
        for i, value in enumerate(values):
            if disallowed_indices and i in disallowed_indices:
                continue
            if value < min_value:
                min_value = value
                min_index = i
        assert min_index >= 0, ""All indices are disallowed""
        return min_index

    def _assign_bucket_subset_to_rank(
        self,
        bucket_index: int,
        bucket_params: List[torch.Tensor],
        bucket_offset: int,
        assigned_rank: int,
        assigned_ranks_per_bucket: List[Set[int]],
    ) -> None:
        r""""""
        Assigns the model parameters given by ``bucket_params``, representing a
        (possibly non-strict) subset of the parameters corresponding to a
        :class:`DistributedDataParallel` bucket, to the rank with the least
        size assigned so far and collects relevant information.
        Arguments:
            bucket_index (int): index of the :class:`DistributedDataParallel`
                gradient bucket.
            bucket_params (List[torch.Tensor]): subset of the parameters
                corresponding to the bucket to assign.
            bucket_offset (int): offset giving the index of the first element
                in ``bucket_params`` in the bucket's full parameter list.
            assigned_rank (int): rank to assign to.
            assigned_ranks_per_bucket (List[Set[int]]): :class:`set` of ranks
                assigned to each bucket.
""""""
        overlap_info = self._overlap_info
        if len(bucket_params) == 0:
            raise ValueError(
                ""Empty bucket assignment""
            )
        params_per_rank = overlap_info.params_per_rank
        offsets = overlap_info.offsets

        self._bucket_assignments_per_rank_cache[assigned_rank][bucket_index] = \
            _DDPBucketAssignment(bucket_index, bucket_params, bucket_offset)
        if self.global_rank == assigned_rank:
            offsets[bucket_index] = len(params_per_rank[assigned_rank])
        params_per_rank[assigned_rank].extend(bucket_params)
        assigned_ranks_per_bucket[bucket_index].add(assigned_rank)
        self._overlap_info.num_bucket_assignments += 1

    @property
    def _bucket_assignments_per_rank(
        self
    ) -> List[Dict[int, _DDPBucketAssignment]]:
        r""""""
        :class:`list` of length world size consisting of :class:`dict` s
        mapping bucket indices to :class:`_DDPBucketAssignment` s for each
        rank.
        """"""
        assert self._overlap_with_ddp, ""`_bucket_assignments_per_rank` "" \
            ""only be used if `overlap_with_ddp=True`""
        if len(self._bucket_assignments_per_rank_cache) > 0:
            return self._bucket_assignments_per_rank_cache
overlap_info = self._overlap_info
        assert overlap_info.status == _OverlapStatus.INITIALIZED
        self._bucket_assignments_per_rank_cache = [{} for _ in range(self.world_size)]
params_per_bucket = overlap_info.params_per_bucket
        if overlap_info.shard_buckets:
            # Define the assignment threshold to approximate uniformity
            assert overlap_info.total_size is not None, \
                ""`total_size` was not computed""
            threshold = overlap_info.total_size / self.world_size  # type: ignore[operator]
            size_per_rank = [0 for _ in range(self.world_size)]

        num_buckets = len(params_per_bucket)
        overlap_info.assigned_ranks_per_bucket = [set() for _ in range(num_buckets)]
        assigned_ranks_per_bucket = overlap_info.assigned_ranks_per_bucket
        if not overlap_info.shard_buckets:
            # Assign each DDP bucket entirely to a single rank
            for bucket_index, bucket_params in enumerate(params_per_bucket):
                assert len(bucket_params) > 0, ""Empty bucket""
                assigned_rank = self._get_assigned_rank(bucket_index)
                self._assign_bucket_subset_to_rank(
                    bucket_index,
                    bucket_params,
                    0,
                    assigned_rank,
                    assigned_ranks_per_bucket,
                )
        else:
            # Assign each DDP bucket to possibly multiple ranks
            # Specifically, sort the DDP buckets by increasing size, and for
            # each bucket, iteratively assign the maximal unassigned subset
            # with size less than `threshold` to the rank with the least total
            # size so far -- each such assignment is represented by a
            # `_DDPBucketAssignment` instance and only contains parameters from
            # a single DDP bucket
            params_per_bucket_enum = sorted(
                enumerate(params_per_bucket),
                key=lambda x: sum(p.numel() for p in x[1])
            )
            for bucket_index, bucket_params in params_per_bucket_enum:
                assert len(bucket_params) > 0, ""Empty bucket""
                bucket_offset = 0
                assignment_size = 0
                for param_index, param in enumerate(bucket_params):
                    param_numel = param.numel()
                    if assignment_size + param_numel >= threshold and param_index > bucket_offset:
                        assigned_rank = self._get_min_index(size_per_rank, assigned_ranks_per_bucket[bucket_index])
                        # Include up to but not including the parameter that
                        # exceeded the threshold
                        self._assign_bucket_subset_to_rank(
                            bucket_index,
                            bucket_params[bucket_offset:param_index],
                            bucket_offset,
                            assigned_rank,
                            assigned_ranks_per_bucket,
                        )
                        size_per_rank[assigned_rank] += assignment_size
                        bucket_offset = param_index
                        assignment_size = 0
                    assignment_size += param_numel
                # Assign the remainder of the bucket so that no assignment
                # spans across two buckets
                assigned_rank = self._get_min_index(size_per_rank, assigned_ranks_per_bucket[bucket_index])
                self._assign_bucket_subset_to_rank(
                    bucket_index,
                    bucket_params[bucket_offset:],
                    bucket_offset,
                    assigned_rank,
                    assigned_ranks_per_bucket,
                )
                size_per_rank[assigned_rank] += assignment_size

        return self._bucket_assignments_per_rank_cache
def _local_step(
self,
"
218,"if use_offsets:
code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);"")
else:
        code.append(
            ""        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);""
        )
for i in range(0, uf):
j = 8 * i
code.append(
","if use_offsets:
code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);"")
else:
        code.append(""        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);"")
for i in range(0, uf):
j = 8 * i
code.append(
"
219,"# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os

# sys.path.insert(0, os.path.abspath('.'))
import textwrap
","# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
# sys.path.insert(0, os.path.abspath('.'))
import textwrap
"
220,"# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = ""pytorch_sphinx_theme""
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
html_theme_options = {
    ""canonical_url"": ""https://pytorch.org/docs/stable/"",
    ""pytorch_project"": ""docs"",
    ""collapse_navigation"": False,
    ""display_version"": True,
    ""logo_only"": True,
}
# NOTE: sharing python docs resources
html_logo = os.path.join(
    repo_root, ""docs"", ""source"", ""_static"", ""img"", ""pytorch-logo-dark-unstable.png""
)
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
# NOTE: sharing python docs resources
html_static_path = [os.path.join(repo_root, ""docs"", ""cpp"", ""source"", ""_static"")]
# Called automatically by Sphinx, making this `conf.py` an ""extension"".
","# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'pytorch_sphinx_theme'
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
html_theme_options = {
    'canonical_url': 'https://pytorch.org/docs/stable/',
    'pytorch_project': 'docs',
    'collapse_navigation': False,
    'display_version': True,
    'logo_only': True,
}
# NOTE: sharing python docs resources
html_logo = os.path.join(
    repo_root, 'docs', 'source', '_static', 'img', 'pytorch-logo-dark-unstable.png'
)
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
# NOTE: sharing python docs resources
html_static_path = [os.path.join(repo_root, 'docs', 'cpp', 'source', '_static')]
# Called automatically by Sphinx, making this `conf.py` an ""extension"".
"
221,"# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        ""PyTorch"",
        ""PyTorch Documentation"",
        author,
        ""PyTorch"",
        ""One line description of project."",
        ""Miscellaneous"",
    ),
]
","# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'PyTorch', 'PyTorch Documentation',
     author, 'PyTorch', 'One line description of project.',
     'Miscellaneous'),
]
"
222,"i += 1
setuptools.command.build_ext.build_ext.build_extensions(self)
def get_outputs(self):
outputs = setuptools.command.build_ext.build_ext.get_outputs(self)
outputs.append(os.path.join(self.build_lib, ""caffe2""))
","i += 1
setuptools.command.build_ext.build_ext.build_extensions(self)

def get_outputs(self):
outputs = setuptools.command.build_ext.build_ext.get_outputs(self)
outputs.append(os.path.join(self.build_lib, ""caffe2""))
"
223,"""""""
_overwrite_module_params_on_conversion = False

def set_overwrite_module_params_on_conversion(value):
global _overwrite_module_params_on_conversion
_overwrite_module_params_on_conversion = value

def get_overwrite_module_params_on_conversion():
return _overwrite_module_params_on_conversion
","""""""
_overwrite_module_params_on_conversion = False
def set_overwrite_module_params_on_conversion(value):
global _overwrite_module_params_on_conversion
_overwrite_module_params_on_conversion = value
def get_overwrite_module_params_on_conversion():
return _overwrite_module_params_on_conversion
"
224,"is_loaded = True
if not is_loaded:
if not path_patched:
                os.environ[""PATH""] = "";"".join(dll_paths + [os.environ[""PATH""]])
path_patched = True
res = kernel32.LoadLibraryW(dll)
if res is None:
","is_loaded = True
if not is_loaded:
if not path_patched:
                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])
path_patched = True
res = kernel32.LoadLibraryW(dll)
if res is None:
"
225,"if isinstance(o, torch.Tensor):
return o.type()
    module = """"
    class_name = """"
    if (
        hasattr(o, ""__module__"")
        and o.__module__ != ""builtins""
        and o.__module__ != ""__builtin__""
        and o.__module__ is not None
    ):
        module = o.__module__ + "".""

    if hasattr(o, ""__qualname__""):
class_name = o.__qualname__
    elif hasattr(o, ""__name__""):
class_name = o.__name__
else:
class_name = o.__class__.__name__
","if isinstance(o, torch.Tensor):
return o.type()
    module = ''
    class_name = ''
    if hasattr(o, '__module__') and o.__module__ != 'builtins' \
            and o.__module__ != '__builtin__' and o.__module__ is not None:
        module = o.__module__ + '.'

    if hasattr(o, '__qualname__'):
class_name = o.__qualname__
    elif hasattr(o, '__name__'):
class_name = o.__name__
else:
class_name = o.__class__.__name__
"
226,"from torch._C._VariableFunctions import *  # type: ignore[misc] # noqa: F403
for name in dir(_C._VariableFunctions):
    if name.startswith(""__""):
continue
globals()[name] = getattr(_C._VariableFunctions, name)
__all__.append(name)
","from torch._C._VariableFunctions import *  # type: ignore[misc] # noqa: F403
for name in dir(_C._VariableFunctions):
    if name.startswith('__'):
continue
globals()[name] = getattr(_C._VariableFunctions, name)
__all__.append(name)
"
227,"path = os.path.join(path, appauthor, appname)
else:
path = os.path.join(path, appname)
    elif system == ""darwin"":
        path = os.path.expanduser(""~/Library/Application Support/"")
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv(""XDG_DATA_HOME"", os.path.expanduser(""~/.local/share""))
if appname:
path = os.path.join(path, appname)
if appname and version:
","path = os.path.join(path, appauthor, appname)
else:
path = os.path.join(path, appname)
    elif system == 'darwin':
        path = os.path.expanduser('~/Library/Application Support/')
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv('XDG_DATA_HOME', os.path.expanduser(""~/.local/share""))
if appname:
path = os.path.join(path, appname)
if appname and version:
"
228,"@property
def user_data_dir(self):
        return user_data_dir(
            self.appname, self.appauthor, version=self.version, roaming=self.roaming
        )
@property
def site_data_dir(self):
        return site_data_dir(
            self.appname, self.appauthor, version=self.version, multipath=self.multipath
        )
@property
def user_config_dir(self):
        return user_config_dir(
            self.appname, self.appauthor, version=self.version, roaming=self.roaming
        )
@property
def site_config_dir(self):
        return site_config_dir(
            self.appname, self.appauthor, version=self.version, multipath=self.multipath
        )
@property
def user_cache_dir(self):
        return user_cache_dir(self.appname, self.appauthor, version=self.version)
@property
def user_state_dir(self):
        return user_state_dir(self.appname, self.appauthor, version=self.version)
@property
def user_log_dir(self):
        return user_log_dir(self.appname, self.appauthor, version=self.version)

# ---- internal support stuff
def _get_win_folder_from_registry(csidl_name):
""""""This is a fallback technique at best. I'm not sure if using the
","@property
def user_data_dir(self):
        return user_data_dir(self.appname, self.appauthor,
                             version=self.version, roaming=self.roaming)
@property
def site_data_dir(self):
        return site_data_dir(self.appname, self.appauthor,
                             version=self.version, multipath=self.multipath)
@property
def user_config_dir(self):
        return user_config_dir(self.appname, self.appauthor,
                               version=self.version, roaming=self.roaming)
@property
def site_config_dir(self):
        return site_config_dir(self.appname, self.appauthor,
                             version=self.version, multipath=self.multipath)
@property
def user_cache_dir(self):
        return user_cache_dir(self.appname, self.appauthor,
                              version=self.version)
@property
def user_state_dir(self):
        return user_state_dir(self.appname, self.appauthor,
                              version=self.version)
@property
def user_log_dir(self):
        return user_log_dir(self.appname, self.appauthor,
                            version=self.version)
#---- internal support stuff
def _get_win_folder_from_registry(csidl_name):
""""""This is a fallback technique at best. I'm not sure if using the
"
229,"if torch.is_storage(obj):
serialized_storages.append(obj)
serialized_dtypes.append(obj.dtype)
            return (""storage"", len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
if _serialized_reduces.get(id(obj)) is None:
","if torch.is_storage(obj):
serialized_storages.append(obj)
serialized_dtypes.append(obj.dtype)
            return ('storage', len(serialized_storages) - 1)
if hasattr(obj, ""__reduce_deploy__""):
if _serialized_reduces.get(id(obj)) is None:
"
230,"result = _deploy_objects[id] = unpickler.load()
return result

def _get_package(zip_reader):
if zip_reader not in _raw_packages:
_raw_packages[zip_reader] = PackageImporter(zip_reader)
","result = _deploy_objects[id] = unpickler.load()
return result
def _get_package(zip_reader):
if zip_reader not in _raw_packages:
_raw_packages[zip_reader] = PackageImporter(zip_reader)
"
231,"return fn
if not isinstance(drop, bool):
        raise RuntimeError(
            ""Argument to @torch.jit.ignore must be a bool or ""
            f""a function but got {drop}""
        )
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(
            ""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
drop = drop_on_export
elif drop:
        warnings.warn(
            ""ignore(True) has been deprecated. TorchScript will now drop the function ""
            ""call on compilation. Use torch.jit.unused now. {}"",
            category=FutureWarning,
        )
def decorator(fn):
if drop:
","return fn
if not isinstance(drop, bool):
        raise RuntimeError(""Argument to @torch.jit.ignore must be a bool or ""
                           f""a function but got {drop}"")
# for backwards compat
drop_on_export = kwargs.pop(""drop_on_export"", None)
if drop_on_export:
        warnings.warn(""ignore(drop_on_export=True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
drop = drop_on_export
elif drop:
        warnings.warn(""ignore(True) has been deprecated. TorchScript will now drop the function ""
                      ""call on compilation. Use torch.jit.unused now. {}"", category=FutureWarning)
def decorator(fn):
if drop:
"
232,"return isinstance(x, ast.Expr) and isinstance(x.value, ast.Ellipsis)
if len(body) != 1 or not (is_pass(body[0]) or is_ellipsis(body[0])):
        msg = (
            ""Only `pass` statement or `...` can be the body of overload declaration:\n""
        )
        msg += ""\n"".join(parsed_def.source.split(""\n"")[:3])
msg += "" <- Expecting `pass` or `...` here!\n"" + _OVERLOAD_EXAMPLE
raise RuntimeError(msg)

def _overload(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
","return isinstance(x, ast.Expr) and isinstance(x.value, ast.Ellipsis)
if len(body) != 1 or not (is_pass(body[0]) or is_ellipsis(body[0])):
        msg = ""Only `pass` statement or `...` can be the body of overload declaration:\n""
        msg += '\n'.join(parsed_def.source.split(""\n"")[:3])
msg += "" <- Expecting `pass` or `...` here!\n"" + _OVERLOAD_EXAMPLE
raise RuntimeError(msg)
def _overload(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
"
233,")
return getattr(ann, ""__origin__"", None) is Future

if torch.distributed.rpc.is_available():
    from torch._C._distributed_rpc import PyRRef
from torch.distributed.rpc import RRef
def is_rref(ann) -> bool:
if ann is RRef:
",")
return getattr(ann, ""__origin__"", None) is Future
if torch.distributed.rpc.is_available():
from torch.distributed.rpc import RRef
    from torch._C._distributed_rpc import PyRRef
def is_rref(ann) -> bool:
if ann is RRef:
"
234,"else:
raise RuntimeError(""Could not get name of python class object"")
    if name == ""<lambda>"":
        name = ""_lambda""  # make name a valid identifier
module_name = obj.__module__
","else:
raise RuntimeError(""Could not get name of python class object"")

    if name == '<lambda>':
        name = '_lambda'  # make name a valid identifier
module_name = obj.__module__
"
235,"module_name = ""__torch__."" + module_name
if ""."" in name:
        raise RuntimeError(
            f""Could not get qualified name for class '{name}': ""
            f""'{name}' is not a valid identifier""
        )
return module_name + ""."" + name
","module_name = ""__torch__."" + module_name
if ""."" in name:
        raise RuntimeError(f""Could not get qualified name for class '{name}': ""
                           f""'{name}' is not a valid identifier"")
return module_name + ""."" + name
"
236,"# symeig backward
if B is None:
            A_grad = _symeig_backward(D_grad, U_grad, A, D, U, largest)
# A has index 0
grads[0] = A_grad
","# symeig backward
if B is None:
            A_grad = _symeig_backward(
                D_grad, U_grad, A, D, U, largest
            )
# A has index 0
grads[0] = A_grad
"
237,"Update or initialize iteration variables when `method == ""basic""`.
""""""
mm = torch.matmul
        ns = self.ivars[""converged_end""]
        nc = self.ivars[""converged_count""]
        n = self.iparams[""n""]
        largest = self.bparams[""largest""]
        if self.ivars[""istep""] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
","Update or initialize iteration variables when `method == ""basic""`.
""""""
mm = torch.matmul
        ns = self.ivars['converged_end']
        nc = self.ivars['converged_count']
        n = self.iparams['n']
        largest = self.bparams['largest']
        if self.ivars['istep'] == 0:
Ri = self._get_rayleigh_ritz_transform(self.X)
M = _utils.qform(_utils.qform(self.A, self.X), Ri)
E, Z = _utils.symeig(M, largest)
"
238,"U = self._get_svqb(U, False, tau_replace)
if torch.numel(U) == 0:
# all initial U columns are B-collinear to V
                    self.ivars[""ortho_i""] = i
                    self.ivars[""ortho_j""] = j
return U
BU = mm_B(self.B, U)
UBU = mm(_utils.transpose(U), BU)
U_norm = torch.norm(U)
BU_norm = torch.norm(BU)
                R = UBU - torch.eye(UBU.shape[-1], device=UBU.device, dtype=UBU.dtype)
R_norm = torch.norm(R)
# https://github.com/pytorch/pytorch/issues/33810 workaround:
rerr = float(R_norm) * float(BU_norm * U_norm) ** -1
                vkey = ""ortho_UBUmI_rerr[{}, {}]"".format(i, j)
self.fvars[vkey] = rerr
if rerr < tau_ortho:
break
","U = self._get_svqb(U, False, tau_replace)
if torch.numel(U) == 0:
# all initial U columns are B-collinear to V
                    self.ivars['ortho_i'] = i
                    self.ivars['ortho_j'] = j
return U
BU = mm_B(self.B, U)
UBU = mm(_utils.transpose(U), BU)
U_norm = torch.norm(U)
BU_norm = torch.norm(BU)
                R = UBU - torch.eye(UBU.shape[-1],
                                    device=UBU.device,
                                    dtype=UBU.dtype)
R_norm = torch.norm(R)
# https://github.com/pytorch/pytorch/issues/33810 workaround:
rerr = float(R_norm) * float(BU_norm * U_norm) ** -1
                vkey = 'ortho_UBUmI_rerr[{}, {}]'.format(i, j)
self.fvars[vkey] = rerr
if rerr < tau_ortho:
break
"
239,"if tensor.has_names():
raise RuntimeError(
""NYI: Named tensors don't support serialization. Please drop ""
            ""names via `tensor = tensor.rename(None)` before serialization.""
        )
def build_dim_map(tensor):
""""""Returns a map of { dim: dim_name } where dim is a name if the dim is named
and the dim index otherwise.""""""
    return OrderedDict(
        [(idx if name is None else name, name) for idx, name in enumerate(tensor.names)]
    )
def unzip_namedshape(namedshape):
if isinstance(namedshape, OrderedDict):
namedshape = namedshape.items()
    if not hasattr(namedshape, ""__iter__"") and not isinstance(namedshape, tuple):
raise RuntimeError(
            ""Expected namedshape to be OrderedDict or iterable of tuples, got: {}"".format(
                type(namedshape)
            )
        )
if len(namedshape) == 0:
        raise RuntimeError(""Expected namedshape to non-empty."")
return zip(*namedshape)
def namer_api_name(inplace):
if inplace:
        return ""rename_""
else:
        return ""rename""
def is_ellipsis(item):
    return item == Ellipsis or item == ""...""

def single_ellipsis_index(names, fn_name):
ellipsis_indices = [i for i, name in enumerate(names) if is_ellipsis(name)]
if len(ellipsis_indices) >= 2:
        raise RuntimeError(
            ""{}: More than one Ellipsis ('...') found in names (""
            ""{}). This function supports up to one Ellipsis."".format(fn_name, names)
        )
if len(ellipsis_indices) == 1:
return ellipsis_indices[0]
return None

def expand_single_ellipsis(numel_pre_glob, numel_post_glob, names):
    return names[numel_pre_glob : len(names) - numel_post_glob]
def replace_ellipsis_by_position(ellipsis_idx, names, tensor_names):
    globbed_names = expand_single_ellipsis(
        ellipsis_idx, len(names) - ellipsis_idx - 1, tensor_names
    )
    return names[:ellipsis_idx] + globbed_names + names[ellipsis_idx + 1 :]
def resolve_ellipsis(names, tensor_names, fn_name):
","if tensor.has_names():
raise RuntimeError(
""NYI: Named tensors don't support serialization. Please drop ""
            ""names via `tensor = tensor.rename(None)` before serialization."")
def build_dim_map(tensor):
""""""Returns a map of { dim: dim_name } where dim is a name if the dim is named
and the dim index otherwise.""""""
    return OrderedDict([(idx if name is None else name, name)
                        for idx, name in enumerate(tensor.names)])
def unzip_namedshape(namedshape):
if isinstance(namedshape, OrderedDict):
namedshape = namedshape.items()
    if not hasattr(namedshape, '__iter__') and not isinstance(namedshape, tuple):
raise RuntimeError(
            'Expected namedshape to be OrderedDict or iterable of tuples, got: {}'
            .format(type(namedshape)))
if len(namedshape) == 0:
        raise RuntimeError('Expected namedshape to non-empty.')
return zip(*namedshape)
def namer_api_name(inplace):
if inplace:
        return 'rename_'
else:
        return 'rename'
def is_ellipsis(item):
    return item == Ellipsis or item == '...'
def single_ellipsis_index(names, fn_name):
ellipsis_indices = [i for i, name in enumerate(names) if is_ellipsis(name)]
if len(ellipsis_indices) >= 2:
        raise RuntimeError('{}: More than one Ellipsis (\'...\') found in names ('
                           '{}). This function supports up to one Ellipsis.'
                           .format(fn_name, names))
if len(ellipsis_indices) == 1:
return ellipsis_indices[0]
return None
def expand_single_ellipsis(numel_pre_glob, numel_post_glob, names):
    return names[numel_pre_glob:len(names) - numel_post_glob]
def replace_ellipsis_by_position(ellipsis_idx, names, tensor_names):
    globbed_names = expand_single_ellipsis(ellipsis_idx, len(names) - ellipsis_idx - 1, tensor_names)
    return names[:ellipsis_idx] + globbed_names + names[ellipsis_idx + 1:]
def resolve_ellipsis(names, tensor_names, fn_name):
"
240,"import contextlib
import ctypes
import sys
import types
import torch._C
import torch._utils_internal
import torch.jit
# Query `hasattr` only once.
_SET_GLOBAL_FLAGS = hasattr(sys, ""getdlopenflags"") and hasattr(sys, ""setdlopenflags"")
@contextlib.contextmanager
","import torch._C

import contextlib
import ctypes
import sys
import types
import torch.jit
import torch._utils_internal
# Query `hasattr` only once.
_SET_GLOBAL_FLAGS = hasattr(sys, 'getdlopenflags') and hasattr(sys, 'setdlopenflags')
@contextlib.contextmanager
"
241,"gradient=gradient,
retain_graph=retain_graph,
create_graph=create_graph,
                inputs=inputs,
            )
        torch.autograd.backward(
            self, gradient, retain_graph, create_graph, inputs=inputs
        )
def register_hook(self, hook):
r""""""Registers a backward hook.
","gradient=gradient,
retain_graph=retain_graph,
create_graph=create_graph,
                inputs=inputs)
        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
def register_hook(self, hook):
r""""""Registers a backward hook.
"
242,"""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.refine_names, (self,), self, *names)
        names = resolve_ellipsis(names, self.names, ""refine_names"")
return super(Tensor, self).refine_names(names)
def align_to(self, *names):
","""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.refine_names, (self,), self, *names)
        names = resolve_ellipsis(names, self.names, 'refine_names')
return super(Tensor, self).refine_names(names)
def align_to(self, *names):
"
243,"""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, ""align_to"")
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)], ellipsis_idx
        )
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
","""""""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
        ellipsis_idx = single_ellipsis_index(names, 'align_to')
if ellipsis_idx is None:
return super(Tensor, self).align_to(names)
return super(Tensor, self).align_to(
            [name for name in names if not is_ellipsis(name)],
            ellipsis_idx)
def unflatten(self, dim, sizes):
r""""""Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions
"
244,"""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.rename, (self,), self, *names, **rename_map
            )
# See Note [rename_ / rename API]
return update_names(self, names, rename_map, inplace=False)
def to_sparse_csr(self):
        """"""Convert a tensor to compressed row storage format. Only works with 2D tensors.
Examples::
","""""""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.rename, (self,), self, *names, **rename_map)
# See Note [rename_ / rename API]
return update_names(self, names, rename_map, inplace=False)
def to_sparse_csr(self):
        """""" Convert a tensor to compressed row storage format. Only works with 2D tensors.
Examples::
"
245,"returned tensor. Default: ``False``.
pin_memory (bool, optional): If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: ``False``.
""""""
)
add_docstr_all(
    ""new_tensor"",
    r""""""
new_tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a new Tensor with :attr:`data` as the tensor data.
","returned tensor. Default: ``False``.
pin_memory (bool, optional): If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: ``False``.
"""""")
add_docstr_all('new_tensor',
               r""""""
new_tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a new Tensor with :attr:`data` as the tensor data.
"
246,".. warning::
Throws an error if :attr:`self` is not a sparse COO tensor.
"""""",
)
add_docstr_all(
    ""contiguous"",
    r""""""
contiguous(memory_format=torch.contiguous_format) -> Tensor
Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If
",".. warning::
Throws an error if :attr:`self` is not a sparse COO tensor.
"""""")
add_docstr_all('contiguous',
               r""""""
contiguous(memory_format=torch.contiguous_format) -> Tensor
Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If
"
247,".. note::
This method can only be called on a coalesced sparse tensor. See
:meth:`Tensor.coalesce` for details.
"""""",
)
add_docstr_all(
    ""get_device"",
    r""""""
get_device() -> Device ordinal (Integer)
For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
",".. note::
This method can only be called on a coalesced sparse tensor. See
:meth:`Tensor.coalesce` for details.
"""""")
add_docstr_all('get_device',
               r""""""
get_device() -> Device ordinal (Integer)
For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
"
248,"size (torch.Size): the desired size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""",
)
add_docstr_all(
    ""sqrt"",
    r""""""
sqrt() -> Tensor
See :func:`torch.sqrt`
"""""",
)
add_docstr_all(
    ""sqrt_"",
    r""""""
sqrt_() -> Tensor
In-place version of :meth:`~Tensor.sqrt`
"""""",
)
add_docstr_all(
    ""square"",
    r""""""
square() -> Tensor
See :func:`torch.square`
"""""",
)
add_docstr_all(
    ""square_"",
    r""""""
square_() -> Tensor
In-place version of :meth:`~Tensor.square`
"""""",
)
add_docstr_all(
    ""squeeze"",
    r""""""
squeeze(dim=None) -> Tensor
See :func:`torch.squeeze`
"""""",
)
add_docstr_all(
    ""squeeze_"",
    r""""""
squeeze_(dim=None) -> Tensor
In-place version of :meth:`~Tensor.squeeze`
"""""",
)
add_docstr_all(
    ""std"",
    r""""""
std(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.std`
","size (torch.Size): the desired size.
sparse_dim (int): the number of sparse dimensions
dense_dim (int): the number of dense dimensions
"""""")
add_docstr_all('sqrt',
               r""""""
sqrt() -> Tensor
See :func:`torch.sqrt`
"""""")
add_docstr_all('sqrt_',
               r""""""
sqrt_() -> Tensor
In-place version of :meth:`~Tensor.sqrt`
"""""")
add_docstr_all('square',
               r""""""
square() -> Tensor
See :func:`torch.square`
"""""")
add_docstr_all('square_',
               r""""""
square_() -> Tensor
In-place version of :meth:`~Tensor.square`
"""""")
add_docstr_all('squeeze',
               r""""""
squeeze(dim=None) -> Tensor
See :func:`torch.squeeze`
"""""")
add_docstr_all('squeeze_',
               r""""""
squeeze_(dim=None) -> Tensor
In-place version of :meth:`~Tensor.squeeze`
"""""")
add_docstr_all('std',
               r""""""
std(dim, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.std`
"
249,">>> tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
[ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""byte"",
    r""""""
byte(memory_format=torch.preserve_format) -> Tensor
``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""bool"",
    r""""""
bool(memory_format=torch.preserve_format) -> Tensor
``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""char"",
    r""""""
char(memory_format=torch.preserve_format) -> Tensor
``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""bfloat16"",
    r""""""
bfloat16(memory_format=torch.preserve_format) -> Tensor
``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""double"",
    r""""""
double(memory_format=torch.preserve_format) -> Tensor
``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""float"",
    r""""""
float(memory_format=torch.preserve_format) -> Tensor
``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""cdouble"",
    r""""""
cdouble(memory_format=torch.preserve_format) -> Tensor
``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""cfloat"",
    r""""""
cfloat(memory_format=torch.preserve_format) -> Tensor
``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""half"",
    r""""""
half(memory_format=torch.preserve_format) -> Tensor
``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""int"",
    r""""""
int(memory_format=torch.preserve_format) -> Tensor
``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""int_repr"",
    r""""""
int_repr() -> Tensor
Given a quantized Tensor,
``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.
"""""",
)
add_docstr_all(
    ""long"",
    r""""""
long(memory_format=torch.preserve_format) -> Tensor
``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""short"",
    r""""""
short(memory_format=torch.preserve_format) -> Tensor
``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""take"",
    r""""""
take(indices) -> Tensor
See :func:`torch.take`
"""""",
)
add_docstr_all(
    ""take_along_dim"",
    r""""""
take_along_dim(indices, dim) -> Tensor
See :func:`torch.take_along_dim`
"""""",
)
add_docstr_all(
    ""tan"",
    r""""""
tan() -> Tensor
See :func:`torch.tan`
"""""",
)
add_docstr_all(
    ""tan_"",
    r""""""
tan_() -> Tensor
In-place version of :meth:`~Tensor.tan`
"""""",
)
add_docstr_all(
    ""tanh"",
    r""""""
tanh() -> Tensor
See :func:`torch.tanh`
"""""",
)
add_docstr_all(
    ""tanh_"",
    r""""""
tanh_() -> Tensor
In-place version of :meth:`~Tensor.tanh`
"""""",
)
add_docstr_all(
    ""tolist"",
    r""""""
tolist() -> list or number
Returns the tensor as a (nested) list. For scalars, a standard
",">>> tensor.to(other, non_blocking=True)
tensor([[-0.5044,  0.0005],
[ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
"""""".format(**common_args))

add_docstr_all('byte',
               r""""""
byte(memory_format=torch.preserve_format) -> Tensor
``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('bool',
               r""""""
bool(memory_format=torch.preserve_format) -> Tensor
``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('char',
               r""""""
char(memory_format=torch.preserve_format) -> Tensor
``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('bfloat16',
               r""""""
bfloat16(memory_format=torch.preserve_format) -> Tensor
``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('double',
               r""""""
double(memory_format=torch.preserve_format) -> Tensor
``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('float',
               r""""""
float(memory_format=torch.preserve_format) -> Tensor
``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('cdouble',
               r""""""
cdouble(memory_format=torch.preserve_format) -> Tensor
``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('cfloat',
               r""""""
cfloat(memory_format=torch.preserve_format) -> Tensor
``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('half',
               r""""""
half(memory_format=torch.preserve_format) -> Tensor
``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('int',
               r""""""
int(memory_format=torch.preserve_format) -> Tensor
``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('int_repr',
               r""""""
int_repr() -> Tensor
Given a quantized Tensor,
``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the
underlying uint8_t values of the given Tensor.
"""""")
add_docstr_all('long',
               r""""""
long(memory_format=torch.preserve_format) -> Tensor
``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('short',
               r""""""
short(memory_format=torch.preserve_format) -> Tensor
``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.
Args:
{memory_format}
"""""".format(**common_args))

add_docstr_all('take',
               r""""""
take(indices) -> Tensor
See :func:`torch.take`
"""""")
add_docstr_all('take_along_dim',
               r""""""
take_along_dim(indices, dim) -> Tensor
See :func:`torch.take_along_dim`
"""""")
add_docstr_all('tan',
               r""""""
tan() -> Tensor
See :func:`torch.tan`
"""""")
add_docstr_all('tan_',
               r""""""
tan_() -> Tensor
In-place version of :meth:`~Tensor.tan`
"""""")
add_docstr_all('tanh',
               r""""""
tanh() -> Tensor
See :func:`torch.tanh`
"""""")
add_docstr_all('tanh_',
               r""""""
tanh_() -> Tensor
In-place version of :meth:`~Tensor.tanh`
"""""")
add_docstr_all('tolist',
               r""""""
tolist() -> list or number
Returns the tensor as a (nested) list. For scalars, a standard
"
250,"[-0.08909505605697632, 0.7729271650314331]]
>>> a[0,0].tolist()
0.012766935862600803
"""""",
)
add_docstr_all(
    ""topk"",
    r""""""
topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)
See :func:`torch.topk`
"""""",
)
add_docstr_all(
    ""to_dense"",
    r""""""
to_dense() -> Tensor
Creates a strided copy of :attr:`self`.
","[-0.08909505605697632, 0.7729271650314331]]
>>> a[0,0].tolist()
0.012766935862600803
"""""")
add_docstr_all('topk',
               r""""""
topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)
See :func:`torch.topk`
"""""")
add_docstr_all('to_dense',
               r""""""
to_dense() -> Tensor
Creates a strided copy of :attr:`self`.
"
251,".. math::
\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}
""""""
    + r""""""
The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be
:ref:`broadcastable <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}
"""""" + r""""""
The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be
:ref:`broadcastable <broadcasting-semantics>`.
"
252,"If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
""""""
    + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers.
","If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
"""""" + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers.
"
253,">>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
[1.0795, 2.1939]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.as_tensor,
    r""""""
as_tensor(data, dtype=None, device=None) -> Tensor
Convert the data into a `torch.Tensor`. If the data is already a `Tensor` with the same `dtype` and `device`,
",">>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
tensor([[0.6291, 0.1586],
[1.0795, 2.1939]])
"""""".format(**common_args))

add_docstr(torch.as_tensor,
           r""""""
as_tensor(data, dtype=None, device=None) -> Tensor
Convert the data into a `torch.Tensor`. If the data is already a `Tensor` with the same `dtype` and `device`,
"
254,".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
""""""
    + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
"""""" + r""""""
{tf32_note}
.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
"
255,"tensor([[1, 2, 3],
[1, 2, 3],
[1, 2, 3]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.stack,
    r""""""
stack(tensors, dim=0, *, out=None) -> Tensor
Concatenates a sequence of tensors along a new dimension.
","tensor([[1, 2, 3],
[1, 2, 3],
[1, 2, 3]])
"""""".format(**common_args))

add_docstr(torch.stack,
           r""""""
stack(tensors, dim=0, *, out=None) -> Tensor
Concatenates a sequence of tensors along a new dimension.
"
256,"Keyword args:
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.clamp,
    r""""""
clamp(input, min=None, max=None, *, out=None) -> Tensor
Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
","Keyword args:
{memory_format}
"""""".format(**common_args))

add_docstr(torch.clamp, r""""""
clamp(input, min=None, max=None, *, out=None) -> Tensor
Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
"
257,">>> y = torch.conj(x)
>>> y.is_conj()
True
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.resolve_conj,
    r""""""
resolve_conj(input) -> Tensor
Returns a new tensor with materialized conjugation if :attr:`input`'s conjugate bit is set to `True`,
",">>> y = torch.conj(x)
>>> y.is_conj()
True
"""""".format(**common_args))

add_docstr(torch.resolve_conj,
           r""""""
resolve_conj(input) -> Tensor
Returns a new tensor with materialized conjugation if :attr:`input`'s conjugate bit is set to `True`,
"
258,".. math::
\text{out}_{i} = \cosh(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \cosh(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
259,"Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""",
)
add_docstr(
    torch.diag,
    r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
 If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
","Args:
tensors (sequence of Tensors): A list of quantized Tensors
"""""")
add_docstr(torch.diag,
           r""""""
diag(input, diagonal=0, *, out=None) -> Tensor
"
260,"tensor([-0.4264,-0.2429,-1.6300])
>>> torch.diag(a, 1)
tensor([ 0.0255, 0.1374])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.diag_embed,
    r""""""
diag_embed(input, offset=0, dim1=-2, dim2=-1) -> Tensor
Creates a tensor whose diagonals of certain 2D planes (specified by
","tensor([-0.4264,-0.2429,-1.6300])
>>> torch.diag(a, 1)
tensor([ 0.0255, 0.1374])
"""""".format(**common_args))

add_docstr(torch.diag_embed,
           r""""""
diag_embed(input, offset=0, dim1=-2, dim2=-1) -> Tensor
Creates a tensor whose diagonals of certain 2D planes (specified by
"
261,">>> torch.diff(c, dim=1)
tensor([[1, 1],
[1, 1]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.digamma,
    r""""""
digamma(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""",
)
add_docstr(
    torch.dist,
    r""""""
dist(input, other, p=2) -> Tensor
Returns the p-norm of (:attr:`input` - :attr:`other`)
",">>> torch.diff(c, dim=1)
tensor([[1, 1],
[1, 1]])
"""""".format(**common_args))

add_docstr(torch.digamma, r""""""
digamma(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""")
add_docstr(torch.dist,
           r""""""
dist(input, other, p=2) -> Tensor
Returns the p-norm of (:attr:`input` - :attr:`other`)
"
262,"tensor([[ 0.3333, 0.5000, 1.0000, 1.3333],
[ 3.3333, 5.0000, 10.0000, 13.3333]]))
"""""",
)
add_docstr(
    torch.geqrf,
    r""""""
geqrf(input, *, out=None) -> (Tensor, Tensor)
This is a low-level function for calling LAPACK's geqrf directly. This function
","tensor([[ 0.3333, 0.5000, 1.0000, 1.3333],
[ 3.3333, 5.0000, 10.0000, 13.3333]]))
"""""")
add_docstr(torch.geqrf,
           r""""""
geqrf(input, *, out=None) -> (Tensor, Tensor)
This is a low-level function for calling LAPACK's geqrf directly. This function
"
263,">>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)
tensor([1., 1., 1.])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.igammac,
    r""""""
igammac(input, other, *, out=None) -> Tensor
Computes the regularized upper incomplete gamma function:
",">>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)
tensor([1., 1., 1.])
"""""".format(**common_args))
add_docstr(torch.igammac,
           r""""""
igammac(input, other, *, out=None) -> Tensor
Computes the regularized upper incomplete gamma function:
"
264,">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.isposinf,
    r""""""
isposinf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is positive infinity or not.
",">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
tensor([False,  True,  False,  True,  False])
"""""".format(**common_args))

add_docstr(torch.isposinf,
           r""""""
isposinf(input, *, out=None) -> Tensor
Tests if each element of :attr:`input` is positive infinity or not.
"
265,"Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with no values is ambiguous
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.kron,
    r""""""
kron(input, other, *, out=None) -> Tensor
Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`.
","Traceback (most recent call last):
...
RuntimeError: bool value of Tensor with no values is ambiguous
"""""".format(**common_args))

add_docstr(torch.kron,
           r""""""
kron(input, other, *, out=None) -> Tensor
Computes the Kronecker product, denoted by :math:`\otimes`, of :attr:`input` and :attr:`other`.
"
266,"[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.lcm,
    r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
","[ 4.,  5.,  6.]])
>>> torch.kthvalue(x, 2, 0, True)
torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))
"""""".format(**single_dim_common))

add_docstr(torch.lcm,
           r""""""
lcm(input, other, *, out=None) -> Tensor
Computes the element-wise least common multiple (LCM) of :attr:`input` and :attr:`other`.
"
267,"tensor([1.2589])
>>> torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.logsumexp,
    r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Returns the log of summed exponentials of each row of the :attr:`input`
","tensor([1.2589])
>>> torch.logspace(start=2, end=2, steps=1, base=2)
tensor([4.0])
"""""".format(**factory_common_args))

add_docstr(torch.logsumexp,
           r""""""
logsumexp(input, dim, keepdim=False, *, out=None)
Returns the log of summed exponentials of each row of the :attr:`input`
"
268,"torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))
>>> a.nanmedian(0)
torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.quantile,
    r""""""
quantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
Computes the q-th quantiles of each row of the :attr:`input` tensor
","torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))
>>> a.nanmedian(0)
torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))
"""""".format(**single_dim_common))

add_docstr(torch.quantile, r""""""
quantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
Computes the q-th quantiles of each row of the :attr:`input` tensor
"
269,"not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
>>> torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.mv,
    r""""""
mv(input, vec, *, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`input` and the vector
","not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320
>>> torch.multinomial(weights, 4, replacement=True)
tensor([ 2,  1,  1,  1])
"""""".format(**common_args))

add_docstr(torch.mv,
           r""""""
mv(input, vec, *, out=None) -> Tensor
Performs a matrix-vector product of the matrix :attr:`input` and the vector
"
270,"[[1, 5],
[3, 7]]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.swapaxes,
    r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
","[[1, 5],
[3, 7]]])
"""""".format(**common_args))

add_docstr(torch.swapaxes, r""""""
swapaxes(input, axis0, axis1) -> Tensor
Alias for :func:`torch.transpose`.
"
271,"{requires_grad}
{memory_format}
"""""".format(
        **factory_like_common_args
    ),
)
add_docstr(
    torch.randint,
    """"""
randint(low=0, high, size, \\*, generator=None, out=None, \
dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
","{requires_grad}
{memory_format}
"""""".format(**factory_like_common_args))
add_docstr(torch.randint,
           """"""
randint(low=0, high, size, \\*, generator=None, out=None, \
dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
"
272,">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])
>>> t.sgn()
tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.sin,
    r""""""
sin(input, *, out=None) -> Tensor
Returns a new tensor with the sine of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sin(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])
>>> t.sgn()
tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])
"""""".format(**common_args))

add_docstr(torch.sin,
           r""""""
sin(input, *, out=None) -> Tensor
Returns a new tensor with the sine of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sin(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
273,".. math::
\text{out}_{i} = \sinh(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \sinh(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
274,"torch.return_types.sort(
values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.argsort,
    r""""""
argsort(input, dim=-1, descending=False) -> LongTensor
Returns the indices that sort a tensor along a given dimension in ascending
","torch.return_types.sort(
values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))
"""""".format(**common_args))

add_docstr(torch.argsort,
           r""""""
argsort(input, dim=-1, descending=False) -> LongTensor
Returns the indices that sort a tensor along a given dimension in ascending
"
275,">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std_mean(a, unbiased=False)
(tensor(0.4188), tensor(-0.8509))
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.sub,
    r""""""
sub(input, other, *, alpha=1, out=None) -> Tensor
Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.
.. math::
\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i
""""""
    + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
",">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])
>>> torch.std_mean(a, unbiased=False)
(tensor(0.4188), tensor(-0.8509))
"""""".format(**multi_dim_common))

add_docstr(torch.sub, r""""""
sub(input, other, *, alpha=1, out=None) -> Tensor
Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.
.. math::
\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i
"""""" + r""""""
Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
:ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
"
276,">>> b = torch.tensor((0, 1))
>>> torch.sub(a, b, alpha=2)
tensor([1, 0])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.subtract,
    r""""""
subtract(input, other, *, alpha=1, out=None) -> Tensor
Alias for :func:`torch.sub`.
"""""",
)
add_docstr(
    torch.sum,
    r""""""
sum(input, *, dtype=None) -> Tensor
Returns the sum of all elements in the :attr:`input` tensor.
",">>> b = torch.tensor((0, 1))
>>> torch.sub(a, b, alpha=2)
tensor([1, 0])
"""""".format(**common_args))

add_docstr(torch.subtract, r""""""
subtract(input, other, *, alpha=1, out=None) -> Tensor
Alias for :func:`torch.sub`.
"""""")
add_docstr(torch.sum,
           r""""""
sum(input, *, dtype=None) -> Tensor
Returns the sum of all elements in the :attr:`input` tensor.
"
277,"tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.flip,
    r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
","tensor([[ 0.4875,  0.3938],
[ 0.9158, -0.6929],
[-0.5872,  0.6932]])
"""""".format(**common_args))

add_docstr(torch.flip,
           r""""""
flip(input, dims) -> Tensor
Reverse the order of a n-D tensor along given axis in dims.
"
278,".. math::
\text{out}_{i} = \tanh(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
",".. math::
\text{out}_{i} = \tanh(\text{input}_{i})
"""""" + r""""""
Args:
{input}
"
279,"tensor([ 1.,  2.,  3.,  4.,  5.])
>>> torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.trace,
    r""""""
trace(input) -> Tensor
Returns the sum of the elements of the diagonal of the input 2-D matrix.
","tensor([ 1.,  2.,  3.,  4.,  5.])
>>> torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
"""""".format(**common_args))

add_docstr(torch.trace,
           r""""""
trace(input) -> Tensor
Returns the sum of the elements of the diagonal of the input 2-D matrix.
"
280,"[ 7.,  8.,  9.]])
>>> torch.trace(x)
tensor(15.)
"""""",
)
add_docstr(
    torch.transpose,
    r""""""
transpose(input, dim0, dim1) -> Tensor
Returns a tensor that is a transposed version of :attr:`input`.
","[ 7.,  8.,  9.]])
>>> torch.trace(x)
tensor(15.)
"""""")
add_docstr(torch.transpose,
           r""""""
transpose(input, dim0, dim1) -> Tensor
Returns a tensor that is a transposed version of :attr:`input`.
"
281,"[ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
[ 0.0000,  0.7986]]))
"""""",
)
add_docstr(
    torch.tril,
    r""""""
tril(input, diagonal=0, *, out=None) -> Tensor
Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices
","[ 1.9320,  0.9270, -1.2826]]),
cloned_coefficient=tensor([[ 1.1527, -1.0753],
[ 0.0000,  0.7986]]))
"""""")
add_docstr(torch.tril,
           r""""""
tril(input, diagonal=0, *, out=None) -> Tensor
Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices
"
282,"the main diagonal. The main diagonal are the set of indices
:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where
:math:`d_{1}, d_{2}` are the dimensions of the matrix.
""""""
    + r""""""
Args:
{input}
diagonal (int, optional): the diagonal to consider
","the main diagonal. The main diagonal are the set of indices
:math:`\lbrace (i, i) \rbrace` for :math:`i \in [0, \min\{d_{1}, d_{2}\} - 1]` where
:math:`d_{1}, d_{2}` are the dimensions of the matrix.
"""""" + r""""""
Args:
{input}
diagonal (int, optional): the diagonal to consider
"
283,">>> torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
"""""".format(
        **factory_common_args
    ),
)

add_docstr(
    torch.zeros_like,
    r""""""
zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `0`, with the same size as
",">>> torch.zeros(5)
tensor([ 0.,  0.,  0.,  0.,  0.])
"""""".format(**factory_common_args))

add_docstr(torch.zeros_like,
           r""""""
zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor
Returns a tensor filled with the scalar value `0`, with the same size as
"
284,"Returns:
Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window
"""""".format(
        **factory_common_args
    ),
)
add_docstr(
    torch.hamming_window,
    """"""
hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, \
layout=torch.strided, device=None, requires_grad=False) -> Tensor
""""""
    + r""""""
Hamming window function.
.. math::
","Returns:
Tensor: A 1-D tensor of size :math:`(\text{{window\_length}},)` containing the window
"""""".format(**factory_common_args))
add_docstr(torch.hamming_window,
           """"""
hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, \
layout=torch.strided, device=None, requires_grad=False) -> Tensor
"""""" + r""""""
Hamming window function.
.. math::
"
285,">>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
tensor([[  0,  10],
[100, 200]], dtype=torch.uint8)
"""""",
)
add_docstr(
    torch.Generator,
    r""""""
Generator(device='cpu') -> Generator
Creates and returns a generator object that manages the state of the algorithm which
",">>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()
tensor([[  0,  10],
[100, 200]], dtype=torch.uint8)
"""""")
add_docstr(torch.Generator,
           r""""""
Generator(device='cpu') -> Generator
Creates and returns a generator object that manages the state of the algorithm which
"
286,">>> g_cpu = torch.Generator()
>>> g_cpu.seed()
1516516984916
"""""",
)
add_docstr(
    torch.Generator.device,
    r""""""
Generator.device -> device
Gets the current device of the generator.
",">>> g_cpu = torch.Generator()
>>> g_cpu.seed()
1516516984916
"""""")
add_docstr(torch.Generator.device,
           r""""""
Generator.device -> device
Gets the current device of the generator.
"
287,">>> g_cpu = torch.Generator()
>>> g_cpu.device
device(type='cpu')
"""""",
)
add_docstr(
    torch._assert_async,
    r""""""
_assert_async(tensor) -> void
Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
",">>> g_cpu = torch.Generator()
>>> g_cpu.device
device(type='cpu')
"""""")
add_docstr(torch._assert_async,
           r""""""
_assert_async(tensor) -> void
Asynchronously assert that the contents of tensor are nonzero.  For CPU tensors,
"
288,"return torch.cuda.current_device()
return -1

def _get_device_index(
    device: Any, optional: bool = False, allow_cpu: bool = False
) -> int:
r""""""Gets the device index from :attr:`device`, which can be a torch.device
object, a Python integer, or ``None``.
","return torch.cuda.current_device()
return -1
def _get_device_index(device: Any, optional: bool = False, allow_cpu: bool = False) -> int:
r""""""Gets the device index from :attr:`device`, which can be a torch.device
object, a Python integer, or ``None``.
"
289,"else:
device_idx = _get_current_device_index()
else:
            raise ValueError(
                ""Expected a torch.device with a specified index ""
                ""or an integer, but got:{}"".format(device)
            )
return device_idx
","else:
device_idx = _get_current_device_index()
else:
            raise ValueError('Expected a torch.device with a specified index '
                             'or an integer, but got:{}'.format(device))
return device_idx
"
290,"# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
    from tensorflow.core.framework import graph_pb2
from tensorflow.core.util import event_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(
        ""TensorBoard visualization of GraphExecutors requires having ""
        ""TensorFlow installed""
    ) from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(
            wall_time=time.time(), graph_def=pb_graph.SerializeToString()
        )
w.add_event(evt)
def visualize(graph, name_prefix="""", pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(
            graph, name_prefix, pb_graph, partial(visualize, pb_graph=pb_graph)
        )
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op=""input"", name=name_prefix + ""input"")
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + ""input:"" + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op=""output"", name=name_prefix + ""output"")
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
","# anything without having TF installed, and so this file has a hard dependency on it
# as well. It really is a debugging tool, so it doesn't matter.
try:
from tensorflow.core.util import event_pb2
    from tensorflow.core.framework import graph_pb2
from tensorflow.python.summary.writer.writer import FileWriter
except ImportError:
    raise ImportError(""TensorBoard visualization of GraphExecutors requires having ""
                      ""TensorFlow installed"") from None
def dump_tensorboard_summary(graph_executor, logdir):
with FileWriter(logdir) as w:
pb_graph = visualize(graph_executor)
        evt = event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
w.add_event(evt)
def visualize(graph, name_prefix='', pb_graph=None, executors_it=None):
""""""Visualizes an independent graph, or a graph executor.""""""
value_map = {}
pb_graph = pb_graph or graph_pb2.GraphDef()
if isinstance(graph, torch._C.GraphExecutorState):
        visualize_graph_executor(graph, name_prefix, pb_graph,
                                 partial(visualize, pb_graph=pb_graph))
return pb_graph
# Set up an input node
    input_node = pb_graph.node.add(op='input', name=name_prefix + 'input')
for i, value in enumerate(graph.param_node().outputs()):
        value_map[value.unique()] = name_prefix + 'input:' + str(i)
visualize_rec(graph, value_map, name_prefix, pb_graph, executors_it)
# Gather all outputs
    return_node = pb_graph.node.add(op='output', name=name_prefix + 'output')
for value in graph.return_node().inputs():
return_node.input.append(value_map[value.unique()])
"
291,"Notice that the symmetric element ``T[-1] == T[1].conj()`` is omitted.
At the Nyquist frequency ``T[-2] == T[2]`` is it's own symmetric pair,
and therefore must always be real-valued.
"""""".format(
        **common_args
    ),
)

irfft = _add_docstr(
    _fft.fft_irfft,
    r""""""
irfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the inverse of :func:`~torch.fft.rfft`.
","Notice that the symmetric element ``T[-1] == T[1].conj()`` is omitted.
At the Nyquist frequency ``T[-2] == T[2]`` is it's own symmetric pair,
and therefore must always be real-valued.
"""""".format(**common_args))

irfft = _add_docstr(_fft.fft_irfft, r""""""
irfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the inverse of :func:`~torch.fft.rfft`.
"
292,"""""""
if has_torch_function_unary(input):
return handle_torch_function(
            istft,
            (input,),
            input,
            n_fft,
            hop_length=hop_length,
            win_length=win_length,
            window=window,
            center=center,
            normalized=normalized,
            onesided=onesided,
            length=length,
            return_complex=return_complex,
        )
    return _VF.istft(
        input,
        n_fft,
        hop_length,
        win_length,
        window,
        center,  # type: ignore[attr-defined]
        normalized,
        onesided,
        length,
        return_complex,
    )
del torch.unique_dim
","""""""
if has_torch_function_unary(input):
return handle_torch_function(
            istft, (input,), input, n_fft, hop_length=hop_length, win_length=win_length,
            window=window, center=center, normalized=normalized, onesided=onesided,
            length=length, return_complex=return_complex)
    return _VF.istft(input, n_fft, hop_length, win_length, window, center,  # type: ignore[attr-defined]
                     normalized, onesided, length, return_complex)
del torch.unique_dim
"
293,"return output, inverse_indices, counts
def _unique_consecutive_impl(
    input: Tensor,
    return_inverse: bool = False,
    return_counts: bool = False,
    dim: Optional[int] = None,
) -> _unique_impl_out:
r""""""Eliminates all but the first element from every consecutive group of equivalent elements.
.. note:: This function is different from :func:`torch.unique` in the sense that this function
","return output, inverse_indices, counts
def _unique_consecutive_impl(input: Tensor, return_inverse: bool = False,
                             return_counts: bool = False,
                             dim: Optional[int] = None) -> _unique_impl_out:
r""""""Eliminates all but the first element from every consecutive group of equivalent elements.
.. note:: This function is different from :func:`torch.unique` in the sense that this function
"
294,"return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2.0, compute_mode=""use_mm_for_euclid_dist_if_necessary""):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
","return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
"
295,"if has_torch_function_unary(input):
return handle_torch_function(
            norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype
        )
ndim = input.dim()
","if has_torch_function_unary(input):
return handle_torch_function(
            norm, (input,), input, p=p, dim=dim, keepdim=keepdim, out=out, dtype=dtype)
ndim = input.dim()
"
296,"else:
return result  # A_LU, pivots, infos

def _lu_no_infos(A, pivot=True, get_infos=False, out=None):
# type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]
# need to check for torch_function here so that we exit if
if has_torch_function_unary(A):
return handle_torch_function(
            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out
        )
result = _lu_impl(A, pivot, get_infos, out)
if out is not None:
_check_list_size(len(out), get_infos, out)
","else:
return result  # A_LU, pivots, infos
def _lu_no_infos(A, pivot=True, get_infos=False, out=None):
# type: (Tensor, bool, bool, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]
# need to check for torch_function here so that we exit if
if has_torch_function_unary(A):
return handle_torch_function(
            lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out)
result = _lu_impl(A, pivot, get_infos, out)
if out is not None:
_check_list_size(len(out), get_infos, out)
"
297,"if self.total is None:
sys.stderr.write(""\r{0:.1f} bytes"".format(self.n))
else:
                    sys.stderr.write(
                        ""\r{0:.1f}%"".format(100 * self.n / float(self.total))
                    )
sys.stderr.flush()
def close(self):
","if self.total is None:
sys.stderr.write(""\r{0:.1f} bytes"".format(self.n))
else:
                    sys.stderr.write(""\r{0:.1f}%"".format(100 * self.n / float(self.total)))
sys.stderr.flush()
def close(self):
"
298,"variable is not set.
""""""
# Issue warning to move data if old env is set
    if os.getenv(""TORCH_HUB""):
        warnings.warn(""TORCH_HUB is deprecated, please use env TORCH_HOME instead"")
if _hub_dir is not None:
return _hub_dir
    return os.path.join(_get_torch_home(), ""hub"")
def set_dir(d):
","variable is not set.
""""""
# Issue warning to move data if old env is set
    if os.getenv('TORCH_HUB'):
        warnings.warn('TORCH_HUB is deprecated, please use env TORCH_HOME instead')
if _hub_dir is not None:
return _hub_dir
    return os.path.join(_get_torch_home(), 'hub')
def set_dir(d):
"
299,"""""""
# Issue warning to move data if old env is set
    if os.getenv(""TORCH_MODEL_ZOO""):
        warnings.warn(
            ""TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead""
        )
if model_dir is None:
hub_dir = get_dir()
        model_dir = os.path.join(hub_dir, ""checkpoints"")
try:
os.makedirs(model_dir)
","""""""
# Issue warning to move data if old env is set
    if os.getenv('TORCH_MODEL_ZOO'):
        warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')
if model_dir is None:
hub_dir = get_dir()
        model_dir = os.path.join(hub_dir, 'checkpoints')
try:
os.makedirs(model_dir)
"
300,"Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
""""""
    + fr""""""
.. note:: This function is computed using :func:`torch.lu`.
{common_notes[""sync_note""]}
""""""
    + r""""""
.. note:: The determinant can be recovered as `sign * exp(logabsdet)`.
","Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
"""""" + fr""""""
.. note:: This function is computed using :func:`torch.lu`.
{common_notes[""sync_note""]}
"""""" + r""""""
.. note:: The determinant can be recovered as `sign * exp(logabsdet)`.
"
301,">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""",
)
eigvalsh = _add_docstr(
    _linalg.linalg_eigvalsh,
    r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
",">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""")
eigvalsh = _add_docstr(_linalg.linalg_eigvalsh, r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
"
302,".. _the resulting vectors will span the same subspace:
https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD
"""""",
)
svdvals = _add_docstr(
    _linalg.linalg_svdvals,
    r""""""
linalg.svdvals(A, *, out=None) -> Tensor
Computes the singular values of a matrix.
",".. _the resulting vectors will span the same subspace:
https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD
"""""")
svdvals = _add_docstr(_linalg.linalg_svdvals, r""""""
linalg.svdvals(A, *, out=None) -> Tensor
Computes the singular values of a matrix.
"
303,">>> Ainv = torch.linalg.inverse(A)
>>> torch.allclose(Atensorinv, Ainv)
True
"""""",
)
tensorsolve = _add_docstr(
    _linalg.linalg_tensorsolve,
    r""""""
linalg.tensorsolve(A, B, dims=None, *, out=None) -> Tensor
Computes the solution `X` to the system `torch.tensordot(A, X) = B`.
",">>> Ainv = torch.linalg.inverse(A)
>>> torch.allclose(Atensorinv, Ainv)
True
"""""")
tensorsolve = _add_docstr(_linalg.linalg_tensorsolve, r""""""
linalg.tensorsolve(A, B, dims=None, *, out=None) -> Tensor
Computes the solution `X` to the system `torch.tensordot(A, X) = B`.
"
304,"torch.max_pool1d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
torch.max_pool2d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
torch.max_pool3d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
        torch.max_pool1d_with_indices: (
            lambda input, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False: -1
        ),
torch.mean: lambda input, dim=None: -1,
torch.median: lambda input, dim=None: -1,
torch.nanmedian: lambda input, dim=None: -1,
","torch.max_pool1d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
torch.max_pool2d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
torch.max_pool3d: lambda input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False: -1,
        torch.max_pool1d_with_indices: (lambda input, kernel_size, stride=None, padding=0, dilation=1,
                                        return_indices=False, ceil_mode=False: -1),
torch.mean: lambda input, dim=None: -1,
torch.median: lambda input, dim=None: -1,
torch.nanmedian: lambda input, dim=None: -1,
"
305,">>> def func(a): # This will make func dispatchable by __torch_function__
...     return a + 0
""""""

def inner(func):
@functools.wraps(func)
def wrapped(*args, **kwargs):
",">>> def func(a): # This will make func dispatchable by __torch_function__
...     return a + 0
""""""
def inner(func):
@functools.wraps(func)
def wrapped(*args, **kwargs):
"
306,"return result
    def draw_base2(
        self,
        m: int,
        out: Optional[torch.Tensor] = None,
        dtype: torch.dtype = torch.float32,
    ) -> torch.Tensor:
r""""""
Function to draw a sequence of :attr:`2**m` points from a Sobol sequence.
Note that the samples are dependent on the previous samples. The size
","return result
    def draw_base2(self, m: int, out: Optional[torch.Tensor] = None,
                   dtype: torch.dtype = torch.float32) -> torch.Tensor:
r""""""
Function to draw a sequence of :attr:`2**m` points from a Sobol sequence.
Note that the samples are dependent on the previous samples. The size
"
307,"import contextlib
import warnings
import torch
from torch._C import default_generator
def set_rng_state(new_state: torch.Tensor) -> None:
","import contextlib
import warnings
from torch._C import default_generator
import torch
def set_rng_state(new_state: torch.Tensor) -> None:
"
308,"Returns:
requirement_is_met: bool
    """"""
try:
        version_strs = module.__version__.split(""."")
# Cast module version fields to match the types of the required version
module_version = tuple(
            type(req_field)(version_strs[idx])
            for idx, req_field in enumerate(req_version_tuple)
)
requirement_is_met = module_version >= req_version_tuple
","Returns:
requirement_is_met: bool
    '''
try:
        version_strs = module.__version__.split('.')
# Cast module version fields to match the types of the required version
module_version = tuple(
            type(req_field)(version_strs[idx]) for idx, req_field in enumerate(req_version_tuple)
)
requirement_is_met = module_version >= req_version_tuple
"
309,".. math::
\mathrm{erfcx}(x) = e^{x^2} \mathrm{erfc}(x)
""""""
    + r""""""
""""""
    + r""""""
Args:
{input}
",".. math::
\mathrm{erfcx}(x) = e^{x^2} \mathrm{erfc}(x)
"""""" + r""""""
"""""" + r""""""
Args:
{input}
"
310,".. note:: This function provides greater precision than exp(x) - 1 for small values of x.
""""""
    + r""""""
Args:
{input}
",".. note:: This function provides greater precision than exp(x) - 1 for small values of x.
"""""" + r""""""
Args:
{input}
"
311,".. math::
\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}
""""""
    + r""""""
Args:
input (Tensor): the input tensor
",".. math::
\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}
"""""" + r""""""
Args:
input (Tensor): the input tensor
"
312,">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(
        **common_args
    ),
)
i0e = _add_docstr(
    _special.special_i0e,
    r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
",">>> torch.i0(torch.arange(5, dtype=torch.float32))
tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])
"""""".format(**common_args))
i0e = _add_docstr(_special.special_i0e,
                  r""""""
i0e(input, *, out=None) -> Tensor
Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
"
313,"# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
# sys.path.insert(0, os.path.abspath('.'))
import textwrap
","# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os

# sys.path.insert(0, os.path.abspath('.'))
import textwrap
"
314,"# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = '1.6'
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.intersphinx',
    'breathe',
    'exhale'
]
intersphinx_mapping = {
    'pytorch': ('https://pytorch.org/docs/master', None)
}
# Setup absolute paths for communicating with breathe / exhale where
# items are expected / should be trimmed by.
","# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = ""1.6""
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [""sphinx.ext.intersphinx"", ""breathe"", ""exhale""]
intersphinx_mapping = {""pytorch"": (""https://pytorch.org/docs/master"", None)}
# Setup absolute paths for communicating with breathe / exhale where
# items are expected / should be trimmed by.
"
315,"exclude_patterns = []
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'
# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True
","exclude_patterns = []
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = ""sphinx""
# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True
"
316,"# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
# import sys
# source code directory, relative to this file, for sphinx-autobuild
","# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os

# import sys
# source code directory, relative to this file, for sphinx-autobuild
"
317,"# documentation.
html_theme_options = {
    'pytorch_project': 'docs',
    'canonical_url': 'https://pytorch.org/docs/stable/',
    'collapse_navigation': False,
    'display_version': True,
    'logo_only': True,
    'analytics_id': 'UA-117752657-2',
}
html_logo = '_static/img/pytorch-logo-dark-unstable.png'
if RELEASE:
    html_logo = '_static/img/pytorch-logo-dark.svg'
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['_static']
html_css_files = [
    'css/jit.css',
]
","# documentation.
html_theme_options = {
    ""pytorch_project"": ""docs"",
    ""canonical_url"": ""https://pytorch.org/docs/stable/"",
    ""collapse_navigation"": False,
    ""display_version"": True,
    ""logo_only"": True,
    ""analytics_id"": ""UA-117752657-2"",
}
html_logo = ""_static/img/pytorch-logo-dark-unstable.png""
if RELEASE:
    html_logo = ""_static/img/pytorch-logo-dark.svg""
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = [""_static""]
html_css_files = [
    ""css/jit.css"",
]
"
318,"old_flags = sys.getdlopenflags()
sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_LAZY)
from torch._C import *  # noqa: F403
sys.setdlopenflags(old_flags)
del old_flags
del _dl_flags
","old_flags = sys.getdlopenflags()
sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_LAZY)
from torch._C import *  # noqa: F403

sys.setdlopenflags(old_flags)
del old_flags
del _dl_flags
"
319,"""""""
_C._set_default_dtype(d)
def use_deterministic_algorithms(mode):
    r"""""" Sets whether PyTorch operations must use ""deterministic""
algorithms. That is, algorithms which, given the same input, and when
run on the same software and hardware, always produce the same output.
When enabled, operations will use deterministic algorithms when available,
","""""""
_C._set_default_dtype(d)

def use_deterministic_algorithms(mode):
    r""""""Sets whether PyTorch operations must use ""deterministic""
algorithms. That is, algorithms which, given the same input, and when
run on the same software and hardware, always produce the same output.
When enabled, operations will use deterministic algorithms when available,
"
320,"""""""
_C._set_warnAlways(b)
def is_warn_always_enabled():
r""""""Returns True if the global warn_always flag is turned on. Refer to
:func:`torch.set_warn_always` documentation for more details.
""""""
return _C._get_warnAlways()
################################################################################
# Define numeric constants
################################################################################
# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and
# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)
from math import e , nan , inf , pi
__all__.extend(['e', 'pi', 'nan', 'inf'])
################################################################################
# Define Storage and Tensor classes
","""""""
_C._set_warnAlways(b)

def is_warn_always_enabled():
r""""""Returns True if the global warn_always flag is turned on. Refer to
:func:`torch.set_warn_always` documentation for more details.
""""""
return _C._get_warnAlways()

################################################################################
# Define numeric constants
################################################################################
# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and
# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)
from math import e, nan, inf, pi

__all__.extend([""e"", ""pi"", ""nan"", ""inf""])
################################################################################
# Define Storage and Tensor classes
"
321,"class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
pass
class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
pass
class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
pass
class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
pass
class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
pass
class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
pass
class QUInt4x2Storage(_C.QUInt4x2StorageBase, _StorageBase):
pass
_storage_classes = {
    DoubleStorage, FloatStorage, LongStorage, IntStorage, ShortStorage,
    CharStorage, ByteStorage, HalfStorage, BoolStorage, QUInt8Storage, QInt8Storage,
    QInt32Storage, BFloat16Storage, ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage
}
# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()
_tensor_classes: Set[Type] = set()
# If you edit these imports, please update torch/__init__.py.in as well
from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed
from .serialization import save, load
from ._tensor_str import set_printoptions
################################################################################
# Initialize extension
################################################################################
def manager_path():
    if platform.system() == 'Windows' or sys.executable == 'torch_deploy':
return b""""
    path = get_file_path('torch', 'bin', 'torch_shm_manager')
    prepare_multiprocessing_environment(get_file_path('torch'))
if not os.path.exists(path):
raise RuntimeError(""Unable to find torch_shm_manager at "" + path)
    return path.encode('utf-8')
# Shared memory manager needs to know the exact location of manager executable
","class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
pass

class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
pass

class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
pass

class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
pass

class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
pass

class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
pass

class QUInt4x2Storage(_C.QUInt4x2StorageBase, _StorageBase):
pass

_storage_classes = {
    DoubleStorage,
    FloatStorage,
    LongStorage,
    IntStorage,
    ShortStorage,
    CharStorage,
    ByteStorage,
    HalfStorage,
    BoolStorage,
    QUInt8Storage,
    QInt8Storage,
    QInt32Storage,
    BFloat16Storage,
    ComplexFloatStorage,
    ComplexDoubleStorage,
    QUInt4x2Storage,
}
# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()
_tensor_classes: Set[Type] = set()
from ._tensor_str import set_printoptions

# If you edit these imports, please update torch/__init__.py.in as well
from .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed
from .serialization import save, load
################################################################################
# Initialize extension
################################################################################

def manager_path():
    if platform.system() == ""Windows"" or sys.executable == ""torch_deploy"":
return b""""
    path = get_file_path(""torch"", ""bin"", ""torch_shm_manager"")
    prepare_multiprocessing_environment(get_file_path(""torch""))
if not os.path.exists(path):
raise RuntimeError(""Unable to find torch_shm_manager at "" + path)
    return path.encode(""utf-8"")
# Shared memory manager needs to know the exact location of manager executable
"
322,"# the public API. The ""regular"" import lines are there solely for the runtime
# side effect of adding to the imported module's members for other users.
from torch import cuda as cuda
from torch import cpu as cpu
from torch import autograd as autograd
from torch.autograd import (
    no_grad as no_grad,
    enable_grad as enable_grad,
    set_grad_enabled as set_grad_enabled,
    inference_mode as inference_mode,
)
from torch import fft as fft
from torch import futures as futures
from torch import nn as nn
import torch.nn.intrinsic
import torch.nn.quantizable
import torch.nn.quantized
# AO depends on nn, as well as quantized stuff -- so should be after those.
from torch import ao as ao
from torch import optim as optim
import torch.optim._multi_tensor
from torch import multiprocessing as multiprocessing
from torch import sparse as sparse
from torch import special as special
import torch.utils.backcompat
from torch import onnx as onnx
from torch import jit as jit
from torch import linalg as linalg
from torch import hub as hub
from torch import random as random
from torch import distributions as distributions
from torch import testing as testing
import torch.backends.cuda
import torch.backends.mkl
import torch.backends.mkldnn
import torch.backends.openmp
import torch.backends.quantized
from torch import quantization as quantization
import torch.utils.data
from torch import __config__ as __config__
from torch import __future__ as __future__
from torch import profiler as profiler
_C._init_names(list(torch._storage_classes))
# attach docstrings to torch and tensor functions
from . import _torch_docs, _tensor_docs, _storage_docs
del _torch_docs, _tensor_docs, _storage_docs
","# the public API. The ""regular"" import lines are there solely for the runtime
# side effect of adding to the imported module's members for other users.
from torch import cuda as cuda
from torch import distributions as distributions
from torch import fft as fft
from torch import futures as futures
from torch import hub as hub
from torch import jit as jit
from torch import linalg as linalg
from torch import multiprocessing as multiprocessing
from torch import nn as nn
from torch import onnx as onnx
from torch import optim as optim
from torch import profiler as profiler
from torch import quantization as quantization
from torch import random as random
from torch import sparse as sparse
from torch import special as special
from torch import testing as testing
from torch.autograd import (
    no_grad as no_grad,
    enable_grad as enable_grad,
    set_grad_enabled as set_grad_enabled,
    inference_mode as inference_mode,
)
_C._init_names(list(torch._storage_classes))
# attach docstrings to torch and tensor functions
from . import _torch_docs, _tensor_docs, _storage_docs

del _torch_docs, _tensor_docs, _storage_docs
"
323,"return _C._GLIBCXX_USE_CXX11_ABI
# Import the ops ""namespace""
from torch._ops import ops
from torch._classes import classes

# Import the quasi random sampler
from torch import quasirandom as quasirandom
# If you are seeing this, it means that this call site was not checked if
# the memory format could be preserved, and it was switched to old default
","return _C._GLIBCXX_USE_CXX11_ABI
# Import the quasi random sampler
from torch import quasirandom as quasirandom
from torch._classes import classes

# Import the ops ""namespace""
from torch._ops import ops
# If you are seeing this, it means that this call site was not checked if
# the memory format could be preserved, and it was switched to old default
"
324,"""""""
if system == ""win32"":
path = user_data_dir(appname, appauthor, None, roaming)
    elif system == 'darwin':
        path = os.path.expanduser('~/Library/Preferences/')
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv('XDG_CONFIG_HOME', os.path.expanduser(""~/.config""))
if appname:
path = os.path.join(path, appname)
if appname and version:
","""""""
if system == ""win32"":
path = user_data_dir(appname, appauthor, None, roaming)
    elif system == ""darwin"":
        path = os.path.expanduser(""~/Library/Preferences/"")
if appname:
path = os.path.join(path, appname)
else:
        path = os.getenv(""XDG_CONFIG_HOME"", os.path.expanduser(""~/.config""))
if appname:
path = os.path.join(path, appname)
if appname and version:
"
325,"import io
import torch
from torch.package._package_pickler import create_pickler
from torch.package._package_unpickler import PackageUnpickler
from torch.package import sys_importer, OrderedImporter, PackageImporter, Importer
from torch.serialization import _maybe_decode_ascii
def _save_storages(importer, obj):
serialized_storages = []
serialized_dtypes = []
","import io

import torch
from torch.package import sys_importer, OrderedImporter, PackageImporter, Importer
from torch.package._package_pickler import create_pickler
from torch.package._package_unpickler import PackageUnpickler
from torch.serialization import _maybe_decode_ascii

def _save_storages(importer, obj):
serialized_storages = []
serialized_dtypes = []
"
326,"# when modules of the same name are in the same file
# qualified_name => class name => list[overload_functions]
_overloaded_methods : Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484
# (qualified_name, class name) => class_fileno
_overloaded_method_class_fileno = {}
def _overload_method(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
","# when modules of the same name are in the same file
# qualified_name => class name => list[overload_functions]
_overloaded_methods: Dict[str, Dict[str, List[Callable]]] = {}  # noqa: T484
# (qualified_name, class name) => class_fileno
_overloaded_method_class_fileno = {}

def _overload_method(func):
_check_overload_body(func)
qual_name = _qualified_name(func)
"
327,"(*A.shape[:-1], A.size(-1) - D.size(-1)),
dtype=A.dtype,
device=A.device,
            generator=gen
)
)
U_ortho_t = U_ortho.transpose(-2, -1).contiguous()
","(*A.shape[:-1], A.size(-1) - D.size(-1)),
dtype=A.dtype,
device=A.device,
            generator=gen,
)
)
U_ortho_t = U_ortho.transpose(-2, -1).contiguous()
"
328,")
# compute the gradient part in span(U)
    res = _symeig_backward_complete_eigenspace(
        D_grad, U_grad, A, D, U
    )
# incorporate the Sylvester equation solution into the full gradient
# it resides in span(U_ortho)
res -= U_ortho.matmul(
        chr_poly_D_at_A_to_U_ortho_sign * torch.cholesky_solve(
            U_ortho_t.matmul(series_acc),
            chr_poly_D_at_A_to_U_ortho_L
)
).matmul(Ut)
return res
def _symeig_backward(D_grad, U_grad, A, D, U, largest):
# if `U` is square, then the columns of `U` is a complete eigenspace
if U.size(-1) == U.size(-2):
        return _symeig_backward_complete_eigenspace(
            D_grad, U_grad, A, D, U
        )
else:
        return _symeig_backward_partial_eigenspace(
            D_grad, U_grad, A, D, U, largest
        )
class LOBPCGAutogradFunction(torch.autograd.Function):
@staticmethod
    def forward(ctx,  # type: ignore[override]
                A: Tensor,
                k: Optional[int] = None,
                B: Optional[Tensor] = None,
                X: Optional[Tensor] = None,
                n: Optional[int] = None,
                iK: Optional[Tensor] = None,
                niter: Optional[int] = None,
                tol: Optional[float] = None,
                largest: Optional[bool] = None,
                method: Optional[str] = None,
                tracker: None = None,
                ortho_iparams: Optional[Dict[str, int]] = None,
                ortho_fparams: Optional[Dict[str, float]] = None,
                ortho_bparams: Optional[Dict[str, bool]] = None
                ) -> Tuple[Tensor, Tensor]:
# makes sure that input is contiguous for efficiency.
# Note: autograd does not support dense gradients for sparse input yet.
",")
# compute the gradient part in span(U)
    res = _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)
# incorporate the Sylvester equation solution into the full gradient
# it resides in span(U_ortho)
res -= U_ortho.matmul(
        chr_poly_D_at_A_to_U_ortho_sign
        * torch.cholesky_solve(
            U_ortho_t.matmul(series_acc), chr_poly_D_at_A_to_U_ortho_L
)
).matmul(Ut)
return res

def _symeig_backward(D_grad, U_grad, A, D, U, largest):
# if `U` is square, then the columns of `U` is a complete eigenspace
if U.size(-1) == U.size(-2):
        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)
else:
        return _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest)
class LOBPCGAutogradFunction(torch.autograd.Function):
@staticmethod
    def forward(
        ctx,  # type: ignore[override]
        A: Tensor,
        k: Optional[int] = None,
        B: Optional[Tensor] = None,
        X: Optional[Tensor] = None,
        n: Optional[int] = None,
        iK: Optional[Tensor] = None,
        niter: Optional[int] = None,
        tol: Optional[float] = None,
        largest: Optional[bool] = None,
        method: Optional[str] = None,
        tracker: None = None,
        ortho_iparams: Optional[Dict[str, int]] = None,
        ortho_fparams: Optional[Dict[str, float]] = None,
        ortho_bparams: Optional[Dict[str, bool]] = None,
    ) -> Tuple[Tensor, Tensor]:
# makes sure that input is contiguous for efficiency.
# Note: autograd does not support dense gradients for sparse input yet.
"
329,"self.bparams = bparams
self.method = method
self.tracker = tracker
        m = iparams['m']
        n = iparams['n']
# variable parameters
self.X = X
        self.E = torch.zeros((n, ), dtype=X.dtype, device=X.device)
self.R = torch.zeros((m, n), dtype=X.dtype, device=X.device)
self.S = torch.zeros((m, 3 * n), dtype=X.dtype, device=X.device)
        self.tvars = {}               # type: Dict[str, Tensor]
        self.ivars = {'istep': 0}     # type: Dict[str, int]
        self.fvars = {'_': 0.0}       # type: Dict[str, float]
        self.bvars = {'_': False}     # type: Dict[str, bool]
def __str__(self):
        lines = ['LOPBCG:']
        lines += ['  iparams={}'.format(self.iparams)]
        lines += ['  fparams={}'.format(self.fparams)]
        lines += ['  bparams={}'.format(self.bparams)]
        lines += ['  ivars={}'.format(self.ivars)]
        lines += ['  fvars={}'.format(self.fvars)]
        lines += ['  bvars={}'.format(self.bvars)]
        lines += ['  tvars={}'.format(self.tvars)]
        lines += ['  A={}'.format(self.A)]
        lines += ['  B={}'.format(self.B)]
        lines += ['  iK={}'.format(self.iK)]
        lines += ['  X={}'.format(self.X)]
        lines += ['  E={}'.format(self.E)]
        r = ''
for line in lines:
            r += line + '\n'
return r
def update(self):
        """"""Set and update iteration variables.
        """"""
        if self.ivars['istep'] == 0:
X_norm = float(torch.norm(self.X))
iX_norm = X_norm ** -1
A_norm = float(torch.norm(_utils.matmul(self.A, self.X))) * iX_norm
B_norm = float(torch.norm(_utils.matmul(self.B, self.X))) * iX_norm
            self.fvars['X_norm'] = X_norm
            self.fvars['A_norm'] = A_norm
            self.fvars['B_norm'] = B_norm
            self.ivars['iterations_left'] = self.iparams['niter']
            self.ivars['converged_count'] = 0
            self.ivars['converged_end'] = 0

        if self.method == 'ortho':
self._update_ortho()
else:
self._update_basic()
        self.ivars['iterations_left'] = self.ivars['iterations_left'] - 1
        self.ivars['istep'] = self.ivars['istep'] + 1
def update_residual(self):
        """"""Update residual R from A, B, X, E.
        """"""
mm = _utils.matmul
self.R = mm(self.A, self.X) - mm(self.B, self.X) * self.E
","self.bparams = bparams
self.method = method
self.tracker = tracker
        m = iparams[""m""]
        n = iparams[""n""]
# variable parameters
self.X = X
        self.E = torch.zeros((n,), dtype=X.dtype, device=X.device)
self.R = torch.zeros((m, n), dtype=X.dtype, device=X.device)
self.S = torch.zeros((m, 3 * n), dtype=X.dtype, device=X.device)
        self.tvars = {}  # type: Dict[str, Tensor]
        self.ivars = {""istep"": 0}  # type: Dict[str, int]
        self.fvars = {""_"": 0.0}  # type: Dict[str, float]
        self.bvars = {""_"": False}  # type: Dict[str, bool]
def __str__(self):
        lines = [""LOPBCG:""]
        lines += [""  iparams={}"".format(self.iparams)]
        lines += [""  fparams={}"".format(self.fparams)]
        lines += [""  bparams={}"".format(self.bparams)]
        lines += [""  ivars={}"".format(self.ivars)]
        lines += [""  fvars={}"".format(self.fvars)]
        lines += [""  bvars={}"".format(self.bvars)]
        lines += [""  tvars={}"".format(self.tvars)]
        lines += [""  A={}"".format(self.A)]
        lines += [""  B={}"".format(self.B)]
        lines += [""  iK={}"".format(self.iK)]
        lines += [""  X={}"".format(self.X)]
        lines += [""  E={}"".format(self.E)]
        r = """"
for line in lines:
            r += line + ""\n""
return r
def update(self):
        """"""Set and update iteration variables.""""""
        if self.ivars[""istep""] == 0:
X_norm = float(torch.norm(self.X))
iX_norm = X_norm ** -1
A_norm = float(torch.norm(_utils.matmul(self.A, self.X))) * iX_norm
B_norm = float(torch.norm(_utils.matmul(self.B, self.X))) * iX_norm
            self.fvars[""X_norm""] = X_norm
            self.fvars[""A_norm""] = A_norm
            self.fvars[""B_norm""] = B_norm
            self.ivars[""iterations_left""] = self.iparams[""niter""]
            self.ivars[""converged_count""] = 0
            self.ivars[""converged_end""] = 0

        if self.method == ""ortho"":
self._update_ortho()
else:
self._update_basic()
        self.ivars[""iterations_left""] = self.ivars[""iterations_left""] - 1
        self.ivars[""istep""] = self.ivars[""istep""] + 1
def update_residual(self):
        """"""Update residual R from A, B, X, E.""""""
mm = _utils.matmul
self.R = mm(self.A, self.X) - mm(self.B, self.X) * self.E
"
330,"E_, Z = _utils.symeig(_utils.qform(self.A, S_), largest)
# Update E, X, P
            self.X[:, nc:] = mm(S_, Z[:, :n - nc])
            self.E[nc:] = E_[:n - nc]
            P = mm(S_, mm(Z[:, n - nc:], _utils.basis(_utils.transpose(Z[:n - nc, n - nc:]))))
np = P.shape[-1]
# check convergence
","E_, Z = _utils.symeig(_utils.qform(self.A, S_), largest)
# Update E, X, P
            self.X[:, nc:] = mm(S_, Z[:, : n - nc])
            self.E[nc:] = E_[: n - nc]
            P = mm(
                S_,
                mm(
                    Z[:, n - nc :],
                    _utils.basis(_utils.transpose(Z[: n - nc, n - nc :])),
                ),
            )
np = P.shape[-1]
# check convergence
"
331,"VBU_norm = torch.norm(VBU)
U_norm = torch.norm(U)
rerr = float(VBU_norm) * float(BV_norm * U_norm) ** -1
            vkey = 'ortho_VBU_rerr[{}]'.format(i)
self.fvars[vkey] = rerr
if rerr < tau_ortho:
break
","VBU_norm = torch.norm(VBU)
U_norm = torch.norm(U)
rerr = float(VBU_norm) * float(BV_norm * U_norm) ** -1
            vkey = ""ortho_VBU_rerr[{}]"".format(i)
self.fvars[vkey] = rerr
if rerr < tau_ortho:
break
"
332,"""""""
if not torch.jit.is_scripting():
tensor_ops = (A, M)
        if (not set(map(type, tensor_ops)).issubset((torch.Tensor, type(None))) and has_torch_function(tensor_ops)):
            return handle_torch_function(svd_lowrank, tensor_ops, A, q=q, niter=niter, M=M)
return _svd_lowrank(A, q=q, niter=niter, M=M)
def _svd_lowrank(A: Tensor, q: Optional[int] = 6, niter: Optional[int] = 2,
                 M: Optional[Tensor] = None) -> Tuple[Tensor, Tensor, Tensor]:
q = 6 if q is None else q
m, n = A.shape[-2:]
matmul = _utils.matmul
","""""""
if not torch.jit.is_scripting():
tensor_ops = (A, M)
        if not set(map(type, tensor_ops)).issubset(
            (torch.Tensor, type(None))
        ) and has_torch_function(tensor_ops):
            return handle_torch_function(
                svd_lowrank, tensor_ops, A, q=q, niter=niter, M=M
            )
return _svd_lowrank(A, q=q, niter=niter, M=M)
def _svd_lowrank(
    A: Tensor,
    q: Optional[int] = 6,
    niter: Optional[int] = 2,
    M: Optional[Tensor] = None,
) -> Tuple[Tensor, Tensor, Tensor]:
q = 6 if q is None else q
m, n = A.shape[-2:]
matmul = _utils.matmul
"
333,"Returns a list of dispatch keys supported by PythonDispatcher.
You can register kernels to these keys.
""""""
def keys(self):
return self.supported_keys
","Returns a list of dispatch keys supported by PythonDispatcher.
You can register kernels to these keys.
""""""

def keys(self):
return self.supported_keys
"
334,"Returns a table(str) including all the registrations from users.
Note this includes registrations to both runtime keys and alias keys.
""""""
def registrations(self):
output = self._format_header(""Registered Kernels"")
state = self.rawRegistrations()
        state_entries = state.split('\n')
for line in state_entries:
first = line.split("":"")[0]
if any(first.startswith(k) for k in self.supported_keys):
","Returns a table(str) including all the registrations from users.
Note this includes registrations to both runtime keys and alias keys.
""""""

def registrations(self):
output = self._format_header(""Registered Kernels"")
state = self.rawRegistrations()
        state_entries = state.split(""\n"")
for line in state_entries:
first = line.split("":"")[0]
if any(first.startswith(k) for k in self.supported_keys):
"
335,"return handle_torch_function(Tensor.resize_as, (self, tensor), self, tensor)
warnings.warn(""non-inplace resize_as is deprecated"")
from torch.autograd._functions import Resize
return Resize.apply(self, tensor.size())
def split(self, split_size, dim=0):
        r""""""See :func:`torch.split`
        """"""
if has_torch_function_unary(self):
            return handle_torch_function(Tensor.split, (self,), self, split_size, dim=dim)
if isinstance(split_size, int):
return super(Tensor, self).split(split_size, dim)
elif isinstance(split_size, Tensor):
","return handle_torch_function(Tensor.resize_as, (self, tensor), self, tensor)
warnings.warn(""non-inplace resize_as is deprecated"")
from torch.autograd._functions import Resize

return Resize.apply(self, tensor.size())
def split(self, split_size, dim=0):
        r""""""See :func:`torch.split`""""""
if has_torch_function_unary(self):
            return handle_torch_function(
                Tensor.split, (self,), self, split_size, dim=dim
            )
if isinstance(split_size, int):
return super(Tensor, self).split(split_size, dim)
elif isinstance(split_size, Tensor):
"
336,"tensor([[ 0,  1],
[ 2,  3]], dtype=torch.int8)
"""""".format(**new_common_args))
add_docstr_all('new_full',
               r""""""
new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.
","tensor([[ 0,  1],
[ 2,  3]], dtype=torch.int8)
"""""".format(
        **new_common_args
    ),
)
add_docstr_all(
    ""new_full"",
    r""""""
new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor
Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.
"
337,"non_blocking (bool): if ``True`` and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.
"""""")
add_docstr_all('conj',
               r""""""
conj() -> Tensor
See :func:`torch.conj`
"""""")
add_docstr_all('conj_physical',
               r""""""
conj_physical() -> Tensor
See :func:`torch.conj_physical`
"""""")
add_docstr_all('conj_physical_',
               r""""""
conj_physical_() -> Tensor
In-place version of :meth:`~Tensor.conj_physical`
"""""")
add_docstr_all('resolve_conj',
               r""""""
resolve_conj() -> Tensor
See :func:`torch.resolve_conj`
"""""")
add_docstr_all('resolve_neg',
               r""""""
resolve_neg() -> Tensor
See :func:`torch.resolve_neg`
"""""")
add_docstr_all('copysign',
               r""""""
copysign(other) -> Tensor
See :func:`torch.copysign`
"""""")
add_docstr_all('copysign_', r""""""
copysign_(other) -> Tensor
In-place version of :meth:`~Tensor.copysign`
"""""")
add_docstr_all('cos',
               r""""""
cos() -> Tensor
See :func:`torch.cos`
"""""")
add_docstr_all('cos_',
               r""""""
cos_() -> Tensor
In-place version of :meth:`~Tensor.cos`
"""""")
add_docstr_all('cosh',
               r""""""
cosh() -> Tensor
See :func:`torch.cosh`
"""""")
add_docstr_all('cosh_',
               r""""""
cosh_() -> Tensor
In-place version of :meth:`~Tensor.cosh`
"""""")
add_docstr_all('cpu',
               r""""""
cpu(memory_format=torch.preserve_format) -> Tensor
Returns a copy of this object in CPU memory.
","non_blocking (bool): if ``True`` and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.
"""""",
)
add_docstr_all(
    ""conj"",
    r""""""
conj() -> Tensor
See :func:`torch.conj`
"""""",
)
add_docstr_all(
    ""conj_physical"",
    r""""""
conj_physical() -> Tensor
See :func:`torch.conj_physical`
"""""",
)
add_docstr_all(
    ""conj_physical_"",
    r""""""
conj_physical_() -> Tensor
In-place version of :meth:`~Tensor.conj_physical`
"""""",
)
add_docstr_all(
    ""resolve_conj"",
    r""""""
resolve_conj() -> Tensor
See :func:`torch.resolve_conj`
"""""",
)
add_docstr_all(
    ""resolve_neg"",
    r""""""
resolve_neg() -> Tensor
See :func:`torch.resolve_neg`
"""""",
)
add_docstr_all(
    ""copysign"",
    r""""""
copysign(other) -> Tensor
See :func:`torch.copysign`
"""""",
)
add_docstr_all(
    ""copysign_"",
    r""""""
copysign_(other) -> Tensor
In-place version of :meth:`~Tensor.copysign`
"""""",
)
add_docstr_all(
    ""cos"",
    r""""""
cos() -> Tensor
See :func:`torch.cos`
"""""",
)
add_docstr_all(
    ""cos_"",
    r""""""
cos_() -> Tensor
In-place version of :meth:`~Tensor.cos`
"""""",
)
add_docstr_all(
    ""cosh"",
    r""""""
cosh() -> Tensor
See :func:`torch.cosh`
"""""",
)
add_docstr_all(
    ""cosh_"",
    r""""""
cosh_() -> Tensor
In-place version of :meth:`~Tensor.cosh`
"""""",
)
add_docstr_all(
    ""cpu"",
    r""""""
cpu(memory_format=torch.preserve_format) -> Tensor
Returns a copy of this object in CPU memory.
"
338,"the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(**common_args))

add_docstr_all('logcumsumexp',
               r""""""
logcumsumexp(dim) -> Tensor
See :func:`torch.logcumsumexp`
"""""")
add_docstr_all('cummax',
               r""""""
cummax(dim) -> (Tensor, Tensor)
See :func:`torch.cummax`
"""""")
add_docstr_all('cummin',
               r""""""
cummin(dim) -> (Tensor, Tensor)
See :func:`torch.cummin`
"""""")
add_docstr_all('cumprod',
               r""""""
cumprod(dim, dtype=None) -> Tensor
See :func:`torch.cumprod`
"""""")
add_docstr_all('cumprod_',
               r""""""
cumprod_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumprod`
"""""")
add_docstr_all('cumsum',
               r""""""
cumsum(dim, dtype=None) -> Tensor
See :func:`torch.cumsum`
"""""")
add_docstr_all('cumsum_',
               r""""""
cumsum_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumsum`
"""""")
add_docstr_all('data_ptr',
               r""""""
data_ptr() -> int
Returns the address of the first element of :attr:`self` tensor.
"""""")
add_docstr_all('dequantize',
               r""""""
dequantize() -> Tensor
Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
"""""")
add_docstr_all('dense_dim',
               r""""""
dense_dim() -> int
Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
","the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: ``False``.
{memory_format}
"""""".format(
        **common_args
    ),
)

add_docstr_all(
    ""logcumsumexp"",
    r""""""
logcumsumexp(dim) -> Tensor
See :func:`torch.logcumsumexp`
"""""",
)
add_docstr_all(
    ""cummax"",
    r""""""
cummax(dim) -> (Tensor, Tensor)
See :func:`torch.cummax`
"""""",
)
add_docstr_all(
    ""cummin"",
    r""""""
cummin(dim) -> (Tensor, Tensor)
See :func:`torch.cummin`
"""""",
)
add_docstr_all(
    ""cumprod"",
    r""""""
cumprod(dim, dtype=None) -> Tensor
See :func:`torch.cumprod`
"""""",
)
add_docstr_all(
    ""cumprod_"",
    r""""""
cumprod_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumprod`
"""""",
)
add_docstr_all(
    ""cumsum"",
    r""""""
cumsum(dim, dtype=None) -> Tensor
See :func:`torch.cumsum`
"""""",
)
add_docstr_all(
    ""cumsum_"",
    r""""""
cumsum_(dim, dtype=None) -> Tensor
In-place version of :meth:`~Tensor.cumsum`
"""""",
)
add_docstr_all(
    ""data_ptr"",
    r""""""
data_ptr() -> int
Returns the address of the first element of :attr:`self` tensor.
"""""",
)
add_docstr_all(
    ""dequantize"",
    r""""""
dequantize() -> Tensor
Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
"""""",
)
add_docstr_all(
    ""dense_dim"",
    r""""""
dense_dim() -> int
Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
"
339,"Throws an error if :attr:`self` is not a sparse tensor.
See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.
"""""")
add_docstr_all('diag',
               r""""""
diag(diagonal=0) -> Tensor
See :func:`torch.diag`
"""""")
add_docstr_all('diag_embed',
               r""""""
diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor
See :func:`torch.diag_embed`
"""""")
add_docstr_all('diagflat',
               r""""""
diagflat(offset=0) -> Tensor
See :func:`torch.diagflat`
"""""")
add_docstr_all('diagonal',
               r""""""
diagonal(offset=0, dim1=0, dim2=1) -> Tensor
See :func:`torch.diagonal`
"""""")
add_docstr_all('fill_diagonal_',
               r""""""
fill_diagonal_(fill_value, wrap=False) -> Tensor
Fill the main diagonal of a tensor that has at least 2-dimensions.
","Throws an error if :attr:`self` is not a sparse tensor.
See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.
"""""",
)
add_docstr_all(
    ""diag"",
    r""""""
diag(diagonal=0) -> Tensor
See :func:`torch.diag`
"""""",
)
add_docstr_all(
    ""diag_embed"",
    r""""""
diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor
See :func:`torch.diag_embed`
"""""",
)
add_docstr_all(
    ""diagflat"",
    r""""""
diagflat(offset=0) -> Tensor
See :func:`torch.diagflat`
"""""",
)
add_docstr_all(
    ""diagonal"",
    r""""""
diagonal(offset=0, dim1=0, dim2=1) -> Tensor
See :func:`torch.diagonal`
"""""",
)
add_docstr_all(
    ""fill_diagonal_"",
    r""""""
fill_diagonal_(fill_value, wrap=False) -> Tensor
Fill the main diagonal of a tensor that has at least 2-dimensions.
"
340,"The :attr:`callable` should have the signature::
def callable(a, b) -> number
"""""")
add_docstr_all('masked_scatter_',
               r""""""
masked_scatter_(mask, source)
Copies elements from :attr:`source` into :attr:`self` tensor at positions where
","The :attr:`callable` should have the signature::
def callable(a, b) -> number
"""""",
)
add_docstr_all(
    ""masked_scatter_"",
    r""""""
masked_scatter_(mask, source)
Copies elements from :attr:`source` into :attr:`self` tensor at positions where
"
341,"The :attr:`mask` operates on the :attr:`self` tensor, not on the given
:attr:`source` tensor.
"""""")
add_docstr_all('masked_fill_',
               r""""""
masked_fill_(mask, value)
Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is
","The :attr:`mask` operates on the :attr:`self` tensor, not on the given
:attr:`source` tensor.
"""""",
)
add_docstr_all(
    ""masked_fill_"",
    r""""""
masked_fill_(mask, value)
Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is
"
342,"types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every
value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`
will be uniform in ``[0, 2^53]``.
"""""")
add_docstr_all('rad2deg',
               r""""""
rad2deg() -> Tensor
See :func:`torch.rad2deg`
"""""")
add_docstr_all('rad2deg_',
               r""""""
rad2deg_() -> Tensor
In-place version of :meth:`~Tensor.rad2deg`
"""""")
add_docstr_all('deg2rad',
               r""""""
deg2rad() -> Tensor
See :func:`torch.deg2rad`
"""""")
add_docstr_all('deg2rad_',
               r""""""
deg2rad_() -> Tensor
In-place version of :meth:`~Tensor.deg2rad`
"""""")
add_docstr_all('ravel',
               r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""")
add_docstr_all('reciprocal',
               r""""""
reciprocal() -> Tensor
See :func:`torch.reciprocal`
"""""")
add_docstr_all('reciprocal_',
               r""""""
reciprocal_() -> Tensor
In-place version of :meth:`~Tensor.reciprocal`
"""""")
add_docstr_all('record_stream',
               r""""""
record_stream(stream)
Ensures that the tensor memory is not reused for another tensor until all
","types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every
value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`
will be uniform in ``[0, 2^53]``.
"""""",
)
add_docstr_all(
    ""rad2deg"",
    r""""""
rad2deg() -> Tensor
See :func:`torch.rad2deg`
"""""",
)
add_docstr_all(
    ""rad2deg_"",
    r""""""
rad2deg_() -> Tensor
In-place version of :meth:`~Tensor.rad2deg`
"""""",
)
add_docstr_all(
    ""deg2rad"",
    r""""""
deg2rad() -> Tensor
See :func:`torch.deg2rad`
"""""",
)
add_docstr_all(
    ""deg2rad_"",
    r""""""
deg2rad_() -> Tensor
In-place version of :meth:`~Tensor.deg2rad`
"""""",
)
add_docstr_all(
    ""ravel"",
    r""""""
ravel(input) -> Tensor
see :func:`torch.ravel`
"""""",
)
add_docstr_all(
    ""reciprocal"",
    r""""""
reciprocal() -> Tensor
See :func:`torch.reciprocal`
"""""",
)
add_docstr_all(
    ""reciprocal_"",
    r""""""
reciprocal_() -> Tensor
In-place version of :meth:`~Tensor.reciprocal`
"""""",
)
add_docstr_all(
    ""record_stream"",
    r""""""
record_stream(stream)
Ensures that the tensor memory is not reused for another tensor until all
"
343,">>> t.size(dim=1)
4
"""""")
add_docstr_all('solve',
               r""""""
solve(A) -> Tensor, Tensor
See :func:`torch.solve`
"""""")
add_docstr_all('sort',
               r""""""
sort(dim=-1, descending=False) -> (Tensor, LongTensor)
See :func:`torch.sort`
"""""")
add_docstr_all('msort',
               r""""""
msort() -> Tensor
See :func:`torch.msort`
"""""")
add_docstr_all('argsort',
               r""""""
argsort(dim=-1, descending=False) -> LongTensor
See :func:`torch.argsort`
"""""")
add_docstr_all('sparse_dim',
               r""""""
sparse_dim() -> int
Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
",">>> t.size(dim=1)
4
"""""",
)
add_docstr_all(
    ""solve"",
    r""""""
solve(A) -> Tensor, Tensor
See :func:`torch.solve`
"""""",
)
add_docstr_all(
    ""sort"",
    r""""""
sort(dim=-1, descending=False) -> (Tensor, LongTensor)
See :func:`torch.sort`
"""""",
)
add_docstr_all(
    ""msort"",
    r""""""
msort() -> Tensor
See :func:`torch.msort`
"""""",
)
add_docstr_all(
    ""argsort"",
    r""""""
argsort(dim=-1, descending=False) -> LongTensor
See :func:`torch.argsort`
"""""",
)
add_docstr_all(
    ""sparse_dim"",
    r""""""
sparse_dim() -> int
Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.
"
344,"Args:
tensor (Tensor): the tensor which has the desired type
"""""")
add_docstr_all('unfold',
               r""""""
unfold(dimension, size, step) -> Tensor
Returns a view of the original tensor which contains all slices of size :attr:`size` from
","Args:
tensor (Tensor): the tensor which has the desired type
"""""",
)
add_docstr_all(
    ""unfold"",
    r""""""
unfold(dimension, size, step) -> Tensor
Returns a view of the original tensor which contains all slices of size :attr:`size` from
"
345,":noindex:
See :func:`torch.var`
"""""")
add_docstr_all('vdot',
               r""""""
vdot(other) -> Tensor
See :func:`torch.vdot`
"""""")
add_docstr_all('view',
               r""""""
view(*shape) -> Tensor
Returns a new tensor with the same data as the :attr:`self` tensor but of a
",":noindex:
See :func:`torch.var`
"""""",
)
add_docstr_all(
    ""vdot"",
    r""""""
vdot(other) -> Tensor
See :func:`torch.vdot`
"""""",
)
add_docstr_all(
    ""view"",
    r""""""
view(*shape) -> Tensor
Returns a new tensor with the same data as the :attr:`self` tensor but of a
"
346,"Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""")
add_docstr_all('view_as',
               r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
","Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.
"""""",
)
add_docstr_all(
    ""view_as"",
    r""""""
view_as(other) -> Tensor
View this tensor as the same size as :attr:`other`.
"
347,"The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`
attribute will be populated, see :attr:`is_leaf` for more details.
"""""")
add_docstr_all('is_leaf',
               r""""""
All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.
For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were
","The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`
attribute will be populated, see :attr:`is_leaf` for more details.
"""""",
)
add_docstr_all(
    ""is_leaf"",
    r""""""
All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.
For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were
"
348,">>> csr.col_indices()
tensor([0, 1, 2, 3, 4], dtype=torch.int32)
"""""")
",">>> csr.col_indices()
tensor([0, 1, 2, 3, 4], dtype=torch.int32)
"""""",
)
"
349,"if self.int_mode:
# in int_mode for floats, all numbers are integers, and we append a decimal to nonfinites
# to indicate that the tensor is of floating type. add 1 to the len to account for this.
                if nonzero_finite_max / nonzero_finite_min > 1000. or nonzero_finite_max > 1.e8:
self.sci_mode = True
for value in nonzero_finite_vals:
                        value_str = ('{{:.{}e}}').format(PRINT_OPTS.precision).format(value)
self.max_width = max(self.max_width, len(value_str))
else:
for value in nonzero_finite_vals:
                        value_str = ('{:.0f}').format(value)
self.max_width = max(self.max_width, len(value_str) + 1)
else:
# Check if scientific representation should be used.
                if nonzero_finite_max / nonzero_finite_min > 1000.\
                        or nonzero_finite_max > 1.e8\
                        or nonzero_finite_min < 1.e-4:
self.sci_mode = True
for value in nonzero_finite_vals:
                        value_str = ('{{:.{}e}}').format(PRINT_OPTS.precision).format(value)
self.max_width = max(self.max_width, len(value_str))
else:
for value in nonzero_finite_vals:
                        value_str = ('{{:.{}f}}').format(PRINT_OPTS.precision).format(value)
self.max_width = max(self.max_width, len(value_str))
if PRINT_OPTS.sci_mode is not None:
","if self.int_mode:
# in int_mode for floats, all numbers are integers, and we append a decimal to nonfinites
# to indicate that the tensor is of floating type. add 1 to the len to account for this.
                if (
                    nonzero_finite_max / nonzero_finite_min > 1000.0
                    or nonzero_finite_max > 1.0e8
                ):
self.sci_mode = True
for value in nonzero_finite_vals:
                        value_str = (
                            (""{{:.{}e}}"").format(PRINT_OPTS.precision).format(value)
                        )
self.max_width = max(self.max_width, len(value_str))
else:
for value in nonzero_finite_vals:
                        value_str = (""{:.0f}"").format(value)
self.max_width = max(self.max_width, len(value_str) + 1)
else:
# Check if scientific representation should be used.
                if (
                    nonzero_finite_max / nonzero_finite_min > 1000.0
                    or nonzero_finite_max > 1.0e8
                    or nonzero_finite_min < 1.0e-4
                ):
self.sci_mode = True
for value in nonzero_finite_vals:
                        value_str = (
                            (""{{:.{}e}}"").format(PRINT_OPTS.precision).format(value)
                        )
self.max_width = max(self.max_width, len(value_str))
else:
for value in nonzero_finite_vals:
                        value_str = (
                            (""{{:.{}f}}"").format(PRINT_OPTS.precision).format(value)
                        )
self.max_width = max(self.max_width, len(value_str))
if PRINT_OPTS.sci_mode is not None:
"
350,"# width for imag_formatter + an extra j for complex
element_length += formatter2.width() + 1
    elements_per_line = max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / (element_length))))
char_per_line = element_length * elements_per_line
def _val_formatter(val, formatter1=formatter1, formatter2=formatter2):
","# width for imag_formatter + an extra j for complex
element_length += formatter2.width() + 1
    elements_per_line = max(
        1, int(math.floor((PRINT_OPTS.linewidth - indent) / (element_length)))
    )
char_per_line = element_length * elements_per_line
def _val_formatter(val, formatter1=formatter1, formatter2=formatter2):
"
351,"return _vector_str(self, indent, summarize, formatter1, formatter2)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        slices = ([_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                   for i in range(0, PRINT_OPTS.edgeitems)] +
                  ['...'] +
                  [_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                   for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])
else:
        slices = [_tensor_str_with_formatter(self[i], indent + 1, summarize, formatter1, formatter2)
                  for i in range(0, self.size(0))]
    tensor_str = (',' + '\n' * (dim - 1) + ' ' * (indent + 1)).join(slices)
    return '[' + tensor_str + ']'
def _tensor_str(self, indent):
if self.numel() == 0:
        return '[]'
if self.has_names():
# There are two main codepaths (possibly more) that tensor printing goes through:
","return _vector_str(self, indent, summarize, formatter1, formatter2)
if summarize and self.size(0) > 2 * PRINT_OPTS.edgeitems:
        slices = (
            [
                _tensor_str_with_formatter(
                    self[i], indent + 1, summarize, formatter1, formatter2
                )
                for i in range(0, PRINT_OPTS.edgeitems)
            ]
            + [""...""]
            + [
                _tensor_str_with_formatter(
                    self[i], indent + 1, summarize, formatter1, formatter2
                )
                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))
            ]
        )
else:
        slices = [
            _tensor_str_with_formatter(
                self[i], indent + 1, summarize, formatter1, formatter2
            )
            for i in range(0, self.size(0))
        ]

    tensor_str = ("","" + ""\n"" * (dim - 1) + "" "" * (indent + 1)).join(slices)
    return ""["" + tensor_str + ""]""
def _tensor_str(self, indent):
if self.numel() == 0:
        return ""[]""
if self.has_names():
# There are two main codepaths (possibly more) that tensor printing goes through:
"
352,"the pinned memory. Works only for CPU tensors. Default: ``False``.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.contiguous_format``.
""""""))
factory_like_common_args = parse_kwargs(""""""
input (Tensor): the size of :attr:`input` will determine size of the output tensor.
layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
Default: if ``None``, defaults to the layout of :attr:`input`.
","the pinned memory. Works only for CPU tensors. Default: ``False``.
memory_format (:class:`torch.memory_format`, optional): the desired memory format of
returned Tensor. Default: ``torch.contiguous_format``.
""""""
    ),
)
factory_like_common_args = parse_kwargs(
    """"""
input (Tensor): the size of :attr:`input` will determine size of the output tensor.
layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
Default: if ``None``, defaults to the layout of :attr:`input`.
"
353,"If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
"""""" + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers
","If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
it will not be propagated.
""""""
    + r""""""
For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
:attr:`alpha` must be real numbers, otherwise they should be integers
"
354,"False
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True
"""""")
add_docstr(torch.all,
           r""""""
all(input) -> Tensor
Tests if all elements in :attr:`input` evaluate to `True`.
","False
>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)
True
"""""",
)
add_docstr(
    torch.all,
    r""""""
all(input) -> Tensor
Tests if all elements in :attr:`input` evaluate to `True`.
"
355,"tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]])
"""""".format(**common_args))

add_docstr(torch.bincount,
           r""""""
bincount(input, weights=None, minlength=0) -> Tensor
Count the frequency of each value in an array of non-negative ints.
","tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.bincount,
    r""""""
bincount(input, weights=None, minlength=0) -> Tensor
Count the frequency of each value in an array of non-negative ints.
"
356,">>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])
"""""".format(**common_args, **tf32_notes))

add_docstr(torch.bitwise_and,
           r""""""
bitwise_and(input, other, *, out=None) -> Tensor
Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
",">>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])
"""""".format(
        **common_args, **tf32_notes
    ),
)

add_docstr(
    torch.bitwise_and,
    r""""""
bitwise_and(input, other, *, out=None) -> Tensor
Computes the bitwise AND of :attr:`input` and :attr:`other`. The input tensor must be of
"
357,"tensor([-2, -2,  0], dtype=torch.int8)
>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, False, False])
"""""".format(**common_args))

add_docstr(torch.bitwise_left_shift,
           r""""""
bitwise_left_shift(input, other, *, out=None) -> Tensor
Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits.
","tensor([-2, -2,  0], dtype=torch.int8)
>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))
tensor([ True, False, False])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.bitwise_left_shift,
    r""""""
bitwise_left_shift(input, other, *, out=None) -> Tensor
Computes the left arithmetic shift of :attr:`input` by :attr:`other` bits.
"
358,">>> torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])
"""""".format(**common_args))

add_docstr(torch.conj,
           r""""""
conj(input) -> Tensor
Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype,
",">>> torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))
tensor([-1 - 1j, -2 - 2j, 3 + 3j])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.conj,
    r""""""
conj(input) -> Tensor
Returns a view of :attr:`input` with a flipped conjugate bit. If :attr:`input` has a non-complex dtype,
"
359,"[-2.4490, -1.5687,  1.9792],
[-0.8304, -1.3037,  0.5650],
[-1.2329,  1.9883,  1.0551]])
"""""".format(**common_args))

add_docstr(torch.logcumsumexp,
           r""""""
logcumsumexp(input, dim, *, out=None) -> Tensor
Returns the logarithm of the cumulative summation of the exponentiation of
elements of :attr:`input` in the dimension :attr:`dim`.
","[-2.4490, -1.5687,  1.9792],
[-0.8304, -1.3037,  0.5650],
[-1.2329,  1.9883,  1.0551]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.logcumsumexp,
    r""""""
logcumsumexp(input, dim, *, out=None) -> Tensor
Returns the logarithm of the cumulative summation of the exponentiation of
elements of :attr:`input` in the dimension :attr:`dim`.
"
360,"[ 0.0000, -0.3018,  0.0000,  0.0000],
[ 0.0000,  0.0000, -0.1516,  0.0000],
[ 0.0000,  0.0000,  0.0000,  1.9342]])
"""""".format(**common_args))

add_docstr(torch.diagonal,
           r""""""
diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor
Returns a partial view of :attr:`input` with the its diagonal elements
","[ 0.0000, -0.3018,  0.0000,  0.0000],
[ 0.0000,  0.0000, -0.1516,  0.0000],
[ 0.0000,  0.0000,  0.0000,  1.9342]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.diagonal,
    r""""""
diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor
Returns a partial view of :attr:`input` with the its diagonal elements
"
361,">>> torch.diff(c, dim=1)
tensor([[1, 1],
[1, 1]])
"""""".format(**common_args))

add_docstr(torch.digamma, r""""""
digamma(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""")
add_docstr(torch.dist,
           r""""""
dist(input, other, p=2) -> Tensor
Returns the p-norm of (:attr:`input` - :attr:`other`)
",">>> torch.diff(c, dim=1)
tensor([[1, 1],
[1, 1]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.digamma,
    r""""""
digamma(input, *, out=None) -> Tensor
Alias for :func:`torch.special.digamma`.
"""""",
)
add_docstr(
    torch.dist,
    r""""""
dist(input, other, p=2) -> Tensor
Returns the p-norm of (:attr:`input` - :attr:`other`)
"
362,">>> torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])
"""""")
add_docstr(torch.frexp,
           r""""""
frexp(input, *, out=None) -> (Tensor mantissa, Tensor exponent)
Decomposes :attr:`input` into mantissa and exponent tensors
",">>> torch.frac(torch.tensor([1, 2.5, -3.2]))
tensor([ 0.0000,  0.5000, -0.2000])
"""""",
)
add_docstr(
    torch.frexp,
    r""""""
frexp(input, *, out=None) -> (Tensor mantissa, Tensor exponent)
Decomposes :attr:`input` into mantissa and exponent tensors
"
363,"The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
"""""" + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
","The backward pass with respect to :attr:`input` is not yet supported.
Please open an issue on PyTorch's Github to request it.
""""""
    + r""""""
Args:
input (Tensor): the first non-negative input tensor
other (Tensor): the second non-negative input tensor
"
364,".. math::
\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)
"""""" + r""""""
The shapes of :attr:`start` and :attr:`end` must be
:ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then
the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.
",".. math::
\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)
""""""
    + r""""""
The shapes of :attr:`start` and :attr:`end` must be
:ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then
the shapes of :attr:`weight`, :attr:`start`, and :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.
"
365,">>> torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
"""""".format(**common_args))
add_docstr(torch.logaddexp,
           r""""""
logaddexp(input, other, *, out=None) -> Tensor
Logarithm of the sum of exponentiations of the inputs.
",">>> torch.log2(a)
tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.logaddexp,
    r""""""
logaddexp(input, other, *, out=None) -> Tensor
Logarithm of the sum of exponentiations of the inputs.
"
366,"tensor([1.4907, 1.0593, 1.5696])
>>> torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1)))
tensor(1.6859e-07)
"""""".format(**multi_dim_common))

add_docstr(torch.lstsq,
           r""""""
lstsq(input, A, *, out=None) -> (Tensor, Tensor)
Computes the solution to the least squares and least norm problems for a full
","tensor([1.4907, 1.0593, 1.5696])
>>> torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1)))
tensor(1.6859e-07)
"""""".format(
        **multi_dim_common
    ),
)

add_docstr(
    torch.lstsq,
    r""""""
lstsq(input, A, *, out=None) -> (Tensor, Tensor)
Computes the solution to the least squares and least norm problems for a full
"
367,">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
"""""".format(**common_args))

add_docstr(torch.lu_unpack, r""""""
lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -> (Tensor, Tensor, Tensor)
Unpacks the data and pivots from a LU factorization of a tensor into tensors ``L`` and ``U`` and a permutation tensor ``P``
",">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
tensor([[False, False], [True, False]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.lu_unpack,
    r""""""
lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None) -> (Tensor, Tensor, Tensor)
Unpacks the data and pivots from a LU factorization of a tensor into tensors ``L`` and ``U`` and a permutation tensor ``P``
"
368,"[ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
>>> torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
"""""".format(**single_dim_common))

add_docstr(torch.nanmedian,
           r""""""
nanmedian(input) -> Tensor
Returns the median of the values in :attr:`input`, ignoring ``NaN`` values.
","[ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
>>> torch.median(a, 1)
torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.nanmedian,
    r""""""
nanmedian(input) -> Tensor
Returns the median of the values in :attr:`input`, ignoring ``NaN`` values.
"
369,"torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))
>>> a.nanmedian(0)
torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))
"""""".format(**single_dim_common))

add_docstr(torch.quantile, r""""""
quantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
Computes the q-th quantiles of each row of the :attr:`input` tensor
","torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))
>>> a.nanmedian(0)
torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))
"""""".format(
        **single_dim_common
    ),
)

add_docstr(
    torch.quantile,
    r""""""
quantile(input, q, dim=None, keepdim=False, *, out=None) -> Tensor
Computes the q-th quantiles of each row of the :attr:`input` tensor
"
370,"[ 359.9894, -359.9894],
[  89.9544,  -89.9544]])
"""""".format(**common_args))
add_docstr(torch.deg2rad,
           r""""""
deg2rad(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
","[ 359.9894, -359.9894],
[  89.9544,  -89.9544]])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.deg2rad,
    r""""""
deg2rad(input, *, out=None) -> Tensor
Returns a new tensor with each of the elements of :attr:`input`
"
371,">>> torch.heaviside(input, values)
tensor([0., -2., 1.])
"""""".format(**common_args))
add_docstr(torch.rand,
           r""""""
rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with random numbers from a uniform distribution
",">>> torch.heaviside(input, values)
tensor([0., -2., 1.])
"""""".format(
        **common_args
    ),
)
add_docstr(
    torch.rand,
    r""""""
rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a tensor filled with random numbers from a uniform distribution
"
372,">>> torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
"""""".format(**factory_data_common_args))

add_docstr(torch.range,
           r""""""
range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1`
",">>> torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
"""""".format(
        **factory_data_common_args
    ),
)

add_docstr(
    torch.range,
    r""""""
range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
Returns a 1-D tensor of size :math:`\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1`
"
373,"tensor([-0.0370,  0.2970,  1.5420, -0.9105])
>>> torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])
"""""".format(**common_args))

add_docstr(torch.scatter,
           r""""""
scatter(input, dim, index, src) -> Tensor
Out-of-place version of :meth:`torch.Tensor.scatter_`
"""""")
add_docstr(torch.scatter_add,
           r""""""
scatter_add(input, dim, index, src) -> Tensor
Out-of-place version of :meth:`torch.Tensor.scatter_add_`
"""""")
add_docstr(torch.set_flush_denormal,
           r""""""
set_flush_denormal(mode) -> bool
Disables denormal floating numbers on CPU.
","tensor([-0.0370,  0.2970,  1.5420, -0.9105])
>>> torch.rsqrt(a)
tensor([    nan,  1.8351,  0.8053,     nan])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.scatter,
    r""""""
scatter(input, dim, index, src) -> Tensor
Out-of-place version of :meth:`torch.Tensor.scatter_`
"""""",
)
add_docstr(
    torch.scatter_add,
    r""""""
scatter_add(input, dim, index, src) -> Tensor
Out-of-place version of :meth:`torch.Tensor.scatter_add_`
"""""",
)
add_docstr(
    torch.set_flush_denormal,
    r""""""
set_flush_denormal(mode) -> bool
Disables denormal floating numbers on CPU.
"
374,">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])
>>> t.sgn()
tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])
"""""".format(**common_args))

add_docstr(torch.sin,
           r""""""
sin(input, *, out=None) -> Tensor
Returns a new tensor with the sine of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sin(\text{input}_{i})
"""""" + r""""""
Args:
{input}
",">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])
>>> t.sgn()
tensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.sin,
    r""""""
sin(input, *, out=None) -> Tensor
Returns a new tensor with the sine of the elements of :attr:`input`.
.. math::
\text{out}_{i} = \sin(\text{input}_{i})
""""""
    + r""""""
Args:
{input}
"
375,">>> torch.fliplr(x)
tensor([[1, 0],
[3, 2]])
"""""".format(**common_args))

add_docstr(torch.flipud,
           r""""""
flipud(input) -> Tensor
Flip tensor in the up/down direction, returning a new tensor.
",">>> torch.fliplr(x)
tensor([[1, 0],
[3, 2]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.flipud,
    r""""""
flipud(input) -> Tensor
Flip tensor in the up/down direction, returning a new tensor.
"
376,"[8, 7],
[2, 1],
[4, 3]])
"""""".format(**common_args))

add_docstr(torch.rot90,
           r""""""
rot90(input, k, dims) -> Tensor
Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.
","[8, 7],
[2, 1],
[4, 3]])
"""""".format(
        **common_args
    ),
)

add_docstr(
    torch.rot90,
    r""""""
rot90(input, k, dims) -> Tensor
Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.
"
377,">>> torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.]])
"""""".format(**factory_like_common_args))
add_docstr(torch.empty,
           """"""
empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, \
memory_format=torch.contiguous_format) -> Tensor
",">>> torch.zeros_like(input)
tensor([[ 0.,  0.,  0.],
[ 0.,  0.,  0.]])
"""""".format(
        **factory_like_common_args
    ),
)
add_docstr(
    torch.empty,
    """"""
empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, \
memory_format=torch.contiguous_format) -> Tensor
"
378,"[2, 3],
[3, 3]])
"""""")
add_docstr(torch.trapezoid,
           r""""""
trapezoid(y, x=None, *, dx=None, dim=-1) -> Tensor
Computes the `trapezoidal rule <https://en.wikipedia.org/wiki/Trapezoidal_rule>`_ along
","[2, 3],
[3, 3]])
"""""",
)
add_docstr(
    torch.trapezoid,
    r""""""
trapezoid(y, x=None, *, dx=None, dim=-1) -> Tensor
Computes the `trapezoidal rule <https://en.wikipedia.org/wiki/Trapezoidal_rule>`_ along
"
379,">>> g_cpu = torch.Generator()
>>> g_cpu_other = torch.Generator()
>>> g_cpu.set_state(g_cpu_other.get_state())
"""""")
add_docstr(torch.Generator.get_state,
           r""""""
Generator.get_state() -> Tensor
Returns the Generator state as a ``torch.ByteTensor``.
",">>> g_cpu = torch.Generator()
>>> g_cpu_other = torch.Generator()
>>> g_cpu.set_state(g_cpu_other.get_state())
"""""",
)
add_docstr(
    torch.Generator.get_state,
    r""""""
Generator.get_state() -> Tensor
Returns the Generator state as a ``torch.ByteTensor``.
"
380,"tensor._backward_hooks = backward_hooks
return tensor
def _rebuild_parameter(data, requires_grad, backward_hooks):
param = torch.nn.Parameter(data, requires_grad)
# NB: This line exists only for backwards compatibility; the
","tensor._backward_hooks = backward_hooks
return tensor

def _rebuild_parameter(data, requires_grad, backward_hooks):
param = torch.nn.Parameter(data, requires_grad)
# NB: This line exists only for backwards compatibility; the
"
381,"def _import_dotted_name(name):
    components = name.split('.')
obj = __import__(components[0])
for component in components[1:]:
obj = getattr(obj, component)
","def _import_dotted_name(name):
    components = name.split(""."")
obj = __import__(components[0])
for component in components[1:]:
obj = getattr(obj, component)
"
382,"# Taken from python 3.5 docs
def _accumulate(iterable, fn=lambda x, y: x + y):
    'Return running totals'
# _accumulate([1,2,3,4,5]) --> 1 3 6 10 15
# _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120
it = iter(iterable)
","# Taken from python 3.5 docs
def _accumulate(iterable, fn=lambda x, y: x + y):
    ""Return running totals""
# _accumulate([1,2,3,4,5]) --> 1 3 6 10 15
# _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120
it = iter(iterable)
"
383,"flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(flat_indices, [torch.Tensor._indices(t) for t in tensors])
    values = torch._C._nn.unflatten_dense_tensors(flat_values, [torch.Tensor._values(t) for t in tensors])
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
","flat.
""""""
flat_indices, flat_values = flat
    indices = torch._C._nn.unflatten_dense_tensors(
        flat_indices, [torch.Tensor._indices(t) for t in tensors]
    )
    values = torch._C._nn.unflatten_dense_tensors(
        flat_values, [torch.Tensor._values(t) for t in tensors]
    )
outputs = []
for t, i, v in zip(tensors, indices, values):
outputs.append(t.new(i, v, t.size()))
"
384,"def annotate(ret, **kwargs):
def dec(fun):
fun.__annotations__ = dict(kwargs)
        fun.__annotations__['return'] = ret
return fun
return dec
","def annotate(ret, **kwargs):
def dec(fun):
fun.__annotations__ = dict(kwargs)
        fun.__annotations__[""return""] = ret
return fun

return dec
"
385,"device = torch.device(device)
device_idx: Optional[int] = None
if isinstance(device, torch.device):
        if not allow_cpu and device.type == 'cpu':
            raise ValueError('Expected a non cpu device, but got: {}'.format(device))
        device_idx = -1 if device.type == 'cpu' else device.index
if isinstance(device, int):
device_idx = device
if device_idx is None:
","device = torch.device(device)
device_idx: Optional[int] = None
if isinstance(device, torch.device):
        if not allow_cpu and device.type == ""cpu"":
            raise ValueError(""Expected a non cpu device, but got: {}"".format(device))
        device_idx = -1 if device.type == ""cpu"" else device.index
if isinstance(device, int):
device_idx = device
if device_idx is None:
"
386,"return tempfile.mkdtemp(suffix=os.path.basename(path))

def prepare_multiprocessing_environment(path: str) -> None:
pass
","return tempfile.mkdtemp(suffix=os.path.basename(path))
def prepare_multiprocessing_environment(path: str) -> None:
pass
"
387,"torch.Size([10, 9])
>>> torch.testing.assert_close(roundtrip, t, check_stride=False)
"""""".format(**common_args))
hfft = _add_docstr(_fft.fft_hfft, r""""""
hfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the one dimensional discrete Fourier transform of a Hermitian
","torch.Size([10, 9])
>>> torch.testing.assert_close(roundtrip, t, check_stride=False)
"""""".format(
        **common_args
    ),
)
hfft = _add_docstr(
    _fft.fft_hfft,
    r""""""
hfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor
Computes the one dimensional discrete Fourier transform of a Hermitian
"
388,"from typing import (
    Tuple, Optional, Union, Any, Sequence, TYPE_CHECKING
)
import torch
import torch.nn.functional as F
from ._lowrank import svd_lowrank, pca_lowrank
from .overrides import (
    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
    handle_torch_function)
from ._jit_internal import boolean_dispatch, List
from ._jit_internal import _overload as overload
from torch._autograd_functions import _LU
Tensor = torch.Tensor
from torch import _VF
__all__ = [
    'atleast_1d',
    'atleast_2d',
    'atleast_3d',
    'align_tensors',
    'broadcast_shapes',
    'broadcast_tensors',
    'cartesian_prod',
    'block_diag',
    'cdist',
    'chain_matmul',
    'einsum',
    'istft',
    'lu',
    'norm',
    'meshgrid',
    'pca_lowrank',
    'split',
    'stft',
    'svd_lowrank',
    'tensordot',
    'unique',
    'unique_consecutive',
]
","from typing import Tuple, Optional, Union, Any, Sequence, TYPE_CHECKING
import torch
import torch.nn.functional as F
from torch._autograd_functions import _LU

from ._jit_internal import boolean_dispatch, List
from ._jit_internal import _overload as overload
from ._lowrank import svd_lowrank, pca_lowrank
from .overrides import (
    has_torch_function,
    has_torch_function_unary,
    has_torch_function_variadic,
    handle_torch_function,
)
Tensor = torch.Tensor
from torch import _VF
__all__ = [
    ""atleast_1d"",
    ""atleast_2d"",
    ""atleast_3d"",
    ""align_tensors"",
    ""broadcast_shapes"",
    ""broadcast_tensors"",
    ""cartesian_prod"",
    ""block_diag"",
    ""cdist"",
    ""chain_matmul"",
    ""einsum"",
    ""istft"",
    ""lu"",
    ""norm"",
    ""meshgrid"",
    ""pca_lowrank"",
    ""split"",
    ""stft"",
    ""svd_lowrank"",
    ""tensordot"",
    ""unique"",
    ""unique_consecutive"",
]
"
389,"return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = None,
         win_length: Optional[int] = None, window: Optional[Tensor] = None,
         center: bool = True, pad_mode: str = 'reflect', normalized: bool = False,
         onesided: Optional[bool] = None,
         return_complex: Optional[bool] = None) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
","return _VF.meshgrid(tensors)  # type: ignore[attr-defined]
def stft(
    input: Tensor,
    n_fft: int,
    hop_length: Optional[int] = None,
    win_length: Optional[int] = None,
    window: Optional[Tensor] = None,
    center: bool = True,
    pad_mode: str = ""reflect"",
    normalized: bool = False,
    onesided: Optional[bool] = None,
    return_complex: Optional[bool] = None,
) -> Tensor:
r""""""Short-time Fourier transform (STFT).
.. warning::
"
390,"# There's no good way to use this type annotation without breaking JIT
# overloads. So leave untyped for mypy for now.
else:
@overload
def tensordot(a, b, dims: int = 2, out: Optional[torch.Tensor] = None):
pass
@overload
    def tensordot(a, b, dims: Tuple[List[int], List[int]], out: Optional[torch.Tensor] = None):  # noqa: F811
pass
@overload
    def tensordot(a, b, dims: List[List[int]], out: Optional[torch.Tensor] = None):  # noqa: F811
pass
@overload
    def tensordot(a, b, dims: torch.Tensor, out: Optional[torch.Tensor] = None):  # noqa: F811
pass
def tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None):  # noqa: F811
r""""""Returns a contraction of a and b over multiple dimensions.
","# There's no good way to use this type annotation without breaking JIT
# overloads. So leave untyped for mypy for now.
else:

@overload
def tensordot(a, b, dims: int = 2, out: Optional[torch.Tensor] = None):
pass
@overload
    def tensordot(
        a, b, dims: Tuple[List[int], List[int]], out: Optional[torch.Tensor] = None
    ):  # noqa: F811
pass
@overload
    def tensordot(
        a, b, dims: List[List[int]], out: Optional[torch.Tensor] = None
    ):  # noqa: F811
pass
@overload
    def tensordot(
        a, b, dims: torch.Tensor, out: Optional[torch.Tensor] = None
    ):  # noqa: F811
pass

def tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None):  # noqa: F811
r""""""Returns a contraction of a and b over multiple dimensions.
"
391,"return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2., compute_mode='use_mm_for_euclid_dist_if_necessary'):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
","return torch._C._VariableFunctions.block_diag(tensors)  # type: ignore[attr-defined]
def cdist(x1, x2, p=2.0, compute_mode=""use_mm_for_euclid_dist_if_necessary""):
# type: (Tensor, Tensor, float, str) -> (Tensor)
r""""""Computes batched the p-norm distance between each pair of the two collections of row vectors.
"
392,"""""""
if has_torch_function_variadic(x1, x2):
return handle_torch_function(
            cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode)
    if compute_mode == 'use_mm_for_euclid_dist_if_necessary':
return _VF.cdist(x1, x2, p, None)  # type: ignore[attr-defined]
    elif compute_mode == 'use_mm_for_euclid_dist':
return _VF.cdist(x1, x2, p, 1)  # type: ignore[attr-defined]
    elif compute_mode == 'donot_use_mm_for_euclid_dist':
return _VF.cdist(x1, x2, p, 2)  # type: ignore[attr-defined]
else:
raise ValueError(f""{compute_mode} is not a valid value for compute_mode"")
def atleast_1d(*tensors):
r""""""
Returns a 1-dimensional view of each input tensor with zero dimensions.
","""""""
if has_torch_function_variadic(x1, x2):
return handle_torch_function(
            cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode
        )
    if compute_mode == ""use_mm_for_euclid_dist_if_necessary"":
return _VF.cdist(x1, x2, p, None)  # type: ignore[attr-defined]
    elif compute_mode == ""use_mm_for_euclid_dist"":
return _VF.cdist(x1, x2, p, 1)  # type: ignore[attr-defined]
    elif compute_mode == ""donot_use_mm_for_euclid_dist"":
return _VF.cdist(x1, x2, p, 2)  # type: ignore[attr-defined]
else:
raise ValueError(f""{compute_mode} is not a valid value for compute_mode"")

def atleast_1d(*tensors):
r""""""
Returns a 1-dimensional view of each input tensor with zero dimensions.
"
393,"...
ValueError: foo
""""""
        assert isinstance(result, Exception), f""{result} is of type {type(result)}, not an Exception.""
def raise_error(fut_result):
raise fut_result
","...
ValueError: foo
""""""
        assert isinstance(
            result, Exception
        ), f""{result} is of type {type(result)}, not an Exception.""
def raise_error(fut_result):
raise fut_result
"
394,"def _download_url_to_file(url, dst, hash_prefix=None, progress=True):
    warnings.warn('torch.hub._download_url_to_file has been renamed to\
torch.hub.download_url_to_file to be a public API,\
            _download_url_to_file will be removed in after 1.3 release')
download_url_to_file(url, dst, hash_prefix, progress)
","def _download_url_to_file(url, dst, hash_prefix=None, progress=True):
    warnings.warn(
        ""torch.hub._download_url_to_file has been renamed to\
torch.hub.download_url_to_file to be a public API,\
            _download_url_to_file will be removed in after 1.3 release""
    )
download_url_to_file(url, dst, hash_prefix, progress)
"
395,"def _legacy_zip_load(filename, model_dir, map_location):
    warnings.warn('Falling back to the old format < 1.6. This support will be '
                  'deprecated in favor of default zipfile format introduced in 1.6. '
                  'Please redo torch.save() to save it in the new zipfile format.')
# Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.
#       We deliberately don't handle tarfile here since our legacy serialization format was in tar.
#       E.g. resnet18-5c106cde.pth which is widely used.
with zipfile.ZipFile(filename) as f:
members = f.infolist()
if len(members) != 1:
            raise RuntimeError('Only one file(not dir) is allowed in the zipfile')
f.extractall(model_dir)
extraced_name = members[0].filename
extracted_file = os.path.join(model_dir, extraced_name)
return torch.load(extracted_file, map_location=map_location)
def load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None):
r""""""Loads the Torch serialized object at the given URL.
If downloaded file is a zip file, it will be automatically
","def _legacy_zip_load(filename, model_dir, map_location):
    warnings.warn(
        ""Falling back to the old format < 1.6. This support will be ""
        ""deprecated in favor of default zipfile format introduced in 1.6. ""
        ""Please redo torch.save() to save it in the new zipfile format.""
    )
# Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.
#       We deliberately don't handle tarfile here since our legacy serialization format was in tar.
#       E.g. resnet18-5c106cde.pth which is widely used.
with zipfile.ZipFile(filename) as f:
members = f.infolist()
if len(members) != 1:
            raise RuntimeError(""Only one file(not dir) is allowed in the zipfile"")
f.extractall(model_dir)
extraced_name = members[0].filename
extracted_file = os.path.join(model_dir, extraced_name)
return torch.load(extracted_file, map_location=map_location)
def load_state_dict_from_url(
    url,
    model_dir=None,
    map_location=None,
    progress=True,
    check_hash=False,
    file_name=None,
):
r""""""Loads the Torch serialized object at the given URL.
If downloaded file is a zip file, it will be automatically
"
396,"Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
"""""" + fr""""""
.. note:: This function is computed using :func:`torch.lu`.
{common_notes[""sync_note""]}
"""""" + r""""""
.. note:: The determinant can be recovered as `sign * exp(logabsdet)`.
","Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
""""""
    + fr""""""
.. note:: This function is computed using :func:`torch.lu`.
{common_notes[""sync_note""]}
""""""
    + r""""""
.. note:: The determinant can be recovered as `sign * exp(logabsdet)`.
"
397,"The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.
"""""" + fr""""""
.. note:: {common_notes[""sync_note""]}
"""""" + r""""""
.. seealso::
","The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.
""""""
    + fr""""""
.. note:: {common_notes[""sync_note""]}
""""""
    + r""""""
.. seealso::
"
398,">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""")
eigvalsh = _add_docstr(_linalg.linalg_eigvalsh, r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
",">>> L, Q = torch.linalg.eigh(A)
>>> torch.dist(Q @ torch.diag_embed(L) @ Q.transpose(-2, -1).conj(), A)
tensor(1.5423e-15, dtype=torch.float64)
"""""",
)
eigvalsh = _add_docstr(
    _linalg.linalg_eigvalsh,
    r""""""
linalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor
Computes the eigenvalues of a complex Hermitian or real symmetric matrix.
"
399,"tensor([ 3.7417, 11.2250])
>>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""")
vector_norm = _add_docstr(_linalg.linalg_vector_norm, r""""""
linalg.vector_norm(A, ord=2, dim=None, keepdim=False, *, dtype=None, out=None) -> Tensor
Computes a vector norm.
","tensor([ 3.7417, 11.2250])
>>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])
(tensor(3.7417), tensor(11.2250))
"""""",
)
vector_norm = _add_docstr(
    _linalg.linalg_vector_norm,
    r""""""
linalg.vector_norm(A, ord=2, dim=None, keepdim=False, *, dtype=None, out=None) -> Tensor
Computes a vector norm.
"
400,"torch.batch_norm_update_stats: lambda input, running_mean, running_var, momentum: -1,
torch.bernoulli: lambda input, generator=None, out=None: -1,
torch.bilinear: lambda input1, input2, weight, bias: -1,
        torch.binary_cross_entropy_with_logits: (lambda input, target, weight=None, size_average=None, reduce=None,
                                                 reduction='mean', pos_weight=None: -1),
torch.bincount: lambda input, weights=None, minlength=0: -1,
torch.binomial: lambda count, prob, generator=None: -1,
torch.bitwise_and: lambda input, other, out=None: -1,
","torch.batch_norm_update_stats: lambda input, running_mean, running_var, momentum: -1,
torch.bernoulli: lambda input, generator=None, out=None: -1,
torch.bilinear: lambda input1, input2, weight, bias: -1,
        torch.binary_cross_entropy_with_logits: (
            lambda input, target, weight=None, size_average=None, reduce=None, reduction=""mean"", pos_weight=None: -1
        ),
torch.bincount: lambda input, weights=None, minlength=0: -1,
torch.binomial: lambda count, prob, generator=None: -1,
torch.bitwise_and: lambda input, other, out=None: -1,
"
401,"torch.isreal: lambda tensor: -1,
torch.isposinf: lambda input, out=None: -1,
torch.isneginf: lambda input, out=None: -1,
        torch.instance_norm: (lambda input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps,
                              cudnn_enabled: -1),
torch.int_repr: lambda input: -1,
torch.inverse: lambda input, out=None: -1,
torch.linalg.inv: lambda input, out=None: -1,
","torch.isreal: lambda tensor: -1,
torch.isposinf: lambda input, out=None: -1,
torch.isneginf: lambda input, out=None: -1,
        torch.instance_norm: (
            lambda input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps, cudnn_enabled: -1
        ),
torch.int_repr: lambda input: -1,
torch.inverse: lambda input, out=None: -1,
torch.linalg.inv: lambda input, out=None: -1,
"
402,"torch.less: lambda input, other, out=None: -1,
torch.lu: lambda A, pivot=True, get_infos=False, out=None: -1,
torch.lu_solve: lambda b, LU_data, LU_pivots, out=None: -1,
        torch.margin_ranking_loss: lambda input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean': -1,  # type: ignore[attr-defined]  # noqa: B950
torch.masked_fill: lambda input, mask, value: -1,
torch.masked_scatter: lambda input, mask, source: -1,
torch.masked_select: lambda input, mask, out=None: -1,
","torch.less: lambda input, other, out=None: -1,
torch.lu: lambda A, pivot=True, get_infos=False, out=None: -1,
torch.lu_solve: lambda b, LU_data, LU_pivots, out=None: -1,
        torch.margin_ranking_loss: lambda input1, input2, target, margin=0, size_average=None, reduce=None, reduction=""mean"": -1,  # type: ignore[attr-defined]  # noqa: B950
torch.masked_fill: lambda input, mask, value: -1,
torch.masked_scatter: lambda input, mask, source: -1,
torch.masked_select: lambda input, mask, out=None: -1,
"
403,"torch.trapezoid: lambda y, x=None, dim=-1: -1,
torch.triangular_solve: lambda input, A, upper=True, transpose=False, unitriangular=False: -1,
torch.tril: lambda input, diagonal=0, out=None: -1,
        torch.triplet_margin_loss: (lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False,

                                    size_average=None, reduce=None, reduction='mean': -1),
torch.triu: lambda input, diagonal=0, out=None: -1,
torch.true_divide: lambda input, other: -1,
torch.trunc: lambda input, out=None: -1,
","torch.trapezoid: lambda y, x=None, dim=-1: -1,
torch.triangular_solve: lambda input, A, upper=True, transpose=False, unitriangular=False: -1,
torch.tril: lambda input, diagonal=0, out=None: -1,
        torch.triplet_margin_loss: (
            lambda anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction=""mean"": -1
        ),
torch.triu: lambda input, diagonal=0, out=None: -1,
torch.true_divide: lambda input, other: -1,
torch.trunc: lambda input, out=None: -1,
"
404,"The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that
the cycles will continue until the profiling is finished.
""""""
def schedule_fn(step: int) -> ProfilerAction:
assert step >= 0
if step < skip_first:
","The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that
the cycles will continue until the profiling is finished.
""""""

def schedule_fn(step: int) -> ProfilerAction:
assert step >= 0
if step < skip_first:
"
405,"import difflib
import os
import io
import shutil
import struct
import sys
import torch
import tarfile
import tempfile
import warnings
from contextlib import closing, contextmanager
from ._utils import _import_dotted_name
from ._six import string_classes as _string_classes
from torch._sources import get_source_lines_and_file
from torch.types import Storage
from typing import Any, BinaryIO, cast, Dict, Optional, Type, Tuple, Union, IO
import copyreg
import pickle
import pathlib
DEFAULT_PROTOCOL = 2
LONG_SIZE = struct.Struct('=l').size
INT_SIZE = struct.Struct('=i').size
SHORT_SIZE = struct.Struct('=h').size
MAGIC_NUMBER = 0x1950a86a20f9469cfc6c
PROTOCOL_VERSION = 1001
STORAGE_KEY_SEPARATOR = ','
class SourceChangeWarning(Warning):
pass
","import copyreg
import difflib
import io
import os
import pathlib
import pickle
import shutil
import struct
import sys
import tarfile
import tempfile
import warnings
from contextlib import closing, contextmanager
from typing import Any, BinaryIO, cast, Dict, Optional, Type, Tuple, Union, IO

import torch
from torch._sources import get_source_lines_and_file
from torch.types import Storage

from ._six import string_classes as _string_classes
from ._utils import _import_dotted_name
DEFAULT_PROTOCOL = 2
LONG_SIZE = struct.Struct(""=l"").size
INT_SIZE = struct.Struct(""=i"").size
SHORT_SIZE = struct.Struct(""=h"").size
MAGIC_NUMBER = 0x1950A86A20F9469CFC6C
PROTOCOL_VERSION = 1001
STORAGE_KEY_SEPARATOR = "",""

class SourceChangeWarning(Warning):
pass
"
406,"_package_registry.sort()
def check_module_version_greater_or_equal(module, req_version_tuple, error_if_malformed=True):
    '''
Check if a module's version satisfies requirements
Usually, a module's version string will be like 'x.y.z', which would be represented
","_package_registry.sort()
def check_module_version_greater_or_equal(
    module, req_version_tuple, error_if_malformed=True
):
    """"""
Check if a module's version satisfies requirements
Usually, a module's version string will be like 'x.y.z', which would be represented
"
407,"class _open_zipfile_reader(_opener):
def __init__(self, name_or_buffer) -> None:
        super(_open_zipfile_reader, self).__init__(torch._C.PyTorchFileReader(name_or_buffer))
class _open_zipfile_writer_file(_opener):
def __init__(self, name) -> None:
        super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
def __exit__(self, *args) -> None:
self.file_like.write_end_of_file()
","class _open_zipfile_reader(_opener):
def __init__(self, name_or_buffer) -> None:
        super(_open_zipfile_reader, self).__init__(
            torch._C.PyTorchFileReader(name_or_buffer)
        )
class _open_zipfile_writer_file(_opener):
def __init__(self, name) -> None:
        super(_open_zipfile_writer_file, self).__init__(
            torch._C.PyTorchFileWriter(str(name))
        )
def __exit__(self, *args) -> None:
self.file_like.write_end_of_file()
"
408,"storage_views = pickle_module.load(f, **pickle_load_args)
for target_cdata, root_cdata, offset, size in storage_views:
root = deserialized_objects[root_cdata]
                    deserialized_objects[target_cdata] = root[offset:offset + size]
            tar.extract('tensors', path=tmpdir)
            with open(os.path.join(tmpdir, 'tensors'), 'rb', 0) as f:
num_tensors = pickle_module.load(f, **pickle_load_args)
for _ in range(num_tensors):
args = pickle_module.load(f, **pickle_load_args)
key, storage_id, original_tensor_type = args
storage = deserialized_objects[storage_id]
tensor_type = storage_to_tensor_type(storage)
                    ndim, = struct.unpack('<i', f.read(4))
# skip next 4 bytes; legacy encoding treated ndim as 8 bytes
f.read(4)
                    size = struct.unpack(f'<{ndim}q', f.read(8 * ndim))
                    stride = struct.unpack(f'<{ndim}q', f.read(8 * ndim))
                    storage_offset, = struct.unpack('<q', f.read(8))
tensor = tensor_type().set_(storage, storage_offset, size, stride)
deserialized_objects[key] = tensor
            pickle_file = tar.extractfile('pickle')
unpickler = pickle_module.Unpickler(pickle_file, **pickle_load_args)
unpickler.persistent_load = persistent_load
result = unpickler.load()
","storage_views = pickle_module.load(f, **pickle_load_args)
for target_cdata, root_cdata, offset, size in storage_views:
root = deserialized_objects[root_cdata]
                    deserialized_objects[target_cdata] = root[offset : offset + size]
            tar.extract(""tensors"", path=tmpdir)
            with open(os.path.join(tmpdir, ""tensors""), ""rb"", 0) as f:
num_tensors = pickle_module.load(f, **pickle_load_args)
for _ in range(num_tensors):
args = pickle_module.load(f, **pickle_load_args)
key, storage_id, original_tensor_type = args
storage = deserialized_objects[storage_id]
tensor_type = storage_to_tensor_type(storage)
                    (ndim,) = struct.unpack(""<i"", f.read(4))
# skip next 4 bytes; legacy encoding treated ndim as 8 bytes
f.read(4)
                    size = struct.unpack(f""<{ndim}q"", f.read(8 * ndim))
                    stride = struct.unpack(f""<{ndim}q"", f.read(8 * ndim))
                    (storage_offset,) = struct.unpack(""<q"", f.read(8))
tensor = tensor_type().set_(storage, storage_offset, size, stride)
deserialized_objects[key] = tensor
            pickle_file = tar.extractfile(""pickle"")
unpickler = pickle_module.Unpickler(pickle_file, **pickle_load_args)
unpickler.persistent_load = persistent_load
result = unpickler.load()
"
409,"return torch._sparse_mm(mat1, mat2)
def sum(input: Tensor, dim: DimOrDims = None,
        dtype: Optional[DType] = None) -> Tensor:
r""""""
Returns the sum of each row of the sparse tensor :attr:`input` in the given
dimensions :attr:`dim`. If :attr:`dim` is a list of dimensions,
","return torch._sparse_mm(mat1, mat2)
def sum(input: Tensor, dim: DimOrDims = None, dtype: Optional[DType] = None) -> Tensor:
r""""""
Returns the sum of each row of the sparse tensor :attr:`input` in the given
dimensions :attr:`dim`. If :attr:`dim` is a list of dimensions,
"
410,"\infty & x < 0
\end{cases}
\end{align}
"""""" + """"""
Args:
input (Tensor): the input tensor.
","\end{cases}
\end{align}
""""""
    + """"""
Args:
input (Tensor): the input tensor.
"
411,".. math::
\mathrm{erfcx}(x) = e^{x^2} \mathrm{erfc}(x)
"""""" + r""""""
"""""" + r""""""
Args:
{input}
",".. math::
\mathrm{erfcx}(x) = e^{x^2} \mathrm{erfc}(x)
""""""
    + r""""""
""""""
    + r""""""
Args:
{input}
"
412,".. math::
\mathrm{erfinv}(\mathrm{erf}(x)) = x
"""""" + r""""""
Args:
{input}
",".. math::
\mathrm{erfinv}(\mathrm{erf}(x)) = x
""""""
    + r""""""
Args:
{input}
"
413,"1 - \text{eps} & \text{if } x_{i} > 1 - \text{eps}
\end{cases}
\end{align}
"""""" + r""""""
Args:
{input}
eps (float, optional): the epsilon for input clamp bound. Default: ``None``
","1 - \text{eps} & \text{if } x_{i} > 1 - \text{eps}
\end{cases}
\end{align}
""""""
    + r""""""
Args:
{input}
eps (float, optional): the epsilon for input clamp bound. Default: ``None``
"
414,"tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
>>> torch.special.expit(t)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])
"""""".format(**common_args))

exp2 = _add_docstr(_special.special_exp2,
                   r""""""
exp2(input, *, out=None) -> Tensor
Computes the base two exponential function of :attr:`input`.
","tensor([ 0.9213,  1.0887, -0.8858, -1.7683])
>>> torch.special.expit(t)
tensor([ 0.7153,  0.7481,  0.2920,  0.1458])
"""""".format(
        **common_args
    ),
)

exp2 = _add_docstr(
    _special.special_exp2,
    r""""""
exp2(input, *, out=None) -> Tensor
Computes the base two exponential function of :attr:`input`.
"
415,".. math::
y_{i} = 2^{x_{i}}
"""""" + r""""""
Args:
{input}
",".. math::
y_{i} = 2^{x_{i}}
""""""
    + r""""""
Args:
{input}
"
416,"Similar to SciPy's `scipy.special.xlogy`.
"""""" + r""""""
Args:
input (Number or Tensor) : Multiplier
","Similar to SciPy's `scipy.special.xlogy`.
""""""
    + r""""""
Args:
input (Number or Tensor) : Multiplier
"
417,"Example::
>>> torch.special.i1(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])
"""""".format(**common_args))

i1e = _add_docstr(_special.special_i1e,
                  r""""""
i1e(input, *, out=None) -> Tensor
Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
","Example::
>>> torch.special.i1(torch.arange(5, dtype=torch.float32))
tensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])
"""""".format(
        **common_args
    ),
)

i1e = _add_docstr(
    _special.special_i1e,
    r""""""
i1e(input, *, out=None) -> Tensor
Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below)
for each element of :attr:`input`.
"
418,"import torch
from typing import Any, List, Sequence, Tuple, Union
import builtins
# Convenience aliases for common composite types that we need
# to talk about in PyTorch
","import builtins
from typing import Any, List, Sequence, Tuple, Union
import torch
# Convenience aliases for common composite types that we need
# to talk about in PyTorch
"
419,"from ..overrides import has_torch_function, handle_torch_function
from . import functional
from . import forward_ad
from . import saved_variable_default_hooks as graph
__all__ = ['Variable', 'Function', 'backward', 'grad_mode']
","from ..overrides import has_torch_function, handle_torch_function
from . import functional
from . import forward_ad
from . import graph
__all__ = ['Variable', 'Function', 'backward', 'grad_mode']
"
420,"import collections
from contextlib import contextmanager
from dataclasses import dataclass
from typing import (
Dict,
List
","import collections
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import (
Dict,
List
"
421,"default_placeholder_observer, default_weight_observer)
from .fake_quantize import (FakeQuantize, default_fake_quant,
default_per_channel_weight_fake_quant,
                            default_weight_fake_quant)
import torch
import torch.nn as nn
","default_placeholder_observer, default_weight_observer)
from .fake_quantize import (FakeQuantize, default_fake_quant,
default_per_channel_weight_fake_quant,
                            default_weight_fake_quant, default_fused_act_fake_quant, default_fused_wt_fake_quant,
                            FusedMovingAvgObsFakeQuantize, default_fused_per_channel_wt_fake_quant)
import torch
import torch.nn as nn
"
422,"from torch._C._autograd import (DeviceType, ProfilerActivity, ProfilerState, ProfilerConfig, ProfilerEvent,
_enable_profiler_legacy, _disable_profiler_legacy, _profiler_enabled,
_enable_record_function, _set_empty_test_observer, kineto_available,
                                _supported_kineto_activities, _add_metadata_json, SavedTensor,
                                _register_default_hooks, _reset_default_hooks)
if kineto_available():
from torch._C._autograd import (_ProfilerResult, _KinetoEvent,
","from torch._C._autograd import (DeviceType, ProfilerActivity, ProfilerState, ProfilerConfig, ProfilerEvent,
_enable_profiler_legacy, _disable_profiler_legacy, _profiler_enabled,
_enable_record_function, _set_empty_test_observer, kineto_available,
                                _supported_kineto_activities, _add_metadata_json, SavedTensor)
if kineto_available():
from torch._C._autograd import (_ProfilerResult, _KinetoEvent,
"
423,"op_to_type_to_weight_extraction_fn: Optional[Dict[str, Dict[Callable, Callable]]] = None,
) -> NSResultsType:
torch._C._log_api_usage_once(""quantization_api._numeric_suite_fx.extract_weights"")
    base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops()
type_a_related_to_b = \
get_type_a_related_to_b(base_name_to_sets_of_related_ops)
","op_to_type_to_weight_extraction_fn: Optional[Dict[str, Dict[Callable, Callable]]] = None,
) -> NSResultsType:
torch._C._log_api_usage_once(""quantization_api._numeric_suite_fx.extract_weights"")
    if base_name_to_sets_of_related_ops is None:
        base_name_to_sets_of_related_ops = \
            get_base_name_to_sets_of_related_ops()
type_a_related_to_b = \
get_type_a_related_to_b(base_name_to_sets_of_related_ops)
"
424,"def step(self, gradients: List[Optional[Tensor]]):
params = self.param_group['params']
grads = []
momentum_buffer_list: List[Optional[Tensor]] = []
lr = self.defaults['lr']
","def step(self, gradients: List[Optional[Tensor]]):
params = self.param_group['params']
        params_with_grad = []
grads = []
momentum_buffer_list: List[Optional[Tensor]] = []
lr = self.defaults['lr']
"
425,"return [SETUP_ANY_REQUIRES_GRAD.substitute(
args_with_derivatives=[arg.name for arg in args_with_derivatives]), ]
    def emit_check_inplace() -> List[str]:
        if not inplace:
            return []
        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]
    def emit_fw_derivatives() -> List[str]:
content: List[str] = []
for derivative in fw_derivatives:
            res = derivative.var_name
            if f.func.name.name.inplace:
                # TODO update this when inplace namings are unified
                res = ""self""

assert derivative.required_inputs_fw_grad is not None
requires_fw_grad = "" || "".join([FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name)
for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])
","return [SETUP_ANY_REQUIRES_GRAD.substitute(
args_with_derivatives=[arg.name for arg in args_with_derivatives]), ]
    def get_any_has_forward_grad_name(var_name: str) -> str:
        return f'_any_has_forward_grad_{var_name}'
    def emit_any_has_forward_grad() -> List[str]:
content: List[str] = []
for derivative in fw_derivatives:
assert derivative.required_inputs_fw_grad is not None
requires_fw_grad = "" || "".join([FW_DERIVATIVE_CHECK_TEMPLATE.substitute(req_inp=inp.name)
for inp in differentiable_inputs if inp.name in derivative.required_inputs_fw_grad])
"
426,"'formula has been defined for it. This case should only happen for function that '
'take a single TensorList as input. All other cases are not supported right now.')
requires_fw_grad = ""true""
unpacked_arguments = """"
for inp in differentiable_inputs:
if inp.name in derivative.required_inputs_fw_grad:
unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp=inp.name)
if inp.name in (derivative.required_inputs_primal or []):
unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp=inp.name)
if inplace:
is_inplace_str = ""true""
","'formula has been defined for it. This case should only happen for function that '
'take a single TensorList as input. All other cases are not supported right now.')
requires_fw_grad = ""true""

            content.append(f""auto {get_any_has_forward_grad_name(derivative.var_name)} = {requires_fw_grad};\n""
                           f""(void){get_any_has_forward_grad_name(derivative.var_name)};"")

        return content

    def emit_check_inplace() -> List[str]:
        if not inplace:
            return []
        return [f'check_inplace({arg.name}, _any_requires_grad);' for arg in differentiable_outputs]

    def emit_fw_derivatives() -> List[str]:
        content: List[str] = []
        for derivative in fw_derivatives:
            res = derivative.var_name
            if f.func.name.name.inplace:
                # TODO update this when inplace namings are unified
                res = ""self""

            assert derivative.required_inputs_fw_grad is not None

unpacked_arguments = """"
for inp in differentiable_inputs:
if inp.name in derivative.required_inputs_fw_grad:
unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp=inp.name)
if inp.name in (derivative.required_inputs_primal or []):
unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp=inp.name)
            if derivative.required_original_self_value:
                unpacked_arguments += FW_DERIVATIVE_DEFINED_GRAD_TEMPLATE.substitute(inp=""original_self"")
                unpacked_arguments += FW_DERIVATIVE_DEFINED_PRIMAL_TEMPLATE.substitute(inp=""original_self"")
            elif inplace and derivative.is_reusing_outplace_formula:
                # The gradient wasn't already cloned, do it if grad mode is enabled
                unpacked_arguments += ""self_t = GradMode::is_enabled() ? self_t.clone() : self_t;""
if inplace:
is_inplace_str = ""true""
"
427,"if dtype is None:
dtype = data.type().scalarType()
dtype = sym_help.scalar_type_to_onnx.index(sym_help.cast_pytorch_to_onnx[dtype])
return g.op(""Cast"", data, to_i=sym_help.scalar_type_to_onnx[dtype])
def as_tensor(g, data, dtype=None, device=None):
","if dtype is None:
dtype = data.type().scalarType()
dtype = sym_help.scalar_type_to_onnx.index(sym_help.cast_pytorch_to_onnx[dtype])
        if sym_help._is_list(data) and (sym_help._is_tensor_list(data) or sym_help._is_scalar_list(data)):
            data = g.op(""ConcatFromSequence"", data, axis_i=0, new_axis_i=1)
return g.op(""Cast"", data, to_i=sym_help.scalar_type_to_onnx[dtype])
def as_tensor(g, data, dtype=None, device=None):
"
428,".. code:: python
        U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()
Args:
input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more
",".. code:: python
        U = torch.linalg.cholesky(A).transpose(-2, -1).conj()

    This transform will produce equivalent results for all valid (symmetric positive definite) inputs.
Args:
input (Tensor): the input tensor :math:`A` of size :math:`(*, n, n)` where `*` is zero or more
"
429,".. note::
Consider using :func:`torch.linalg.lstsq` if possible for multiplying a matrix on the left by
    the the pseudoinverse, as::
torch.linalg.lstsq(A, B).solution == A.pinv() @ B
",".. note::
Consider using :func:`torch.linalg.lstsq` if possible for multiplying a matrix on the left by
    the pseudoinverse, as::
torch.linalg.lstsq(A, B).solution == A.pinv() @ B
"
430,">>> torch.allclose(A @ X, B)
True
Broadcasting::

>>> A = torch.randn(2, 3, 3)
>>> b = torch.randn(3, 1)
>>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3, 1)
",">>> torch.allclose(A @ X, B)
True
>>> A = torch.randn(2, 3, 3)
>>> b = torch.randn(3, 1)
>>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3, 1)
"
431,"Examples::
    >>> a = torch.eye(4 * 6).reshape((4, 6, 8, 3))
    >>> ainv = torch.linalg.tensorinv(a, ind=2)
    >>> ainv.shape
torch.Size([8, 3, 4, 6])
    >>> b = torch.randn(4, 6)
    >>> torch.allclose(torch.tensordot(ainv, b), torch.linalg.tensorsolve(a, b))
True
    >>> a = torch.randn(4, 4)
    >>> a_tensorinv = torch.linalg.tensorinv(a, ind=1)
    >>> a_inv = torch.inverse(a)
    >>> torch.allclose(a_tensorinv, a_inv)
True
"""""")
","Examples::
    >>> A = torch.eye(4 * 6).reshape((4, 6, 8, 3))
    >>> Ainv = torch.linalg.tensorinv(A, ind=2)
    >>> Ainv.shape
torch.Size([8, 3, 4, 6])
    >>> B = torch.randn(4, 6)
    >>> torch.allclose(torch.tensordot(Ainv, B), torch.linalg.tensorsolve(A, B))
True
    >>> A = torch.randn(4, 4)
    >>> Atensorinv = torch.linalg.tensorinv(A, ind=1)
    >>> Ainv = torch.linalg.inverse(A)
    >>> torch.allclose(Atensorinv, Ainv)
True
"""""")
"
432,"async def quick(self, files: List[str]) -> CommandResult:
return await shell_cmd(
            [sys.executable, ""tools/linter/clang_tidy"", ""--paths""]
)
async def full(self) -> None:
await shell_cmd(
            [sys.executable, ""tools/linter/clang_tidy""] + self.common_options
)
","async def quick(self, files: List[str]) -> CommandResult:
return await shell_cmd(
            [sys.executable, ""-m"", ""tools.linter.clang_tidy"", ""--paths""]
 [os.path.join(REPO_ROOT, f) for f in files]
 self.common_options,
)
async def full(self) -> None:
await shell_cmd(
            [sys.executable, ""-m"", ""tools.linter.clang_tidy""] + self.common_options,
            redirect=False,
)
"
433,"to modify the behavior of the join hook at run time; all
:class:`_Joinable` instances sharing the same join context
manager are forwarded the same value for ``kwargs``.
""""""
return _ZeROJoinHook(self)
","to modify the behavior of the join hook at run time; all
:class:`_Joinable` instances sharing the same join context
manager are forwarded the same value for ``kwargs``.

        This hook does not support any keyword arguments; i.e. ``kwargs`` is
        unused.
""""""
return _ZeROJoinHook(self)
"
434,"if backend_key is not None and autograd_key is not None:
backend_dispatch_key: DispatchKey = backend_key
autograd_dispatch_key: DispatchKey = autograd_key
generated_comment = 'Autogenerated file by gen_backend_stubs.py. Do not edit directly!'
fm.write_with_template(f'{backend_dispatch_key}NativeFunctions.h', 'DispatchKeyNativeFunctions.h', lambda: {
'generated_comment': generated_comment,
'cpp_namespace': cpp_namespace,
# Convert to a set first to remove duplicate kernel names.
# Backends are allowed to repeat kernel names; only generate the declaration once!
'dispatch_declarations': list(set(concatMap(
","if backend_key is not None and autograd_key is not None:
backend_dispatch_key: DispatchKey = backend_key
autograd_dispatch_key: DispatchKey = autograd_key
        class_name = backend_indices[backend_dispatch_key].native_function_class_name()
        assert class_name is not None
generated_comment = 'Autogenerated file by gen_backend_stubs.py. Do not edit directly!'
fm.write_with_template(f'{backend_dispatch_key}NativeFunctions.h', 'DispatchKeyNativeFunctions.h', lambda: {
'generated_comment': generated_comment,
'cpp_namespace': cpp_namespace,
            'class_name': class_name,
# Convert to a set first to remove duplicate kernel names.
# Backends are allowed to repeat kernel names; only generate the declaration once!
'dispatch_declarations': list(set(concatMap(
"
435,"from typing import List, Optional, Union
from setuptools.command.build_ext import build_ext
from pkg_resources import packaging  # type: ignore[attr-defined]
IS_WINDOWS = sys.platform == 'win32'
LIB_EXT = '.pyd' if IS_WINDOWS else '.so'
","from typing import List, Optional, Union
from setuptools.command.build_ext import build_ext
from pkg_resources import packaging, parse_version  # type: ignore[attr-defined]
IS_WINDOWS = sys.platform == 'win32'
LIB_EXT = '.pyd' if IS_WINDOWS else '.so'
"
436,"object_type_dict[all_qat_mappings[k]] = v
return qconfig_dict
def insert_observer(
node: Node,
observer: torch.quantization.ObserverBase,
","object_type_dict[all_qat_mappings[k]] = v
return qconfig_dict
def update_qconfig_for_fusion(
    model: GraphModule,
    qconfig_dict: Any,
) -> Any:
    """"""
    Update the qconfig_dict to account for fused modules such as LinearReLU.
    """"""
    object_type_dict = qconfig_dict.get(""object_type"", None)
    if object_type_dict is None:
        return qconfig_dict

    modules = dict(model.named_modules())

    for node in model.graph.nodes:
        if node.op == 'call_module':
            module_type = type(modules[str(node.target)])
            if module_type not in list(DEFAULT_OP_LIST_TO_FUSER_METHOD.values()):
                continue

            for ops, fuser in DEFAULT_OP_LIST_TO_FUSER_METHOD.items():
                if module_type == fuser:
                    fused_qconfig = object_type_dict.get(ops[0], None)

                    # Raise an error if the modules in the fused module have
                    # different qconfigs specified in the qconfig_dict
                    for op in ops:
                        if object_type_dict.get(op, None) != fused_qconfig:
                            raise LookupError(""During fusion, we need to specify the same "" +
                                              f""qconfigs for both modules in {module_type}."")

                    if fused_qconfig is not None:
                        object_type_dict[module_type] = fused_qconfig

    return qconfig_dict

def insert_observer(
node: Node,
observer: torch.quantization.ObserverBase,
"
437,"for arg in tensor_args:
if arg in args_with_derivatives:
continue
            name = arg.name
            if info and name in info.non_differentiable_arg_names:
continue
            if name == 'output':
# Double-backwards definitions sometimes take in 'input' and
# 'output', but only define the derivative for input.
continue
            body.append(f'check_no_requires_grad({name}, ""{name}"");')
return body
def save_variables(
","for arg in tensor_args:
if arg in args_with_derivatives:
continue
            arg_name = arg.name
            if info and arg_name in info.non_differentiable_arg_names:
continue
            if arg_name == 'output':
# Double-backwards definitions sometimes take in 'input' and
# 'output', but only define the derivative for input.
continue
            body.append(f'check_no_requires_grad({arg_name}, ""{arg_name}"", ""{name}"");')
return body
def save_variables(
"
438,"arg_a = return_first_non_observer_node(node_a_arg, gm_a)
node_a_arg_copy = _copy_node_from_a_to_c(arg_a, gm_a, gm_b, graph_c)
new_args.append(node_a_arg_copy)
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
","arg_a = return_first_non_observer_node(node_a_arg, gm_a)
node_a_arg_copy = _copy_node_from_a_to_c(arg_a, gm_a, gm_b, graph_c)
new_args.append(node_a_arg_copy)
        elif isinstance(node_a_arg, (int, float)):
            new_args.append(node_a_arg)
        elif isinstance(node_a_arg, (list, tuple)):
            for el in node_a_arg:
                assert not isinstance(el, Node), \
                    ""handling of Node inside list is not implemented""
            new_args.append(node_a_arg)
else:
raise AssertionError(
f""handling for arg of type {type(node_a_arg)} is not implemented"")
"
439,"add_docstr_all('index_put',
r""""""
index_put(tensor1, indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
`tensor1` corresponds to `self` in :meth:`torch.Tensor.index_put_`.
"""""")
add_docstr_all('index_select',
","add_docstr_all('index_put',
r""""""
index_put(indices, values, accumulate=False) -> Tensor
Out-place version of :meth:`~Tensor.index_put_`.
"""""")
add_docstr_all('index_select',
"
440,"message = ""import of {} halted; "" ""None in sys.modules"".format(name)
raise ModuleNotFoundError(message, name=name)
return module
def _gcd_import(self, name, package=None, level=0):
","message = ""import of {} halted; "" ""None in sys.modules"".format(name)
raise ModuleNotFoundError(message, name=name)
        # To handle https://github.com/pytorch/pytorch/issues/57490, where os's
        # creation of os.path via the hacking of sys.modules is not import friendly
        if name == ""os"":
            self.modules[""os.path""] = cast(Any, module).path

return module
def _gcd_import(self, name, package=None, level=0):
"
441,">>> # All tensors below are of torch.cfloat dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]
>>> tensor_list
[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1
>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)
",">>> # All tensors below are of torch.cfloat dtype.
>>> # We have 2 process groups, 2 ranks.
        >>> tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]
>>> tensor_list
[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1
>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)
"
442,"'legacy_th_headers': '',
'external_backend_headers': f'''#include ""{output_dir}/{backend_key}NativeFunctions.h""
#include <torch_xla/csrc/aten_xla_type_default.h>''',
'DispatchKey': dispatch_key,
'dispatch_namespace': dispatch_key.lower(),
'dispatch_namespaced_definitions': list(concatMap(
","'legacy_th_headers': '',
'external_backend_headers': f'''#include ""{output_dir}/{backend_key}NativeFunctions.h""
#include <torch_xla/csrc/aten_xla_type_default.h>''',
                'namespaced_headers': '',
'DispatchKey': dispatch_key,
'dispatch_namespace': dispatch_key.lower(),
'dispatch_namespaced_definitions': list(concatMap(
"
443,"backend_key: Optional[DispatchKey] = None
if len(supported) > 0:
        with context(f'The provided value for ""backend"" must be a valid DispatchKey, but got {backend}.'):
backend_key = DispatchKey.parse(backend)
backend_idx = create_backend_index(supported, backend_key)
","backend_key: Optional[DispatchKey] = None
if len(supported) > 0:
        with context(lambda: f'The provided value for ""backend"" must be a valid DispatchKey, but got {backend}.'):
backend_key = DispatchKey.parse(backend)
backend_idx = create_backend_index(supported, backend_key)
"
444,"BATCHNORM = 4
def optimize_for_mobile(
        script_module,
        optimization_blocklist: Set[MobileOptimizerType] = None,
        preserved_methods: List[AnyStr] = None,
        backend: str = 'CPU'):
""""""
Args:
script_module: An instance of torch script module with type of ScriptModule.
","BATCHNORM = 4
def optimize_for_mobile(
        script_module: torch.jit.ScriptModule,
        optimization_blocklist: Optional[Set[MobileOptimizerType]] = None,
        preserved_methods: Optional[List[AnyStr]] = None,
        backend: str = 'CPU') -> torch.jit.RecursiveScriptModule:
""""""
Args:
script_module: An instance of torch script module with type of ScriptModule.
"
445,"if isinstance(qhandler, CatQuantizeHandler):
adjust_observers_for_cat(node, model, modules)
else:  # output
maybe_insert_observers_before_graph_output(
node, output_quantized_idxs,
","if isinstance(qhandler, CatQuantizeHandler):
adjust_observers_for_cat(node, model, modules)
                            if isinstance(qhandler, CustomModuleQuantizeHandler):
                                swap_custom_module_to_observed(node, qconfig, modules, prepare_custom_config_dict)

else:  # output
maybe_insert_observers_before_graph_output(
node, output_quantized_idxs,
"
446,")
# This class exists solely for Transformer; it has an annotation stating
# that bias is never None, which appeases TorchScript
class _LinearWithBias(Linear):
    bias: Tensor  # type: ignore[assignment]

    def __init__(self, in_features: int, out_features: int) -> None:
        super().__init__(in_features, out_features, bias=True)
class Bilinear(Module):
",")
# This class exists solely to avoid triggering an obscure error when scripting
# an improperly quantized attention layer. See this issue for details:
# https://github.com/pytorch/pytorch/issues/58969
# TODO: fail fast on quantization API usage error, then remove this class
# and replace uses of it with plain Linear
class NonDynamicallyQuantizableLinear(Linear):
    def __init__(self, in_features: int, out_features: int, bias: bool = True,
                 device=None, dtype=None) -> None:
        super().__init__(in_features, out_features, bias=bias,
                         device=device, dtype=dtype)
class Bilinear(Module):
"
447,"# Reentering the quantized zone
attn_output = self.quant_attn_output(attn_output)
        attn_output = self.out_proj(attn_output)
attn_output_weights = self.quant_attn_output_weights(attn_output_weights)
if need_weights:
","# Reentering the quantized zone
attn_output = self.quant_attn_output(attn_output)
        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969
        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]
attn_output_weights = self.quant_attn_output_weights(attn_output_weights)
if need_weights:
"
448,"nn.Conv2d: nnqat.Conv2d,
nn.Conv3d: nnqat.Conv3d,
nn.Linear: nnqat.Linear,
    nn.modules.linear._LinearWithBias: nnqat.Linear,
# Intrinsic modules:
nni.ConvBn1d: nniqat.ConvBn1d,
nni.ConvBn2d: nniqat.ConvBn2d,
","nn.Conv2d: nnqat.Conv2d,
nn.Conv3d: nnqat.Conv3d,
nn.Linear: nnqat.Linear,
    nn.modules.linear.NonDynamicallyQuantizableLinear: nnqat.Linear,
# Intrinsic modules:
nni.ConvBn1d: nniqat.ConvBn1d,
nni.ConvBn2d: nniqat.ConvBn2d,
"
449,"def __init__(
self,
remote_device: str,
        module_cls: Type[nn.Module],
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
","def __init__(
self,
remote_device: str,
        module_cls: nn.Module,
args: Tuple = None,
kwargs: Dict[str, Any] = None,
_module_interface_cls: Any = None,
"
450,"WORKFLOW_DATA = [
    IOSJob(XCODE_VERSION, ArchVariant(""x86_64""), is_org_member_context=False),
    IOSJob(XCODE_VERSION, ArchVariant(""x86_64"", ""lite_interpreter""), is_org_member_context=False, extra_props={
""lite_interpreter"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"")),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""metal""), extra_props={""use_metal"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""lite_interpreter""), extra_props={
""lite_interpreter"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""custom""), extra_props={""op_list"": ""mobilenetv2.yaml""}),
]
","WORKFLOW_DATA = [
    IOSJob(XCODE_VERSION, ArchVariant(""x86_64""), is_org_member_context=False, extra_props={
""lite_interpreter"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""x86_64"", ""full_jit""), is_org_member_context=False, extra_props={
        ""lite_interpreter"": miniutils.quote(str(int(False)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64""), extra_props={
        ""lite_interpreter"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""metal""), extra_props={
        ""use_metal"": miniutils.quote(str(int(True))),
        ""lite_interpreter"": miniutils.quote(str(int(True)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""full_jit""), extra_props={
        ""lite_interpreter"": miniutils.quote(str(int(False)))}),
    IOSJob(XCODE_VERSION, ArchVariant(""arm64"", ""custom""), extra_props={
        ""op_list"": ""mobilenetv2.yaml"",
""lite_interpreter"": miniutils.quote(str(int(True)))}),
]
"
451,"we don't know the value of the proxy, but a custom tracer can attach more
information to the graph node using create_node and can choose to return an iterator.
""""""
        raise TraceError('Proxy object cannot be iterated. '
                         'This can be attempted when used in a for loop or as a *args or **kwargs function argument.')
def keys(self, obj: 'Proxy') -> Any:
""""""Called when a proxy object is has the keys() method called.
","we don't know the value of the proxy, but a custom tracer can attach more
information to the graph node using create_node and can choose to return an iterator.
""""""
        raise TraceError('Proxy object cannot be iterated. This can be '
                         'attempted when the Proxy is used in a loop or'
                         ' as a *args or **kwargs function argument. '
                         'See the torch.fx docs on pytorch.org for a '
                         'more detailed explanation of what types of '
                         'control flow can be traced, and check out the'
                         ' Proxy docstring for help troubleshooting '
                         'Proxy iteration errors')
def keys(self, obj: 'Proxy') -> Any:
""""""Called when a proxy object is has the keys() method called.
"
452,"def supported_activities():
""""""
    Returns a set of supported profiler activities
""""""
return torch.autograd.supported_kineto_activities()
","def supported_activities():
""""""
    Returns a set of supported profiler tracing activities.

    Note: profiler uses CUPTI library to trace on-device CUDA kernels.
    In case when CUDA is enabled but CUPTI is not available, passing
    ``ProfilerActivity.CUDA`` to profiler results in using the legacy CUDA
    profiling code (same as in the legacy ``torch.autograd.profiler``).
    This, in turn, results in including CUDA time in the profiler table output,
    but not in the JSON trace.
""""""
return torch.autograd.supported_kineto_activities()
"
453,"'tan', 'pow', 'rsqrt', 'tanh', 'tanh_backward', 'asinh', 'acosh', 'atanh', 'take', 'fill_',
'exp', 'nonzero', 'mean', 'inverse', 'solve', 'linalg_cholesky', 'addcmul', 'addcdiv',
'matrix_exp', 'linalg_eigh', 'cholesky_solve', 'linalg_qr', '_svd_helper', '_fft_c2c', '_fft_r2c',
    'linalg_solve', 'sqrt', 'stack', 'gather', 'index_select', 'index_add_', 'linalg_inv',
'l1_loss_backward', 'baddbmm', 'addbmm', 'addmm', 'addmv', 'addr', 'linalg_householder_product',
'constant_pad_nd', 'reflection_pad1d', 'reflection_pad2d', 'linalg_cholesky_ex', 'linalg_eig',
'reflection_pad1d_backward', 'reflection_pad2d_backward', 'symeig',
","'tan', 'pow', 'rsqrt', 'tanh', 'tanh_backward', 'asinh', 'acosh', 'atanh', 'take', 'fill_',
'exp', 'nonzero', 'mean', 'inverse', 'solve', 'linalg_cholesky', 'addcmul', 'addcdiv',
'matrix_exp', 'linalg_eigh', 'cholesky_solve', 'linalg_qr', '_svd_helper', '_fft_c2c', '_fft_r2c',
    'linalg_solve', 'sqrt', 'stack', 'gather', 'index_select', 'index_add_', 'linalg_inv', 'linalg_inv_ex',
'l1_loss_backward', 'baddbmm', 'addbmm', 'addmm', 'addmv', 'addr', 'linalg_householder_product',
'constant_pad_nd', 'reflection_pad1d', 'reflection_pad2d', 'linalg_cholesky_ex', 'linalg_eig',
'reflection_pad1d_backward', 'reflection_pad2d_backward', 'symeig',
"
454,"RECORD_AND_SAVE = 3
def schedule(*, wait: int, warmup: int, active: int, repeat: int = 0) -> Callable:
""""""
    Returns a callable that can be used as profiler ``schedule`` argument. The profiler will wait for ``wait`` steps, then
    do the warmup for the next ``warmup`` steps, then
    do the active recording for the next ``active`` steps and then
    repeat the cycle starting with the next step. The number of cycles is specified by the ``repeat`` parameter.
    When the parameter's value is zero, the cycles will continue until the profiling is finished.
""""""
def schedule_fn(step: int) -> ProfilerAction:
assert step >= 0
num_steps = wait + warmup + active
if repeat > 0 and step / num_steps >= repeat:
return ProfilerAction.NONE
","RECORD_AND_SAVE = 3
def schedule(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -> Callable:
""""""
    Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip
    the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,
    then do the active recording for the next ``active`` steps and then repeat the cycle starting with ``wait`` steps.
    The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that
    the cycles will continue until the profiling is finished.
""""""
def schedule_fn(step: int) -> ProfilerAction:
assert step >= 0
        if step < skip_first:
            return ProfilerAction.NONE
        else:
            step -= skip_first
num_steps = wait + warmup + active
if repeat > 0 and step / num_steps >= repeat:
return ProfilerAction.NONE
"
455,"add_docstr_all('std',
r""""""
std(dim=None, unbiased=True, keepdim=False) -> Tensor
See :func:`torch.std`
"""""")
","add_docstr_all('std',
r""""""
std(dim, unbiased=True, keepdim=False) -> Tensor

See :func:`torch.std`

.. function:: std(unbiased=True) -> Tensor
   :noindex:
See :func:`torch.std`
"""""")
"
456,".. function:: normal(mean, std=1.0, *, out=None) -> Tensor
Similar to the function above, but the standard-deviations are shared among
all drawn elements.
Args:
",".. function:: normal(mean, std=1.0, *, out=None) -> Tensor
Similar to the function above, but the standard deviations are shared among
all drawn elements.
Args:
"
457,"else:
self.message = f""Process failed with exitcode {self.exitcode}""
def _set_no_reply_file(self):
self.error_file = _NOT_AVAILABLE
self.error_file_data = _EMPTY_ERROR_DATA
","else:
self.message = f""Process failed with exitcode {self.exitcode}""
    def _get_error_data(self, error_file_data: Dict[str, Any]) -> Tuple[str, int]:
        message = error_file_data[""message""]
        if isinstance(message, str):
            timestamp = error_file_data.get(""timestamp"", 0)
        else:
            timestamp = int(message[""extraInfo""][""timestamp""])
        return (message, timestamp)

def _set_no_reply_file(self):
self.error_file = _NOT_AVAILABLE
self.error_file_data = _EMPTY_ERROR_DATA
"
458,"f""child error file ({rootcause_error_file}) does not have field `message`. \n""
f""cannot override error code: {error_code}""
)
else:
rootcause_error[""message""][""errorCode""] = error_code
","f""child error file ({rootcause_error_file}) does not have field `message`. \n""
f""cannot override error code: {error_code}""
)
                elif isinstance(rootcause_error[""message""], str):
                    log.warning(
                        f""child error file ({rootcause_error_file}) has a new message format. \n""
                        f""skipping error code override""
                    )
else:
rootcause_error[""message""][""errorCode""] = error_code
"
459,"if found_tensor:
n.meta['tensor_meta'] = meta
        n.meta['type'] = create_type_hint(result)
return result
def propagate(self, *args):
","if found_tensor:
n.meta['tensor_meta'] = meta
        n.meta['type'] = type(result)
return result
def propagate(self, *args):
"
460,"for node_a_k, node_a_kwarg in node_a.kwargs.items():
if isinstance(node_a_kwarg, Node):
kwarg_a = return_first_non_observer_node(node_a_kwarg, gm_a)
            node_a_kwarg_copy = _copy_node_from_a_to_c(kwarg_a, gm_a, gm_b, graph_c)
new_kwargs[node_a_k] = node_a_kwarg_copy
else:
new_kwargs[node_a_k] = node_a_kwarg
","for node_a_k, node_a_kwarg in node_a.kwargs.items():
if isinstance(node_a_kwarg, Node):
kwarg_a = return_first_non_observer_node(node_a_kwarg, gm_a)
            kwarg_a_copy_name = \
                get_new_attr_name_with_prefix(kwarg_a.name + '_shadow_copy_')(gm_b)
            kwarg_a_obj = getattr_from_fqn(gm_a, kwarg_a.target)  # type: ignore[arg-type]
            setattr(gm_b, kwarg_a_copy_name, kwarg_a_obj.detach())
            node_a_kwarg_copy = graph_c.create_node(
                'get_attr', kwarg_a_copy_name, (), {}, kwarg_a_copy_name)
new_kwargs[node_a_k] = node_a_kwarg_copy
else:
new_kwargs[node_a_k] = node_a_kwarg
"
461,"# TODO(next): make this code handle matching by what is before the base op
if node_a.op != node_b.op:
        # for now, comparing call_module to call_function is not supported
        # this can be added later if needed
        return SubgraphTypeRelationship.NOT_RELATED
    if node_a.op == 'call_function':
if node_a.target == node_b.target:
node_a_has_prev = subgraph_a.base_op_node == subgraph_a.start_node
node_b_has_prev = subgraph_b.base_op_node == subgraph_b.start_node
","# TODO(next): make this code handle matching by what is before the base op
if node_a.op != node_b.op:
        if not (
            node_a.op in ('call_function', 'call_method') and
            node_b.op in ('call_function', 'call_method')
        ):
            return SubgraphTypeRelationship.NOT_RELATED
    if node_a.op in ('call_function', 'call_method'):
if node_a.target == node_b.target:
node_a_has_prev = subgraph_a.base_op_node == subgraph_a.start_node
node_b_has_prev = subgraph_b.base_op_node == subgraph_b.start_node
"
462,"error_file = os.path.join(clogdir, ""error.json"")
error_files[local_rank] = error_file
envs[local_rank][""TORCHELASTIC_ERROR_FILE""] = error_file
context: PContext
","error_file = os.path.join(clogdir, ""error.json"")
error_files[local_rank] = error_file
        log.info(f""Setting worker{local_rank} reply file to: {error_file}"")
envs[local_rank][""TORCHELASTIC_ERROR_FILE""] = error_file
context: PContext
"
463,"Shape:
 Input: :math:`(N, *)` where :math:`*` means any number of additional
dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Var: :math:`(N, 1)` or :math:`(N, *)`, same shape as the input
 Output: scalar if :attr:`reduction` is ``'mean'`` (default) or
          ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N)`
Examples::

>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
","Shape:
dimensions
        - Target: :math:`(N, *)`, same shape as the input, or same shape as the input
          but with one dimension equal to 1 (to allow for broadcasting)
        - Var: :math:`(N, *)`, same shape as the input, or same shape as the input but
          with one dimension equal to 1, or same shape as the input but with one fewer
          dimension (to allow for broadcasting)
          ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
          shape as the input
Examples::
>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
"
464,">>> output = loss(input, target, var)
>>> output.backward()

>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
",">>> output = loss(input, target, var)
>>> output.backward()
>>> loss = nn.GaussianNLLLoss()
>>> input = torch.randn(5, 2, requires_grad=True)
>>> target = torch.randn(5, 2)
"
465,"type_definition_body=emit_inplace_or_view_body(fn),
)
def inplace_or_view_method_registration(fn: NativeFunctionWithDifferentiabilityInfo) -> Optional[str]:
f = fn.func
if get_view_info(fn) is None and (not modifies_arguments(f) or is_foreach_op(str(f.func.name))):
","type_definition_body=emit_inplace_or_view_body(fn),
)
@with_native_function_with_differentiability_info
def inplace_or_view_method_registration(fn: NativeFunctionWithDifferentiabilityInfo) -> Optional[str]:
f = fn.func
if get_view_info(fn) is None and (not modifies_arguments(f) or is_foreach_op(str(f.func.name))):
"
466,"OptionalCType, tensorT, scalarT, layoutT,
deviceT, boolT, scalarTypeT)
from tools.codegen.api import cpp
from typing import Union, Sequence, List, Optional
","OptionalCType, tensorT, scalarT, layoutT,
deviceT, boolT, scalarTypeT)
from tools.codegen.api import cpp
from tools.codegen import local
from typing import Union, Sequence, List, Optional
"
467,"stdout_rd = stdout_redirects[local_rank]
stderr_rd = stderr_redirects[local_rank]
    stdout_cm = redirect_stdout(stdout_rd) if stdout_rd else _nullcontext()
    stderr_cm = redirect_stderr(stderr_rd) if stderr_rd else _nullcontext()
for k, v in env_.items():
os.environ[k] = v
","stdout_rd = stdout_redirects[local_rank]
stderr_rd = stderr_redirects[local_rank]
    stdout_cm = get_std_cm(stdout_rd, redirect_stdout)
    stderr_cm = get_std_cm(stderr_rd, redirect_stderr)
for k, v in env_.items():
os.environ[k] = v
"
468,"py_ast = ast.parse(dedent_src)
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, False)
    return build_class_def(ctx, py_ast.body[0], methods, properties, self_name)
def normalize_source_lines(sourcelines: List[str]) -> List[str]:
","py_ast = ast.parse(dedent_src)
leading_whitespace_len = len(source.split('\n', 1)[0]) - len(dedent_src.split('\n', 1)[0])
ctx = SourceContext(source, filename, file_lineno, leading_whitespace_len, False)
    class_ast = py_ast.body[0]
    assert isinstance(class_ast, ast.ClassDef)
    assigns = get_class_assigns(ctx, class_ast)

    return build_class_def(ctx, class_ast, methods, properties, self_name, assigns)
def normalize_source_lines(sourcelines: List[str]) -> List[str]:
"
469,"return quantizer.quantized_graph.create_node(
""call_function"", torch.nn.functional.relu, tuple(relu_args), relu_kwargs)
else:
                        return quantizer.quantized_graph.node_copy(node, load_arg(quantized=None))
@register_quant_pattern(torch.nn.BatchNorm2d)
@register_quant_pattern(torch.nn.BatchNorm3d)
","return quantizer.quantized_graph.create_node(
""call_function"", torch.nn.functional.relu, tuple(relu_args), relu_kwargs)
else:
                        return quantizer.quantized_graph.node_copy(node, load_arg(quantized=False))
@register_quant_pattern(torch.nn.BatchNorm2d)
@register_quant_pattern(torch.nn.BatchNorm3d)
"
470,"examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
.. note::
    An earlier version of the API in ``torch.autograd`` module is considered legacy and will be deprecated.
'''
","examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.
.. note::
    An earlier version of the API in :mod:`torch.autograd` module is considered legacy and will be deprecated.
'''
"
471,"# For each user-defined class that subclasses ScriptModule, this meta-class:
# (1) finds all the methods annotated with @script_method in a ScriptModule and
#     removes them from the class attributes
# (2) puts a wrapper around the class's __init__ method to recusively compile
#     all of the script_methods with the module after the original __init__ has
#     run. This has to occur after the user-defined __init__ so that submodules and
#     parameters are initialized _before_ the script compiler resolve references to
","# For each user-defined class that subclasses ScriptModule, this meta-class:
# (1) finds all the methods annotated with @script_method in a ScriptModule and
#     removes them from the class attributes
# (2) puts a wrapper around the class's __init__ method to recursively compile
#     all of the script_methods with the module after the original __init__ has
#     run. This has to occur after the user-defined __init__ so that submodules and
#     parameters are initialized _before_ the script compiler resolve references to
"
472,"observed_nodes: Set[Node] = set()
copy_nodes: Set[Node] = set()
non_tensor_input_binary_op_nodes: Set[Node] = set()
app_to_remove: Set[Node] = set()
env: Dict[Any, Any] = {}
","observed_nodes: Set[Node] = set()
copy_nodes: Set[Node] = set()
non_tensor_input_binary_op_nodes: Set[Node] = set()
    unmatched_nodes: Set[Node] = set()
app_to_remove: Set[Node] = set()
env: Dict[Any, Any] = {}
"
473,"def _no_error_file_warning_msg(rank: int, failure: ProcessFailure) -> str:
msg = [
        ""CHILD PROCESS FAILED WITH NO ERROR_FILE""
        f""Child process {failure.pid} (local_rank {rank}) FAILED (exitcode {failure.exitcode})""
f""Error msg: {failure.message}"",
f""Without writing an error file to {failure.error_file}."",
""While this DOES NOT affect the correctness of your application,"",
","def _no_error_file_warning_msg(rank: int, failure: ProcessFailure) -> str:
msg = [
        ""CHILD PROCESS FAILED WITH NO ERROR_FILE"",
        f""Child process {failure.pid} (local_rank {rank}) FAILED (exitcode {failure.exitcode})"",
f""Error msg: {failure.message}"",
f""Without writing an error file to {failure.error_file}."",
""While this DOES NOT affect the correctness of your application,"",
"
474,"EmbeddingBag also supports per-sample weights as an argument to the forward
pass. This scales the output of the Embedding before performing a weighted
    reduction as specified by ``mode``. If :attr:`per_sample_weights`` is passed, the
only supported ``mode`` is ``""sum""``, which computes a weighted sum according to
:attr:`per_sample_weights`.
","EmbeddingBag also supports per-sample weights as an argument to the forward
pass. This scales the output of the Embedding before performing a weighted
    reduction as specified by ``mode``. If :attr:`per_sample_weights` is passed, the
only supported ``mode`` is ``""sum""``, which computes a weighted sum according to
:attr:`per_sample_weights`.
"
475,"operand_id = self.jitval_operand_map[jitval]
return (operand_id, self.operands[operand_id])
def get_tensor_operand_or_constant(self, jitval):
operand_id = self.jitval_operand_map.get(jitval)
if operand_id is None:
","operand_id = self.jitval_operand_map[jitval]
return (operand_id, self.operands[operand_id])
    def get_tensor_operand_by_jitval_fixed_size(self, jitval):
        op_id, oper = self.get_tensor_operand_by_jitval(jitval)
        for s in oper.shape:
            if s <= 0:
                # TODO: Improve this error message, possibly after converting
                # many callsites to support flexible size.
                raise Exception(""Flexible size is not supported for this operand."")
        return op_id, oper

def get_tensor_operand_or_constant(self, jitval):
operand_id = self.jitval_operand_map.get(jitval)
if operand_id is None:
"
476,"scale=raw_weight.q_scale(),
zero_point=raw_weight.q_zero_point() + 128)
weight_scale = unsigned_weight.q_scale()
        _, image_oper = self.get_tensor_operand_by_jitval(jit_image)
bias_scale = image_oper.scale * weight_scale
int_bias = torch.quantize_per_tensor(raw_bias, bias_scale, 0, torch.qint32)
bias_id = self.add_tensor_operand_for_weight(int_bias)
","scale=raw_weight.q_scale(),
zero_point=raw_weight.q_zero_point() + 128)
weight_scale = unsigned_weight.q_scale()
        _, image_oper = self.get_tensor_operand_by_jitval_fixed_size(jit_image)
bias_scale = image_oper.scale * weight_scale
int_bias = torch.quantize_per_tensor(raw_bias, bias_scale, 0, torch.qint32)
bias_id = self.add_tensor_operand_for_weight(int_bias)
"
477,"api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
    inplace_view_body.append(THROW_IF_VARIABLETYPE_ON)
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
","api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
"
478,"(void)_any_requires_grad;
"""""")
SETUP_ASSERT_NO_INFERENCE_TENSOR = CodeTemplate(""""""\
assert_no_inference_tensor( ${tensor_args} );
"""""")

SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
","(void)_any_requires_grad;
"""""")
SETUP_DERIVATIVE = CodeTemplate(""""""\
if (_any_requires_grad) {
${setup}
"
479,"api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
","api_name = sig_group.faithful_signature.name()
else:
api_name = sig_group.signature.name()
    inplace_view_body.append(THROW_IF_VARIABLETYPE_ON)
if modifies_arguments(f):  # inplace op
inplace_view_body.append(INPLACE_REDISPATCH.substitute(
api_name=api_name,
"
480,"if self.dispatch_key is DispatchKey.Meta:
class_name = f""structured_{meta.name(self.g)}_meta_{k.name}""
parent_class = f""at::meta::{meta.name(self.g)}""
            elif self.dispatch_key is DispatchKey.DefaultBackend:
# TODO: dedup this branch
class_name = f""structured_{meta.name(self.g)}_default_backend_{k.name}""
parent_class = f""at::meta::{meta.name(self.g)}""
","if self.dispatch_key is DispatchKey.Meta:
class_name = f""structured_{meta.name(self.g)}_meta_{k.name}""
parent_class = f""at::meta::{meta.name(self.g)}""
            elif self.dispatch_key is DispatchKey.CompositeExplicitAutograd:
# TODO: dedup this branch
class_name = f""structured_{meta.name(self.g)}_default_backend_{k.name}""
parent_class = f""at::meta::{meta.name(self.g)}""
"
481,"DispatchKey.CPU,
DispatchKey.CUDA,
DispatchKey.CompositeImplicitAutograd,
        DispatchKey.DefaultBackend,
}
if options.backend_whitelist:
dispatch_keys = [k for k in dispatch_keys if is_generic_dispatch_key(k) or str(k) in options.backend_whitelist]
","DispatchKey.CPU,
DispatchKey.CUDA,
DispatchKey.CompositeImplicitAutograd,
        DispatchKey.CompositeExplicitAutograd,
}
if options.backend_whitelist:
dispatch_keys = [k for k in dispatch_keys if is_generic_dispatch_key(k) or str(k) in options.backend_whitelist]
"
482,"elif not structured and structured_delegate is None:
dispatch[DispatchKey.CompositeImplicitAutograd] = cpp.name(func)
        assert not (DispatchKey.DefaultBackend in dispatch and DispatchKey.CompositeImplicitAutograd in dispatch), \
            ""cannot specify both DefaultBackend and CompositeImplicitAutograd on a single kernel; each "" \
""strictly subsumes the other.  If you wanted to provide an explicit autograd "" \
            ""implementation, specify DefaultBackend; otherwise specify CompositeImplicitAutograd only""
e.pop('__line__')
assert not e, f""leftover entries: {e}""
","elif not structured and structured_delegate is None:
dispatch[DispatchKey.CompositeImplicitAutograd] = cpp.name(func)
        assert not (DispatchKey.CompositeExplicitAutograd in dispatch and DispatchKey.CompositeImplicitAutograd in dispatch), \
            ""cannot specify both CompositeExplicitAutograd and CompositeImplicitAutograd on a single kernel; each "" \
""strictly subsumes the other.  If you wanted to provide an explicit autograd "" \
            ""implementation, specify CompositeExplicitAutograd; otherwise specify CompositeImplicitAutograd only""
e.pop('__line__')
assert not e, f""leftover entries: {e}""
"
483,"def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
        # Supported combinations are:
        # quant_type | activation (compute_type) | weight
        #  static       quint8                      qint8

        # tuple (activation_dtype, weight_dtype, compute_dtype)
        # these are supported types for common binary ops like add/mul etc.
        all_bop_dtypes = [
            (torch.quint8, torch.qint8, None),
            (torch.float16, torch.float16, None),
        ]
        float16_dtypes = [
            (torch.float16, torch.float16, None)
        ]
        supported_dtypes : Dict[Union[Callable, str], List[Tuple[torch.dtype, torch.dtype, None]]] = {
            operator.add: all_bop_dtypes,
            torch.add: all_bop_dtypes,
            operator.mul: all_bop_dtypes,
            torch.mul: all_bop_dtypes,
            torch.bmm: float16_dtypes,
            torch.sub: float16_dtypes,
            operator.sub: float16_dtypes,
            torch.div: float16_dtypes,
            operator.truediv: float16_dtypes,
            torch.sum: float16_dtypes
        }
qconfig = quantizer.qconfig_map[node.name]
dtypes = get_qconfig_dtypes(qconfig)
# leave the op unquantized if the dtype combination is not supported
        if dtypes not in supported_dtypes[self.binary_op]:
warnings.warn(
""dtype combination: {} is not ""
""supported by {} ""
                ""supported dtype combinations are: {}"".format(dtypes, self.binary_op, supported_dtypes[self.binary_op]))
if self.relu_node:
op_out = quantizer.quantized_graph.node_copy(self.binary_op_node, load_arg(quantized=False))
relu_args = [op_out]
","def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
qconfig = quantizer.qconfig_map[node.name]
dtypes = get_qconfig_dtypes(qconfig)
# leave the op unquantized if the dtype combination is not supported
        if dtypes not in binary_op_supported_dtypes[self.binary_op]:
warnings.warn(
""dtype combination: {} is not ""
""supported by {} ""
                ""supported dtype combinations are: {}"".format(dtypes, self.binary_op, binary_op_supported_dtypes[self.binary_op]))
if self.relu_node:
op_out = quantizer.quantized_graph.node_copy(self.binary_op_node, load_arg(quantized=False))
relu_args = [op_out]
"
484,"warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '
'Passing a tensor of different shape won\'t change the number of '
'iterations executed (and might lead to errors or silently give '
                          'incorrect results).', category=RuntimeWarning)
return iter(self.unbind(0))
def __hash__(self):
","warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '
'Passing a tensor of different shape won\'t change the number of '
'iterations executed (and might lead to errors or silently give '
                          'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)
return iter(self.unbind(0))
def __hash__(self):
"
485,"parent_name, name = _parent_name(node.target)
setattr(modules[parent_name], name, new_module)
def fuse(model: torch.nn.Module, inplace=True) -> torch.nn.Module:
""""""
Fuses convolution/BN layers for inference purposes. Will deepcopy your
model by default, but can modify the model inplace as well.
","parent_name, name = _parent_name(node.target)
setattr(modules[parent_name], name, new_module)
def fuse(model: torch.nn.Module, inplace=False) -> torch.nn.Module:
""""""
Fuses convolution/BN layers for inference purposes. Will deepcopy your
model by default, but can modify the model inplace as well.
"
486,"square_avgs.append(state['square_avg'])
acc_deltas.append(state['acc_delta'])
                lr, rho, eps, weight_decay = group['lr'], group['rho'], group['eps'], group['weight_decay']

state['step'] += 1
F.adadelta(params_with_grad,
","square_avgs.append(state['square_avg'])
acc_deltas.append(state['acc_delta'])
state['step'] += 1
F.adadelta(params_with_grad,
"
487,"max_exp_avg_sqs = []
state_steps = []
amsgrad = group['amsgrad']
for p in group['params']:
if p.grad is None:
","max_exp_avg_sqs = []
state_steps = []
amsgrad = group['amsgrad']
            beta1, beta2 = group['betas']
for p in group['params']:
if p.grad is None:
"
488,"'DistributedSampler', 'Dataset', 'IterableDataset', 'TensorDataset',
'ConcatDataset', 'ChainDataset', 'BufferedShuffleDataset', 'Subset',
'random_split', 'DataLoader', '_DatasetKind', 'get_worker_info',
           'IterDataPipe']
","'DistributedSampler', 'Dataset', 'IterableDataset', 'TensorDataset',
'ConcatDataset', 'ChainDataset', 'BufferedShuffleDataset', 'Subset',
'random_split', 'DataLoader', '_DatasetKind', 'get_worker_info',
           'IterDataPipe', 'functional_datapipe']
"
489,"from ._importlib import _normalize_line_endings, _resolve_name, _sanity_check, _calc___package__, \
_normalize_path
from ._mock_zipreader import MockZipReader
from ._mangling import PackageMangler, demangle
from .importer import Importer
","from ._importlib import _normalize_line_endings, _resolve_name, _sanity_check, _calc___package__, \
_normalize_path
from ._file_structure_representation import _create_folder_from_file_list, Folder
from ._glob_group import GlobPattern
from ._mock_zipreader import MockZipReader
from ._mangling import PackageMangler, demangle
from .importer import Importer
"
490,"@register_quant_pattern(torch.nn.functional.elu)
class ELU(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                debug: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
","@register_quant_pattern(torch.nn.functional.elu)
class ELU(QuantizeHandler):
def convert(self, quantizer: QuantizerCls, node: Node, load_arg: Callable,
                is_reference: bool = False,
convert_custom_config_dict: Dict[str, Any] = None) -> Node:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
"
491,"input_quantized_idxs, output_quantized_idxs, please
see docs for prepare_fx for details
""""""
    return _convert_fx(graph_module, debug, convert_custom_config_dict, is_standalone_module=True)
","input_quantized_idxs, output_quantized_idxs, please
see docs for prepare_fx for details
""""""
    return _convert_fx(graph_module, is_reference, convert_custom_config_dict, is_standalone_module=True)
"
492,"assert all(i.device.type != 'cpu' for i in inputs), (
'Gather function not implemented for CPU tensors'
)
        target_device = _get_device_index(target_device, True)
        ctx.target_device = target_device
ctx.dim = dim
ctx.input_gpus = tuple(i.get_device() for i in inputs)
if all(t.dim() == 0 for t in inputs) and dim == 0:
","assert all(i.device.type != 'cpu' for i in inputs), (
'Gather function not implemented for CPU tensors'
)
        if (target_device == 'cpu'):
            ctx.target_device = 'cpu'
        else:
            target_device = _get_device_index(target_device, True)
            ctx.target_device = target_device
ctx.dim = dim
ctx.input_gpus = tuple(i.get_device() for i in inputs)
if all(t.dim() == 0 for t in inputs) and dim == 0:
"
493,"return helper_name in self._registry
sys.modules[__name__] = HelperWrapper(sys.modules[__name__])
","return helper_name in self._registry
# pyre-fixme[6]: incompatible parameter type: expected ModuleType, got HelperWrapper
sys.modules[__name__] = HelperWrapper(sys.modules[__name__])
"
494,"return wrapper
Tags.TRAIN_ONLY = [Tags.EXCLUDE_FROM_PREDICTION, Tags.EXCLUDE_FROM_EVAL,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
Tags.EVAL_ONLY = [Tags.EXCLUDE_FROM_PREDICTION, Tags.EXCLUDE_FROM_TRAIN,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
Tags.PREDICTION_ONLY = [Tags.EXCLUDE_FROM_TRAIN, Tags.EXCLUDE_FROM_EVAL,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
","return wrapper
# pyre-fixme[16]: Tags has no attribute `TRAIN_ONLY`
Tags.TRAIN_ONLY = [Tags.EXCLUDE_FROM_PREDICTION, Tags.EXCLUDE_FROM_EVAL,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
# pyre-fixme[16]: Tags has no attribute `EVAL_ONLY`
Tags.EVAL_ONLY = [Tags.EXCLUDE_FROM_PREDICTION, Tags.EXCLUDE_FROM_TRAIN,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
# pyre-fixme[16]: Tags has no attribute `PREDICTION_ONLY`
Tags.PREDICTION_ONLY = [Tags.EXCLUDE_FROM_TRAIN, Tags.EXCLUDE_FROM_EVAL,
Tags.EXCLUDE_FROM_ACCUMULATE_PRED]
"
495,"add_docstr_all('index_fill_',
r""""""
index_fill_(dim, index, val) -> Tensor
Fills the elements of the :attr:`self` tensor with value :attr:`val` by
selecting the indices in the order given in :attr:`index`.
Args:
dim (int): dimension along which to index
index (LongTensor): indices of :attr:`self` tensor to fill in
    val (float): the value to fill with
Example::
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
","add_docstr_all('index_fill_',
r""""""
index_fill_(dim, index, value) -> Tensor
Fills the elements of the :attr:`self` tensor with value :attr:`value` by
selecting the indices in the order given in :attr:`index`.
Args:
dim (int): dimension along which to index
index (LongTensor): indices of :attr:`self` tensor to fill in
    value (float): the value to fill with
Example::
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
"
496,"from typing import TYPE_CHECKING, Union, Callable, Any, Tuple, List, Optional, Dict
from .immutable_collections import immutable_dict, immutable_list
import torch
if TYPE_CHECKING:
from .graph import Graph
","from typing import TYPE_CHECKING, Union, Callable, Any, Tuple, List, Optional, Dict
from .immutable_collections import immutable_dict, immutable_list
import torch
import builtins
import types
if TYPE_CHECKING:
from .graph import Graph
"
497,"where :math:`L` is a lower-triangular matrix and :math:`L^H` is the conjugate transpose of :math:`L`,
which is just a transpose for the case of real-valued input matrices.
In code it translates to ``input = L @ L.t()` if :attr:`input` is real-valued and
``input = L @ L.conj().t()`` if :attr:`input` is complex-valued.
The batch of :math:`L` matrices is returned.
Supports real-valued and complex-valued inputs.
.. note:: If :attr:`input` is not a Hermitian positive-definite matrix, or if it's a batch of matrices
and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown.
If :attr:`input` is a batch of matrices, then the error message will include the batch index
of the first matrix that is not Hermitian positive-definite.
.. warning:: This function always checks whether :attr:`input` is a Hermitian positive-definite matrix
             using `info` argument to LAPACK/MAGMA call. For CUDA this causes cross-device memory synchronization.

Args:
input (Tensor): the input tensor of size :math:`(*, n, n)` consisting of Hermitian positive-definite
                    :math:`n \times n` matrices, where `*` is zero or more batch dimensions.
Keyword args:
out (Tensor, optional): The output tensor. Ignored if ``None``. Default: ``None``
","where :math:`L` is a lower-triangular matrix and :math:`L^H` is the conjugate transpose of :math:`L`,
which is just a transpose for the case of real-valued input matrices.
In code it translates to ``input = L @ L.t()`` if :attr:`input` is real-valued and
``input = L @ L.conj().t()`` if :attr:`input` is complex-valued.
The batch of :math:`L` matrices is returned.
Supports real-valued and complex-valued inputs.
.. note:: When given inputs on a CUDA device, this function synchronizes that device with the CPU.

.. note:: LAPACK's `potrf` is used for CPU inputs, and MAGMA's `potrf` is used for CUDA inputs.

.. note:: If :attr:`input` is not a Hermitian positive-definite matrix, or if it's a batch of matrices
and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown.
If :attr:`input` is a batch of matrices, then the error message will include the batch index
of the first matrix that is not Hermitian positive-definite.
Args:
input (Tensor): the input tensor of size :math:`(*, n, n)` consisting of Hermitian positive-definite
                    :math:`n \times n` matrices, where :math:`*` is zero or more batch dimensions.
Keyword args:
out (Tensor, optional): The output tensor. Ignored if ``None``. Default: ``None``
"
498,"AndroidJob([""x86_64""], ""pytorch_linux_build""),
AndroidJob([""arm"", ""v7a""], ""pytorch_linux_build""),
AndroidJob([""arm"", ""v8a""], ""pytorch_linux_build""),
    AndroidJob([""vulkan"", ""x86_32""], ""pytorch_linux_build"", is_master_only=False),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build-x86_32"",
""pytorch_android_gradle_build-x86_32"",
","AndroidJob([""x86_64""], ""pytorch_linux_build""),
AndroidJob([""arm"", ""v7a""], ""pytorch_linux_build""),
AndroidJob([""arm"", ""v8a""], ""pytorch_linux_build""),
AndroidGradleJob(
""pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build-x86_32"",
""pytorch_android_gradle_build-x86_32"",
"
499,"Example::
>>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
    return allreduce_fut(process_group, bucket.get_tensors()[0])
def fp16_compress_hook(
","Example::
>>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
    group_to_use = process_group if process_group is not None else dist.group.WORLD
    world_size = group_to_use.size()

    tensor = bucket.get_tensors()[0]
    fut = dist.all_reduce(tensor, group=group_to_use, async_op=True).get_future()

    def then_callback(fut):
        return [fut.value()[0].div_(world_size)]

    return fut.then(then_callback)
def fp16_compress_hook(
"
500,"total_length
)
)
            state.error_dict[bucket_index] = torch.zeros(total_length, device=device)
# Keep a copy of the input tensor,
# so that we can compute the local error caused by compression later,
","total_length
)
)
            state.error_dict[bucket_index] = torch.zeros(total_length, device=device, dtype=dtype)
# Keep a copy of the input tensor,
# so that we can compute the local error caused by compression later,
"
501,")
if state.use_error_feedback:
            # memoize the local errors.
state.error_dict[bucket_index] = input_tensor_cp - input_tensor
if torch.cuda.is_available():
torch.cuda.synchronize(device)
",")
if state.use_error_feedback:
            # Memorize the local errors.
state.error_dict[bucket_index] = input_tensor_cp - input_tensor
if torch.cuda.is_available():
torch.cuda.synchronize(device)
"
502,"preserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.
Attributes modified in preserved methods will also be preserved.
Returns:
Frozen :class:`ScriptModule`.
","preserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.
Attributes modified in preserved methods will also be preserved.
        optimize (bool): If ``True``, a set of optimization passes will be run to prepare the graph for inference,
        in addition to the graph cleanup that already occurs. The details of the optimizations can be found in
        `torch.jit.optimize_frozen_module.`


Returns:
Frozen :class:`ScriptModule`.
"
503,"free_vars: List[str] = []
modules_used : Set[str] = set()
body: List[str] = []
        maybe_return_annotation : str = ''
def register_modules_used(qualified_name : str):
if '.' in qualified_name:
","free_vars: List[str] = []
modules_used : Set[str] = set()
body: List[str] = []

        # Wrap string in list to pass by reference
        maybe_return_annotation : List[str] = ['']
def register_modules_used(qualified_name : str):
if '.' in qualified_name:
"
504,"def argument_type(a: Argument, *, binds: ArgName) -> CType:
return argumenttype_type(a.type, mutable=a.is_write, binds=binds)
def argument(a: Union[Argument, SelfArgument, TensorOptionsArguments]) -> List[Binding]:
if isinstance(a, Argument):
return [Binding(
ctype=argument_type(a, binds=a.name),
name=a.name,
            default=cpp.default_expr(a.default, a.type) if a.default is not None else None,
argument=a,
)]
elif isinstance(a, SelfArgument):
# Erase SelfArgument from the distinction
        return argument(a.argument)
elif isinstance(a, TensorOptionsArguments):
if local.use_c10_dispatcher() in [UseC10Dispatcher.hacky_wrapper_for_legacy_signatures,
UseC10Dispatcher.with_codegenerated_unboxing_wrapper]:
# TODO: expunge this logic entirely
default = None
            if all(x.default == ""None"" for x in a.all()):
                default = '{}'
            elif a.dtype.default == ""long"":
                default = 'at::kLong'  # TODO: this is wrong
return [Binding(
ctype=ConstRefCType(BaseCType('TensorOptions', 'options')),
name='options',
","def argument_type(a: Argument, *, binds: ArgName) -> CType:
return argumenttype_type(a.type, mutable=a.is_write, binds=binds)
def argument(a: Union[Argument, SelfArgument, TensorOptionsArguments], *, is_out: bool) -> List[Binding]:
    # Ideally, we NEVER default native functions.  However, there are a number
    # of functions that call native:: directly and rely on the defaulting
    # existing.  So for BC, we generate defaults for non-out variants (but not
    # for out variants, where it is impossible to generate an appropriate
    # default)
    should_default = not is_out or local.use_c10_dispatcher() is not UseC10Dispatcher.full
if isinstance(a, Argument):
        default: Optional[str] = None
        if should_default and a.default is not None:
            default = cpp.default_expr(a.default, a.type)
return [Binding(
ctype=argument_type(a, binds=a.name),
name=a.name,
            default=default,
argument=a,
)]
elif isinstance(a, SelfArgument):
# Erase SelfArgument from the distinction
        return argument(a.argument, is_out=is_out)
elif isinstance(a, TensorOptionsArguments):
if local.use_c10_dispatcher() in [UseC10Dispatcher.hacky_wrapper_for_legacy_signatures,
UseC10Dispatcher.with_codegenerated_unboxing_wrapper]:
# TODO: expunge this logic entirely
default = None
            if should_default:
                if all(x.default == ""None"" for x in a.all()):
                    default = '{}'
                elif a.dtype.default == ""long"":
                    default = 'at::kLong'  # TODO: this is wrong
return [Binding(
ctype=ConstRefCType(BaseCType('TensorOptions', 'options')),
name='options',
"
505,")]
else:
assert local.use_c10_dispatcher() == UseC10Dispatcher.full
return [
Binding(
ctype=OptionalCType(BaseCType('ScalarType', 'dtype')),
name='dtype',
                    default='{}',
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('Layout', 'layout')),
name='layout',
                    default='{}',
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('Device', 'device')),
name='device',
                    default='{}',
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('bool', 'pin_memory')),
name='pin_memory',
                    default='{}',
argument=a,
)]
else:
",")]
else:
assert local.use_c10_dispatcher() == UseC10Dispatcher.full
            default = None
            if should_default:
                default = '{}'
            # TODO: Not sure why the arguments assigned here are for
            # TensorOptionsArguments and not the constituent pieces.  It seems
            # to matter
return [
Binding(
ctype=OptionalCType(BaseCType('ScalarType', 'dtype')),
name='dtype',
                    default=default,
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('Layout', 'layout')),
name='layout',
                    default=default,
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('Device', 'device')),
name='device',
                    default=default,
argument=a,
),
Binding(
ctype=OptionalCType(BaseCType('bool', 'pin_memory')),
name='pin_memory',
                    default=default,
argument=a,
)]
else:
"
506,"cpp_schema_order_types = [
# NB: method here doesn't matter
        r.type for a in schema_order_jit_arguments for r in cpp.argument(a, method=False)
]
cpp_returns = cpp.returns_type(f.func.returns)
","cpp_schema_order_types = [
# NB: method here doesn't matter
        r.type for a in schema_order_jit_arguments
        for r in cpp.argument(
            a, method=False, cpp_no_default_args=set(), faithful=False, has_tensor_options=False)
]
cpp_returns = cpp.returns_type(f.func.returns)
"
507,""""""".format(**common_args))
# TODO: see https://github.com/pytorch/pytorch/issues/43667
add_docstr(torch.ones,
r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
",""""""".format(**common_args))
add_docstr(torch.ones,
r""""""
ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor
"
508,"Arguments:
y (Tensor): The values of the function to integrate
dx (float): The distance between points at which `y` is sampled.
dim (int): The dimension along which to integrate.
By default, use the last dimension.
","Arguments:
y (Tensor): The values of the function to integrate

Keyword args:
dx (float): The distance between points at which `y` is sampled.
dim (int): The dimension along which to integrate.
By default, use the last dimension.
"
509,"(set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |
set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |
set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |
         _INCLUDE_QCONFIG_PROPAGATE_LIST) -
        _EXCLUDE_QCONFIG_PROPAGATE_LIST
)
return QCONFIG_PROPAGATE_MODULE_CLASS_LIST
","(set(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS.keys()) |
set(DEFAULT_QAT_MODULE_MAPPINGS.keys()) |
set(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS.keys()) |
         _INCLUDE_QCONFIG_PROPAGATE_LIST)
)
return QCONFIG_PROPAGATE_MODULE_CLASS_LIST
"
510,"* :class:`torch.nn.FractionalMaxPool2d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.FractionalMaxPool3d` when called on a CUDA tensor that requires grad
* :func:`torch.nn.functional.interpolate` when called on a CUDA tensor that requires grad
            and one of the following modes is used:
            - `linear`
            - `bilinear`
            - `bicubic`
            - `trilinear`
* :class:`torch.nn.ReflectionPad1d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.ReflectionPad2d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.ReplicationPad1d` when called on a CUDA tensor that requires grad
","* :class:`torch.nn.FractionalMaxPool2d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.FractionalMaxPool3d` when called on a CUDA tensor that requires grad
* :func:`torch.nn.functional.interpolate` when called on a CUDA tensor that requires grad
          and one of the following modes is used:

          - `linear`
          - `bilinear`
          - `bicubic`
          - `trilinear`

* :class:`torch.nn.ReflectionPad1d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.ReflectionPad2d` when called on a CUDA tensor that requires grad
* :class:`torch.nn.ReplicationPad1d` when called on a CUDA tensor that requires grad
"
511,"self.modules = dict(input_root.named_modules())
additional_fusion_patterns = \
            fuse_custom_config_dict.get(""additional_quant_pattern"", {})
fusion_patterns = get_combined_dict(
get_default_fusion_patterns(), additional_fusion_patterns)
# find fusion
","self.modules = dict(input_root.named_modules())
additional_fusion_patterns = \
            fuse_custom_config_dict.get(""additional_fusion_pattern"", {})
fusion_patterns = get_combined_dict(
get_default_fusion_patterns(), additional_fusion_patterns)
# find fusion
"
512,"# Factories are a bit special because their out-of-place overloads
# take an extra TensorOptions argument, which is missing in the _out function
has_tensor_return = any(r.type.is_tensor_like() for r in f.func.returns)
        has_tensor_input_arg = any(a.type.is_tensor_like()
                                   for a in itertools.chain(f.func.arguments.positional, f.func.arguments.kwarg_only))
is_factory_method = f.category_override == 'factory' or (has_tensor_return and not has_tensor_input_arg)
# HACK: preserve old codegen behavior - the old codegen set the `is_factory_method`
","# Factories are a bit special because their out-of-place overloads
# take an extra TensorOptions argument, which is missing in the _out function
has_tensor_return = any(r.type.is_tensor_like() for r in f.func.returns)
        has_tensor_input_arg = any(a.type.is_tensor_like() for a in f.func.arguments.flat_non_out)
is_factory_method = f.category_override == 'factory' or (has_tensor_return and not has_tensor_input_arg)
# HACK: preserve old codegen behavior - the old codegen set the `is_factory_method`
"
513,"import tools.codegen.api.dispatcher as dispatcher
from typing import Sequence
import itertools
# Follows dispatcher calling convention, but:
#   - Mutable arguments not allowed.  Meta functions are always
","import tools.codegen.api.dispatcher as dispatcher
from typing import Sequence
# Follows dispatcher calling convention, but:
#   - Mutable arguments not allowed.  Meta functions are always
"
514,"# NB: doesn't contain out arguments
@property
    def kwarg_only(self) -> Sequence[Argument]:
ret: List[Argument] = []
ret.extend(self.pre_tensor_options_kwarg_only)
if self.tensor_options is not None:
","# NB: doesn't contain out arguments
@property
    def flat_kwarg_only(self) -> Sequence[Argument]:
ret: List[Argument] = []
ret.extend(self.pre_tensor_options_kwarg_only)
if self.tensor_options is not None:
"
515,"def find_device_for(partition: Partition):
""""""Given a partition, find a logical device for the partition
            The algorithm is that:
            #1. sort all the devices based on left mem size
            #2. put the partition on the device that has just enought mem
            for that partition
""""""
for d in device_to_left_mem_bytes:
extra_size_needed = calculate_extra_mem_bytes_needed_for(partition, device_to_partitions[d])
","def find_device_for(partition: Partition):
""""""Given a partition, find a logical device for the partition
           The algorithm is to put the partition on the device
           that has just enough mem left for that partition.
           device_to_left_mem_bytes is a dictionary between device and its left mem size
           sorted by its left mem size
""""""
for d in device_to_left_mem_bytes:
extra_size_needed = calculate_extra_mem_bytes_needed_for(partition, device_to_partitions[d])
"
516,"node_to_latency_mapping: Dict[Node, NodeLatency]
) -> None:
""""""This method is to partition the fx module based on the cost.
           The cost is the total latency of running the whole graph.
In partitioner_utils.py, the cost model is built.
           The algorithm is:
#1. At every begining, each node is a partition.
Then we map all the partitions to the devices
and calculate the cost
","node_to_latency_mapping: Dict[Node, NodeLatency]
) -> None:
""""""This method is to partition the fx module based on the cost.
           The cost is the total latency of running the whole fx module.
In partitioner_utils.py, the cost model is built.
           The cost aware partition algorithm is:
#1. At every begining, each node is a partition.
Then we map all the partitions to the devices
and calculate the cost
"
517,"in_channels, out_channels, kernel_size, stride, padding, dilation,
False, _single(0), groups, bias, padding_mode)
    def _conv_forward(self, input, weight):
if self.padding_mode != 'zeros':
return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
                            weight, self.bias, self.stride,
_single(0), self.dilation, self.groups)
        return F.conv1d(input, weight, self.bias, self.stride,
self.padding, self.dilation, self.groups)
def forward(self, input: Tensor) -> Tensor:
        return self._conv_forward(input, self.weight)
class Conv2d(_ConvNd):
","in_channels, out_channels, kernel_size, stride, padding, dilation,
False, _single(0), groups, bias, padding_mode)
    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
if self.padding_mode != 'zeros':
return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
                            weight, bias, self.stride,
_single(0), self.dilation, self.groups)
        return F.conv1d(input, weight, bias, self.stride,
self.padding, self.dilation, self.groups)
def forward(self, input: Tensor) -> Tensor:
        return self._conv_forward(input, self.weight, self.bias)
class Conv2d(_ConvNd):
"
518,"self.freeze_bn = freeze_bn if self.training else True
self.bn = _BN_CLASS_MAP[dim](out_channels, eps, momentum, True, True)
self.weight_fake_quant = self.qconfig.weight()
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
else:
","self.freeze_bn = freeze_bn if self.training else True
self.bn = _BN_CLASS_MAP[dim](out_channels, eps, momentum, True, True)
self.weight_fake_quant = self.qconfig.weight()
        self.zero_bias = torch.zeros(out_channels)
if bias:
self.bias = Parameter(torch.Tensor(out_channels))
else:
"
519,")
from .stubs import DeQuantStub, QuantWrapper
from .qconfig import default_dynamic_qconfig, float16_dynamic_qconfig, float_qparams_dynamic_qconfig
def is_activation_post_process(module):
return (isinstance(module, torch.quantization.ObserverBase) or
",")
from .stubs import DeQuantStub, QuantWrapper
from .qconfig import default_dynamic_qconfig, float16_dynamic_qconfig, float_qparams_weight_only_qconfig
def is_activation_post_process(module):
return (isinstance(module, torch.quantization.ObserverBase) or
"
520,"#
tensor = tensor.new_empty([0]).set_(tensor.storage())
        tensor.record_stream(as_cuda(stream))
def is_cuda(stream: AbstractStream) -> bool:
","#
tensor = tensor.new_empty([0]).set_(tensor.storage())
        # Typechecking: torch.cuda.Stream is incompatible with torch._C.Stream
        tensor.record_stream(as_cuda(stream))  # type: ignore[arg-type]
def is_cuda(stream: AbstractStream) -> bool:
"
521,"def quantization_perchannel_hook(
    process_group: object, bucket: dist._GradBucket, bucket_size=512
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_channel`` logic to DDP using ``allgather``
","def quantization_perchannel_hook(
    process_group: dist.ProcessGroup, bucket: dist._GradBucket, bucket_size=512
) -> torch.futures.Future:
""""""
Applies the ``torch.quantize_per_channel`` logic to DDP using ``allgather``
"
522,"return ValueError(""Error initializing torch.distributed using "" + msg)
def _file_rendezvous_handler(url, **kwargs):
def _error(msg):
return _rendezvous_error(""file:// rendezvous: "" + msg)
","return ValueError(""Error initializing torch.distributed using "" + msg)
def _file_rendezvous_handler(url: str, **kwargs):
def _error(msg):
return _rendezvous_error(""file:// rendezvous: "" + msg)
"
523,"raise RuntimeError(""Unable to perform rerendezvous using file:// method"")
def _tcp_rendezvous_handler(url, timeout=default_pg_timeout, **kwargs):
def _error(msg):
return _rendezvous_error(""tcp:// rendezvous: "" + msg)
result = urlparse(url)
if not result.port:
raise _error(""port number missing"")
    query = dict(pair.split(""="") for pair in filter(None, result.query.split(""&"")))
if ""rank"" not in query:
raise _error(""rank parameter missing"")
if ""world_size"" not in query:
","raise RuntimeError(""Unable to perform rerendezvous using file:// method"")
def _tcp_rendezvous_handler(url: str, timeout: timedelta = default_pg_timeout, **kwargs):
def _error(msg):
return _rendezvous_error(""tcp:// rendezvous: "" + msg)
result = urlparse(url)
if not result.port:
raise _error(""port number missing"")
    query: Dict[str, Union[int, str]]
    # mypy doesn't allow dict() to accept List of values (#257)
    query = dict(pair.split(""="") for pair in filter(None, result.query.split(""&"")))  # type: ignore[misc, arg-type]
if ""rank"" not in query:
raise _error(""rank parameter missing"")
if ""world_size"" not in query:
"
524,"rank = int(query[""rank""])
world_size = int(query[""world_size""])
start_daemon = rank == 0
store = TCPStore(result.hostname, result.port, world_size, start_daemon, timeout)
yield (store, rank, world_size)
","rank = int(query[""rank""])
world_size = int(query[""world_size""])
start_daemon = rank == 0
    assert result.hostname is not None
store = TCPStore(result.hostname, result.port, world_size, start_daemon, timeout)
yield (store, rank, world_size)
"
525,"raise RuntimeError(""Unable to perform rerendezvous using tcp:// method"")
def _env_rendezvous_handler(url, timeout=default_pg_timeout, **kwargs):
def _error(msg):
return _rendezvous_error(""env:// rendezvous: "" + msg)
","raise RuntimeError(""Unable to perform rerendezvous using tcp:// method"")
def _env_rendezvous_handler(url: str, timeout: timedelta = default_pg_timeout, **kwargs):
def _error(msg):
return _rendezvous_error(""env:// rendezvous: "" + msg)
"
526,"The backend of the given process group as a lower case string.
""""""
    _check_default_pg()

if group == GroupMember.WORLD:
        pg = _default_pg
else:
pg = group
if _rank_not_in_group(pg):
raise RuntimeError(""Invalid process group specified"")
    return _pg_map.get(pg, None)[0]
def init_process_group(backend,
","The backend of the given process group as a lower case string.
""""""
if group == GroupMember.WORLD:
        pg = _check_default_pg()
else:
pg = group
if _rank_not_in_group(pg):
raise RuntimeError(""Invalid process group specified"")
    pg_store = _pg_map.get(pg, None)
    assert pg_store is not None
    return pg_store[0]
def init_process_group(backend,
"
527,"is_default_group = (len(group_ranks) == 0)
backend = Backend(backend)
if backend == Backend.MPI:
if not is_mpi_available():
raise RuntimeError(
","is_default_group = (len(group_ranks) == 0)
backend = Backend(backend)
    pg: Union[ProcessGroupGloo, ProcessGroupMPI, ProcessGroupNCCL]
if backend == Backend.MPI:
if not is_mpi_available():
raise RuntimeError(
"
528,"return
if group == GroupMember.WORLD:
        _check_default_pg()
        _default_pg.send([tensor], dst, tag).wait()
else:
group_dst_rank = _get_group_rank(group, dst)
group.send([tensor], group_dst_rank, tag).wait()
","return
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        default_pg.send([tensor], dst, tag).wait()
else:
group_dst_rank = _get_group_rank(group, dst)
group.send([tensor], group_dst_rank, tag).wait()
"
529,"opts.rootRank = dst
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.reduce([tensor], opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
","opts.rootRank = dst
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.reduce([tensor], opts)
else:
group_dst_rank = _get_group_rank(group, dst)
opts.rootRank = group_dst_rank
"
530,"input_tensor_list = [t if not t.is_complex() else torch.view_as_real(t) for t in input_tensor_list]
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.allgather(output_tensor_lists, input_tensor_list)
else:
work = group.allgather(output_tensor_lists, input_tensor_list)
","input_tensor_list = [t if not t.is_complex() else torch.view_as_real(t) for t in input_tensor_list]
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.allgather(output_tensor_lists, input_tensor_list)
else:
work = group.allgather(output_tensor_lists, input_tensor_list)
"
531,"tensor = tensor if not tensor.is_complex() else torch.view_as_real(tensor)
if group == GroupMember.WORLD:
        _check_default_pg()
        work = _default_pg.allgather([tensor_list], [tensor])
else:
work = group.allgather([tensor_list], [tensor])
","tensor = tensor if not tensor.is_complex() else torch.view_as_real(tensor)
if group == GroupMember.WORLD:
        default_pg = _check_default_pg()
        work = default_pg.allgather([tensor_list], [tensor])
else:
work = group.allgather([tensor_list], [tensor])
"
532,"if not sym_help._is_fp(self):
other = g.op(""Cast"", other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
two_pow = g.op('Pow', two, other)

lshift = g.op('Mul', self, two_pow)
return lshift
","if not sym_help._is_fp(self):
other = g.op(""Cast"", other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
two_pow = g.op('Pow', two, other)
    two_pow = g.op('Cast', two_pow, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
lshift = g.op('Mul', self, two_pow)
return lshift
"
533,"print(""No CUDA available"")
sys.exit()
    print(""Payload: {}, {} iterations; timer min. runtime = {}\n"".format(
        args.workload, args.internal_iter, args.timer_min_run_time))
INTERNAL_ITER = args.internal_iter
for profiling_enabled in [False, True]:
","print(""No CUDA available"")
sys.exit()
    print(""Payload: {}; {} iterations, N = {}\n"".format(
        args.workload, args.internal_iter, args.n))
INTERNAL_ITER = args.internal_iter
for profiling_enabled in [False, True]:
"
534,"asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``
will provide errors to the user which can be caught and handled,
but due to its blocking nature, it has a performance overhead. On
            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has little
performance overhead, but crashes the process on errors. This is
done since CUDA execution is async and it is no longer safe to
continue executing user code since failed async NCCL operations
            might result in subsequent CUDA operations to run on corrupted
data. Only one of these two environment variables should be set.
group_name (str, optional, deprecated): Group name.
","asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``
will provide errors to the user which can be caught and handled,
but due to its blocking nature, it has a performance overhead. On
            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little
performance overhead, but crashes the process on errors. This is
done since CUDA execution is async and it is no longer safe to
continue executing user code since failed async NCCL operations
            might result in subsequent CUDA operations running on corrupted
data. Only one of these two environment variables should be set.
group_name (str, optional, deprecated): Group name.
"
535,"process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
        Similar to ``allreduce_hook``, this hook first gathers ``GradBucket`` tensors
        and its ``then`` callback aggregates the gathered gradient tensors and takes
        mean. Instead of ``allreduce`` this hook uses ``allgather``. Note that with
        W workers, both the computation and communication time scale as O(W) for
        allgather compared to O(logW) for allreduce. Therefore, this hook is expected
        to be much slower than ``allreduce_hook`` although both essentially do the
        same thing with the gradients.

        .. warning ::
            This is for test and experiments. User is suggested to use a faster
            alternative called ``allreduce_hook``  that uses ``allreduce`` protocol
            instead of ``allgather`` protocol.

        Example::
            >>> ddp_model._register_comm_hook(process_group, allreduce_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
","process_group: object, bucket: dist._GradBucket
) -> torch.futures.Future:
""""""
    Similar to ``allreduce_hook``, this hook first gathers ``GradBucket`` tensors
    and its ``then`` callback aggregates the gathered gradient tensors and takes
    mean. Instead of ``allreduce`` this hook uses ``allgather``. Note that with
    W workers, both the computation and communication time scale as O(W) for
    allgather compared to O(logW) for allreduce. Therefore, this hook is expected
    to be much slower than ``allreduce_hook`` although both essentially do the
    same thing with the gradients.

    .. warning ::
        This is for test and experiments. User is suggested to use a faster
        alternative called ``allreduce_hook``  that uses ``allreduce`` protocol
        instead of ``allgather`` protocol.

    Example::
        >>> ddp_model.register_comm_hook(process_group, allreduce_hook)
""""""
group_to_use = process_group if process_group is not None else dist.group.WORLD
rank = process_group.rank() if process_group is not None else dist.get_rank()
"
536,"# All procs joined. Agree on authoritative rank and broadcast the model.
self._sync_final_model(is_last_joiner)
    def _register_comm_hook(self, state: object, hook: callable):
r""""""
Registers a communication hook which is an enhancement that provides a
flexible hook to users where they can specify how DDP aggregates gradients
","# All procs joined. Agree on authoritative rank and broadcast the model.
self._sync_final_model(is_last_joiner)
    def register_comm_hook(self, state: object, hook: callable):
r""""""
Registers a communication hook which is an enhancement that provides a
flexible hook to users where they can specify how DDP aggregates gradients
"
537,"import torch.nn as nn
class MyModule(nn.Module):
                def __init__(self, use_memory_efficent):
super(MyModule, self).__init__()
                    self.use_memory_efficent = use_memory_efficent
@torch.jit.unused
def memory_efficient(self, x):
","import torch.nn as nn
class MyModule(nn.Module):
                def __init__(self, use_memory_efficient):
super(MyModule, self).__init__()
                    self.use_memory_efficient = use_memory_efficient
@torch.jit.unused
def memory_efficient(self, x):
"
538,"result = []
for x, dictionary in sorted(grouped.items()):
if 'base' not in dictionary:
raise RuntimeError(
                ""'base' not in dictionary for {}. keys are {}"".format(
                    x, list(dictionary.keys())))
result.append(dictionary)
return sort_declarations(result)
","result = []
for x, dictionary in sorted(grouped.items()):
if 'base' not in dictionary:
            candidates = []
            non_out_name = dictionary['out']['operator_name']
            for declaration in declarations:
                if declaration['name'] == non_out_name and not declaration['deprecated']:
                    signature = get_python_signature(declaration, is_python_method, skip_outputs=True)
                    candidates.append(signature)
raise RuntimeError(
                ""While identifying overloads, we found an out schema {} without a corresponding non-out variant. ""
                ""We expected the non-out variant to have schema: \n- {}\nPlease check that you spelled the schema ""
                ""correctly in native_functions.yaml. We discovered the following candidate(s): \n""
                .format(dictionary['signature'], x) + ""\n"".join(""- {}"".format(candidate) for candidate in candidates))
result.append(dictionary)
return sort_declarations(result)
"
539,"weight_is_quantized,
weight_dtype,
get_linear_prepack_op_for_dtype,
)
from abc import ABC, abstractmethod
","weight_is_quantized,
weight_dtype,
get_linear_prepack_op_for_dtype,
    get_qconfig_dtypes,
)
from abc import ABC, abstractmethod
"
540,"tensors: Tuple[Tensor, ...]
def __init__(self, *tensors: Tensor) -> None:
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)
self.tensors = tensors
def __getitem__(self, index):
","tensors: Tuple[Tensor, ...]
def __init__(self, *tensors: Tensor) -> None:
        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), ""Size mismatch between tensors""
self.tensors = tensors
def __getitem__(self, index):
"
541,"# this is an indicator of whether all the inputs are Node or not
# since some op might be quantized differently depending on whether
# all inputs are tensors or not, e.g. add/mul
        self.all_nodes = True
@abstractmethod
def convert(self, quantizer, node, load_arg, debug=False, convert_custom_config_dict=None):
","# this is an indicator of whether all the inputs are Node or not
# since some op might be quantized differently depending on whether
# all inputs are tensors or not, e.g. add/mul
        self.num_node_args = len(node.args)
        self.all_node_args = True
@abstractmethod
def convert(self, quantizer, node, load_arg, debug=False, convert_custom_config_dict=None):
"
542,"node = node.args[0]
assert node.op == 'call_function' and node.target in [operator.add, torch.add]
self.add_node = node
        self.all_nodes = all([isinstance(a, Node) for a in self.add_node.args[:2]])
def convert(self, quantizer, node, load_arg, debug=False, convert_custom_config_dict=None):
        if not self.all_nodes:
# add scalar
if self.relu_node is not None:
op = torch.ops.quantized.add_relu
else:
op = torch.ops.quantized.add
return quantizer.quantized_graph.create_node(
'call_function', op,
                load_arg(quantized=[0])(self.add_node.args), self.add_node.kwargs)
else:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
","node = node.args[0]
assert node.op == 'call_function' and node.target in [operator.add, torch.add]
self.add_node = node
        self.num_node_args = len([a for a in self.add_node.args[:2] if isinstance(a, Node)])
def convert(self, quantizer, node, load_arg, debug=False, convert_custom_config_dict=None):
        if self.num_node_args == 1:
# add scalar
if self.relu_node is not None:
op = torch.ops.quantized.add_relu
else:
op = torch.ops.quantized.add

            if isinstance(self.add_node.args[0], Node):
                quantized_index = 0
            else:
                quantized_index = 1

return quantizer.quantized_graph.create_node(
'call_function', op,
                load_arg(quantized=[quantized_index])(self.add_node.args), self.add_node.kwargs)
else:
activation_post_process = quantizer.activation_post_process_map[node.name]
scale, zero_point = activation_post_process.calculate_qparams()
"
543,"global _group_count
global _pg_names
    group_name_ = group_name
if not group_name:
group_name = str(_group_count)
if group_name in _pg_names.values():
raise RuntimeError(""The specified group name has already been ""
","global _group_count
global _pg_names
if not group_name:
group_name = str(_group_count)
        _group_count += 1
if group_name in _pg_names.values():
raise RuntimeError(""The specified group name has already been ""
"
544,"# If you want to register a kernel to Autograd, you must make the op abstract.
# In other words, this op must have dispatch section in native_functions.yaml.
if declaration['name'] in MANUAL_AUTOGRAD_AND_TRACER or declaration['derivative']:
            msg = (f'Did you add a formula for {declaration[""name""]}(or its functional variant) in derivatives.yaml?'
                   f'If so please add a dispatch section for it with DefaultBackend in native_functions.yaml.')
assert declaration['abstract'], msg
# Emit TraceType code
","# If you want to register a kernel to Autograd, you must make the op abstract.
# In other words, this op must have dispatch section in native_functions.yaml.
if declaration['name'] in MANUAL_AUTOGRAD_AND_TRACER or declaration['derivative']:
            msg = (f'There\'s a formula for {declaration[""name""]}(or its functional variant) in derivatives.yaml. '
                   f'It\'s required to add a dispatch section for it with explicit supported backends e.g CPU/CUDA '
                   f'or DefaultBackend in native_functions.yaml. Please see '
                   f'https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native#choosing-the-right-dispatch-keyword '
                   f'for instructions to choose the right dispatch keyword.')
assert declaration['abstract'], msg
# Emit TraceType code
"
545,"cuda_post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags(cuda_post_cflags)
                    cuda_post_cflags = cuda_post_cflags + COMMON_HIPCC_FLAGS
else:
cuda_post_cflags = unix_cuda_flags(cuda_post_cflags)
append_std14_if_no_std_present(cuda_post_cflags)
","cuda_post_cflags = list(extra_postargs)
if IS_HIP_EXTENSION:
cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags(cuda_post_cflags)
                    cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags
else:
cuda_post_cflags = unix_cuda_flags(cuda_post_cflags)
append_std14_if_no_std_present(cuda_post_cflags)
"
546,"from typing import Dict
from .quantization_mappings import (
    get_compare_output_module_list,
)
NON_LEAF_MODULE_TO_ADD_OBSERVER_ALLOW_LIST = {
","from typing import Dict
from .quantization_mappings import (
    get_default_compare_output_module_list,
)
NON_LEAF_MODULE_TO_ADD_OBSERVER_ALLOW_LIST = {
"
547,"(nn.BatchNorm3d, nn.ReLU): nni.BNReLU3d,
}
def register_fuser_method(op_list, fuser_method):
    ''' Register a fuser method for a tuple of ops, will be called
    during fusion step
    '''
    assert isinstance(op_list, tuple), 'op list must be a tuple'
    OP_LIST_TO_FUSER_METHOD[op_list] = fuser_method

def get_fuser_method(op_list):
''' Get fuser method for the given list of module types,
return None if fuser method does not exist
'''
    return OP_LIST_TO_FUSER_METHOD.get(op_list, None)
","(nn.BatchNorm3d, nn.ReLU): nni.BNReLU3d,
}
# TODO: remove
def get_fuser_method(op_list):
''' Get fuser method for the given list of module types,
return None if fuser method does not exist
'''
    return DEFAULT_OP_LIST_TO_FUSER_METHOD.get(op_list, None)
"
548,"from .pattern_utils import (
is_match,
    get_fusion_patterns,
)
from .fusion_patterns import *  # noqa: F401
","from .pattern_utils import (
is_match,
    get_default_fusion_patterns,
)
from .fusion_patterns import *  # noqa: F401
"
549,"""""""
return cast(Future[S], super().then(callback))
def set_result(self, result: T) -> None:
r""""""
Set the result for this ``Future``, which will mark this ``Future`` as
","""""""
return cast(Future[S], super().then(callback))
    # Have to use string annotations because  PEP-0563 is not available in 3.6
    def _add_done_callback(self, callback):  # type: (Callable[[Future[T]], None]) -> None
        r""""""
        Append the given callback function to this ``Future``, which will be run
        when the ``Future`` is completed.  Multiple callbacks can be added to
        the same ``Future``, and will be invoked in the same order as they were
        added. The callback must take one argument, which is the reference to
        this ``Future``. The callback function can use the ``Future.wait()`` API
        to get the value.

        We recommend that you use the ``then`` API as it provides a way to synchronize
        after your callback has completed. ``add_done_callback`` can be cheaper if your
        callback does not return anything. But both ``then`` and ``add_done_callback``
        use the same callback registration API under the hood, and thus the order of
        their callbacks will be maintained even if their calls are interleaved.

        Arguments:
            callback(``None``): a ``Callable`` that takes in no arguments

        Example::
            >>> import torch
            >>>
            >>> def callback():
            >>>     print(f""This will run after the future has finished."")
            >>>
            >>> fut = torch.futures.Future()
            >>> fut.add_done_callback(callback)
            >>> fut.set_result(5)
            >>>
            >>> # Outputs are:
            >>> # This will run after the future has finished.
        """"""
        super().add_done_callback(callback)

def set_result(self, result: T) -> None:
r""""""
Set the result for this ``Future``, which will mark this ``Future`` as
"
550,"assert returns_type == dispatcher.returns_type(f.func.returns)
dispatcher_args = dispatcher.arguments(f.func)
dispatcher_args_types_str = ', '.join(map(lambda a: a.type, dispatcher_args))
            if dispatch is None or dispatch == 'Math' or dispatch == 'DefaultBackend':
type_name = f'TypeDefault::{name}'
else:
type_name = f'{dispatch}Type::{name}'
","assert returns_type == dispatcher.returns_type(f.func.returns)
dispatcher_args = dispatcher.arguments(f.func)
dispatcher_args_types_str = ', '.join(map(lambda a: a.type, dispatcher_args))
            if dispatch is None or dispatch in KEYWORD_ALL_BACKENDS:
type_name = f'TypeDefault::{name}'
else:
type_name = f'{dispatch}Type::{name}'
"
551,"After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise
identical in all processes.
Only nccl and gloo backend is currently supported
tensors should only be GPU tensors
","After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise
identical in all processes.
    Complex tensors are supported.

Only nccl and gloo backend is currently supported
tensors should only be GPU tensors
"
552,"After the call each tensor in tensors is going to bitwise identical
in all processes.
Arguments:
tensors (List[Tensor]): Input and output of the collective. The function
operates in-place.
","After the call each tensor in tensors is going to bitwise identical
in all processes.
    Complex tensors are supported.

Arguments:
tensors (List[Tensor]): Input and output of the collective. The function
operates in-place.
"
553,"""""""
Gathers tensors from the whole group in a list.
Arguments:
tensor_list (list[Tensor]): Output list. It should contain
correctly-sized tensors to be used for output of the collective.
","""""""
Gathers tensors from the whole group in a list.
    Complex tensors are supported.

Arguments:
tensor_list (list[Tensor]): Output list. It should contain
correctly-sized tensors to be used for output of the collective.
"
554,"from numbers import Number
import torch
from torch.distributions import constraints
from torch.distributions.distribution import Distribution
","import torch
from torch.distributions import constraints
from torch.distributions.distribution import Distribution
"
555,"self._validate_sample(value)
# compute the variance
var = (self.scale ** 2)
        log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()
return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))
def cdf(self, value):
","self._validate_sample(value)
# compute the variance
var = (self.scale ** 2)
        log_scale = math.log(self.scale) if isinstance(self.scale, Real) else self.scale.log()
return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))
def cdf(self, value):
"
556,"returned tensor. If specified, the input tensor is casted to
:attr:'dtype' while performing the operation. Default: None.
Example::
","returned tensor. If specified, the input tensor is casted to
:attr:'dtype' while performing the operation. Default: None.
    .. note::
        Even though ``p='fro'`` supports any number of dimensions, the true
        mathematical definition of Frobenius norm only applies to tensors with
        exactly two dimensions. :func:`torch.linalg.norm` with ``ord='fro'`` aligns
        with the mathematical definition, since it can only be applied across
        exactly two dimensions.
Example::
"
557,"for k in ks.split("",""):
dispatch[k.strip()] = v
e.pop('__line__')
assert not e, f""leftover entries: {e}""
","for k in ks.split("",""):
dispatch[k.strip()] = v
        # Throws if both DefaultBackend and Math are provided
        assert not (dispatch is not None and 'DefaultBackend' in dispatch and 'Math' in dispatch)

e.pop('__line__')
assert not e, f""leftover entries: {e}""
"
558,"'None': 'c10::nullopt',  # UGH this one is type directed
'Mean': 'at::Reduction::Mean',
'[]': '{}',
    '[0,1]': '{0,1}',  # TODO: stop special casing
'contiguous_format': 'MemoryFormat::Contiguous',
'long': 'at::kLong',
}
","'None': 'c10::nullopt',  # UGH this one is type directed
'Mean': 'at::Reduction::Mean',
'[]': '{}',
'contiguous_format': 'MemoryFormat::Contiguous',
'long': 'at::kLong',
}
"
559,"for r in func(x):
yield r
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#
#                        C++ CODE GENERATION
","for r in func(x):
yield r
def cpp_string(s: str) -> str:
    """"""Convert a python string into a c++ string literal """"""
    s = s.replace('\\', '\\\\')
    s = s.replace('""', '\\""')
    s = s.replace('\a', '\\a')
    s = s.replace('\b', '\\b')
    s = s.replace('\f', '\\f')
    s = s.replace('\n', '\\n')
    s = s.replace('\v', '\\v')
    s = s.replace('\t', '\\t')
    return f'""{s}""'

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #
#
#                        C++ CODE GENERATION
"
560,"for _ in range(num_runs):
start_time = time.time()
self.output = self.op_bench.forward()
                if cuda_sync:
torch.cuda.synchronize(torch.cuda.current_device())
end_time = time.time()
self.time_series.append((end_time - start_time) * 1e3)
else:
for _ in range(num_runs):
self.output = self.op_bench.forward()
            if cuda_sync:
torch.cuda.synchronize(torch.cuda.current_device())
def _output_mean(self):
","for _ in range(num_runs):
start_time = time.time()
self.output = self.op_bench.forward()
                if cuda_sync:
torch.cuda.synchronize(torch.cuda.current_device())
end_time = time.time()
self.time_series.append((end_time - start_time) * 1e3)
else:
for _ in range(num_runs):
self.output = self.op_bench.forward()
            if cuda_sync:
torch.cuda.synchronize(torch.cuda.current_device())
def _output_mean(self):
"
561,"if op[0].isdigit():
op = f'_{op}'
if op not in self._used_names:
self._used_names[op] = 0
# Avoid shadowing PyTorch and Python builtins.
","if op[0].isdigit():
op = f'_{op}'
        return self._register_name_used(op)

    def _register_name_used(self, op : str) -> str:
        """"""
        Even if a user provides us with a name, we must register that that
        name is used to prevent duplication of names from further nodes as
        well as ensure that the name provided does not shadow a builtin.
        """"""
if op not in self._used_names:
self._used_names[op] = 0
# Avoid shadowing PyTorch and Python builtins.
"
562,"cuda_guard = """"""\
const DeviceGuard device_guard(options.device());
""""""
                    # See Note [Byte-for-byte compatibility]
                    if dispatch is not None:
                        cuda_guard = f""\n{cuda_guard}""
elif f.device_guard and dispatch is not None and 'CUDA' in dispatch and has_tensor_options:
cuda_guard = """"""\
globalContext().lazyInitCUDA();
","cuda_guard = """"""\
const DeviceGuard device_guard(options.device());
""""""
elif f.device_guard and dispatch is not None and 'CUDA' in dispatch and has_tensor_options:
cuda_guard = """"""\
globalContext().lazyInitCUDA();
"
563,")
with _all_gather_dict_lock:
        states = _all_gather_sequence_id_to_states[
            sequence_id
        ]
states.proceed_signal.wait()
# Phase 2: Leader broadcast gathered results to all followers
",")
with _all_gather_dict_lock:
        states = _all_gather_sequence_id_to_states[sequence_id]
states.proceed_signal.wait()
# Phase 2: Leader broadcast gathered results to all followers
"
564,"return (isinstance(module, torch.quantization.ObserverBase) or
isinstance(module, torch.quantization.FakeQuantize))
# A dictionary for querying the weight index for a given op
WEIGHT_INDEX_DICT = {
torch.nn.functional.conv2d : [1],
","return (isinstance(module, torch.quantization.ObserverBase) or
isinstance(module, torch.quantization.FakeQuantize))
def is_submodule_of_fake_quant(name, module, named_modules):
    parent_name, _ = _parent_name(name)
    return is_activation_post_process(named_modules[parent_name])

# A dictionary for querying the weight index for a given op
WEIGHT_INDEX_DICT = {
torch.nn.functional.conv2d : [1],
"
565,"pytorch_root = dirname(dirname(abspath(__file__)))
sys.path.append(pytorch_root)
# If you want to modify flags or environmental variables that is set when
# building torch, you should do it in tools/setup_helpers/configure.py.
# Please don't add it here unless it's only used in LibTorch.
from tools.build_pytorch_libs import build_caffe2
from tools.setup_helpers.cmake import CMake
","pytorch_root = dirname(dirname(abspath(__file__)))
sys.path.append(pytorch_root)
from tools.build_pytorch_libs import build_caffe2
from tools.setup_helpers.cmake import CMake
"
566,"env[node.name] = observed_graph.node_copy(node, load_arg)
def insert_observer(node, observer, device):
observer_name = get_new_observer_name(model)
setattr(model, observer_name, observer)
self.activation_post_process_map[node.name] = observer
","env[node.name] = observed_graph.node_copy(node, load_arg)
def insert_observer(node, observer, device):
                    get_new_observer_name = get_new_attr_name_with_prefix(prefix)
observer_name = get_new_observer_name(model)
setattr(model, observer_name, observer)
self.activation_post_process_map[node.name] = observer
"
567,":attr:`dim`. And ``indices`` is the index location of each minimum value found
(argmin).
.. warning::
    ``indices`` does not necessarily contain the first occurrence of each
    minimal value found, unless it is unique.
    The exact implementation details are device-specific.
    Do not expect the same result when run on CPU and GPU in general.
    For the same reason do not expect the gradients to be deterministic.

If :attr:`keepdim` is ``True``, the output tensors are of the same size as
:attr:`input` except in the dimension :attr:`dim` where they are of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensors having 1 fewer dimension than :attr:`input`.
Args:
{input}
{dim}
",":attr:`dim`. And ``indices`` is the index location of each minimum value found
(argmin).
If :attr:`keepdim` is ``True``, the output tensors are of the same size as
:attr:`input` except in the dimension :attr:`dim` where they are of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensors having 1 fewer dimension than :attr:`input`.
.. note:: If there are multiple minimal values in a reduced row then
          the indices of the first minimal value are returned.

Args:
{input}
{dim}
"
568,"from ..util.setting import (
JSON_FOLDER_BASE_DIR,
LOG_DIR,
Option,
Test,
TestList,
","from ..util.setting import (
JSON_FOLDER_BASE_DIR,
LOG_DIR,
    CompilerType,
Option,
Test,
TestList,
"
569,"""cudaHostRegisterIoMemory"",
(""hipHostRegisterIoMemory"", CONV_MEM, API_RUNTIME),
),
        # (""warpSize"", (""hipWarpSize"", CONV_SPECIAL_FUNC, API_RUNTIME), (HIP actually uses warpSize...),
(""cudaEventCreate"", (""hipEventCreate"", CONV_EVENT, API_RUNTIME)),
(
""cudaEventCreateWithFlags"",
","""cudaHostRegisterIoMemory"",
(""hipHostRegisterIoMemory"", CONV_MEM, API_RUNTIME),
),
        # (""warpSize"", (""hipWarpSize"", CONV_SPECIAL_FUNC, API_RUNTIME), (HIP actually uses warpSize...)),
(""cudaEventCreate"", (""hipEventCreate"", CONV_EVENT, API_RUNTIME)),
(
""cudaEventCreateWithFlags"",
"
570,"class TestPlatform(Enum):
FBCODE: str = ""fbcode""
OSS: str = ""oss""
","class TestPlatform(Enum):
FBCODE: str = ""fbcode""
OSS: str = ""oss""


# compiler type
class CompilerType(Enum):
    CLANG: str = ""clang""
    GCC: str = ""gcc""
"
571,"| _INCLUDE_QCONFIG_PROPAGATE_LIST
) - _EXCLUDE_QCONFIG_PROPAGATE_LIST
NON_LEAF_MODULE_TO_ADD_OBSERVER_WHITE_LIST = {
nnqd.Linear,
nnq.Linear,
nnqd.LSTM,
","| _INCLUDE_QCONFIG_PROPAGATE_LIST
) - _EXCLUDE_QCONFIG_PROPAGATE_LIST
NON_LEAF_MODULE_TO_ADD_OBSERVER_ALLOW_LIST = {
nnqd.Linear,
nnq.Linear,
nnqd.LSTM,
"
572,"float_module,
q_module,
Logger=OutputLogger,
    white_list=DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_WHITE_LIST,
):
r""""""Prepare the model by attaching the logger to both float module
    and quantized module if they are in the white_list.
Args:
float_module: float module used to generate the q_module
q_module: module quantized from float_module
Logger: type of logger to be attached to float_module and q_module
        white_list: list of module types to attach logger
""""""
qconfig_debug = torch.quantization.QConfig(activation=Logger, weight=None)
float_module.qconfig = qconfig_debug
    prepare(float_module, inplace=True, white_list=white_list)
q_module.qconfig = qconfig_debug
prepare(
q_module,
inplace=True,
        white_list=white_list,
        observer_non_leaf_module_list=NON_LEAF_MODULE_TO_ADD_OBSERVER_WHITE_LIST,
)
","float_module,
q_module,
Logger=OutputLogger,
    allow_list=DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_ALLOWED_LIST,
):
r""""""Prepare the model by attaching the logger to both float module
    and quantized module if they are in the allow_list.
Args:
float_module: float module used to generate the q_module
q_module: module quantized from float_module
Logger: type of logger to be attached to float_module and q_module
        allow_list: list of module types to attach logger
""""""
qconfig_debug = torch.quantization.QConfig(activation=Logger, weight=None)
float_module.qconfig = qconfig_debug
    prepare(float_module, inplace=True, allow_list=allow_list)
q_module.qconfig = qconfig_debug
prepare(
q_module,
inplace=True,
        allow_list=allow_list,
        observer_non_leaf_module_list=NON_LEAF_MODULE_TO_ADD_OBSERVER_ALLOW_LIST,
)
"
573,"r""""""
quantize_per_channel(input, scales, zero_points, axis, dtype) -> Tensor
Converts a float tensor to per-channel quantized tensor with given scales and zero points.
Arguments:
input (Tensor): float tensor to quantize
","r""""""
quantize_per_channel(input, scales, zero_points, axis, dtype) -> Tensor
Converts a float tensor to a per-channel quantized tensor with given scales and zero points.
Arguments:
input (Tensor): float tensor to quantize
"
574,"output, _, counts = _unique_consecutive_impl(input, return_inverse, return_counts, dim)
return output, counts
def _consecutive_return_output(input, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, Optional[int]) -> Tensor
","output, _, counts = _unique_consecutive_impl(input, return_inverse, return_counts, dim)
return output, counts

def _consecutive_return_output(input, return_inverse=False, return_counts=False, dim=None):
# type: (Tensor, bool, bool, Optional[int]) -> Tensor
"
575,"return handle_torch_function(atleast_3d, tensors, *tensors)
if len(tensors) == 1:
tensors = tensors[0]
    return _VF.atleast_3d(tensors)
# TODO: type dim as BroadcastingList when https://github.com/pytorch/pytorch/issues/33782 is fixed
@overload  # noqa: 749
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
    # type: (Tensor, str, Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
    pass
@overload  # noqa: 749
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
    # type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
pass
@overload  # noqa: 749
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
    # type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
    pass

@overload  # noqa: 749
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
    # type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
    pass
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
r""""""Returns the matrix norm or vector norm of a given tensor.
","return handle_torch_function(atleast_3d, tensors, *tensors)
if len(tensors) == 1:
tensors = tensors[0]
    return _VF.atleast_3d(tensors)  # type: ignore
if TYPE_CHECKING:
pass
    # There's no good way to use this type annotation; cannot rename norm() to
    # _norm_impl() in a way that doesn't break JIT overloads. So leave untyped
    # for mypy for now.
    #    def norm(input: Tensor,
    #             p: Optional[Union[str, Number]] = ""fro"",
    #             dim: Optional[Union[int, List[int]]] = None,
    #             keepdim: bool = False,
    #             out: Optional[Tensor] = None,
    #             dtype: _dtype = None) -> Tensor:
    #        return _norm_impl(input, p, dim, keepdim, out, dtype)
else:
    # TODO: type dim as BroadcastingList when
    # https://github.com/pytorch/pytorch/issues/33782 is fixed
    @overload  # noqa: 749
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
        # type: (Tensor, str, Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
        pass

    @overload  # noqa: 749
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
        # type: (Tensor, Optional[number], Optional[List[int]], bool, Optional[Tensor], Optional[int]) -> Tensor
        pass

    @overload  # noqa: 749
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
        # type: (Tensor, Optional[number], Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
        pass

    @overload  # noqa: 749
    def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
        # type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor
        pass
def norm(input, p=""fro"", dim=None, keepdim=False, out=None, dtype=None):  # noqa: 749
r""""""Returns the matrix norm or vector norm of a given tensor.
"
576,"tensor([ 3.,  0., -0., -0.])
"""""".format(**common_args))
add_docstr(torch.unsqueeze,
r""""""
unsqueeze(input, dim) -> Tensor
","tensor([ 3.,  0., -0., -0.])
"""""".format(**common_args))
add_docstr(torch.fix,
           r""""""
fix(input, *, out=None) -> Tensor

Alias for :func:`torch.trunc`
"""""".format(**common_args))

add_docstr(torch.unsqueeze,
r""""""
unsqueeze(input, dim) -> Tensor
"
577,"Stack tensors in sequence depthwise (along third axis).
This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by :func:`torch.atleast_3d`.
Args:
tensors (sequence of Tensors): sequence of tensors to concatenate
","Stack tensors in sequence depthwise (along third axis).
This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by :func:`torch.atleast_3d`.
Args:
tensors (sequence of Tensors): sequence of tensors to concatenate
"
578,"@functools.wraps(func)
def wrapped(*args):
        fn_name = func.__name__
        _check_out_dims_is_int_or_int_tuple(out_dims, fn_name)
vmap_level = torch._C._vmapmode_increment_nesting()
try:
            batched_inputs, batch_size = _create_batched_inputs(in_dims, args, vmap_level, fn_name)
batched_outputs = func(*batched_inputs)
            _validate_outputs(batched_outputs, fn_name)
            return _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, fn_name)
finally:
torch._C._vmapmode_decrement_nesting()
return wrapped
","@functools.wraps(func)
def wrapped(*args):
        _check_out_dims_is_int_or_int_tuple(out_dims, func)
vmap_level = torch._C._vmapmode_increment_nesting()
try:
            batched_inputs, batch_size = _create_batched_inputs(in_dims, args, vmap_level, func)
batched_outputs = func(*batched_inputs)
            _validate_outputs(batched_outputs, func)
            return _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)
finally:
torch._C._vmapmode_decrement_nesting()
return wrapped
"
579,"qconfig)
def forward(self, input):
        return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))
@classmethod
def from_float(cls, mod, qconfig=None):
","qconfig)
def forward(self, input):
        return F.relu(ConvBn2d._forward(self, input))
@classmethod
def from_float(cls, mod, qconfig=None):
"
580,"groups=groups, bias=bias, padding_mode=padding_mode)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self._conv_forward(input, self.weight_fake_quant(self.weight))
@classmethod
def from_float(cls, mod, qconfig=None):
","groups=groups, bias=bias, padding_mode=padding_mode)
assert qconfig, 'qconfig must be provided for QAT module'
self.qconfig = qconfig
        self.activation_post_process = qconfig.activation()
self.weight_fake_quant = qconfig.weight()
def forward(self, input):
        return self.activation_post_process(
            self._conv_forward(input, self.weight_fake_quant(self.weight)))
@classmethod
def from_float(cls, mod, qconfig=None):
"
581,"class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for weight,
    used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
","class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
"
582,"super(LinearReLU, self).__init__(in_features, out_features, bias, qconfig)
def forward(self, input):
        return self.activation_post_process(F.relu(
            F.linear(input, self.weight_fake_quant(self.weight), self.bias)))
@classmethod
def from_float(cls, mod, qconfig=None):
","super(LinearReLU, self).__init__(in_features, out_features, bias, qconfig)
def forward(self, input):
        return F.relu(F.linear(input, self.weight_fake_quant(self.weight), self.bias))
@classmethod
def from_float(cls, mod, qconfig=None):
"
583,"class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
","class Linear(nn.Linear):
r""""""
    A linear module attached with FakeQuantize modules for weight,
    used for quantization aware training.
We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
"
584,"import torch
import torch.overrides
import linecache
","# type: ignore
import torch
import torch.overrides
import linecache
"
585,"import ast
import enum
import inspect
import os
import re
import torch
","import ast
import enum
import inspect
import warnings
import os
import re
import torch
"
586,"for b in n.blocks():
new_block = new_node.addBlock()
torch._C._jit_pass_onnx_block(b, new_block, operator_export_type, env)
return new_op_outputs
else:
symbolic_name = 'prim_' + op_name
","for b in n.blocks():
new_block = new_node.addBlock()
torch._C._jit_pass_onnx_block(b, new_block, operator_export_type, env)
                new_op_outputs = torch._C._jit_pass_fixup_onnx_controlflow_node(new_node, opset_version)
return new_op_outputs
else:
symbolic_name = 'prim_' + op_name
"
587,"Applies a 3D adaptive average pooling over a quantized input signal composed
of several quantized input planes.
    .. note:: The input quantization paramteres propagate to the output.
See :class:`~torch.nn.quantized.AdaptiveAvgPool3d` for details and output shape.
","Applies a 3D adaptive average pooling over a quantized input signal composed
of several quantized input planes.
    .. note:: The input quantization parameters propagate to the output.
See :class:`~torch.nn.quantized.AdaptiveAvgPool3d` for details and output shape.
"
588,"def find_closure_group(input_string, start, group):
""""""Generalization for finding a balancing closure group
         if group = [""("", "")""], then finds the first balanced parantheses.
if group = [""{"", ""}""], then finds the first balanced bracket.
Given an input string, a starting position in the input string, and the group type,
","def find_closure_group(input_string, start, group):
""""""Generalization for finding a balancing closure group
         if group = [""("", "")""], then finds the first balanced parentheses.
if group = [""{"", ""}""], then finds the first balanced bracket.
Given an input string, a starting position in the input string, and the group type,
"
589,"class CudaError(RuntimeError):
def __init__(self, code: int) -> None:
        msg = _cudart.cudaGetErrorString(code).decode('utf-8')
super(CudaError, self).__init__('{0} ({1})'.format(msg, code))
","class CudaError(RuntimeError):
def __init__(self, code: int) -> None:
        msg = _cudart.cudaGetErrorString(_cudart.cudaError(code))
super(CudaError, self).__init__('{0} ({1})'.format(msg, code))
"
590,"return v
elif isinstance(v, tuple) or isinstance(v, list):
return tuple(_get_valid_constant(attr, x) for x in v)
    constants = "", "".join(typ.__name__ for typ in _constant_types)
raise TypeError(textwrap.dedent(""""""
'{}' object for attribute '{}' is not a valid constant.
Valid constants are:
1. a nn.ModuleList
2. a value of type {{{}}}
3. a list or tuple of (2)
        """""".format(type(v).__name__, attr, constants)))
class SourceContext(torch._C._jit_tree_views.SourceRangeFactory):
","return v
elif isinstance(v, tuple) or isinstance(v, list):
return tuple(_get_valid_constant(attr, x) for x in v)
    constants = "", "".join(torch.typename(typ) for typ in _constant_types)
raise TypeError(textwrap.dedent(""""""
'{}' object for attribute '{}' is not a valid constant.
Valid constants are:
1. a nn.ModuleList
2. a value of type {{{}}}
3. a list or tuple of (2)
        """""".format(torch.typename(type(v)), attr, constants)))
class SourceContext(torch._C._jit_tree_views.SourceRangeFactory):
"
591,"from typing import Any, Callable, Optional, Tuple, Union
import warnings
REQUIRE_SAME_MAP_SIZE = (
    'vmap: Expected all tensors to have the same size in the mapped dimension, '
    'got sizes {sizes} for the mapped dimension'
)

ELEMENT_MUST_BE_TENSOR = (
    'vmap({fn}, ...): `{fn}` must only return Tensors, got '
    'type {out} for return {idx}.'
)

MUST_RETURN_TENSORS = (
    'vmap({fn}, ...): `{fn}` must only return Tensors, got '
    'type {out} as the return.'
)

NO_INPUTS = (
    'vmap({fn})(<inputs>): got no inputs. Maybe you forgot '
    'to add inputs, or you are trying to vmap over a '
    'function with no inputs. The latter is unsupported.'
)

OUT_DIMS_MUST_BE_INT_OR_TUPLE_OF_INT = (
    'vmap({fn}, ..., out_dims={out_dims}): `out_dims` must be an int or a tuple '
    'of int representing where in the outputs the vmapped dimension should appear.'
)

OUT_DIMS_AND_NUM_OUTPUTS_MISMATCH = (
    'vmap({fn}, ..., out_dims={out_dims}): `out_dims` must have one dim per '
    'output (got {num_outputs} outputs) of {fn}.'
)

EXPECTED_IN_DIMS_TO_BE_INT_OR_TUPLE = (
    'vmap({fn}, in_dims={in_dims}, ...): expected `in_dims` to be int or tuple, '
    'got: {actual_type}.'
)

IN_DIMS_AND_NUM_INPUTS_MISMATCH = (
    'vmap({fn}, in_dims={in_dims}, ...)(<inputs>): expected one `in_dim` per '
    'input (got {num_inputs} inputs) of {fn}'
)

IN_DIMS_MUST_BE_FLAT_TUPLE = (
    'vmap({fn}, in_dims={in_dims}, ...)(<inputs>): in_dims must be a flat '
    'tuple containing ints and/or Nones. If you were trying to vmap over a '
    'Tensor inside a Python collection in `inputs`, we do not yet support that.'
)

CANT_VMAP_A_NONTENSOR = (
    'vmap({fn}, in_dims={in_dims}, ...)(<inputs>): Got in_dim={in_dim} for '
    'input {idx}, but input {idx} is not a Tensor (got {arg_type}) so it '
    'cannot be vmap\'ed over. If you were trying to vmap over a Tensor inside '
    'a Python collection in `inputs`, we do not yet support that; otherwise, '
    'use None as the respective in_dim for input {idx}.'
)

IN_DIM_NOT_IN_TENSOR = (
    'vmap({fn}, in_dims={in_dims}, ...)(<inputs>): Got in_dim={in_dim} for '
    'input {idx}, but input {idx} is a Tensor of dimensionality {tensor_dim} '
    'so expected in_dim to satisfy 0 <= in_dim < {tensor_dim}.'
)

in_dims_t = Union[int, Tuple[Optional[int], ...]]
out_dims_t = Union[int, Tuple[int, ...]]
","from typing import Any, Callable, Optional, Tuple, Union
import warnings
in_dims_t = Union[int, Tuple[Optional[int], ...]]
out_dims_t = Union[int, Tuple[int, ...]]
"
592,"else:
# use the default recursive rule to compile the module
scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
cpp_module.setattr(name, scripted)
script_module._modules[name] = scripted
","else:
# use the default recursive rule to compile the module
scripted = create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)

cpp_module.setattr(name, scripted)
script_module._modules[name] = scripted
"
593,".. warning ::
This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes.
The current implementation will not have the presented behavior
for complex :class:`Module` that perform many operations.
",".. warning ::
This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes.
The current implementation will not have the presented behavior
for complex :class:`Module` that perform many operations.
"
594,"else:
cuda_path = ''
    if not is_conda and sys.version_info >= (3, 8):
        dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]))
        for dll_path in dll_paths:
os.add_dll_directory(dll_path)
    else:
        dll_paths = [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]
        dll_paths = list(filter(os.path.exists, dll_paths)) + [os.environ['PATH']]
        os.environ['PATH'] = ';'.join(dll_paths)
","else:
cuda_path = ''
    import ctypes
    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]))
    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
    prev_error_mode = kernel32.SetErrorMode(0x0001)
    kernel32.LoadLibraryW.restype = ctypes.c_void_p
    if with_load_library_flags:
        kernel32.AddDllDirectory.restype = ctypes.c_void_p
        kernel32.LoadLibraryExW.restype = ctypes.c_void_p

    for dll_path in dll_paths:
        if sys.version_info >= (3, 8):
os.add_dll_directory(dll_path)
        elif with_load_library_flags:
            res = kernel32.AddDllDirectory(dll_path)
            if res is None:
                err = ctypes.WinError(ctypes.get_last_error())
                err.strerror += ' Error adding ""{}"" to the DLL directories.'.format(dll_path)
                raise err

    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))
    path_patched = False
    for dll in dlls:
        is_loaded = False
        if with_load_library_flags:
            res = kernel32.LoadLibraryExW(dll, None, 0x00001100)
            last_error = ctypes.get_last_error()
            if res is None and last_error != 126:
                err = ctypes.WinError(last_error)
                err.strerror += ' Error loading ""{}"" or one of its dependencies.'.format(dll)
                raise err
            elif res is not None:
                is_loaded = True
        if not is_loaded:
            if not path_patched:
                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])
                path_patched = True
            res = kernel32.LoadLibraryW(dll)
            if res is None:
                err = ctypes.WinError(ctypes.get_last_error())
                err.strerror += ' Error loading ""{}"" or one of its dependencies.'.format(dll)
                raise err
    kernel32.SetErrorMode(prev_error_mode)
"
595,"[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""")
add_docstr(torch.reciprocal,
r""""""
","[-0.6561, -1.6623]])
>>> torch.view_as_complex(x)
tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
"""""".format(**common_args))
add_docstr(torch.reciprocal,
r""""""
"
596,"get_worker_info().name,
dst_worker_info.name,
)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
","get_worker_info().name,
dst_worker_info.name,
)
        RemoteProfilerManager.set_current_profiling_key(rpc_profiling_key)
ctx_manager = torch.autograd.profiler.record_function(rpc_profiling_key)
with ctx_manager as rf:
"
597,"legacy_class_hints = []
for c in ('DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',
              'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor'):
legacy_class_hints.append('class {}(Tensor): ...'.format(c))
# Generate type signatures for dtype classes
","legacy_class_hints = []
for c in ('DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',
              'ShortTensor', 'HalfTensor', 'CharTensor', 'ByteTensor', 'BoolTensor'):
legacy_class_hints.append('class {}(Tensor): ...'.format(c))
# Generate type signatures for dtype classes
"
598,"try:
from torch._C import _cudnn
except ImportError:
    _cudnn = None
# Write:
#
","try:
from torch._C import _cudnn
except ImportError:
    _cudnn = None  # type: ignore
# Write:
#
"
599,"parameters = list(filter(lambda p: p.grad is not None, parameters))
max_norm = float(max_norm)
norm_type = float(norm_type)
if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max() for p in parameters)
else:
        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type)
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
            p.grad.detach().mul_(clip_coef)
return total_norm
","parameters = list(filter(lambda p: p.grad is not None, parameters))
max_norm = float(max_norm)
norm_type = float(norm_type)
    if len(parameters) == 0:
        return torch.tensor(0.)
    device = parameters[0].grad.device
if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)
else:
        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
clip_coef = max_norm / (total_norm + 1e-6)
if clip_coef < 1:
for p in parameters:
            p.grad.detach().mul_(clip_coef.to(p.grad.device))
return total_norm
"
600,"def __str__(self):
if self.function_events is None:
return '<unfinished torch.autograd.profile>'
return str(self.function_events)
def _check_finish(self):
","def __str__(self):
if self.function_events is None:
return '<unfinished torch.autograd.profile>'
        self.function_events.populate_cpu_children()
return str(self.function_events)
def _check_finish(self):
"
601,"def preprocessor(output_directory, filepath, stats, hip_clang_launch, is_pytorch_extension, clean_ctx):
"""""" Executes the CUDA -> HIP conversion on the specified file. """"""
fin_path = os.path.join(output_directory, filepath)
    with open(fin_path, 'r') as fin:
output_source = fin.read()
fout_path = os.path.join(output_directory, get_hip_file_path(filepath))
","def preprocessor(output_directory, filepath, stats, hip_clang_launch, is_pytorch_extension, clean_ctx):
"""""" Executes the CUDA -> HIP conversion on the specified file. """"""
fin_path = os.path.join(output_directory, filepath)
    with open(fin_path, 'r', encoding='utf-8') as fin:
output_source = fin.read()
fout_path = os.path.join(output_directory, get_hip_file_path(filepath))
"
602,"python_args_s = ', '.join(python_args)
python_returns = [type_to_python(r['dynamic_type']) for r in decl['returns']]

        if len(python_returns) > 1:
python_returns_s = 'Tuple[' + ', '.join(python_returns) + ']'
elif len(python_returns) == 1:
python_returns_s = python_returns[0]
","python_args_s = ', '.join(python_args)
python_returns = [type_to_python(r['dynamic_type']) for r in decl['returns']]
        field_names = namedtuple_fieldnames(decl)

        if field_names:
            namedtuple_name = '_'.join(['namedtuple'] + field_names)
            tuple_args = ['(""{}"", {})'.format(name, typ) for name, typ in zip(field_names, python_returns)]
            namedtuple_def = 'NamedTuple(""{}"", [{}])'.format(namedtuple_name, ', '.join(tuple_args))
            if namedtuple_name in namedtuples:
                assert namedtuples[namedtuple_name] == namedtuple_def
            else:
                namedtuples[namedtuple_name] = namedtuple_def
            python_returns_s = namedtuple_name
        elif len(python_returns) > 1:
python_returns_s = 'Tuple[' + ', '.join(python_returns) + ']'
elif len(python_returns) == 1:
python_returns_s = python_returns[0]
"
603,"# Load information from YAML
declarations = load_aten_declarations(declarations_path)
# Generate type signatures for top-level functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
","# Load information from YAML
declarations = load_aten_declarations(declarations_path)
    # Dictionary for NamedTuple definitions
    namedtuples = {}

# Generate type signatures for top-level functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
"
604,"(4) registering the grad hooks
(5) passing a handle of DDP to SyncBatchNorm Layer
""""""
if self.device_ids and len(self.device_ids) > 1:
import warnings
","(4) registering the grad hooks
(5) passing a handle of DDP to SyncBatchNorm Layer
""""""

        def parameters(m, recurse=True):
            def model_parameters(m):
                ps = m._former_parameters.values() \
                    if hasattr(m, ""_former_parameters"") \
                    else m.parameters(recurse=False)
                for p in ps:
                    yield p

            for m in m.modules() if recurse else [m]:
                for p in model_parameters(m):
                    yield p

if self.device_ids and len(self.device_ids) > 1:
import warnings
"
605,"import sys
import platform
import ctypes
from ._utils import _import_dotted_name
from ._utils_internal import get_file_path, prepare_multiprocessing_environment, \
USE_RTLD_GLOBAL_WITH_LIBTORCH
","import sys
import platform
import ctypes

if sys.version_info < (3,):
    raise Exception(""Python 2 has reached end-of-life and is no longer supported by PyTorch."")

from ._utils import _import_dotted_name
from ._utils_internal import get_file_path, prepare_multiprocessing_environment, \
USE_RTLD_GLOBAL_WITH_LIBTORCH
"
606,"""Mismatched gradient shapes: estimated ({}), grad ({})"".format(
grad_estimate.shape, grad.shape))
dims_to_check = inputs[input_to_check].size
for current_dim in range(dims_to_check):
# Positive gradient
","""Mismatched gradient shapes: estimated ({}), grad ({})"".format(
grad_estimate.shape, grad.shape))
        if ensure_outputs_are_inferred:
            self._assertInferTensorChecks(op, grad_ops)

dims_to_check = inputs[input_to_check].size
for current_dim in range(dims_to_check):
# Positive gradient
"
607,"* ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.
* ``scaler.update()`` updates ``scaler``'s scale factor.
    Example::
# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()
","* ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.
* ``scaler.update()`` updates ``scaler``'s scale factor.
    Typical use::
# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()
"
608,"output = model(input)
loss = loss_fn(output, target)
                # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
scaler.scale(loss).backward()
                # scaler.step() first unscales gradients of the optimizer's params.
                # If gradients don't contain infs/NaNs, optimizer.step() is then called,
# otherwise, optimizer.step() is skipped.
scaler.step(optimizer)
# Updates the scale for next iteration.
scaler.update()
    See the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage
    (along with autocasting) in more complex cases like gradient clipping, gradient penalty,
    and multiple losses/optimizers.
``scaler`` dynamically estimates the scale factor each iteration.  To minimize gradient underflow,
    a large scale factor should be used.  However, ``float16`` values can ""overflow"" (become inf or NaN) if
the scale factor is too large.  Therefore, the optimal scale factor is the largest factor that can be used
without incurring inf or NaN gradient values.
``scaler`` approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every
","output = model(input)
loss = loss_fn(output, target)
                # Scales the loss, and calls backward() on the scaled loss to create scaled gradients.
scaler.scale(loss).backward()
                # scaler.step() first unscales the gradients of the optimizer's assigned params.
                # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
# otherwise, optimizer.step() is skipped.
scaler.step(optimizer)
# Updates the scale for next iteration.
scaler.update()
    See the :ref:`Gradient Scaling Examples<gradient-scaling-examples>` for usage in more complex cases like
    gradient clipping, gradient penalty, and multiple losses/optimizers.
``scaler`` dynamically estimates the scale factor each iteration.  To minimize gradient underflow,
    a large scale factor should be used.  However, ``torch.float16`` values can ""overflow"" (become inf or NaN) if
the scale factor is too large.  Therefore, the optimal scale factor is the largest factor that can be used
without incurring inf or NaN gradient values.
``scaler`` approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every
"
609,"backoff_factor=0.5,
growth_interval=2000,
enabled=True):
        if enabled and not torch.cuda.is_available():
            warnings.warn(""torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling."")
            self._enabled = False
        else:
            self._enabled = enabled

        if self._enabled:
assert growth_factor > 1.0, ""The growth factor must be > 1.0.""
assert backoff_factor < 1.0, ""The backoff factor must be < 1.0.""
","backoff_factor=0.5,
growth_interval=2000,
enabled=True):
        self._enabled = enabled
        if enabled:
assert growth_factor > 1.0, ""The growth factor must be > 1.0.""
assert backoff_factor < 1.0, ""The backoff factor must be < 1.0.""
"
610,"],
'item': [""def item(self) -> Number: ...""],
})
    for binop in ['add', 'sub', 'mul', 'div']:
        for inplace in [True, False]:
out_suffix = ', *, out: Optional[Tensor]=None'
if inplace:
binop += '_'
","],
'item': [""def item(self) -> Number: ...""],
})
    for binop in ['mul', 'div']:
        for inplace in [False, True]:
out_suffix = ', *, out: Optional[Tensor]=None'
if inplace:
binop += '_'
"
611,"offending_node.col_offset,
offending_node.col_offset + range_len)
feature_name = pretty_node_names.get(node_type, node_type.__name__)
        msg = ""{} aren't supported"".format(feature_name)
super(UnsupportedNodeError, self).__init__(source_range, msg)
","offending_node.col_offset,
offending_node.col_offset + range_len)
feature_name = pretty_node_names.get(node_type, node_type.__name__)
        msg = ""{} {}aren't supported"".format(feature_name, reason + ' ' if reason else '')
super(UnsupportedNodeError, self).__init__(source_range, msg)
"
612,"f.write('{""name"": ""%s"", '
'""ph"": ""f"", '
'""ts"": %s, '
                            'tid"": %s, '
'""pid"": ""CUDA functions"", '
'""id"": %s, '
'""cat"": ""cpu_to_cuda"", '
","f.write('{""name"": ""%s"", '
'""ph"": ""f"", '
'""ts"": %s, '
                            '""tid"": %s, '
'""pid"": ""CUDA functions"", '
'""id"": %s, '
'""cat"": ""cpu_to_cuda"", '
"
613,"f.write('{""name"": ""%s"", '
'""ph"": ""f"", '
'""ts"": %s, '
                            '""tid"": %s, '
'""pid"": ""CUDA functions"", '
'""id"": %s, '
'""cat"": ""cpu_to_cuda"", '
","f.write('{""name"": ""%s"", '
'""ph"": ""f"", '
'""ts"": %s, '
                            'tid"": %s, '
'""pid"": ""CUDA functions"", '
'""id"": %s, '
'""cat"": ""cpu_to_cuda"", '
"
614,"pad = int(n_fft // 2)
input = F.pad(input.view(extended_shape), (pad, pad), pad_mode)
input = input.view(input.shape[-signal_dim:])
    return torch._C._VariableFunctions.stft(input, n_fft, hop_length, win_length, window, normalized, onesided)
del torch.unique_dim
","pad = int(n_fft // 2)
input = F.pad(input.view(extended_shape), (pad, pad), pad_mode)
input = input.view(input.shape[-signal_dim:])
    return _VF.stft(input, n_fft, hop_length, win_length, window, normalized, onesided)
del torch.unique_dim
"
615,"from torch._jit_internal import Tuple, Optional, List  # noqa: F401
from torch import Tensor  # noqa: F401
from torch.nn import _VF
from torch.nn.utils.rnn import PackedSequence
","from torch._jit_internal import Tuple, Optional, List  # noqa: F401
from torch import Tensor, _VF  # noqa: F401
from torch.nn.utils.rnn import PackedSequence
"
616,"from ..parameter import Parameter
from ..utils.rnn import PackedSequence
from .. import init
from .. import _VF
_rnn_impls = {
'RNN_TANH': _VF.rnn_tanh,
","from ..parameter import Parameter
from ..utils.rnn import PackedSequence
from .. import init
from ... import _VF
_rnn_impls = {
'RNN_TANH': _VF.rnn_tanh,
"
617,"def symbolic_fn(g, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=None):
if ceil_mode and not input.isCompleteTensor():
return _unimplemented(name, ""input size not accessible"")
padding = sym_help._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
if ceil_mode:
padding_ceil = get_pool_ceil_padding(input, kernel_size, stride, padding)
","def symbolic_fn(g, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=None):
if ceil_mode and not input.isCompleteTensor():
return _unimplemented(name, ""input size not accessible"")
        if not stride:
            stride = kernel_size
padding = sym_help._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
if ceil_mode:
padding_ceil = get_pool_ceil_padding(input, kernel_size, stride, padding)
"
618,"pickle_module.dump(serialized_storage_keys, f, protocol=pickle_protocol)
f.flush()
for key in serialized_storage_keys:
        serialized_storages[key]._write_file(f, _should_read_directly(f))
def _save(obj, zip_file, pickle_module, pickle_protocol):
","pickle_module.dump(serialized_storage_keys, f, protocol=pickle_protocol)
f.flush()
for key in serialized_storage_keys:
        serialized_storages[key]._write_file(f, _should_read_directly(f), True)
def _save(obj, zip_file, pickle_module, pickle_protocol):
"
619,":attr:`errors=...`.
.. warning::
        :func:`torch.load()` uses ``pickle`` module implicitly, which is known to be insecure.
It is possible to construct malicious pickle data which will execute arbitrary code
during unpickling. Never load data that could have come from an untrusted
source, or that could have been tampered with. **Only load data you trust**.
",":attr:`errors=...`.
.. warning::
        :func:`torch.load()` uses ``pickle`` module implicitly, which is known to be insecure.
It is possible to construct malicious pickle data which will execute arbitrary code
during unpickling. Never load data that could have come from an untrusted
source, or that could have been tampered with. **Only load data you trust**.
"
620,">>> output = loss(m(conv(data)), target)
>>> output.backward()
""""""
    __constants__ = ['ignore_index', 'weight', 'reduction']
def __init__(self, weight=None, size_average=None, ignore_index=-100,
reduce=None, reduction='mean'):
",">>> output = loss(m(conv(data)), target)
>>> output.backward()
""""""
    __constants__ = ['ignore_index', 'reduction']
def __init__(self, weight=None, size_average=None, ignore_index=-100,
reduce=None, reduction='mean'):
"
621,"Args:
config:         contains number of warmup and benchmark iterations.
module_config:  module_config which contains op, number of parameters that op takes
                    and wether graph mode is enabled or not.
module_type:    Type of the module to be wrapped. e.g. SimpleAddModule for add op.
result:         dictionary instance to be populated with the benchmark result (latency per iter).
""""""
","Args:
config:         contains number of warmup and benchmark iterations.
module_config:  module_config which contains op, number of parameters that op takes
                    and whether graph mode is enabled or not.
module_type:    Type of the module to be wrapped. e.g. SimpleAddModule for add op.
result:         dictionary instance to be populated with the benchmark result (latency per iter).
""""""
"
622,"return '\n'.join(names)
def wrap_output(call):
        # Returns a 2-tuple `(wrapped_call, extra_wrapping_stmts)`, where
        # `wrapped_call` is to drop-in replace `call`, and
        # `extra_wrapping_stmts` is a list of extra statements to run after
        # `call`.
if 'Tensor' not in declaration['return_type']:
            return call, []
elif view_info is not None:
# See NOTE [ Autograd View Variables ] in variable.h for details.
differentiable_output_vars = {r['name'] for r in differentiable_outputs}
            tensor_output_vars = {r['name'] for r in returns if 'Tensor' in r['type']}
            if not isinstance(view_info, dict):
                if len(differentiable_output_vars) == len(tensor_output_vars):
                    # all outputs are differentiable
                    return 'as_view({}, {}, true)'.format(view_info, call), []
                elif len(differentiable_output_vars) == 0:
                    # no output is differentiable
                    return 'as_view({}, {}, false)'.format(view_info, call), []
                else:
                    # some of the outputs are differentiable
                    # need to expand to dict mode, i.e., one entry per output
                    base_name = view_info
                    view_info_dict = {}
                    for i, return_info in enumerate(returns):
                        if 'Tensor' in return_info['type']:
                            view_info_dict[i] = base_name
else:
                view_info_dict = view_info

            def wrap_view_single(output_var, base_var):
                fmt = '{output_var} = as_view({base_var}, {output_var}, {is_differentiable});'
                if output_var in differentiable_output_vars:
                    # If `GradMode::is_enabled()` is False, this is a
                    # non-differentiable view. Gradients should not flow through.
                    is_differentiable = 'true'
                else:
                    # This output is non-differentiable, so it is a
                    # non-differentiable view. Gradients should not flow through.
                    is_differentiable = 'false'
                return fmt.format(output_var=output_var, base_var=base_var,
                                  is_differentiable=is_differentiable)

            extra_wrapping_stmts = []
            for output_idx, return_info in enumerate(returns):
                if 'Tensor' not in return_info['type']:
                    assert output_idx not in view_info_dict, 'Can not wrap non-Tensor output as a view'
                    continue
                output_var = return_info['name']
                if output_idx in view_info_dict:
                    stmt = wrap_view_single(output_var, view_info_dict[output_idx])
                extra_wrapping_stmts.append(stmt)
            return call, extra_wrapping_stmts
else:
            return 'std::move({})'.format(call), []
def enforce_same_tensorimpl_and_storage(env, call):
save_ptrs_stmts = []
","return '\n'.join(names)
def wrap_output(call):
        # Returns `wrapped_call` which is a drop-in replacement for `call`
if 'Tensor' not in declaration['return_type']:
            return call
elif view_info is not None:
# See NOTE [ Autograd View Variables ] in variable.h for details.
differentiable_output_vars = {r['name'] for r in differentiable_outputs}

            if not isinstance(view_info, str):
                raise TypeError(""The view info should be a string for {}, but it is: {}"".format(base_name, view_info))

            if len(differentiable_output_vars) == 0:
                # no output is differentiable (.indices() for SparseTensors for example)
                return 'as_non_differentiable_view({}, {})'.format(view_info, call)
            elif len(differentiable_output_vars) == 1:
                # Single differentiable output (Tensor or Tensor[])
                return_info = returns[0]
                # We only support simple Tensor or a TensorList for functions that return views
                if not return_info['dynamic_type'] in ['Tensor', 'TensorList']:
                    raise RuntimeError(""{} that return differentiable views can only return Tensor or Tensor[]"".format(base_name))
                # Only allow rebasing of the history if we return a single Tensor
                # Or if the function is marked as a pure view
                allow_rebase_history = 'true'
                if return_info['dynamic_type'] == 'TensorList' and base_name not in PURE_VIEW_FUNCTIONS:
                    allow_rebase_history = 'false'
                wrapped_call = (""as_differentiable_view(/* base */{}, /* output */ {}, ""
                                ""/* allow_rebase_history */ {})"").format(view_info, call, allow_rebase_history)
                return wrapped_call
else:
                # This could be supported but we don't need it at the moment, so keeping things simple.
                raise RuntimeError(""Function that return multiple differentiable output ""
                                   ""when at least one of them is view is not supported."")
else:
            return 'std::move({})'.format(call)
def enforce_same_tensorimpl_and_storage(env, call):
save_ptrs_stmts = []
"
623,"Inputs:
output_dims -- output feature dimensions
sigma -- bandwidth for the Gaussian kernel estimator
        w_init -- initalization options for weight parameter
        b_init -- initalization options for bias parameter
""""""
def __init__(
","Inputs:
output_dims -- output feature dimensions
sigma -- bandwidth for the Gaussian kernel estimator
        w_init -- initialization options for weight parameter
        b_init -- initialization options for bias parameter
""""""
def __init__(
"
624,"candidate_scope = scope.CurrentNameScope()
best_scope = self._resolve_scope_overrides(candidate_scope)
if best_scope != candidate_scope:
            logger.info(""Overwiting scope {0} with scope {1}"".format(
candidate_scope, best_scope))
return best_scope + name
","candidate_scope = scope.CurrentNameScope()
best_scope = self._resolve_scope_overrides(candidate_scope)
if best_scope != candidate_scope:
            logger.info(""Overwriting scope {0} with scope {1}"".format(
candidate_scope, best_scope))
return best_scope + name
"
625,"have some subtle differences. :class:`InstanceNorm2d` is applied
on each channel of channeled data like RGB images, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionaly, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm2d` usually don't apply affine
transform.
","have some subtle differences. :class:`InstanceNorm2d` is applied
on each channel of channeled data like RGB images, but
:class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
transform, while :class:`InstanceNorm2d` usually don't apply affine
transform.
"
626,"r""""""This file provides a location for operators that help exporting
models via onnx. E.g. shape_as_tensor and reshape_from_tensor_shape
are to make all dynamic sizes operations traceble.
NOTE: at one point these functions were implemented differently.
Since then we have implemented these directly in ATen, so this
","r""""""This file provides a location for operators that help exporting
models via onnx. E.g. shape_as_tensor and reshape_from_tensor_shape
are to make all dynamic sizes operations traceable.
NOTE: at one point these functions were implemented differently.
Since then we have implemented these directly in ATen, so this
"
627,"above the diagonal.
The argument :attr:`diagonal` controls which diagonal to consider. If
:attr:`diagonal` = 0, all elements on and below the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
","above the diagonal.
The argument :attr:`diagonal` controls which diagonal to consider. If
:attr:`diagonal` = 0, all elements on and above the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
"
628,"# and later one we can config docker.pytorch.org to point to the location
client.put_object(
        Bucket=""ossci-docker"",
ACL=""public-read"",
Key=""{project}.html"".format(project=project),
Body=html_body,
","# and later one we can config docker.pytorch.org to point to the location
client.put_object(
        Bucket=""docker.pytorch.org"",
ACL=""public-read"",
Key=""{project}.html"".format(project=project),
Body=html_body,
"
